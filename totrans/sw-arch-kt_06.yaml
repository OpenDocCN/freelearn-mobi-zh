- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Microservices, Serverless, and Microfrontends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ll delve into the architectural styles of microservices,
    serverless, and microfrontends. They have revolutionized the way we design, develop,
    and deploy applications. They also empower organizations to build robust, flexible,
    and scalable systems. We’ll explore the fundamentals, unique features, and tangible
    benefits of each approach. By the end, you’ll have a comprehensive understanding
    of these architectures and their application in modern software engineering.
  prefs: []
  type: TYPE_NORMAL
- en: Our exploration begins with describing the traditional monolith, where the entire
    system is designed and developed as a single unit. Then, we’ll discuss the challenges
    of this approach that are faced by many developers to bring about the need to
    break down this monolith into smaller and loosely coupled components.
  prefs: []
  type: TYPE_NORMAL
- en: After, we’ll turn our attention to microservices architecture and examine how
    microservices solve the challenges of monoliths. We’ll cover how a monolith can
    be transformed into microservices before discussing the benefits, challenges,
    and trade-offs that come with this distributed architectural style.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discuss how serverless computing helps developers focus on writing
    code without the concerns of infrastructure. We’ll discuss the optimal use cases
    for serverless architectures and address the challenges associated with this paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll look at how a monolithic application can be transformed into
    microservices and microfrontends. We’ll discuss how self-contained components
    benefit the developers and integrate with microservices and serverless backends.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Monoliths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nanoservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microfrontends
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code files used in this chapter on GitHub: [https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-6](https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-6%20)'
  prefs: []
  type: TYPE_NORMAL
- en: Monoliths
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Monolith** means *made of one stone*. In the context of software architecture,
    a monolith refers to a large system designed and developed as a single unit. In
    a monolithic architecture, there’s typically a single code base, a unified database,
    and one deployable artifact.'
  prefs: []
  type: TYPE_NORMAL
- en: Having a single code base means it usually relies on individual developers’
    efforts to keep the code tidy and clean. There’s little room to enforce separation
    of concerns by design since all code is hosted in a single place. This often results
    in all the components, modules, and functionalities of the application being tightly
    coupled and interdependent.
  prefs: []
  type: TYPE_NORMAL
- en: A unified database, typically relational only, in a monolithic application is
    likely to produce a major – if not only one – schema that contains all the entities
    from all functionalities. Moreover, each entity table will contain all the columns
    that address all business concerns related to the entity. There may also be a
    spiderweb of foreign key constraints among tables. It often combines the concerns
    of a database used for transactions and reporting into one.
  prefs: []
  type: TYPE_NORMAL
- en: 'While monoliths have a wide range of code quality and database designs, they
    have one thing in common: there’s one deployable artifact for the monolithic application,
    which contains the entire system. It’s big and takes a long time to release. The
    release procedure usually requires all monolithic application instances to shut
    down first, after which infrastructure changes are made, the new deployable artifact
    replaces the old one, and the updated version of the application starts. Each
    release also often requires intensive pre-planning and coordination among teams
    to identify dependencies. The release plan can look like a Gantt chart or a project
    plan, as shown in *Figure 6**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Release plan as a Gantt chart](img/B21737_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Release plan as a Gantt chart
  prefs: []
  type: TYPE_NORMAL
- en: In this release plan, the dev team shuts down the monolithic application first.
    Then, the database team backs up the database. Afterward, the dev team starts
    deploying the application, while the infrastructure team applies changes such
    as network or middleware upgrades. When this is finished, the dev team starts
    the application. Finally, the QA team verifies the environment and signs off the
    release. Some organizations may have a dedicated release team or operation team
    to formulate a playbook for each release and execute the plan as one team.
  prefs: []
  type: TYPE_NORMAL
- en: The release plan is visualized as a Gantt chart to show the dependencies among
    teams and show the timeline of the release from left to right.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll cover the benefits and challenges of the monolithic approach as
    a background for the upcoming three architectural styles.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Nowadays, few organizations would openly advocate monolithic architecture as
    an optimal style. However, there are still a few benefits that allow it to be
    considered a choice of architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: Monolithic applications have the simplicity of one code base,
    one database, and one deployable artifact. Once the routines for build, deploy,
    and run are in place, developers can follow that one pattern that repeats over
    time. As we mentioned in [*Chapter 2*](B21737_02.xhtml#_idTextAnchor045), this
    is known as the **You aren’t gonna need it** (**YAGNI**) principle. If this simplicity
    is all we need now, then it’s not a bad idea to run with the monolithic approach
    initially.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Short time to market**: This situation typically resonates with developers
    if they work in a startup company, where time to market is the number one priority
    and nothing else matters. Sometimes, it’s even a do-it-now-or-quit situation.
    It could also go well with experimental applications where sophisticated architecture
    may not be necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build first, optimize later**: Another benefit of monolithic architecture
    is situational. If the area of business is new and everyone is finding their paths
    on how to build the system as a product, then it’s beneficial to delay any refactoring
    or optimization – that is, until everyone, including technical and non-technical
    stakeholders, has more experience with the subject matter and recognizes the need
    to break down the monolithic application. Also, it’s better to understand the
    business ecosystem before trying to break it down by reasonable boundaries and
    which features naturally go together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monolith-first
  prefs: []
  type: TYPE_NORMAL
- en: This approach of making a conscious choice toward monolithic architecture is
    called **monolith-first**. This term was made popular by Martin Fowler in his
    engineering blog in 2015.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While monolithic architecture has been widely used, it has some drawbacks.
    The tight coupling between components makes it difficult to update specific parts
    of the application independently. In other words, changing one part of the system
    would affect other parts unintentionally:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Slow development**: This makes any change bigger than it needs to be, and
    therefore increases the risk of release. Since every change is bigger, the probability
    of having code conflicts among engineers is significantly higher. Engineers would
    spend more time reviewing code and resolving code conflicts. More areas of testing
    are required to ensure the system is functional. Combined, these factors lead
    to slower development cycles and reduced flexibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Difficult to scale and tune performance**: Additionally, scaling resources
    in a monolith can be inefficient since the entire application needs to be replicated
    instead of individual components that require more resources being scaled. It’s
    also harder to tune performance precisely since there could be other processes
    contending with the same resource, which affect the performance to be tuned intentionally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time-consuming test suite**: In a monolithic application, the different components
    and modules are tightly coupled, meaning that changes in one part of the application
    can have unintended consequences in other parts. Tests need to verify no unintended
    changes were made for business cases, leading to more complex test scenarios and
    a longer test execution time. The inter-dependencies also make it difficult to
    isolate and run tests independently, limiting the potential for parallel execution
    and shorter test execution times. Even a minor change in a monolithic application
    would need regression testing. This requires a comprehensive suite of test cases,
    which can be time-consuming.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Risky, long, and big releases**: Releasing a monolithic application usually
    takes a long time because the entire system is deployed as a unit. Even a minor
    change would result in the entire monolith having to be redeployed, which means
    it becomes harder to continuously deliver the system. Worse, it could end up accumulating
    more changes to release periodically since the monolith can’t be deployed quickly
    and continuously. Engineers could be spending a long night aiming to release a
    monolith application, and these long and late hours may introduce more human errors
    due to fatigue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On the contrary, releasing a small application in a distributed system can be
    equally challenging. However, due to the smaller scope of change, it’s possible
    to use modern strategies such as rolling releases, **blue-green releases**, or
    **canary releases**. These release strategies can be performed during business
    hours where most help is available, and thus reduce the risk of human errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Technology lock-in**: A monolithic application usually has a long lifespan.
    That means it may have chosen a technology stack a long time ago. Developers are
    faced with either upgrading the technology, which results in a substantial change
    in the code base or adopting a different technology in some parts of the codebase,
    which results in multiple tools doing the same job. This also means the space
    for experimenting with technology is severely limited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Total system failure**: A failure in a monolithic application easily results
    in total system failure. Even the operational part may shut down since it’s part
    of one monolith unit. It’s harder to isolate and contain failures because there’s
    no clear separation between components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Team dependencies**: Lastly, multiple teams sharing one deployable artifact
    and probably one code base create a lot of dependencies. One team may have completed
    a feature that needs to be released as soon as possible, and the other team may
    still be working on a feature that isn’t ready yet.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Slow time to market**: As they share one deployable artifact and one monolith,
    the first team may not be able to have their feature land on the market in time
    until other teams have completed theirs. This slow time to market could mean competitors
    may have taken opportunities and customers by the time the monolith is released.
    It hurts the business if the system is constantly catching up with competitors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With that, we’ve set the context of the challenges that are faced by engineers
    when implementing monolithic applications. Next, we’re going to look at architectural
    styles that aim to conquer these challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before the term microservice architecture was coined, the concept of **service-oriented
    architecture** (**SOA**) became popular in the 2000s as an early response to the
    challenges posed by monolithic architectures.
  prefs: []
  type: TYPE_NORMAL
- en: SOA emphasizes encapsulating business functionality into independent services.
    Each service has a well-defined interface and communicates with other services.
    Standard protocols such as **Enterprise Service Bus** (**ESB**) are used for communication.
    The principles and concepts formalized in SOA to break down monoliths provided
    a basis for the future development of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: In 2011, the term microservices was brought up in software architecture workshops
    as the participants increasingly became aware of the emergence of a new architecture.
    In 2012, the term microservices was officially decided. *James Lewis* and *Fred
    George* were the major initial contributors to this style.
  prefs: []
  type: TYPE_NORMAL
- en: Around the same time, companies such as *Netflix* and *Amazon* were also experimenting
    with similar architectural patterns. Netflix played a significant role in popularizing
    microservices through their adoption of the architecture for their scalable streaming
    platform. They shared their experiences and insights at various conferences and
    through blog posts, contributing to the growing interest and understanding of
    microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’re going to cover the key principles of microservices that shape their
    design and implementation.
  prefs: []
  type: TYPE_NORMAL
- en: Key principles
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Primarily, a microservice should conform to the **single responsibility principle**
    (**SRP**) at the business capability or functionality level.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider the example we’ve used throughout this book about village households
    exchanging services with a contract. Functions such as record-keeping household
    information, negotiating the contract, exercising the contract, and notifying
    households are all microservice candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 'A microservice should have a well-defined responsibility that handles a single
    concern or business domain. For this example, we can define each microservice
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Household service**: Masters the records of households'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contract service**: Maintains the workflow of contract negotiation from drafted
    to fully exercised'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification service**: Sends proxy notification requests to email service
    providers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The details of how to break down a system into appropriate business domains
    will be discussed in depth in [*Chapter 8*](B21737_08.xhtml#_idTextAnchor289),
    where we’ll cover **domain-driven development** (**DDD**). However, some architectural
    smells indicate whether microservices have well-defined responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Each microservice shouldn’t be developed by more than one team of engineers.
    However, there could be exceptions, such as if responsibilities aren’t delegated
    to the teams. This takes us back to [*Chapter 1*](B21737_01.xhtml#_idTextAnchor013)
    when **Conway’s law** was mentioned. It’s recommended to re-organize the teams
    so that each team has well-defined responsibilities and those responsibilities
    don’t overlap with other teams. The approach of re-organizing teams to aim for
    a better architecture is called the **Inverse** **Conway Maneuver**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A microservice shouldn’t communicate too often with another to complete its
    feature. What’s worse is if it invokes another microservice’s endpoints iteratively.
    This is likely to indicate a “leak” of the service boundary. Perhaps the part
    that this microservice needs from another should be brought back and owned by
    the microservice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A microservice shouldn’t depend on another microservice during a release. If
    a microservice is unavailable because other microservices are unavailable, this
    indicates a possible technical dependency.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two microservices having inter-dependency in terms of exchanging messages could
    indicate that responsibilities haven’t been defined well enough. This is more
    of a problem if the communication is synchronous rather than asynchronous. If
    service A calls service B synchronously, when service B handles the call from
    service A, service B calls service A synchronously. This case would easily exhaust
    all threads in the request handler pool.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservices shouldn’t share code repositories, build processes, database schemas,
    and deployable artifacts with other microservices. Sharing them may hint at potential
    dependencies among microservices during build and release. The last thing you
    want is that your microservice can’t be deployed until another microservice is
    deployed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having any of the symptoms discussed so far might suggest these microservices
    have emerged as **distributed monoliths**, which is worse than a traditional monolithic
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Communication and integration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservices communicate with each other through well-defined interfaces, namely
    **application programming interfaces** (**APIs**). The communication is either
    synchronous or asynchronous.
  prefs: []
  type: TYPE_NORMAL
- en: 'The synchronous and asynchronous communications that occur via APIs are specified
    by the popular **Open API** and **Async API** standards, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous communication is often achieved by one microservice sending a request
    to another microservice and waiting for a response before continuing its execution.
    The APIs can be exposed as **Hypertext Transfer Protocol/Secure** (**HTTP/HTTPS)**,
    **Remote Procedure Call** (**RPC**), **Simple Object Access Protocol** (**SOAP**),
    and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous communication usually involves messaging systems so that a microservice
    can send a message and immediately continue its execution. The other microservices
    receive the message when it’s available.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Webhooks** are an alternative and popular way to communicate among microservices
    asynchronously without the need for messaging systems. Instead of responding to
    a request, a microservice usually sends a message to another microservice via
    the HTTP/REST protocol but doesn’t require a response to continue its execution.
    Webhooks usually require static configuration to be used with the target endpoints.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the example of the four potential microservices, they could communicate
    in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – An example of microservice communication](img/B21737_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – An example of microservice communication
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, **Contract service** needs to get household data from
    **Household service** to validate requests and manage their workflows. Then, **Contract
    service** sends requests to **Notification service** so that emails can be sent
    to households to inform them of any changes in their workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Scalability and resilience
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservice architecture provides inherent scalability due to its modular nature.
    Each microservice can be scaled independently based on its specific resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example of microservices, the usage patterns can be hugely different.
    The following may apply:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Household service**: Low traffic as there’s only a limited number of households'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contract service**: Most frequently used by users'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification service**: Medium throughput of email requests but no strict
    latency requirements'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Assuming this is a true reflection of its usage, we might need more instances
    of the **contract service** than any other service. Alternatively, other microservices
    can send asynchronous messages to request notifications, combined with queuing
    and batching mechanisms on the receiver side. Then, the **contract service** will
    be able to process a large volume of email requests in batches; other microservices
    don’t need to wait to get a response from the **contract service** to continue
    their processes.
  prefs: []
  type: TYPE_NORMAL
- en: The **household service** is requested often in getting household data from
    the **contract service**. In other words, the availability of the **household
    service** has become more important than that of other services. However, we can
    also consider having a local cache of household data in the **contract service**.
    The **household service** would need to send asynchronous messages when household
    data is created, updated, and deleted. It can broadcast messages to all interested
    microservices, though the messages can be kept in a **last-value queue** messaging
    structure so that other microservices can copy the data to their local storage.
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining these changes to address the concerns of scalability, performance,
    availability, and resilience, these services may communicate like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Example of updated microservice communication](img/B21737_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Example of updated microservice communication
  prefs: []
  type: TYPE_NORMAL
- en: Note that there are no synchronous request-response messages in the system anymore.
    This implies that each microservice can operate on its own without other microservices
    being available, increasing the resilience of the system. For instance, if **Household
    service** is down, maintenance operations regarding household data are unavailable,
    but all other microservices are still operational as they use the last known household
    kept in their local storage.
  prefs: []
  type: TYPE_NORMAL
- en: However, it does rely more on messaging systems to provide features such as
    queuing, batching, and last-value queues. Messaging systems are typically configured
    and deployed as infrastructure so that they’re more resilient than microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Even with synchronous communication, there are techniques such as circuit breakers,
    bulkheads, and graceful degradation that can be employed to respond to failures
    gracefully.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that it was just a monolithic application of four modules. In that case,
    these concerns can’t be addressed separately. Moreover, if the monolithic application
    is down, instead of having a partial system failure occur, we end up with a total
    system failure. No operation will be available.
  prefs: []
  type: TYPE_NORMAL
- en: Maintainability and technology choices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Each microservice should have a code repository. A microservice focuses on a
    single responsibility at the business capability level, so it’s unlikely that
    a code change in one microservice would change something in another microservice.
    Moreover, keeping code changes small creates fewer chances of code conflicts among
    engineers, which also reduces the time required to review pull requests. The productivity
    of engineers increases as the code using the microservice style is more maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: As each microservice has its own project, build script, and code repository,
    any choice that’s made regarding the use of technology and libraries is confined
    within the project. When it comes to using a new library as a dependency or a
    newer version of the library, engineers can try it in one microservice to start
    with, learn and get familiar with it, prove it’s working with one microservice,
    and then apply the proven approach to other microservices. A dedicated code repository
    for a microservice reduces many risks when experimenting with recent technologies.
    It also increases the chance of them being used in the system and keeps the system
    modern.
  prefs: []
  type: TYPE_NORMAL
- en: Testing and quality assurance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing microservices as black boxes has a smaller scope than testing a monolithic
    application. End-to-end test cases may involve a combination of behaviors from
    different modules.
  prefs: []
  type: TYPE_NORMAL
- en: However, in microservice architecture, these modules would have become microservices.
    With well-defined APIs, it’s now possible to mock the behaviors of external interfaces
    so that the test case focuses on testing how it uses and responds to external
    APIs. This simplifies the test suite and each microservice focuses on testing
    its own behaviors instead. The exhaustive combination of behaviors from different
    microservices’ communication can be inferred and thus remove the need for comprehensively
    testing all business cases via end-to-end testing.
  prefs: []
  type: TYPE_NORMAL
- en: Having said that, end-to-end testing may still be required to validate the overall
    communication among microservices while considering other factors such as URL
    routing, security controls, and API compatibility. Usually, end-to-end testing
    only contains critical business cases and focuses on overall system correctness.
  prefs: []
  type: TYPE_NORMAL
- en: As microservice communication using APIs is vital to ensure the overall system
    is functional, **contract testing** can be brought in to verify that these APIs
    are as per their specifications and that the microservices conform to them. This
    involves testing on both the consumer and provider sides of the APIs.
  prefs: []
  type: TYPE_NORMAL
- en: Consumers create contract tests based on their expectations, simulating the
    interactions with the microservice via APIs. These tests verify that the consumer’s
    requirements are met. On the other hand, providers execute the consumer’s contract
    tests against their implementation. These tests ensure that the provider meets
    the API’s requirements and doesn’t introduce any breaking changes.
  prefs: []
  type: TYPE_NORMAL
- en: These improvements allow engineers to verify the quality of the system faster
    and thus reduce the time to market of changes. We’ll cover software testing in
    depth in [*Chapter 13*](B21737_13.xhtml#_idTextAnchor418).
  prefs: []
  type: TYPE_NORMAL
- en: Deployment and infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the key characteristics of monolithic applications is the single major
    deployable artifact. In contrast, one microservice should have its own deployable
    artifact. This allows us to implement the practice of **continuous integration
    and continuous deployment** (**CI/CD**) and, as a result, reduce or even eliminate
    downtime during release.
  prefs: []
  type: TYPE_NORMAL
- en: In our real-life example, and with the updated communications among microservices
    shown previously, each microservice can be released in isolation. Together with
    a rolling deployment procedure, it’s possible to keep the system operating while
    the release is ongoing.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, each microservice can be deployed at its own cadence. There’s no need
    to wait for other microservices to be deployed like in the days of monolithic
    applications. This encourages engineers to deploy microservices if they’re ready
    to go, and thus speed up the time to market of software as products.
  prefs: []
  type: TYPE_NORMAL
- en: Microservice architecture often uses containerization technologies such as **Docker**
    and container orchestration platforms such as **Kubernetes**. Typically, building
    a microservice would generate its own Docker image, in which the dependencies
    and configurations are already set up. This results in consistent, repeatable,
    and predictable deployment, and each microservice has an isolated environment.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides a declarative approach to managing how microservices are
    deployed. The desired state of each microservice is defined using Kubernetes manifest
    files and includes the Docker image to be used as a running microservice. The
    diverse needs of each microservice, as illustrated in our real-life example, can
    be realized by declaring the number of replicas and resource requirements in these
    manifest files.
  prefs: []
  type: TYPE_NORMAL
- en: The replicas setting defines the desirable number of instances of the **household
    service** that are running. Kubernetes’ **Horizontal Pod Autoscalers** (**HPAs**)
    use this number to scale up and down based on resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes also provides mechanisms for microservices to discover and communicate
    with each other, such as **Domain Name System** (**DNS**) or environment variables.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these tools allow you to provision, scale, and manage the infrastructure
    needed to deploy and run microservices in an automated fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Team organization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Microservice architecture goes hand in hand with the modern organization of
    teams. The system is broken down into microservices, and so should the teams.
  prefs: []
  type: TYPE_NORMAL
- en: The golden rule is that a microservice should be owned by one – and only one
    – team. This team is responsible and accountable for the full development cycle
    of the designated microservice.
  prefs: []
  type: TYPE_NORMAL
- en: The team should be given the autonomy to make small technical decisions within
    their scope while adhering to broader guidelines on the choice of technology to
    be used in the system.
  prefs: []
  type: TYPE_NORMAL
- en: Broader guidelines are there to maintain a certain degree of consistency among
    teams, such as the choice of commercial messaging, so that the firm reduces the
    complexity and cost of having too many technologies. Also, these guidelines provide
    agreed-upon principles and conventions to all teams, but each team has the power
    to decide how to execute and adhere to those guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: The drawbacks of microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While microservices offer several benefits, such as modularity, maintainability,
    and testability, they also come with some drawbacks that must be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Increased complexity**: Microservices architecture introduces a distributed
    system with more moving parts. Communication among microservices leads to additional
    complexity in development, testing, and deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Engineers will have to consider API versioning and compatibility. Introducing
    a breaking change in an API would break other microservices that stay in the older
    API versions. Maintaining a backward-compatible API or transitioning to a new
    major API version can be a challenge.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Multiple microservices are used in the end-to-end test suite, which means these
    services need to be operational before the end-to-end test cases can be run. To
    make things worse, each microservice is usually managed by its own team, which
    means there are multiple streams of changes happening at the same time. There
    are more reasons to fail end-to-end tests now. It could be that one of the microservices
    failed to start, a change in one microservice ended up not being compatible with
    other microservices, and so on. It may end up that the engineering teams spend
    a lot of energy fixing the end-to-end tests.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Microservices architecture requires a more complex infrastructure, including
    service routing, load balancing, and container orchestration. This infrastructure
    as configuration can also act as a boilerplate as each microservice may have similar
    configurations but only differ in a few sections. The overhead of managing and
    maintaining this infrastructure can be significant, especially for smaller organizations
    or teams.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Network latency and overhead**: Breaking down a monolithic application into
    microservices implies local function invocations become remote. This can introduce
    latency and performance issues, especially if the services are geographically
    distributed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overhead of network communication, including protocols, serialization, and
    deserialization, can impact the system’s overall performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Distributed data management**: In a microservices architecture, data is often
    distributed across multiple microservices, which brings the challenges of keeping
    data manageable and consistent. Data could be split between two microservices,
    and there could be data that’s represented differently in multiple microservices.
    Worse, there could be data among microservices that isn’t consistent, so it’s
    hard to understand the overall picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and observability**: Tracing a business journey that travels through
    multiple microservices can be more challenging compared to monolithic applications.
    There are techniques to overcome this problem, but they require additional tooling
    and effort. These techniques will be covered in [*Chapter 11*](B21737_11.xhtml#_idTextAnchor358).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-grained, infrequent, on-demand, or small tasks**: The overhead of microservices
    architecture could outweigh the benefits if you’re running specific processes.
    For example, if a summary report of user activities needs to be exported as a
    file and uploaded to an SFTP folder monthly, it can hardly justify standing up
    a long-running microservice that’s only used once per month. Similar situations
    apply to small tasks that are triggered upon request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s an alternative approach to microservices architecture when the scope
    is too small to justify the overhead. We’ll discuss this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Nanoservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Nanoservices architecture, as the name suggests, takes the principles of microservices
    and makes them even more fine-grained. Microservices focus on breaking down monolithic
    applications into small, independent, and loosely coupled services, with one service
    for one business capability. Nanoservices take this concept even further by decomposing
    the system into extremely fine-grained, single-purpose components.
  prefs: []
  type: TYPE_NORMAL
- en: Nanoservices are designed to handle highly specific, autonomous, independent,
    and atomic functionalities. A nanoservice is often responsible for a single task
    or a tiny piece of logic within the overall system. Each nanoservice has a deployable
    artifact and can be deployed independently. Some of these nanoservices, when combined,
    can be seen as microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the previous example regarding microservices, the **household service**
    can be broken down into several nanoservices:'
  prefs: []
  type: TYPE_NORMAL
- en: Get a Household record by name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create or update a Household record.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These two nanoservices share the same database schema, while only one of them
    focuses on returning a Household record by name. The other nanoservice focuses
    on performing a create/update operation and sending asynchronous messages after
    a Household record is updated.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nanoservices have much smaller code bases, fewer dependencies, and less resource
    utilization (CPU, memory, and disk) compared to microservices. This reduced footprint
    results in simpler deployment and configurations. Scaling nanoservices is efficient
    as it only concerns the needs of one function.
  prefs: []
  type: TYPE_NORMAL
- en: Since nanoservices require less coordination and communication between each
    other, a nanoservice can be seen as a plumbing unit that only focuses on inputs,
    processes, and output.
  prefs: []
  type: TYPE_NORMAL
- en: The reduced complexity and overhead of nanoservices can make them particularly
    well-suited for resource-constrained environments, real-time systems, or scenarios
    where fault tolerance and rapid scaling are critical.
  prefs: []
  type: TYPE_NORMAL
- en: Drawbacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though managing a nanoservice is easier than managing a microservice, a
    system that’s been broken down into nanoservices has a significantly higher number
    of services to manage. This high number may outweigh the ease of managing one
    nanoservice.
  prefs: []
  type: TYPE_NORMAL
- en: The high number of nanoservices may bring significant network communication
    and collaboration challenges. The small resource footprint of one nanoservice
    may not translate well regarding overall resource consumption (CPU, memory, and
    disks).
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, some nanoservices may share the same database schema for different
    operations. In our example of the read and update nanoservices for the **household
    service**, they share the same database schema. If the schema needs to evolve,
    there’s additional complexity regarding how to apply the change in the database
    schema, and then the changes in both nanoservices. It’s due to these fragmental
    concerns that some people would consider nanoservices architecture an anti-pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining data consistency and coherence across a large number of highly autonomous
    nanoservices can be a significant challenge that requires careful design and coordination
    mechanisms. Imagine that there’s one nanoservice for the create operation and
    another for the update operation; keeping the validation logic of the two nanoservices
    consistent can be a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Deciding between microservices and nanoservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The decision of whether to have one microservice or multiple nanoservices is
    a balancing act on different overheads. In general, nanoservices work better if
    the function is simple and isolated. Any effort to coordinate with other nanoservices
    should be considered carefully.
  prefs: []
  type: TYPE_NORMAL
- en: In our previous example, the **notification service** has a simple objective
    compared to the other services. It merely translates an internal event into an
    email that contains an address and requests an email service provider to send
    the email. This is a suitable candidate to be a nanoservice as its task is simple.
    If we tweak the **notification service** so that it accepts email requests of
    a universal structure, then it can be an autonomous and independent nanoservice
    that doesn’t depend on other services.
  prefs: []
  type: TYPE_NORMAL
- en: The overhead of managing a high number of nanoservices is still a concern to
    many engineers. However, the emergence of serverless architecture may have addressed
    this concern by having nanoservices managed by cloud providers. Next, we’ll consider
    serverless architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless architecture is a computing style in which engineers are no longer
    concerned with capacity planning, configuration, management, maintenance, resilience,
    scalability, physical servers, or virtual servers. There are still servers running
    but they’re abstracted away.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying web services used to be an expensive process. Physical server machines
    (the **bare-metal** aspect), network cables, and other accessories need to be
    purchased with the correct amount of storage, memory, and bandwidth. Operational
    engineers need to install and keep them on-site in a data center. Physical servers
    need to be set up correctly and connected to the network. Only then can web services
    be deployed to and hosted on these servers.
  prefs: []
  type: TYPE_NORMAL
- en: Bare-metal servers come with not only the initial cost of purchase but also
    ongoing costs such as electricity, renting from data centers, and visits by engineers
    to keep your server up and online every time. There are also security concerns
    as these physical machines are at risk of being damaged or stolen.
  prefs: []
  type: TYPE_NORMAL
- en: Most engineers aren’t server specialists. Companies may need to either train
    their engineers to become system administrators, use hardware and network specialist
    contractors, or hire these specialist engineers to work with the application.
  prefs: []
  type: TYPE_NORMAL
- en: Every time the system needs to scale, it requires buying new machines or upgrading
    existing ones. They need to be configured so that they fit the other machines
    and be used by the application. Sometimes, purchasing and delivering the new machines
    takes time, so scaling doesn’t happen when it’s needed the most by the system.
  prefs: []
  type: TYPE_NORMAL
- en: Today, bare-metal servers are still the main choice for systems that require
    ultra-low latency and high-frequency processing, such as trading systems.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless architecture aims to solve the issues from the days of bare-metal
    servers. Let’s explore how.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts of serverless architecture are deeply rooted in distributed computing.
    The history of evolution can be traced back to **grid computing**, in which computing
    tasks were distributed across a network of machines.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless architecture wasn’t popular in commercial systems until *Amazon*
    launched **Amazon Web Services** (**AWS**) in 2006\. AWS provides a set of services
    for businesses to access computing resources over the internet (the **cloud**).
    Initially, AWS offered **Elastic Compute Cloud** (**EC2**) as virtual servers
    to run computation and **Simple Storage Service** (**S3**) as distributed file
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: In 2010, *Microsoft* launched **Azure** and offered cloud services such as AWS,
    including virtual servers and storage. In 2011, *Google* launched **Google Cloud
    Platform** (**GCP**) to compete with Microsoft and Amazon when it came to cloud
    services. AWS, Azure, and GCP remain the three most popular cloud services nowadays,
    and cloud services are also provided by big companies such as *IBM*, *Oracle*,
    *Alibaba*, and *Tencent*. With the variety of cloud service offerings available,
    serverless architecture has come to fruition and is still evolving.
  prefs: []
  type: TYPE_NORMAL
- en: By using cloud services to run applications, the users are the **tenants** who
    subscribe to the service. Tenants rent and use the computing resources in the
    cloud on an on-demand basis, and hence the cost is arguably more flexible. Cloud
    services replace the need to procure and provision computer hardware and host
    it in a data center.
  prefs: []
  type: TYPE_NORMAL
- en: There are four major categories of serverless services provided by cloud service
    providers. Let’s take a closer look.
  prefs: []
  type: TYPE_NORMAL
- en: Infrastructure-as-a-Service (IaaS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IaaS offers computing resources in the cloud such as virtual servers, storage,
    and networking on an on-demand basis. It’s like renting an empty space where tenants
    must configure everything in it.
  prefs: []
  type: TYPE_NORMAL
- en: Users are given an administrator account so that they can set up the infrastructure
    via a management console graphical interface, **command-line interface** (**CLI**),
    or declarative configuration tools such as **Terraform**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following typical infrastructures are offered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Virtual servers**: These are virtualized machines that can run anything set
    up by tenants. Tenants need to specify basic requirements such as CPU, RAM, disk
    spaces, and network addresses to reach the servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secrets management**: There are various situations where we need to keep
    sensitive data as configuration for an application. This varies from encryption
    keys and API keys to foreign systems or credentials to access a database. With
    IaaS, these secrets can be managed separately and are injected into the runtime
    of the application that runs in, for example, virtual servers. As a result, these
    secrets can be viewed and managed by fewer people and be abstracted out of the
    code base. The cloud providers also offered advanced features such as key rotation
    and expiration for extra security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed file storage**: Cloud service providers offer scalable and durable
    storage services that can be accessed by applications. They allow tenants to store
    files of almost any size, and their storage scales as needed. They can replicate
    the files to multiple locations for redundancy and recovery purposes. They also
    support file versioning, so it’s possible to retrieve previous versions of the
    same file object. Finally, they support fine-grained access control to the file
    and grant time-limited access to specific files for download purposes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Databases**: Managed database services are a big category since there’s a
    diverse range of choices. Most cloud providers offer relational and NoSQL databases,
    while some of them offer special types of databases, such as data warehouses.
    There’s also a list of vendors and versions available that provides a smooth path
    for applications to move from bare metal to the cloud. They provide managed services
    that handle infrastructure provisioning, upgrades, scaling, replication, failover,
    and monitoring. Some of them provide advanced features such as data encryption
    for handling sensitive information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Messaging**: Like databases, managed messaging services also have a big category
    of services per cloud provider. There are four main types of messaging available.
    The first type is a simple queuing service where a message is sent from a sender
    to a recipient. The second type is the **Publish/Subscribe** (**Pub/Sub**) pattern,
    where a message is published to a topic via a broker, and all subscribers to the
    topic receive the message. The third type is streaming, where messages are consumed
    as a continuous flow as they’re sent. The last type is specialized messaging services,
    which target specific use cases such as emails and mobile application notifications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud providers abstract away the complexity of setting up and managing the
    messaging infrastructure required. These managed services scale up and down on
    demand and take care of replication, security, and monitoring concerns.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss another big category of services that serve
    as a platform instead of infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Platform-as-a-Service (PaaS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PaaS provides engineers with a cloud platform so that they can develop, run,
    and manage applications without setting up the infrastructure themselves. Engineers
    still need to configure and manage their applications, runtimes, data, and services.
    The details of the configuration are abstracted away and specified in a declarative
    way. The cloud provider takes care of the lower-level concerns, such as hardware,
    operating systems, and network settings.
  prefs: []
  type: TYPE_NORMAL
- en: These services support specific programming languages and frameworks that enable
    engineers to focus on the application itself. The service handles the details
    of provisioning servers, load balancing, scaling, and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: Going further with this, if we don’t want to build systems on a platform, perhaps
    we can simply use and integrate with the existing software available. This is
    the topic of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Software-as-a-Service (SaaS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SaaS offers software that’s ready to be used by end users or integrated with
    applications. This service involves tenants just using some software without having
    to code, manage environments, or even possess technical knowledge. Services in
    this category range from complete usable software solutions in the cloud and no-code
    application building to headless systems that can integrate with applications
    via APIs. In the latter case, tenants are still expected to run their applications
    via other means and set up a network connection to the SaaS service.
  prefs: []
  type: TYPE_NORMAL
- en: This category has the largest variety of software applications. Most companies
    would use at least one SaaS service, and a lot of companies aim to provide SaaS
    services in this open space.
  prefs: []
  type: TYPE_NORMAL
- en: Using SaaS gives us a holistic package of business functionalities. This is
    particularly popular when the business functionalities are necessities but not
    at the core of the organization.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve discussed bigger units, such as software, we’re going to look
    at smaller units, which are functions in the serverless architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Function-as-a-Service (FaaS)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: FaaS allows engineers to write code and deploy it as a function, typically when
    reacting to events or triggers. These functions don’t store states themselves,
    but they can make use of other resources, such as file storage and databases.
    Engineers don’t need to manage any infrastructure. They’re intended to be reusable
    functions so that higher-order functions can be built on top of other functions.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud providers scale the runtime environment that’s executing a function based
    on workload. They also charge based on the usage of the function, which is optimized
    for cost. Note that some of the services have restrictions, such as maximum execution
    time, memory usage, and the number of concurrent processes.
  prefs: []
  type: TYPE_NORMAL
- en: With these services, we can drop our functions as code into the cloud environment
    for execution. The cloud providers will do the rest for us.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we’ve covered the four categories of cloud computing services that
    enable serverless architecture. Next, we’re going to delve into how to build systems
    using these services. We’ll discuss the benefits of these services and explain
    how to use them to create a modern, scalable, and easy-to-maintain system.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The core value of serverless architecture is infrastructure concerns being
    abstracted and implemented by cloud providers. Here are the benefits that come
    with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: Serverless architecture can automatically scale resources
    based on demand. When the application is under a heavy workload, the cloud provider
    dynamically allocates resources to handle the increased load and to ensure optimal
    performance. When the workload decreases or becomes idle, resources are scaled
    down, resulting in optimized cost and efficient resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost efficiency**: Cloud providers offer a pay-per-use pricing model where
    tenants are billed by their actual usage. This pricing model optimizes costs and
    eliminates the need to purchase and maintain idle resources. This is attractive
    for organizations looking for cost-efficient solutions, particularly startups
    and small companies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time to market**: Since it’s a lot quicker to spin up infrastructure to host
    applications, engineers can focus their time on developing business functions
    and specific functions. Plus, infrastructure settings now have a more declarative
    configuration than them having to work on the details of each infrastructure component.
    This results in a faster development and deployment cycle, the ability to continuously
    deploy changes, and a shorter time to market.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adaptability and migration**: The range of available services allows engineers
    to host from big applications to small functions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of companies migrate their systems from bare-metal monolithic applications
    to virtual servers in the cloud as the first step is to break them down into microservices
    and functions. This is more cost-efficient and quicker than breaking down monolithic
    applications first and moving to virtual servers afterward due to the comprehensive
    support cloud providers offer regarding infrastructure services.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: On the other side of the spectrum, there’s a lot of FaaS support to just write
    a small function to perform a small task. The wide range of support from cloud
    providers concerning the size of the application makes it quite easy for engineers
    to adapt and migrate existing systems toward serverless architectures.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Diverse support for business domains**: Serverless architecture is suitable
    for event-driven and highly scalable systems. It’s commonly used for building
    microservices, real-time processing systems, web and mobile backends, **Internet
    of Things** (**IoT**) applications, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many ready-to-use SaaS services that engineers could focus on in their
    business domains. For example, Amazon **Simple Email Service** (**SES**) can be
    used to send emails to customers, Azure Notification Hub can be used to send push
    notifications to mobile devices, and Google Cloud IAM can provide **multi-factor
    authentication** (**MFA**) and reCAPTCHA to verify users’ identities.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: However, it’s important to note that serverless architecture might not be suitable
    for all use cases. Also, many services are provided by each cloud provider, so
    you need to err on the side of caution to ensure a suitable service is chosen
    to meet the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Cautions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While serverless architecture offers numerous benefits, there are important
    drawbacks to consider when adopting this approach:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cold start latency**: FaaS functions are initiated on-demand, meaning that
    when a function is triggered for the first time or after a period of being idle,
    it might take a while to start up. This delay is known as a “cold start” and occurs
    while the cloud provider provisions the necessary resources on the fly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the function is triggered infrequently, which often causes cold starts, latency
    is increased for normal requests. If the application is expected to respond without
    noticeable delays, then PaaS or virtual servers should be used instead.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Vendor lock-in**: Cloud providers offer a wide range of services, while a
    lot of them provide proprietary APIs, frameworks, runtime environments, and even
    languages. While it’s handy to have support from cloud providers in various areas,
    it’s easy to rely too heavily on a specific cloud provider. This causes vendor
    lock-in, which makes it challenging to migrate to another provider.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This creates barriers to migrating to another cloud provider or switching back
    to bare-metal infrastructure. While most cloud providers keep their pricing competitive,
    many companies find it crucial to have the ability to migrate if the situation
    arises. In response to this, some companies choose to use multi-cloud architecture
    with data synchronization processes between different cloud platforms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Function granularity**: Decomposing an application into smaller functions
    appropriately is a key aspect of serverless architecture. However, breaking down
    functionality into excessively fine-grained FaaS functions can result in increased
    overhead due to the invocation and coordination of numerous functions, leading
    to release dependency, higher costs, higher latency, and more complex systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dividing applications and grouping functions so that they’re the right size
    is the key factor for a scalable and cost-efficient system. We’re going to explore
    this aspect in detail while considering our real-life example shortly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**State management**: FaaS functions are typically designed to be stateless,
    meaning they don’t retain their memory of previous executions. When there’s a
    chain of functions and triggers working together with data to share, it poses
    a challenge to how the state is shared across multiple function invocations. This
    pattern often involves other IaaS services, such as queues, databases, or in-memory
    caches. State management must be addressed carefully as it involves concerns such
    as concurrency, data housekeeping, and compatibility while evolving.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and debugging**: Troubleshooting and monitoring serverless applications
    can be more complex than traditional architectures. When a business workflow is
    distributed among multiple functions and processes, it becomes challenging to
    diagnose, reproduce, and resolve issues. We should invest in observability tools
    such as log aggregation, monitoring, dashboards, and alerting. We also need to
    design the system so that it can handle errors gracefully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost management**: While serverless architectures often optimize cost due
    to the pay-per-use model, it’s essential to monitor and optimize resource consumption.
    Granular billing based on usage can lead to unexpected costs if the applications
    are designed inefficiently or experience unexpected spikes in traffic. This can
    be caused by inefficient system design or simply because the pattern of usage
    has changed over time. Adequate monitoring, performance testing, and optimization
    strategies need to be in place to control costs effectively. This is also an opportunity
    to discover system inefficiency so that the system can improve with new findings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long-running processes**: FaaS functions are typically execution time limits
    that are imposed by cloud providers. In other words, FaaS functions are meant
    to be small and executed quickly. If an operation requires significant processing
    time or must run continuously, it might be better to look for PaaS or IaaS alternatives,
    such as virtual servers. Careful consideration is needed to decide on the appropriate
    approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and compliance**: Serverless architectures introduce new security
    considerations. Ensuring secure function invocations, managing access controls,
    and protecting sensitive data within the serverless environment are critical.
    Compliance with regulations and industry standards should be thoroughly evaluated
    to ensure proper security measures are in place.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-functional requirements (NFRs)**: Even cloud providers offer a wide range
    of services that take away the concerns of infrastructure that are often parts
    of the NFRs. Our choice of serverless service needs to meet these requirements.
    Sometimes, it’s hard to be in control of meeting these requirements since engineers
    can only configure the desired resources, and in the end, it’s the platform that
    provisions the resources to meet the desired resources as configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an extreme case, it might be justified to go back to bare-metal servers to
    have full control of the hardware and network that could fulfill the high-end
    NFRs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: By understanding and addressing these cautions, organizations can make informed
    decisions when adopting serverless architecture and mitigate potential risks associated
    with its implementation. Next, we’re going to run through an exercise of adopting
    serverless architecture while utilizing the real-life example provided in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Adopting serverless architectures in our real-life example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider the same real-life example that we used earlier in this chapter, where
    households exchange services with one another. Previously, we identified three
    potential microservices. Let’s recapitulate what they are:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Household service**: Masters the records of households'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contract service**: Maintains the workflow of contract negotiation from drafted
    to fully exercised'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Notification service**: Sends proxy notification requests to email service
    providers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 6**.4* shows how these four microservices communicate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Recap example of updated microservice communication](img/B21737_06_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Recap example of updated microservice communication
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we’ll assume that we want to have the system hosted in AWS.
    We need to decide which cloud service to use and what the desired setup is.
  prefs: []
  type: TYPE_NORMAL
- en: Function granularity and choosing the computing service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previously, we mentioned we need to be cautious about function granularity as
    this will impact system efficiency and cost. We’re going to review how these services
    can be executed and which computing service we might use.
  prefs: []
  type: TYPE_NORMAL
- en: The **household service** provides the classic **Create, Read, Update, and Delete**
    (**CRUD**) operations and connects to a relational database for persistent storage.
    These operations are highly cohesive as they cover the life cycle of households.
    They all assume the same database schema. Moreover, to ensure there’s a way for
    the schema to evolve reliably, it seems reasonable to have the **household service**
    own its schema. This implies that incremental database migration tools such as
    **Flyway** can be used and that the incremental **Data Definition Language** (**DDL**)
    files should be hosted together with the source code that translates into the
    **Structured Query Language** (**SQL**) commands to perform the CRUD operations
    of households.
  prefs: []
  type: TYPE_NORMAL
- en: The incremental database migration runs as part of the service startup. The
    migration tool will check if the latest DDL file is of the same version as the
    version registered in the migration history records. If it’s the same version,
    migration will finish as a **no operation** (**no-op**); if the script version
    is higher than the recorded version, then the tool will run the incremental script
    until the version matches again. With this setup, any evolution in the database
    schema is released in one deployment action, with the schema changes and corresponding
    code changes going in a synchronized manner.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, there are three operations (Create, Update, and Delete) that would
    need an updated household event to be published. These operations would assume
    a specific message format. If the message format is going to change, it may affect
    all three operations. This also suggests they should be grouped as one deployable
    artifact to ensure smooth, reliable changes.
  prefs: []
  type: TYPE_NORMAL
- en: On the contrary, if the CRUD operations are separated into four FaaS functions
    or nanoservices, any change in data structure or message structure would require
    a coordinated release of these functions. This means function coupling, release
    dependency, downtime, and the risk of partial deployment failures.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this example, the `GET`, `PUT` (create and update), and `DELETE`
    verbs.
  prefs: []
  type: TYPE_NORMAL
- en: The messaging technology in this example will be Kafka. We intend to publish
    events on a normal topic and a compacted topic. The normal topic is used to announce
    creation, updates, and soft deletion, while the compacted topic is used as a last-value
    queue to keep the last snapshot of the Household records.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next consideration is about which serverless computing the **household
    service** should use. Here’s an example of using AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The household service using AWS](img/B21737_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – The household service using AWS
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use Amazon **Elastic Kubernetes Service** (**EKS**) to run the **household
    service**. This requires us to implement the following infrastructure setup:'
  prefs: []
  type: TYPE_NORMAL
- en: Specify the AWS region
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a **Virtual Private Cloud** (**VPC**) and subnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a role in AWS **Identity and Access Management** (**IAM**) for EKS to
    assume a role for the EKS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define the EKS cluster that makes use of the VPC, subnet, and the IAM role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create a security group for the EKS cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attach the EKS cluster and EKS service policies to the IAM role
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create an Amazon **Relational Database Service** (**RDS**) with PostgreSQL and
    its subnet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the Kubernetes provider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the Kubernetes namespace and config map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the Kubernetes secrets to be imported from AWS Secrets Manager, such
    as passwords
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure the ingress (incoming traffic route) and **Application Load Balancer**
    (**ALB**) so that the requests can reach the REST endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configure Amazon **Managed Kafka Service** (**MSK**) on Kafka topics and security
    groups to allow an EKS pod to publish and consume messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given that the **household service** has the infrastructure set up, we can
    start a Kotlin project. There are many ready-to-go project creators available
    on the internet, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Spring Boot
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ktor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP4K
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vert.x
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These tools all create a skeleton project that can be built using their respective
    server frameworks. In this example, we’re using Ktor as the server framework and
    REST endpoint routing. In Ktor, endpoint routing is defined like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: We’re using a declarative configuration as Kotlin code, and we’re expected to
    define a payload format and set up corresponding serialization by using Ktor content
    negotiation.
  prefs: []
  type: TYPE_NORMAL
- en: The Kafka topics can be defined using Terraform, which provides a standard declarative
    format for specifying infrastructure. The normal topic will need to set a retention
    policy that determines how long the message should remain on the topic.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compacted topic has a different setup. The message in the compacted topic
    should be retained for as long as possible. The newer message of the same key
    will replace the older one by compacting the logs. The cleanup policy of compacted
    topics should be set to `"compact"`, with the retention period set to `–1`. Here’s
    an example of how a compacted topic is specified in Terraform:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The updated Household records are going to be sent to both topics. This is
    illustrated in the following code using the **Apache** **Kafka API**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The message that’s sent to the compacted topic contains a key to identify and
    remove older messages of the same key. In this instance, household names are used
    as keys.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **contract service** provides controlled operations on the workflow regarding
    contract negotiation and contract exercise. It uses some form of persistent storage
    to keep a local copy of households and to maintain the state of the contract in
    its workflows. It uses a serverless computing service similar to that of the **household
    service**, as shown in *Figure 6**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The contract service using AWS](img/B21737_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – The contract service using AWS
  prefs: []
  type: TYPE_NORMAL
- en: The **contract service** receives Household records from the topics published
    by the **household service**. Initially, it consumes all messages in the compacted
    topic to build its local cache of households and subsequently receives updates
    of Household records from the normal topic. It also sends notification requests
    to a topic to be consumed by the **notification service**.
  prefs: []
  type: TYPE_NORMAL
- en: In this setting, when a household is updated while the `household-v1` and `household-snapshot-v1`
    topics. The event in the queue is now waiting for the **contract service** to
    come back up. One instance of the **contract service** becomes available and takes
    this event for further processing.
  prefs: []
  type: TYPE_NORMAL
- en: The **contract service** uses the same AWS components as the **household service**
    in terms of IAM, Secrets Management, ALB, Kubernetes, and database services.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the **notification service** is simple in that it takes a
    notification request and proxies the request to an email service provider. There’s
    no tight requirement to send emails instantly and it’s OK to have the emails be
    sent a couple of minutes late. There’s no need to maintain state either as the
    request message already contains the household email addresses and the content
    of the messages.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a suitable candidate for a FaaS service. While we can use AWS Lambda
    to meet the requirements, a nanoservice is an equally suitable choice. For this
    reason, the **notification service** is called **Email notifier** in physical
    deployment, as shown in *Figure 6**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The notification service using AWS](img/B21737_06_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – The notification service using AWS
  prefs: []
  type: TYPE_NORMAL
- en: We can configure the function so that it’s triggered by a new message in the
    Kafka topic. The function converts a request into email format and passes it to
    Amazon SES so that it can send the email.
  prefs: []
  type: TYPE_NORMAL
- en: The function must be configured to use a constant Kafka consumer group so that
    multiple instances of the same function don’t consume the same message and effectively
    behave as a queue.
  prefs: []
  type: TYPE_NORMAL
- en: The function is stateless, and AWS provides all the means to connect to SQS
    and SES. There’s no concern regarding a cold startup occurring as there’s no tight
    latency requirement. The function also scales automatically based on traffic and
    is controlled by AWS. Similarly, this function uses IAM and Secrets Management
    to control accessible resources and secrets.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve covered the basic principles of serverless architecture, in conjunction
    with what’s offered by the four major categories of services provided by major
    cloud providers. We’ve also discussed the benefits and cautions of applying serverless
    architecture. Finally, we ran an exercise in adopting serverless architecture
    for the real-life example specified in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’re going to briefly cover the microfrontend architecture, which works
    similarly to microservices in principle.
  prefs: []
  type: TYPE_NORMAL
- en: Microfrontends
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The microfrontend architecture aims to enhance modularity, scalability, and
    autonomy by breaking down the UI into smaller, self-contained frontend modules.
  prefs: []
  type: TYPE_NORMAL
- en: The term *microfrontend* first appeared in 2016 under Thoughtworks Technology
    Radar with the recommendation of *Assess*. It’s often compared to the concept
    of microservices on the backend. The microfrontend architecture promotes decomposing
    the frontend into independently deployable and maintainable units, each responsible
    for a specific part of the UI.
  prefs: []
  type: TYPE_NORMAL
- en: Same symptoms from the days of monolithic applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A traditional monolithic frontend application has a single code base that handles
    the entire UI. Usually. There’s more than one team of engineers working on it,
    which causes code conflicts, release dependency, slow build time, limited autonomy
    for each team, and challenges in scaling and maintaining large applications. This
    situation is the same regarding microservices, except this happens in the frontend.
  prefs: []
  type: TYPE_NORMAL
- en: The microfrontend architecture addresses these issues by enabling different
    teams to work independently on distinct parts of the UI, allowing them to choose
    their own technologies, frameworks, and release cycles.
  prefs: []
  type: TYPE_NORMAL
- en: Many small frontend modules as individual applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a microfrontend architecture, the UI is composed of multiple frontend modules.
    Each module is an application that can be developed, tested, deployed, and scaled
    independently. Like microservices, one frontend module should be owned by one
    – and only one – team, but as a common library.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each module is a grouping of cohesive functionalities. Considering the real-life
    example provided in this chapter, there should be two frontend applications:'
  prefs: []
  type: TYPE_NORMAL
- en: The **household application**, which manages household account creation, updates,
    and deletion. Each household can manage its own account details via this application.
    This application primarily communicates with the **household service** in the
    backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **contract application**, which allows two households to progress from a
    draft contract to an agreement. It supports drafting a contract, similar to the
    screens we saw in [*Chapter 5*](B21737_05.xhtml#_idTextAnchor125). The two households
    that are involved can agree to the contract or amend it until both households
    agree with the details. It also tracks how the agreed-upon contract is exercised
    by the two households involved. Regarding the polymorphic nature of the different
    services that can be mentioned in the contract, as described in [*Chapter 3*](B21737_03.xhtml#_idTextAnchor067),
    there may be multiple screens for households to report the status of the services
    that are exercised based on the contract. This application primarily communicates
    with the **contract service** in the backend.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these applications are bundled as self-contained artifacts that can be launched
    on their own, allowing separate teams to focus on their business domains.
  prefs: []
  type: TYPE_NORMAL
- en: However, there should be one more application that integrates with all other
    applications. This application doesn’t contain business logic; instead, it’s merely
    an over-delegation module that typically provides a menu for users to access other
    applications. This is the application that creates a unified UI at build time
    or runtime, depending on whether it’s a web or mobile platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, these frontend applications can be illustrated like so, together with
    the backend services they communicate with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – The frontend and backend communication of the real-life example](img/B21737_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – The frontend and backend communication of the real-life example
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, each frontend application has a primary microservice
    in the backend to communicate with. Each frontend application communicates with
    other frontend applications. Microservices also communicate with each other. The
    roles and responsibilities of each frontend application and each microservice
    are well-defined and clear.
  prefs: []
  type: TYPE_NORMAL
- en: This architecture enables each frontend application and its primary microservice
    to be owned by only one team. This is coherent with the idea of organizing teams
    by business function, as mentioned in [*Chapter 1*](B21737_01.xhtml#_idTextAnchor013).
  prefs: []
  type: TYPE_NORMAL
- en: Communication among frontend modules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having communication and coordination among the frontend modules is critical
    in a microfrontend architecture. There are various techniques and patterns to
    facilitate this, such as asynchronous messaging, event-driven architectures, or
    shared state management.
  prefs: []
  type: TYPE_NORMAL
- en: In our example, a shared state managed by the collection of backend microservices
    is used to serve each frontend module.
  prefs: []
  type: TYPE_NORMAL
- en: These techniques enable seamless integration and collaboration between different
    modules while maintaining loose coupling and encapsulation. A well-integrated
    UI with a microfrontend architecture brings a connected and consistent user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Design systems for consistent user experiences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization is an essential element of any frontend application. While having
    smaller frontend applications autonomously run by their responsible teams, it’s
    paramount to ensure the integration UI has a consistent look and feel. A design
    system provides the common UI components, such as buttons, checkboxes, text fields,
    and interaction styles, that align all frontend modules so that they behave the
    same way. In that sense, end users have a seamless experience with little learning
    required when navigating to another frontend module.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The microfrontend architecture offers several benefits to engineering teams
    and organizations. By allowing teams to work autonomously, it promotes faster
    development cycles, easier maintenance, and the ability to adopt new technologies
    and frameworks without affecting the entire application. It results in higher
    productivity and quicker time to market. It also enables scalability by allowing
    individual modules to be scaled independently, providing flexibility in managing
    traffic and resources.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, the microfrontend architecture advocates code reusability by reusing
    UI components brought from design systems that can be shared across multiple applications.
    This can lead to improved consistency, reduced duplication, reduced user learning,
    and increased productivity in frontend development.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the microfrontend architecture offers numerous advantages, it also introduces
    complexities and challenges such as module communication, versioning, and orchestration
    when integrating all frontend modules into one application. Successful implementation
    requires careful planning, design considerations, and selecting the appropriate
    tools and frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Variations among the web and other platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The web and other platforms have a slight difference when applying the microfrontend
    architecture. The web platform can achieve independent release as frontend modules
    are integrated primarily using hyperlinks.
  prefs: []
  type: TYPE_NORMAL
- en: Mobile and desktop applications are trickier because they need to generate a
    monolithic artifact for users to download and install. Releasing a frontend module
    would require regenerating the monolithic artifact and bumping the build versions.
    Some organizations might choose to release the application in a specific cadence
    to avoid excessive updates being required by the application.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the microfrontend architecture is a powerful paradigm that empowers
    engineering teams to create scalable, modular, and maintainable frontend applications
    by decomposing the frontend into smaller, independent modules. By embracing this
    architectural style, organizations can achieve greater flexibility, agility, and
    scalability in their frontend development processes.
  prefs: []
  type: TYPE_NORMAL
- en: An overall perspective
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we’ve discussed how monolithic applications evolve from a historical
    perspective. We’ve discussed that the monolithic architecture evolves into the
    SOA, where one big application is decomposed into chunks of smaller applications.
    Then, the era of microservice and microfrontend architectures begins as they’re
    broken down into even smaller applications.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the serverless architecture emerges, which allows a single function
    to be executed as a unit in the cloud infrastructure. Meanwhile, it still supports
    bigger cloud applications and lets them run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Putting them all together into one perspective, we can start to see how the
    sizes differ in each architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – A size comparison among covered architecture styles](img/B21737_06_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – A size comparison among covered architecture styles
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that serverless architecture can adapt to all sizes. It can
    even adapt to monolithic applications, though usually, the first step is to break
    the monolithic application into smaller services. This is a typical example of
    “*First make the change easy, then make the* *easy change.*”
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: “*First make the change easy, then make the easy change*” is a quote attributed
    to Kent Beck, a pioneer of **Extreme Programming** (**XP**) and Agile methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a few cases where monolithic architecture is justified.
    We discussed the basic principles of how to divide a monolithic application into
    microservices and nanoservices, as well as how to detect when the division isn’t
    right. We used a real-life example to delve into the thought process of designing
    microservices and nanoservices.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we introduced the serverless architecture and the most popular cloud providers.
    We covered the four major categories of cloud computing services (IaaS, PaaS,
    SaaS, and FaaS) and discussed the benefits and cautions when using serverless
    architecture. After, we conducted an exercise where we adopted serverless architecture
    and chose appropriate cloud computing services to meet the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we briefly covered the microfrontend architecture, where a monolithic
    frontend application is broken down into frontend applications. We used the same
    real-life example from before to illustrate the decomposition and how each frontend
    module communicates with the backend components. Finally, we covered the need
    for design systems to ensure a consistent user experience and briefly mentioned
    the benefits and challenges of using the microfrontend architecture.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to dive into the practice of separating concerns
    using selected methodologies to help drive us toward efficient, scalable, and
    maintainable applications.
  prefs: []
  type: TYPE_NORMAL
