<html><head></head><body><div class="chapter" title="Chapter&#xA0;12.&#xA0;Real-time Shadows and Particle System"><div class="titlepage"><div><div><h1 class="title"><a id="ch12"/>Chapter 12. Real-time Shadows and Particle System</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Creating shadows with shadow mapping</li><li class="listitem" style="list-style-type: disc">Softening the shadow edges using PCF</li><li class="listitem" style="list-style-type: disc">Using variance shadow mapping</li><li class="listitem" style="list-style-type: disc">Simulating the particle system</li><li class="listitem" style="list-style-type: disc">Transform feedback particle system with sync objects and fences</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec98"/>Introduction</h1></div></div></div><p>Shadows play an important role in real-time rendering; they add depths to a rendering scene. The perceived light information on the 3D object looks much more realistic when rendered with shadows. Overall, shadows improve the realism of the rendering scene and provide a spatial relationship among objects. Rendering smooth and realistic shadows is a great topic of research in the field of computer graphics. The rendering process consumes a large performance. Therefore, the approach to render it must be a balanced trade-off between quality and performance. This even becomes more challenging on the embedded-side due to limited constraints on the memory and performance.</p><p>In this chapter, we will implement shadows using shadow mapping. This technique is relatively cheap as far as performance is considered and produces good results on embedded devices. We will make these shadows appear smoother using another technique called percentile closer filtering (PCF). In another technique called variance shadow mapping, we will improve the performance and quality of the generated real-time shadow.</p><p>This chapter will also help us understand the basics of particle rendering. We will implement two techniques to render the particle system. The first technique is bound to the CPU, whereas particles are updated and processed on the CPU-side and sent to the GPU only for rendering purposes. The second technique is implemented with a new feature of OpenGL ES 3.0 called transform feedback. This feature allows you to capture the vertex shader output to feedback again to the GPU for next frame rendering. The particle system is processed and rendered on the GPU-side. This way, it avoids the CPU intervention and makes the rendering process highly efficient.</p></div></div>
<div class="section" title="Creating shadows with shadow mapping"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec99"/>Creating shadows with shadow mapping</h1></div></div></div><p>In this <a id="id884" class="indexterm"/>recipe, we will bring more realism to <a id="id885" class="indexterm"/>scenes with shadows using a simple and widely accepted shadowing technique called shadow mapping to produce real-time shadows. This technique is called shadow mapping because it uses the depth information of the scene stored or mapped to a dynamically created depth buffer to produce real-time shadows.</p><p>This technique works in two passes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>First pass</strong></span>: During the first pass, a scene is rendered from the perspective of light. Here, the scene is viewed from the position of light in the 3D space. In this way, it's clear to figure out what objects fall under the path of light. In other words, it provides the information of objects that are directly visible from the perspective of light. The scene's depth information is recorded in a FBO texture; this texture is called the shadow map. Certainly, if a light ray from the position of light passes through one or more objects, the object with the higher depth (behind the first object) from the perspective of light will be in the shadow. This technique heavily relies on the depth information captured in the shadow map; it stores the distance or depth of visible objects from the light position.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Second pass</strong></span>: In the second pass, the scene is rendered from the intended camera position. Here, first the depth of each fragment is compared with the depth stored in the shadow map. This comparison checks whether or not the incoming fragment is under the light or not. If the fragment does not fall under the light, then fragment is colored with the ambient shadow color.</li></ul></div><p>The following image shows the rendering of shadows as a result of the shadow mapping technique:</p><div class="mediaobject"><img src="graphics/5527OT_12_01.jpg" alt="Creating shadows with shadow mapping"/></div><p>This section provides a high-level overview on how to implement shadow mapping:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Create cameras</strong></span>: This <a id="id886" class="indexterm"/>creates two cameras, one is placed at the light source position called light camera and another is placed for normal scene rendering.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Shadow map</strong></span>: This creates a FBO with depth texture, as we are only interested in recording the depth and do not require any color buffer here. The dimension of the depth texture is user-defined as per application requirements. In the current recipe, we have used dimensions similar to the render buffer, which is same as viewport dimensions.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Render from light's view</strong></span>: This attaches the FBO as a current framebuffer and renders the scene from the perspective of light using the first pass and records the <a id="id887" class="indexterm"/>depth information in the shadow map. As we are only interested in the depth value, we can avoid the rasterization process in the first pass.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Render normal scene</strong></span>: This again renders the scene, but this time from the normal <a id="id888" class="indexterm"/>camera view and shares the produced shadow map with the fragment shader during the second pass.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Vertex transformation</strong></span>: During the second pass, vertex coordinates are transformed twice in the vertex shader to produce the following:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Normal scene's eye coordinates</strong></span>: The MVP matrix of the normal scene is used to produce eye coordinates to be used in <code class="literal">gl_position</code>.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Eye coordinates from light's perspective</strong></span>: Use the MVP matrix from the perspective of light (use light's camera) to produce eye coordinates, which is exactly the same as the one stored in the shadow map. These eye coordinates are called as shadow coordinates.</li></ul></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Homogeneous to texture coordinates</strong></span>: Shadow coordinates are in the normalize coordinate system [-1, 1]. These are converted to the texture coordinate space [0, 1]. This is done by using premultiplied-based matrix in which a unit matrix is scaled by a factor of half and displaced by half-logical dimensions <a id="id889" class="indexterm"/>in the positive <a id="id890" class="indexterm"/>direction:<div class="mediaobject"><img src="graphics/5527OT_12_02.jpg" alt="Creating shadows with shadow mapping"/></div></li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Depth comparison</strong></span>: This transformed shadow coordinate is shared with the fragment shader, where the current fragment determines whether it falls under the shadow or not using the <code class="literal">textureProj</code> API.</li></ul></div><p>The following image on the left-hand side of the following image shows the rendering of the scene from the light's perspective, which produces the shadow map represented by the right-hand side image. The <a id="id891" class="indexterm"/>shadow map contains the depth information on a scale of 0.0 to 1.0. The values closer to 0.0 represents nearby objects. On the grayscale image, objects appearing darker are closer to the light camera:</p><div class="mediaobject"><img src="graphics/5527OT_12_03.jpg" alt="Creating shadows with shadow mapping"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec317"/>Getting ready</h2></div></div></div><p>Unlike previous recipes, this recipe contains two custom classes for scene and model called <code class="literal">CustomScene</code> and <code class="literal">CustomModel</code>. The custom model contains other mesh models, which makes handling of model rendering very easy in the <code class="literal">NativeTemplate.cpp</code>. Similarly, the custom scene class simplifies the job of the scene, it's responsible for creating the shadow map, managing the light and normal view camera, and performing rendering in a two pass way.</p><p>This recipe uses Phong shading. There are two new uniform variables added: <code class="literal">LightCoordMatrix</code> and <code class="literal">ModelMatrix</code>. The former contains the product of the bias matrix, projection matrix and the view matrix from the perspective of light, whereas the latter contains model transformations. The product of these two variables are stored in <code class="literal">shadowCoord</code> and shared with the fragment shader. The <code class="literal">isLightPerspectivePass</code> uniform variable tells the fragment shader if it's in the first or second pass. The fragment shader contains the shadow map in <code class="literal">ShadowMap</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec318"/>How to do it...</h2></div></div></div><p>Here are the <a id="id892" class="indexterm"/>steps to implement shadow mapping:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Make the <a id="id893" class="indexterm"/>following changes in the Phong vertex shader. Here, shadow coordinates are calculated in the <code class="literal">shadowCoord</code> variable:<div class="informalexample"><pre class="programlisting">// VERTEX SHADER – PhongVertex.glsl
// Reuse old code.. many lines skipped.
// Model View Project matrix
uniform mat4 LightCoordsMatrix, ModelViewMatrix, NormalMatrix;
uniform mat4 ModelMatrix;

out vec3 normalCoord, eyeCoord;
out vec4 shadowCoord;

void main()
{
    normalCoord = NormalMatrix * Normal;
    eyeCoord    = vec3 ( ModelViewMatrix * VertexPosition );
    shadowCoord = LightCoordsMatrix 
                         * ModelMatrix * VertexPosition;
    gl_Position = ModelViewProjectionMatrix * VertexPosition;
}</pre></div></li><li class="listitem">Similarly, implement the Phong fragment shader as follows. Here, fragments are colored based on their displace from the light and scene perspective:<div class="informalexample"><pre class="programlisting">// FRAGMENT SHADER – PhongFragment.glsl
// Many line skipped contain Material and light properties
in vec3  normalCoord, eyeCoord;
in vec4 shadowCoord;
uniform lowp sampler2DShadow ShadowMap;

layout(location = 0) out vec4 FinalColor;
vec3 normalizeNormal, normalizeEyeCoord, normalizeLightVec, V, R, ambient, diffuse, specular;
float sIntensity, cosAngle;
uniform int isLightPerspectivePass;

vec3 PhongShading(){ /* Reuse existing code */ }

void main() {
    if(isLightPerspectivePass == 1){ return; }

    vec3 diffAndSpec = PhongShading();
    float shadow = textureProj(ShadowMap, shadowCoord);
    
    //If the fragment is in shadow, use ambient light
    FinalColor = vec4(diffAndSpec * shadow + ambient, 1.0);
    
    // Correct the Gamma configuration
    FinalColor = pow( FinalColor, vec4(1.0 / 2.2) );
    return;
}</pre></div></li><li class="listitem">In the <code class="literal">CustomScene</code> function's constructor, create the shadow map buffer. For this, use the <code class="literal">FrameBufferObjectSurface</code> class to create an FBO with the depth <a id="id894" class="indexterm"/>texture. This is a high-level <a id="id895" class="indexterm"/>FBO class that encapsulates the creation of FBO:<div class="informalexample"><pre class="programlisting">CustomScene::CustomScene(std::string name, Object* parentObj)
            :Scene(name, parentObj){
   // Create the FBO
   fbo = new FrameBufferObjectSurface(); 

    // Generate the FBO ID
   fbo-&gt;GenerateFBO();
    
    depthTexture.generateTexture2D(GL_TEXTURE_2D, fbo-&gt;
      GetWidth(), fbo-&gt;GetHeight(), GL_DEPTH_COMPONENT32F,
      GL_FLOAT, GL_DEPTH_COMPONENT, 0, true, 0, 0,
      GL_CLAMP_TO_EDGE, GL_CLAMP_TO_EDGE,GL_NEAREST,
      GL_NEAREST );
    
   // Attached Depth Buffer
   fbo-&gt;AttachTexture(depthTexture, GL_DEPTH_ATTACHMENT);

   // Check the status of the FBO
   fbo-&gt;CheckFboStatus();
   lightPerspective = camera = NULL;
}</pre></div></li><li class="listitem">Initialize <a id="id896" class="indexterm"/>light and normal view cameras in the <code class="literal">initializeScene()</code> function:<div class="informalexample"><pre class="programlisting">void CustomScene::initializeScene(){
// Create camera view from lights perspective    lightPerspective = new Camera("lightPerspective", this);
   lightPerspective-&gt;SetClearBitFieldMask(GL_DEPTH_BUFFER_BIT);
   lightPerspective-&gt;SetPosition
               (vec3(this-&gt;lights.at(0)-&gt;position));
    lightPerspective-&gt;SetTarget(vec3 (0.0, 0.0,0.0));
    this-&gt;addCamera(lightPerspective);
    
    // Create scene's camera view.
    viewersPerspective = new Camera("Camera1", this);
    viewersPerspective-&gt;SetClearBitFieldMask
            (GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);
    viewersPerspective-&gt;SetPosition(vec3 (25.0, 25.0,25.0));
    viewersPerspective-&gt;SetTarget(vec3 (0.0, 0.0,0.0));
    this-&gt;addCamera(viewersPerspective);
    Scene::initializeScene(); // Call the base class.
}</pre></div></li><li class="listitem">Render <a id="id897" class="indexterm"/>the scene to the first pass using the perspective of light and the second pass as normal:<div class="informalexample"><pre class="programlisting">void CustomScene::render(){
    // Set Framebuffer to the FBO    
    fbo-&gt;Push(); 

    // Render the scene from lights perspective
    lightPerspective-&gt;Render();
    
     // Cull the front faces to produce 
    glEnable(GL_CULL_FACE);
    glCullFace(GL_FRONT);

    glEnable(GL_POLYGON_OFFSET_FILL);
    glPolygonOffset(2.5f, 20.0f);
    
    for( int i=0; i&lt;models.size();  i++ ){
        currentModel = models.at(i);
        if(!currentModel){ continue; }
        
       // Set LIGHT PASS (PASS ONE) to True
        ((ObjLoader*)currentModel)-&gt;SetLightPass(true);
        currentModel-&gt;Render();
    }
    fbo-&gt;Pop();// Reset to previous framebuffer

    // Bind the texture unit 0 to depth texture of FBO
    glActiveTexture (GL_TEXTURE0);
    glBindTexture(GL_TEXTURE_2D, depthTexture.getTextureID());

    camera-&gt;Render();    // View the scene from camera
    glCullFace(GL_BACK); // Cull objects back face.
    glDisable(GL_POLYGON_OFFSET_FILL);
    
    for( int i=0; i&lt;models.size();  i++ ){
        currentModel = models.at(i);
        if(!currentModel){ continue; }
        
        // PASS TWO =&gt; Normal scene rendering
        ((ObjLoader*)currentModel)-&gt;SetLightPass(!true);
        currentModel-&gt;Render();
    }
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec319"/>How it works...</h2></div></div></div><p>In shadow mapping, a scene is constructed in the <code class="literal">CustomScene</code> class. This class creates an offscreen <a id="id898" class="indexterm"/>surface (FBO) to record the depth information of the scene. During the initialization (<code class="literal">InitializeScene</code>) phase, two camera objects <a id="id899" class="indexterm"/>are created (<code class="literal">lightPerspective</code> and <code class="literal">viewersPerspective</code>). The former camera is placed at the global light position from where the scene is lighted and the latter camera is placed at the viewer's position. The scene is rendered using two passes: one from the perspective of light and another from the perspective of a viewer. In order to let the rendering objects know about the current pass, the <code class="literal">ObjLoader::setLightPass</code> function is used; this function ensures that the object level states under these two passes.</p><p>The given scene is first rendered using the light's perspective pass, where it's bound to a FBO containing the depth buffer (<code class="literal">depthTexture</code>). The depth buffer captures the z-level or depth information of all rendering objects from the view generated by the camera placed in the light position. During this pass, front faces need to be culled and the polygon offset filling must be enabled in order to avoid the shadows acnes artefact. For more information, refer to <span class="emphasis"><em>There's more…</em></span> section at the end of this recipe. In the vertex shader, eye coordinates positions are calculated in <code class="literal">gl_position</code> and captured in the depth buffer. This shader also contains calculations of shadow coordinates that are not necessary for the first pass and can be avoided. We consider this as an optimization and leave it to our reader to implement it. As the first pass only captures the depth information, any fragment shading operation will be unnecessary, therefore rasterization can be avoided here; we used a uniform variable (<code class="literal">isLightPerspectivePass</code>) to bypass rendering of the fragment shader. However, users can also use the <code class="literal">glEnable</code> (<code class="literal">GL_RASTERIZER_DISCARD</code>) API. This API turns off the rasterization process. For more information on the working of this API please refer to <span class="emphasis"><em>Transform feedback particle system with sync objects and fences</em></span> recipe later in this chapter.</p><p>During the <a id="id900" class="indexterm"/>second pass, the viewer's camera is used to <a id="id901" class="indexterm"/>render the scene. This scene is rendered normally with back face culled and disabled polygon offset filling. The scene shares the captured depth information from the first pass to the fragment shader in the <code class="literal">sampler2DShadow ShadowMap</code> uniform variable.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note66"/>Note</h3><p>The <code class="literal">sampler2DShadow</code> is a special type of sampler. A sampler in a program represents a single texture of a specific type. The <code class="literal">sampler2DShadow</code> is used to represent the depth texture type, which contains the depth information of scene objects. It's very important to use the correct sampler; using a normal texture with the shadow map may give unpredictable results as the lookup function is different in this case. Each sampler has a different lookup function that is responsible for computing results based on input texture coordinates.</p></div></div><p>During this pass, normalize coordinates from the light (already contains projection and view info) are converted to the texture coordinate space using a premultiplied bias matrix, as mentioned in the introduction of this recipe. This coordinate is fed to the <code class="literal">textureProj</code> API, which performs a texture lookup with projection. Texture coordinates consumed from <code class="literal">shadowCoord</code> are in the texture coordinate form. In the <code class="literal">textureProj</code> API, these are converted to a homogenous form, where <code class="literal">shadowCoord.xyz</code> is divided by the last component, namely <code class="literal">shadowCoord.w</code>. The resulting third component (z) of <code class="literal">shadowCoord</code> in the shadow forms is used as the depth reference. After these values are computed, the texture lookup proceeds as in texture.</p><p>If the z value is greater than the value stored in the shadow map at a given position (x, y), the object is considered to be behind some surface. In this case, it renders to the shadow color (ambient); otherwise, it's rendered in the respective Phong shading.</p></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec320"/>There's more...</h2></div></div></div><p>This section will describe some important aspects and limitations of shadow mapping.</p><div class="section" title="The shadow map resolution"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec04"/>The shadow map resolution</h3></div></div></div><p>The quality of the <a id="id902" class="indexterm"/>shadow generated is highly dependent on the resolution of the texture to which the shadow map is constructed. The choice of the resolution depends on various factors. For example, a low-spec hardware may have limited memory or slow process power, choosing a high resolution shadow map may degrade the performance. In another case, the requirement is of higher quality in which only the high <a id="id903" class="indexterm"/>resolution shadow map makes sense. The following image shows the quality of shadows generated using various screen resolutions:</p><div class="mediaobject"><img src="graphics/5527OT_12_04.jpg" alt="The shadow map resolution"/></div></div><div class="section" title="Aliasing affects"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec05"/>Aliasing affects</h3></div></div></div><p>The shadow mapping technique suffers from an aliasing effect, which can be easily notified in various given images in this recipe. The reason for this aliasing is the sharp transition from the object color to ambient shadow color. There are various ways to reduce aliasing <a id="id904" class="indexterm"/>effects, such as increasing the resolution of the <a id="id905" class="indexterm"/>shadow map. See the preceding image. The quality of the shadow degrades as the resolution becomes low. The downside here is decrease in the performance as more samplings are taking place. The other effective and popular technique to fix aliasing artefacts is called <span class="strong"><strong>percentage closer filtering</strong></span> (<span class="strong"><strong>PCF</strong></span>). In this technique, edges soften by means of sampling. For more information, refer to the next recipe <span class="emphasis"><em>Softening the shadow edges using PCF</em></span>.</p></div><div class="section" title="Shadow acne"><div class="titlepage"><div><div><h3 class="title"><a id="ch12lvl3sec06"/>Shadow acne</h3></div></div></div><p>A very common problem that arises as a result of implementing the current technique is called shadow acne. The following image shows how the acne effect looks. This is caused when the first pass is executed with the back face culling enabled. The recorded depth texture stores the z value of the front face, which later when compared with the second pass produces <a id="id906" class="indexterm"/>large differences in depth values. These large differences are responsible for the shadow acne effect. This can be eliminated by rendering only back faces, which will result in more accurate depth comparison.</p><p>Therefore, the first pass must be performed using the front face culling. The depth texture formed using the front face culling in the first pass may still not be the same or close enough that are generated with the second pass. As a consequence of this, it results in rendering artefacts in which faces show the fade in and out effect. This visual unpleasantness can be eliminated by using the (<code class="literal">glEnable( GL_POLYGON_OFFSET_FILL)</code>) polygon offset. This polygon offset adds an appropriate offset (<code class="literal">glPolygonOffset(2.5f, 20.0f)</code>) to force resultant z values (in pass 1) to be closer enough (to pass 2) to mitigate the problem:</p><div class="mediaobject"><img src="graphics/5527OT_12_05.jpg" alt="Shadow acne"/></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec321"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to the <span class="emphasis"><em>Phong shading – the per-vertex shading technique</em></span> recipe in <a class="link" href="ch05.html" title="Chapter 5. Light and Materials">Chapter 5</a>, <span class="emphasis"><em>Light and Materials</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Transform feedback particle system with sync objects and fences</em></span></li></ul></div></div></div>
<div class="section" title="Softening the shadow edges using PCF"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec100"/>Softening the shadow edges using PCF</h1></div></div></div><p>PCF stands <a id="id907" class="indexterm"/>for percentage-closer filtering. It is a well-known and simple technique to produce smooth shadow edges. The shadow mapping technique <a id="id908" class="indexterm"/>implemented in the previous recipe shows very sharp transitions among light and shadow pixels, thereby producing aliasing effects. The PCF technique averages these sharp transitions and results in smoother shadows. Unlike the other texture that provides the capability for texture filtering, which is basically a smoothening method to determine the color of a texture-mapped pixel, unfortunately, such filtering techniques cannot be applied to shadow mapping. Alternatively, multiple comparisons are made per pixels and averaged together.</p><p>As the PCF name depicts, it samples the shadow map using the current fragment and compares it with surrounding samples. The rule is to give more weightage to samples closer to the light source. In order words, it calculates the percentage of the area closer to the illuminated surface and not in the shadow. This is how the technique got its name.</p><div class="mediaobject"><img src="graphics/5527OT_12_06.jpg" alt="Softening the shadow edges using PCF"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec322"/>Getting ready</h2></div></div></div><p>For this recipe, we <a id="id909" class="indexterm"/>have reused the shadow mapping. The following steps provide a high-level overview on how to implement this recipe:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The prefiltered shadow map</strong></span>: This shadow map needs to be prefiltered before using it in the PCF. Therefore, apply the linear texture filtering for texture minification and magnification. In the previous recipe, this corresponds to step two of the same section. This time, the 2D depth texture is created using the <code class="literal">GL_LINEAR</code> filter.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Depth comparison with PCF</strong></span>: Shared transformed shadow coordinates in the fragment shader are used to produce multiple samples based on the filter size; multiple samples are always surrounded by the current fragment. Calculate the average result of all samples and use this value to scale the intensity of diffuse and specular components computed from Phong shading in the present recipe.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>The filter size</strong></span>: The choice of dimension of the kernel filter makes a great impact on the quality of the anti-aliased edge, but this comes at the cost of performance. Bigger the filter size, better the quality and slower will be the performance. For embedded platforms, the processing capability is a considerable factor. Therefore, based on our needs, the present recipe produces acceptable results with 2 x 2 filter (four samples).</li></ul></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec323"/>How to do it...</h2></div></div></div><p>As this technique is based on the shadow map, we will advise you to reuse previous recipes and add a few changes addressed in this section. Here are the steps to implement the shadow mapping source:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the <code class="literal">CustomScene</code> constructor, create the depth texture with linear filtering this time; the <a id="id910" class="indexterm"/>last recipe uses the nearest option. This <a id="id911" class="indexterm"/>linear filtering samples depth values in an interpolated manner, which reduces the sharpness of the stored values based on the sampling of nearby depth samples:<div class="informalexample"><pre class="programlisting">// Inside CustomScene::CustomScene
fbo = new FrameBufferObjectSurface();
fbo-&gt;GenerateFBO();

// Generate the depth texture with linear filtering
depthTexture.generateTexture2D( GL_TEXTURE_2D, 
   fbo-&gt;GetWidth(), fbo-&gt;GetHeight(),
   GL_DEPTH_COMPONENT32F, GL_FLOAT, GL_DEPTH_COMPONENT, 
   0, true, 0, 0,GL_CLAMP_TO_EDGE, GL_CLAMP_TO_EDGE,
   GL_LINEAR, GL_LINEAR );

// Attached the Depth Buffer to FBO's depth attachment
fbo-&gt;AttachTexture(depthTexture, GL_DEPTH_ATTACHMENT);</pre></div></li><li class="listitem">Take the average of neighboring shadow coordinates. Make the following changes in the main function under <code class="literal">PhongFragment.glsl</code>:<div class="informalexample"><pre class="programlisting">  // Many lines below skipped, please refer to the recipe code 
  void main() { 
  vec3 diff_Spec = PhongShading();

  // APPLY the Percentage Closer filtering and use sum 
  // of the contributions from 4 texels around it
  float sum = 0.0;
  sum += textureProjOffset(ShadowMap, shadowCoord, ivec2(-1,-1));
  sum += textureProjOffset(ShadowMap, shadowCoord, ivec2(-1,1));
  sum += textureProjOffset(ShadowMap, shadowCoord, ivec2(1,1));
  sum += textureProjOffset(ShadowMap, shadowCoord, ivec2(1,-1));
    
  ambient    = MaterialAmbient  * LightAmbient;
  // If the fragment is under shadow, use ambient light
  FinalColor = vec4(diff_Spec * sum * 0.25+ ambient, 1.0);
    
  // Correct the Gamma configuration
  FinalColor = pow( FinalColor, vec4(1.0/2.2) );
 }</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec324"/>How it works...</h2></div></div></div><p>In the percentile close <a id="id912" class="indexterm"/>filtering technique for each incoming fragment, a set of samples are obtained from the filtering region. Each of these samples are <a id="id913" class="indexterm"/>projected to a shadow map with the reference depth to obtain binary depth results from the underlying lookup function. The shadow map texture contains the closest fragments from the light source. These depth comparisons are combined to compute the percentage of texels in the filtered region that are closer to the reference path. This percentage is used to attenuate the light.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec325"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Using variance shadow mapping</em></span></li></ul></div></div></div>
<div class="section" title="Using variance shadow mapping"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec101"/>Using variance shadow mapping</h1></div></div></div><p>In the previous recipe, we understood the implementation of PCF. It produces good quality soft shadows. The <a id="id914" class="indexterm"/>problem with PCF is that it requires more samples to produce better quality results. In addition, like standard textures, it's impossible to use prefiltered mipmapping to boost the process. Therefore, we must sample multiple texels to average out the resultant to compute the light attenuation on the current texel. The overall process to render the shadow can be slow.</p><p>Such drawbacks of PCF can be overcome by using variance shadow mapping. This technique relies on Chebyshev Probabilist Prediction, which makes use of mean and variation. The mean can be simply get from the shadow map texture and the (<span class="strong"><strong>σ<sup>2</sup></strong></span>) variance can be calculated from the average value (<span class="strong"><strong>E(x)</strong></span>) and the average square value (<span class="strong"><strong>E(x<sup>2</sup>)</strong></span>):</p><div class="mediaobject"><img src="graphics/5527OT_12_07.jpg" alt="Using variance shadow mapping"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec326"/>Getting ready</h2></div></div></div><p>To implement this recipe, we will reuse our first recipe on shadow mapping. The following guidelines will help you to understand the overall concept of variance shadow mapping:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the color buffer. In contrast to generic shadow mapping, this recipe uses the color buffer instead of the depth buffer. Therefore, the FBO now contains the <a id="id915" class="indexterm"/>color buffer instead of the depth buffer.</li><li class="listitem">This is the pass one phase, where the depth of the scene will be recorded in the color buffer, which will store the <span class="strong"><strong>E(x)</strong></span> and <span class="strong"><strong>E(x<sup>2</sup>)</strong></span> values.</li><li class="listitem">Compute variance and quantity. Use the preceding equation and calculate the variance and quantity for pass two.</li></ol></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec327"/>How to do it...</h2></div></div></div><p>In this recipe, we will create a new shader to record the depth information. Here are the steps to implement the shadow mapping source:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the <code class="literal">CustomScene</code> class, define a new <code class="literal">Texture</code> variable called <code class="literal">colorTexture</code> for the color buffer. In the constructor, create a color buffer with a linear filtering of 16-bit floating precision. The format type must be in the RGB format:<div class="informalexample"><pre class="programlisting">// Inside CustomScene::CustomScene
fbo = new FrameBufferObjectSurface();
fbo-&gt;GenerateFBO();

// Generate the depth texture with linear filtering
colorTexture.generateTexture2D( GL_TEXTURE_2D, 
   fbo-&gt;GetWidth(), fbo-&gt;GetHeight(),
   GL_RGB16F, GL_FLOAT, GL_RGB, 
   0, true, 0, 0,GL_CLAMP_TO_EDGE, GL_CLAMP_TO_EDGE,
   GL_LINEAR, GL_LINEAR );

// Attached the Depth Buffer to FBO's depth attachment
fbo-&gt;AttachTexture(colorTexture, GL_COLOR_ATTACHMENT0);</pre></div></li><li class="listitem">Create a new vertex shader called <code class="literal">VSMDepthVertex.glsl</code> and share the computed vertex positions with the fragment shader:<div class="informalexample"><pre class="programlisting">#version 300 es
layout(location = 0) in vec4  VertexPosition;
uniform mat4    ModelViewProjectionMatrix;
out vec4    position;

void main(){
    gl_Position = ModelViewProjectionMatrix * VertexPosition;
    position = gl_Position;
}</pre></div></li><li class="listitem">Similarly, create a <a id="id916" class="indexterm"/>fragment shader called <code class="literal">VSMDepthFragment.glsl</code> and store the depth square information in the first two coordinates of the output fragment:<div class="informalexample"><pre class="programlisting">#version 300 es
precision mediump float;
in vec4    position;
layout(location = 0) out vec4 FinalColor;

void main() {
    float depth = position.z / position.w ;
    //Homogenous to texture coordinate system ([-1,1]) to [0,1] 
    depth = depth * 0.5 + 0.5;
    
    float M1 = depth;           // Moment 1
    float M2 = depth * depth;   // Moment 2
    
    float dx = dFdx(depth);
    float dy = dFdy(depth);
    moment2 += 0.25*(dx*dx+dy*dy) ;
    
    
    FinalColor = vec4( moment1,moment2, 0.0, 0.0 );
}</pre></div></li><li class="listitem">Execute pass one and render the scene to FBO. This will use the preceding shaders and the depth value from the color buffer.</li><li class="listitem">Modify the existing <code class="literal">PhongFragment.glsl</code> as follows. This time, instead of <code class="literal">sampler2DShadow</code>, we <a id="id917" class="indexterm"/>will use sample 2D as we will use the color buffer to store the depth information:<div class="informalexample"><pre class="programlisting">  // Many line below skipped

in vec4    shadowCoord;

uniform sampler2D ShadowMap;
layout(location = 0) out vec4 FinalColor;


vec3 PhongShading(){ . . . }
vec4 homogenShadowCoords;

float chebyshevComputeQuantity( float distance){
    // Get the two moments M1 and M2 in moments.x 
    // and moment.y respectively
    vec2 moments = texture(ShadowMap,
                    homogenShadowCoords.xy).rg;
    
    // Current fragment is ahead of the object surface,
    // therefore must be lighted
    if (distance &lt;= moments.x)
        return 1.0 ;
    
    float E_x2 = moments.y;
    float Ex_2 = moments.x * moments.x;

    // Computer the variance
    float variance = E_x2 - (Ex_2);
    
    float t = distance - moments.x;
    float pMax = variance / (variance + t*t);
    
    return pMax;
}

void main() {
    vec3 diff_Spec = PhongShading();
    
    // Calculate the homogenous coordinates
    homogenShadowCoords = shadowCoord/shadowCoord.w;
    
    // Calculate the quantity
    float shadow = chebyshevComputeQuantity(
                          homogenShadowCoords.z);

    ambient    = MaterialAmbient  * LightAmbient;

    // If the fragment is in shadow, use ambient light only.
    FinalColor = vec4(diff_Spec * shadow + ambient, 1.0);
    
    // Correct the Gamma configuration
    FinalColor = pow( FinalColor, vec4(1.0 / 2.2) );
    return;
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec328"/>How it works...</h2></div></div></div><p>The variance shadow mapping overcomes the limitation of PCF by providing the depth data in a form where it can be filtered linearly and can be used with algorithms and modern graphics hardware that support linear data. Like our first recipe, the overall algorithm is the same apart from the fact that now, we will use two component depth and its square to store it in the <a id="id918" class="indexterm"/>16-bit precession color buffer. During the first pass, this color buffer stores the M1 and M2 moments sampled in the depth distribution of the filtered region. This computation takes place in the <code class="literal">VSMDepthFragment.glsl</code> fragment shader.</p><p>In the second pass, the color buffer is shared with the <code class="literal">phongFragment.glsl</code> fragment shader as a sample 2D uniform. Incoming shadow coordinates are converted to the homogenous form before performing any texture lookup. The z component of this transform coordinate gives the depth from the fragment from light's perspective. This depth value is used in the <code class="literal">chebyshevComputeQuantity</code> function to look up the texture. Lookup values are used to find the variance as per the previously mentioned equations, that is, equation 3. Finally, equation 5 is used to find the quantity that is exactly the same quantity we wish to compute in order to perform percentile close filtering. The returned quantity or weight value from this function is used to produce shadows as per the shadow mapping.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec329"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Creating shadows with shadow mapping</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Softening the shadow edges using PCF</em></span></li></ul></div></div></div>
<div class="section" title="Simulating the particle system"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec102"/>Simulating the particle system</h1></div></div></div><p>In computer <a id="id919" class="indexterm"/>graphics, the simulating particle system is a simulation of the natural phenomena, such as dust, smoke, rain, fireworks, and so on. This particle system contains large number of tiny particles, which can vary from few hundreds to millions in numbers. Each of the unit particles possess the same characteristics, such as velocity, color, lifespan, and so on. These particles are updated once every frame. During the update, the respective characteristics of particles are computed and updated. As a result, it makes them move or appear to change its color.</p><p>In this recipe, we will implement the particle systems. Each particle is made up of a quad and textured with translucent texture. Each particle possesses a specific color that changes with the update of time. Let's take an overview of this recipe to understand the implementation of the simulation of the particle system:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Define particle attributes</strong></span>: This creates the data structure, which contains the important attributes <a id="id920" class="indexterm"/>of the vertex that includes particle position, color, and so on.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Particle geometry</strong></span>: This defines the geometry of a single particle. It's represented by four vertices in the shape of a perfect square and contains respective texture coordinates. This particle object is used in conjunction with the view-projection matrix to produce several instances of particle in the 3D space.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Initialization</strong></span>: This allocates the space for each particle's respective attribute and loads the texture. Compile the vertex and fragment shader.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Update</strong></span>: This updates particles on each frame, calculates the new position of the particles and the remaining life of each, spawns new particle on each frame as the older particles dies</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Render</strong></span>: This renders the updated particles:<div class="mediaobject"><img src="graphics/5527OT_12_08.jpg" alt="Simulating the particle system"/></div></li></ul></div><div class="section" title="Getting ready..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec330"/>Getting ready...</h2></div></div></div><p>This recipe uses the following data structures to manage particle properties and geometries:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">The <code class="literal">Particle</code> data structure:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">pos</code>: This represents the current particle position.</li><li class="listitem" style="list-style-type: disc"><code class="literal">vel</code>: This contains the current velocity of the particle.</li><li class="listitem" style="list-style-type: disc"><code class="literal">life</code>: This represents the remaining life of the particle.</li><li class="listitem" style="list-style-type: disc"><code class="literal">transform</code>: This contains the transformation information.</li></ul></div></li><li class="listitem" style="list-style-type: disc">The <code class="literal">Vertex</code> data structure:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">pos</code>: This contains the vertex position in the 3D space.</li><li class="listitem" style="list-style-type: disc"><code class="literal">texCoord</code>: This <a id="id921" class="indexterm"/>is the texture coordinate that corresponds to <code class="literal">pos</code>.</li></ul></div></li><li class="listitem" style="list-style-type: disc">The <code class="literal">MeshParticle</code> data structure:<div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><code class="literal">vertices</code>: This contains the list of vertex objects.</li><li class="listitem" style="list-style-type: disc"><code class="literal">vertexCount</code>: This represents the number of vertices in the list.</li></ul></div></li></ul></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec331"/>How to do it...</h2></div></div></div><p>Here are the steps to implement the particle system:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a class called <code class="literal">ParticleSystem</code> derived from the <code class="literal">Model</code> class. In the constructor, load the texture image that needs to be textured on the particle quad surface. All the particles will share the same texture image:<div class="informalexample"><pre class="programlisting">    image = new PngImage();
    image-&gt;loadImage(fname);</pre></div></li><li class="listitem">Create the <code class="literal">ParticleVertex.glsl</code> vertex shader. This shader is responsible for updating vertice positions with the transformation information and sharing the remaining lifetime and texture coordinate information with the fragment shader:<div class="informalexample"><pre class="programlisting">// ParticleVertex.glsl
#version 300 es

// Vertex information
layout(location = 0) in vec3  position;
layout(location = 1) in vec2  texcoord;

uniform mat4 worldMatrix;
uniform mat4 viewProjectionMatrix;
uniform float lifeFactor;

out vec2 texCoord;
out float life;

void main( void ) {
    texCoord         = texcoord;
    life             = lifeFactor;
    gl_Position      = viewProjectionMatrix*vec4(position, 1.0 );
}</pre></div></li><li class="listitem">Create the <code class="literal">ParticleFragment.glsl</code> fragment shader. This shader renders the textured quad. In <a id="id922" class="indexterm"/>addition, it uses a lifetime to control the opacity of the particle. The particle diminishes as it reaches its end:<div class="informalexample"><pre class="programlisting">// ParticleFragment.glsl

#version 300 es
precision mediump float;

uniform sampler2D Tex1;
in vec2 texCoord;
in float life;

layout(location = 0) out vec4 outColor;

void main() {
    // directional light
    vec3 lightDir = normalize( vec3( 1.0, 1.0, 1.0 ) );
    // diffuse
    vec4 diffuseColor = vec4( 1, 1.0 - life, 0, 1 );
    vec4 texColor = texture( Tex1, texCoord );
    diffuseColor *= texColor;
    
    // final color
    vec4 color = vec4( 0.0, 0.0, 0.0, 1.0 );
    color.rgb = clamp( diffuseColor.rgb, 0.0, 1.0 );
    color.a = diffuseColor.a * life;
    
    // save it out
    outColor = vec4(texColor.xyz, 1.0);
    outColor = diffuseColor;
}</pre></div></li><li class="listitem">During the initialization of the particle system, compile and link the shader program using <code class="literal">DrawShader()</code>. Also, initialize particles with <code class="literal">InitParticles()</code>:<div class="informalexample"><pre class="programlisting">void ParticleSystem::InitModel(){
    DrawShader();      // Initialize the shader
    InitParticles();   // Initialize the particles
    Model::InitModel();// Call the base class
}</pre></div></li><li class="listitem">Implement the <code class="literal">DrawShader</code> function; this function compiles and links the shader. It loads <a id="id923" class="indexterm"/>necessary uniform variables from the vertex and fragment shader program:<div class="informalexample"><pre class="programlisting">void ParticleSystem::DrawShader(){

   // Load the shader file here, many lines skipped below
   . . . . . .      

   // Use the compiled program 
   glUseProgram( program-&gt;ProgramID );
   
   // Load the uniform variable from the shader files.
   TEX = GetUniform( program, (char *) "Tex1" );
   worldUniform = GetUniform(program,(char*)"worldMatrix");
   viewProjectionUniform = GetUniform( program, 
   (char *) "viewProjectionMatrix" );
   life = GetUniform( program, (char *) "lifeFactor" );

   // Allocate the memory for Particle System. The
   // particle count are contained in the MAX_PARTICLES.
   particles = (Particle*)malloc(sizeof(Particle)*MAX_PARTICLES);
   
   // Start position of each particle (0.0, 0.0, 0.0)
   sourcePosition = glm::vec3(0.0, 0.0, 0.0);
}</pre></div></li><li class="listitem">The <code class="literal">InitParticles</code> function defines the geometry of particles. There is no need to create <code class="literal">N</code> number of geometries for <code class="literal">N</code> particles. We will create one and reuse it for all the particles. Additionally, this function also initializes all the particles. It provides <a id="id924" class="indexterm"/>random velocities to each particle, which varies from <code class="literal">-2</code> to <code class="literal">2</code> units per microsecond in the horizontal direction and <code class="literal">4</code> to <code class="literal">8</code> in the vertical direction:<div class="informalexample"><pre class="programlisting">void ParticleSystem::InitParticles(){
    // define the type of mesh to use for the particles
    particleMesh        = CreateQuadrilateral();

   // define the type of mesh to use for the particles
    particleMesh        = CreateQuadrilateral();
    
    float lowestSpeed, highestSpeed, rangeSpeed;
    lowestSpeed = highestSpeed = rangeSpeed = 1.0f;
    
    for( ii = 0; ii &lt; MAX_PARTICLES; ++ii ){
        Particle* p  = &amp;particles[ ii ];
        p-&gt;transform = mat4();
        p-&gt;pos       = sourcePosition;
        p-&gt;life      = -1.0f;
        p-&gt;transform = translate(p-&gt;transform,p-&gt;pos);
        lowestSpeed  = -2.0;
        highestSpeed = 2.0f;
        rangeSpeed   = ( highestSpeed - lowestSpeed ) + 1;
        float f      = (float)(lowestSpeed + (rangeSpeed * 
                            rand() / (RAND_MAX + 1.0f) ) );
        p-&gt;vel.x     = f;
        lowestSpeed  = 4.0;
        highestSpeed = 8.0f;
        rangeSpeed   = ( highestSpeed - lowestSpeed ) + 1;
        f            = (float)(lowestSpeed + (rangeSpeed *
                            rand() / (RAND_MAX + 1.0f) ) );
        p-&gt;vel.y     = f;
        p-&gt;vel.z     = 0;
    }
}</pre></div></li><li class="listitem">Define the geometry <a id="id925" class="indexterm"/>of the particle in the <code class="literal">CreateQuadrilateral</code> function:<div class="informalexample"><pre class="programlisting">    MeshParticle* ParticleSystem::CreateQuadrilateral( void )
{
    // Quadrilateral made of 2 triangle=&gt;[0,1,2] &amp; [0,2,3]
    //  1-------0
    //  |     / |
    //  |   /   |
    //  | /     |
    //  2-------3
    
    // Interleaved square vertices with position &amp; tex 
    const Vertex quadVertices[] ={
    // Triangle 1: Orientation [ 0, 1, 2 ]
    { {  1.0f,  1.0f,  0.0f },  { 1.0f, 1.0f } },
    { { -1.0f,  1.0f,  0.0f },  { 0.0f, 1.0f } },
    { { -1.0f, -1.0f,  0.0f },  { 0.0f, 0.0f } },
    
    // Triangle 2: Orientation [ 0, 2, 3 ]
    { {  1.0f,  1.0f,  0.0f },  { 1.0f, 1.0f } },
    { { -1.0f, -1.0f,  0.0f },  { 0.0f, 0.0f } },
    { {  1.0f, -1.0f,  0.0f },  { 1.0f, 0.0f } },
    };

    // Allocate memory for particle geometry datastructure
    const int Count   = 6;
    MeshParticle* quad = ( MeshParticle* )malloc
    ( sizeof( MeshParticle ) );
    quad-&gt;vertices     = (Vertex*)malloc(sizeof(Vertex) * Count);
    memcpy( quad-&gt;vertices, quadVertices, Count*sizeof(Vertex) );
    quad-&gt;vertexCount  = quadVertexCount;
    return quad;
}</pre></div></li><li class="listitem">In the <code class="literal">Update()</code> function, calculate the relative difference between the current and last <a id="id926" class="indexterm"/>frame. This time, the difference is used by the <code class="literal">EmitParticles</code> function to update new position of a given particles based on its velocity:<div class="informalexample"><pre class="programlisting">void ParticleSystem::Update (){
    static clock_t lastTime = clock();
    clock_t currentTime     = clock();
    float deltaTime  = (currentTime - lastTime) /
                       (float)(CLOCKS_PER_SEC);
    lastTime         = currentTime;
    
    // update attribute for the particle emission 
    EmitParticles( deltaTime );
    return;
}</pre></div></li><li class="listitem">Implement the <code class="literal">EmitParticles()</code> function, as given in the following code. This function is responsible for updating particles. This function iterates each and every particle and updates its position and reduces the life span. As the life span of a particle <a id="id927" class="indexterm"/>becomes zero or less, it's considered to be dead. In the event of particles death, new particles are respawned:<div class="informalexample"><pre class="programlisting">void ParticleSystem::EmitParticles(float elapsedTime ){
    static float fRotation = 0.0f;
    if(fRotation&gt;360.0){
        fRotation = 0.0;
    }
    
    int spawn   = 0;
    
    for(unsigned ii = 0; ii &lt; MAX_PARTICLES; ++ii ){
        Particle* p = &amp;particles[ ii ];
        
        // Living particles
        if(particle-&gt;life &gt; 0.0f){
            unsigned int bIsEven = ( ( ii % 2 ) == 0 ) ? 1 : 0;
            particle-&gt;transform  = rotate( particle-&gt;transform, 
           (bIsEven) ? fRotation : -fRotation, vec3(0.0,0.0,1.0));
           vec3 vel              = p-&gt;vel/100.0f * elapsedTime;
            p-&gt;pos               = p-&gt;pos + vel;
            
            p-&gt;life           -= p-&gt;vel.y * elapsedTime;
            p-&gt;transform       = translate( p-&gt;transform, p-&gt;pos);
        }

       // Dead particles. Re-spawn more
        else{
            // Re-Spawn a max of 10 particles every frame
            if( spawn++ &gt; 10 ) { continue; }
            particle-&gt;pos       = sourcePosition;
            particle-&gt;life      = MAX_LIFE;
            particle-&gt;transform = mat4();
        }
        
        float fScaleFactor = 1.0+(particle-&gt;pos.y * 0.25f);
        p-&gt;transform = scale(p-&gt;transform, 
           vec3( fScaleFactor, fScaleFactor, fScaleFactor ));
    }
}</pre></div></li><li class="listitem">Implement the <a id="id928" class="indexterm"/><code class="literal">RenderParticles()</code>. This function first updates the particles before rendering:<div class="informalexample"><pre class="programlisting">void ParticleSystem::RenderParticles(){
    // Set the shader program
    glUseProgram( program-&gt;ProgramID );
    
    // All the particles are using the same texture, so it
    // only needs to be set once for all the particles
    glEnable(GL_BLEND);
    glBlendFunc(GL_SRC_ALPHA,GL_ONE_MINUS_SRC_ALPHA);
    glActiveTexture( GL_TEXTURE0 );
    if(image){
    glBindTexture( GL_TEXTURE_2D, image-&gt;getTextureID() );
    // Apply texture filter, below many lines are skipped...

    }

    glUniform1i( TEX, 0 );
    mat4 viewProj=*TransformObj-&gt; 
                   TransformGetModelViewProjectionMatrix();
    
    // Loop through the particles
    unsigned int ii = 0;
    for( ii = 0; ii &lt; MAX_PARTICLES; ++ii )
    {
        // Current particle
        Particle* p = &amp;particles[ ii ];
        
        // Pointer to the particle mesh
        MeshParticle* pMesh = particleMesh;
        
        // Only draw the particle if it is alive
        if( p-&gt;life &gt; 0.0f ){
           // Set the particle transform uniform
            glUniformMatrix4fv( worldUniform, 1, 
           GL_FALSE, ( const GLfloat* )&amp;p-&gt;transform );
            
           // Set view and projection matrices
            glm::mat4 mvp = viewProj * p-&gt;transform ;
            glUniformMatrix4fv( viewProjectionUniform, 
                1, GL_FALSE, ( const GLfloat* )&amp;mvp );
            
            // Send the remaining life span.
            glUniform1f( life, p-&gt;life / MAX_LIFE );

            // Enable and Set the vertex attributes:-
            // position, texture coords
            glEnableVertexAttribArray( VERTEX_POSITION );
            glEnableVertexAttribArray( TEX_COORD );
            glVertexAttribPointer( VERTEX_POSITION, 3, GL_FLOAT, 
            GL_FALSE, sizeof( Vertex ), &amp;pMesh-&gt;vertices-&gt;pos );
            glVertexAttribPointer( TEX_COORD, 2, GL_FLOAT, 
            GL_FALSE, sizeof( Vertex ), 
            &amp;pMesh-&gt;vertices-&gt;texCoord );
            
            glDrawArrays( GL_TRIANGLES, 0, pMesh-&gt;vertexCount );
        }
    }
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec332"/>How it works...</h2></div></div></div><p>The <code class="literal">ParticleSystem</code> class manages the life cycle of the particle system. During the initialization of the program, each particle is given a specific position, velocity, life time, and color. The particles in the system are stored as an array format, forming a data pool. The CPU is responsible for <a id="id929" class="indexterm"/>updating the particle information and is sent across the updated information to the GPU to render them onscreen. This is not a very efficient mechanism because the CPU is very busy in processing particles and sending them to the GPU. In the next recipe, you will learn an efficient way of how to render the particle system with the transform feedback. Here, we will also implement particles using point sprites instead of treating them as textured quadrilaterals:</p><div class="mediaobject"><img src="graphics/5527OT_12_09.jpg" alt="How it works..."/></div><p>During the initialization process, all particles are distributed with random velocities along the <code class="literal">X-Y</code> direction in the range of <code class="literal">[-2 2]</code> and <code class="literal">[4 8]</code> respectively. The next position of the particle is updated by adding the current position with the product of the delta time (difference with respect to the last time the particle was updated) and its respective velocity. Blue arrows show the random distribution of velocity vectors in the 2D space.</p><p>Each particle's life span get condensed every time it's updated and finally reaches to its extinct point where the particles are no more active or visible onscreen. The dead particles still remains in the data pool and can be reinitiated again. This way, we reuse the same memory efficiently instead of allocating a new one. In this recipe, we respawn 10 particles in a go while rendering.</p><p>The size of particles are made to scale as they rise up in the <code class="literal">Y</code> direction. This information is gathered from the <span class="emphasis"><em>y</em></span> component of the current position of the particle. We have used some adjustments in the code <a id="id930" class="indexterm"/>with some constants to control the scaling in a controlled manner mechanism. Finally, as the positions are updated and transformations are applied, particles can be sent to the GPU-side for rendering purposes.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec333"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to <span class="emphasis"><em>Applying texture with UV mapping</em></span> recipe <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span></li></ul></div></div></div>
<div class="section" title="Transform feedback particle system with sync objects and fences"><div class="titlepage"><div><div><h1 class="title"><a id="ch12lvl1sec103"/>Transform feedback particle system with sync objects and fences</h1></div></div></div><p>The previous <a id="id931" class="indexterm"/>example of the particle system demonstrated <a id="id932" class="indexterm"/>that the animation of the particles with highly CPU bounded operations. Typically, the core parameters of the vertex, such as color, position, and velocity are always computed on the CPU-side. The vertex information flows in the forward direction. In this, the data information is always sent from the CPU to the GPU and is repeated for subsequent frames. This fashion incurs delays as one has to pay for the latency it takes from the CPU to the GPU.</p><p>However, it will be wonderful if the vertices got processed on the GPU and reused in the next frame. This is where the new OpenGL ES 3.0 feature called transform feedback comes into play. It's the process to capture the output from the vertex shader and feedback again to the GPU for the next frame. This way, it avoids the CPU intervention and makes the rendering efficient by vast GPU parallel processing. Typically, in this process, a VBO buffer acts as a special buffer and is connected to the vertex shader and collects the transformed primitives' vertices in it. In addition, we can also decide whether the primitives will continue their regular route to the rasterizer.</p><p>In this recipe, we will implement the particle system using the transform feedback feature where vertex parameters, such as velocity, life time, acceleration, and so on are computed on the vertex shader. The translated parameters are stored in the GPU memory and are fed to the next frame <a id="id933" class="indexterm"/>iteration. In addition, we will make it more efficient by using point sprites instead of quads.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note67"/>Note</h3><p>This recipe also implements another new feature of OpenGL ES 3.0 called Sync Object and Fences. Fence is a mechanism by which an application informs the GPU to wait until a certain OpenGL ES specific operation is not completed. This way, the GPU can be prevented to pile up more operation into the command queues. A fence command can be inserted into the GL command stream like any other command. It needs to be associated with the sync object to be waited on. Sync objects are highly efficient as they allow you to wait on partial completion of GL commands.</p></div></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec334"/>Getting ready</h2></div></div></div><p>This section <a id="id934" class="indexterm"/>provides a high-level overview on how to implement the particle system using the transform feedback:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">There are two shaders required: <code class="literal">Update</code> and <code class="literal">Draw</code>. The former updates or processes the data for the particle emission and the latter uses the updated data to render particles.</li><li class="listitem">At the initialization process, allocate two buffer objects to hold the particle data. It includes position, size, velocity, color, and life time. These buffers will be used in a ping-pong fashion, where one output of one buffer becomes the input of other in the next cycle or frame and vice versa.</li><li class="listitem">While rendering, use one VBO as the input and the other as the output by bounding the former as <code class="literal">GL_ARRAY_BUFFER</code> and latter as <code class="literal">GL_TRANSFORM_FEEDBACK</code>.</li><li class="listitem">Prohibits the drawing of fragments by disabling <code class="literal">GL_RASTERIZER_DISCARD</code>.</li><li class="listitem">Executes the update shader with point primitives (<code class="literal">GL_POINTS</code>). Each particle is represented as a point. The vertex shader takes input from the first VBO and sends the processed data to the second VBO, which acts as a transform feedback output buffer.</li><li class="listitem">This enables <code class="literal">GL_RASTERIZER_DISCARD</code> for fragments draw.</li><li class="listitem">This uses the second VBO, which contains the processed data and sends it to draw shader by bounding as <code class="literal">GL_ARRAY_BUFFER</code>, render the particles.</li><li class="listitem">Finally, once the frame is rendered, swap the two VBOs.<div class="mediaobject"><img src="graphics/5527OT_12_11.jpg" alt="Getting ready"/></div></li></ol></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec335"/>How to do it...</h2></div></div></div><p>Here are the steps to implement the transform feedback recipe:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create the update vertex shader called <code class="literal">TFUpdateVert.glsl</code> with the following code. This shader defines various attributes used for particle system; each attribute is given a specific location. This shader is responsible for receiving the <a id="id935" class="indexterm"/>attributes data and update <a id="id936" class="indexterm"/>them. The updated attributes are sent to the next stage using out variables:<div class="informalexample"><pre class="programlisting">#version 300 es
#define NUM_PARTICLES           200
#define ATTRIBUTE_POSITION      0                                 
#define ATTRIBUTE_VELOCITY      1                                 
#define ATTRIBUTE_SIZE          2                                 
#define ATTRIBUTE_CURTIME       3                                 
#define ATTRIBUTE_LIFETIME      4                                 
uniform float               time;
uniform float               emissionRate;
uniform mediump sampler3D   noiseTex;
                                                                  
layout(location = ATTRIBUTE_POSITION) in vec2   inPosition;
layout(location = ATTRIBUTE_VELOCITY) in vec2   inVelocity;
layout(location = ATTRIBUTE_SIZE) in float      inSize;
layout(location = ATTRIBUTE_CURTIME) in float   inCurrentTime;
layout(location = ATTRIBUTE_LIFETIME) in float  inLifeTime;
                                                                  
out vec2    position;
out vec2    velocity;
out float   size;
out float   currentTime;
out float   lifeTime;
                                                                  
float randomValue( inout float seed ){                                                                 
   float vertexId   = float(gl_VertexID) / float(NUM_PARTICLES);
   vec3 texCoord    = vec3( time, vertexId, seed );
   seed             += 0.41;//(.10/float( NUM_PARTICLES ));
   return texture( noiseTex, texCoord ).r;
}                                                                 

void main(){                                                                 
    float seed      = time;
    float lifetime  = (inCurrentTime - time)*10.0;
    if( lifetime &lt;= 0.0 &amp;&amp; randomValue(seed) &lt; emissionRate )
    {
        position       = vec2( 0.0, -1.0 );
        velocity       = vec2( randomValue(seed) * 2.0 - 1.00,
                              randomValue(seed)  + 3.0 );
        size           = randomValue(seed) * 20.0;
        currentTime    = time;
        lifeTime       = 5.0;
    }
    else{
        position = inPosition; velocity   = inVelocity;
        size      = inSize;  currentTime  = inCurrentTime;
        lifeTime = inLifeTime;
    }
    gl_Position = vec4( position, 0.0, 1.0 );
}</pre></div></li><li class="listitem">Create the update fragment shader called <code class="literal">TFUpdateFrag.glsl</code>. This shader is only a place <a id="id937" class="indexterm"/>holder for fragment shading <a id="id938" class="indexterm"/>so that the compilation of shader can be performed. This shader never comes into picture as the rasterization is turned off during the update:<div class="informalexample"><pre class="programlisting">#version 300 es                         
precision mediump float;                
layout(location = 0) out vec4 fragColor;
void main(){                                       
  fragColor = vec4(1.0);                
}</pre></div></li><li class="listitem">Create a vertex shader called <code class="literal">TFDrawVert.glsl</code> for the render phase. This shader is responsible for rendering the updated data onscreen:<div class="informalexample"><pre class="programlisting">#version 300 es                                              
#define ATTRIBUTE_POSITION      0                             
#define ATTRIBUTE_VELOCITY      1                             
#define ATTRIBUTE_SIZE          2                             
#define ATTRIBUTE_CURTIME       3                             
#define ATTRIBUTE_LIFETIME      4                             
                                                              
layout(location = ATTRIBUTE_POSITION) in vec2   inPosition;
layout(location = ATTRIBUTE_VELOCITY) in vec2   inVelocity;
layout(location = ATTRIBUTE_SIZE) in float      inSize;
layout(location = ATTRIBUTE_CURTIME) in float   inCurrentTime;
layout(location = ATTRIBUTE_LIFETIME) in float  inLifeTime;
                                                              
uniform float   time;
uniform vec2    acceleration;
uniform mat4    ModelViewProjectMatrix;

void main(){                                                             
  float deltaTime = (time - inCurrentTime)/10.0;
  if ( deltaTime &lt;= inLifeTime ){ 
     vec2 velocity = inVelocity + deltaTime * acceleration;
     vec2 position = inPosition + deltaTime * velocity;
     gl_Position   = ModelViewProjectMatrix
                                *vec4(position, 0.0, 1.0);
     gl_PointSize  = inSize * ( 1.0 - deltaTime / inLifeTime );
  }                                                           
  else{                                                     
     gl_Position    = vec4( -1000, -1000, 0, 0 );
     gl_PointSize   = 0.0;
  }
}</pre></div></li><li class="listitem">Shade the <a id="id939" class="indexterm"/>fragment while rendering <a id="id940" class="indexterm"/>to <code class="literal">TFDrawFrag.glsl</code>:<div class="informalexample"><pre class="programlisting">#version 300 es                                  
precision mediump float;                         
layout(location = 0) out vec4 fragColor;         
uniform vec4 color;
uniform sampler2D tex;

void main(){                                                
  vec4 texColor = texture( tex, gl_PointCoord );
  fragColor     = texColor * color;
}</pre></div></li><li class="listitem">Create <code class="literal">ParticleSystem.h</code>/<code class="literal">.cpp</code> derived from the <code class="literal">Model</code> base class and implement <a id="id941" class="indexterm"/>the <code class="literal">EmitShader()</code> function. This function will compile the <code class="literal">TFUpdateVert.glsl</code> and <code class="literal">TFUpdateFrag.glsl</code> shader files:<div class="informalexample"><pre class="programlisting">   void ParticleSystem::EmitShader(){
   program = ProgramManagerObj-&gt;ProgramLoad((char*) "TFEmit",
   VERTEX_SHADER_PRG_EMIT, FRAGMENT_SHADER_PRG_EMIT);

   glUseProgram( program-&gt;ProgramID );
   emitProgramObject = program-&gt;ProgramID;
 
   const char *feedbackVaryings[5] = { "position", "velocity", 
   "size", "currentTime", "lifeTime" };
   
   // Set vertex shader outputs as transform feedback
   glTransformFeedbackVaryings ( emitProgramObject, 5,
   feedbackVaryings, GL_INTERLEAVED_ATTRIBS );
   
   // Link program after calling glTransformFeedbackVaryings
   glLinkProgram ( program );

   emitTimeLoc = GetUniform(program,"time");
   emitEmissionRateLoc = GetUniform( program, "emissionRate" );
   emitNoiseSamplerLoc = GetUniform(program, "noiseTex" );
}</pre></div><p>After the <a id="id942" class="indexterm"/>shader is compiled, specify the attribute that you want to capture in the transform feedback using the <code class="literal">glTransformFeedbackVaryings</code> API:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Syntax</strong></span>:<div class="informalexample"><pre class="programlisting">void glTransformFeedbackVaryings(GLuint program, GLsizei count, const char ** varyings, GLenum bufferMode);</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">program</code></p>
</td><td style="text-align: left" valign="top">
<p>This is the handle of the program object.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">count</code></p>
</td><td style="text-align: left" valign="top">
<p>This specifies the number of the vertex output variable used in the transform feedback process.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">varying</code></p>
</td><td style="text-align: left" valign="top">
<p>This is an array of count zero-terminated strings that specifies the names of varying variables to be used for the transform feedback.</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">bufferMode</code></p>
</td><td style="text-align: left" valign="top">
<p>This specifies the mode under which vertex the output variable data is captured when the transform feedback is active. This variable can accept two enum: <code class="literal">GL_INTERLEAVED_ATTRIBS</code> or <code class="literal">GL_SEPARATE_ATTRIBS</code>. The former specifies how to capture the output variables in a single buffer. However, the latter captures each vertex variable output in its own buffer.</p>
</td></tr></tbody></table></div></li></ul></div><p>We are interested in capturing five vertex output variables: <code class="literal">position</code>, <code class="literal">velocity</code>, <code class="literal">size</code>, <code class="literal">currentTime</code>, and <code class="literal">lifeTime</code> in the transform feedback.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note68"/>Note</h3><p>The <code class="literal">glTransformFeedbackVarying</code> is always called before linking the program. Therefore, it's necessary to link the program object using <code class="literal">glLinkProgram</code>.</p></div></div></li><li class="listitem">In the same file, implement <a id="id943" class="indexterm"/>the <code class="literal">DrawShader()</code> function. This function will compile <code class="literal">TFDrawVert.glsl</code> and <code class="literal">TFDrawFrag.glsl</code>:<div class="informalexample"><pre class="programlisting">void ParticleSystem::DrawShader(){
    program = ProgramManagerObj-&gt;ProgramLoad((char*)"TFDraw", 
         VERTEX_SHADER_PRG_DRAW, FRAGMENT_SHADER_PRG_DRAW); 
    glUseProgram( program-&gt;ProgramID );
    
    MVP = GetUniform( program,(char*)"ModelViewProjectMatrix");

    // Load the shaders and get a linked program object
    drawProgramObject = program-&gt;ProgramID;
    
    // Get the uniform locations
    drawTimeLoc   = GetUniform(drawProgramObject,"time");
    drawColorLoc  = GetUniform(drawProgramObject,"color");
    drawAccelerationLoc = GetUniform(program, "acceleration");
    samplerLoc  = GetUniform (program, "tex");
}</pre></div></li><li class="listitem">Initialize the particle system in <code class="literal">ParticleSystem::InitParticles()</code>. This function initializes the array of particle object containing various particle properties. After <a id="id944" class="indexterm"/>initialization, these objects are stored in two different VBO buffer objects particle VBOs. These <a id="id945" class="indexterm"/>buffers are used by the transform feedback to update elements in VBOs in a ping-pong fashion, as mentioned in the preceding code:<div class="informalexample"><pre class="programlisting">   void ParticleSystem::InitParticles(){
   
   time        = 0.0f; 
   curSrcIndex  = 0; 
   textureId    = image-&gt;getTextureID();
   
   if(textureId &lt;= 0){ return; }
   
   // Create a 3D noise texture for random values
   noiseTextureId = Create3DNoiseTexture ( 128, 50.0 );
   Particle particleData[ NUM_PARTICLES ];

   // Initialize particle data
   for ( int i = 0; i &lt; NUM_PARTICLES; i++ ){
      Particle *particle     = &amp;particleData[i];
      particle-&gt;position[0]  = 0.0f; 
      particle-&gt;position[1]  = 0.0f;
      particle-&gt;velocity[0]  = 0.0f; 
      particle-&gt;velocity[1]  = 0.0f;
      particle-&gt;size         = 0.0f;  
      particle-&gt;curtime      = 0.0f;
        particle-&gt;lifetime       = 0.0f;
   }
   
   // Create the particle VBOs
   glGenBuffers ( 2, &amp;particleVBOs[0] );
   
   for ( int i = 0; i &lt; 2; i++ ) {
   glBindBuffer ( GL_ARRAY_BUFFER, particleVBOs[i] );
   glBufferData ( GL_ARRAY_BUFFER, sizeof ( Particle ) * 
   NUM_PARTICLES, particleData, GL_DYNAMIC_COPY );
   }
}</pre></div></li><li class="listitem">In <code class="literal">InitModel</code>, initialize <a id="id946" class="indexterm"/>the system as follows:<div class="informalexample"><pre class="programlisting">void ParticleSystem::InitModel(){
    UpdateShader();
    DrawShader();
    InitParticles();
    Model::InitModel();
    return;
}</pre></div></li><li class="listitem">Use the time and <a id="id947" class="indexterm"/>update the particle system in the <code class="literal">Emitparticles()</code> function:<div class="informalexample"><pre class="programlisting">    void ParticleSystem::Update (){
    static clock_t lastTime = clock();
    clock_t currentTime     = clock();
    float deltaTime         = (currentTime - lastTime)/
                               CLOCKS_PER_SEC*0.10;
    lastTime                = currentTime;
    time                    += deltaTime;
    
    EmitParticles ( deltaTime );
}</pre></div></li><li class="listitem">The <code class="literal">Emitparticles()</code> function flips the two VBO buffers each time the frame is rendered. This way, one VBO becomes the input (called source VBO) to the update shader. However, the other captures the processed output variables (called the destination VBO) and vice versa. Use the updated shader program and send the source VBO data and set up the destination VBO as the transform feedback buffer to capture results with the <code class="literal">glBindBuffer</code> API using <code class="literal">GL_TRANSFORM_FEEDBACK</code> and <code class="literal">glBindBufferBase</code> to bound to and index in the destination VBO.<p>During the update <a id="id948" class="indexterm"/>phase, we are only interested in computing the particle data. Therefore, we can disable the rasterization process:</p><div class="informalexample"><pre class="programlisting">    void ParticleSystem::EmitParticles(float deltaTime ){
    //UserData *userData = esContext-&gt;userData;
    GLuint srcVBO = particleVBOs[ curSrcIndex ];
    GLuint dstVBO = particleVBOs[ ( curSrcIndex + 1 ) % 2 ];
    
    glUseProgram ( emitProgramObject );
    
    // transform feedback buffer
    SetupVertexAttributes ( srcVBO );
    
    // Set transform feedback buffer
    glBindBuffer(GL_TRANSFORM_FEEDBACK_BUFFER, dstVBO);
    glBindBufferBase (GL_TRANSFORM_FEEDBACK_BUFFER, 0, dstVBO);
    
    // Turn off rasterization - we are not drawing
    glEnable(GL_RASTERIZER_DISCARD);
    
    // Set uniforms
    glUniform1f(emitTimeLoc, time);
    glUniform1f(emitEmissionRateLoc, EMISSION_RATE);
    
    // Bind the 3D noise texture
    glActiveTexture(GL_TEXTURE0);
    glBindTexture(GL_TEXTURE_3D, noiseTextureId);
    glUniform1i(emitNoiseSamplerLoc, 0);
    
    // Emit particles using transform feedback
    glBeginTransformFeedback(GL_POINTS);
    glDrawArrays(GL_POINTS, 0, NUM_PARTICLES);
    glEndTransformFeedback();
    
    // Ensure transform feedback results are completed
    // before the draw that uses them.
    emitSync = glFenceSync(GL_SYNC_GPU_COMMANDS_COMPLETE, 0);
    
    //Allows fragment drawing
    glDisable ( GL_RASTERIZER_DISCARD ); 
    glUseProgram ( 0 );
    glBindBufferBase ( GL_TRANSFORM_FEEDBACK_BUFFER, 0, 0 );
    glBindBuffer ( GL_ARRAY_BUFFER, 0 );
    glBindTexture ( GL_TEXTURE_3D, 0 );
    
    // Ping pong the buffers
    curSrcIndex = ( curSrcIndex + 1 ) % 2;
}</pre></div><p>The transform feedback can begin and end with the help of the following APIs syntax:</p><div class="informalexample"><pre class="programlisting">void glBeginTransformFeedback(GLenum primitiveMode);
void glEndTransformFeedback();</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">primitiveMode</code></p>
</td><td style="text-align: left" valign="top">
<p>This specifies the type of the primitive that needs to be captured in the transform feedback attached buffer. The acceptable parameters are <code class="literal">GL_POINT</code>, <code class="literal">GL_LINES</code>, and <code class="literal">GL_TRIANGLES</code>.</p>
</td></tr></tbody></table></div><p>It's very important to ensure that vertex output variables are written in the transform feedback attached buffers, so that the drawing command can use it safely. This requirement to ensure consistency between the <a id="id949" class="indexterm"/>update and drawing operation can be achieved by creating fences. A fence is created just after the transform feedback operation is activated. This fence is associated with a sync object, which waits in the rendering routine until the <a id="id950" class="indexterm"/>transform feedback operation is not completed.</p></li><li class="listitem">The <code class="literal">RenderParticles()</code> function performs the drawing job. It waits for the sync object to ensure the successful completion of the transform feedback operation. Once done, the sync object is deleted and the drawing API are called to render the scene with the particle system:<div class="informalexample"><pre class="programlisting">void ParticleSystem::RenderParticles(){
    // Make sure that the GL server blocked until
    // transform feedback output is not captured.
    glWaitSync ( emitSync, 0, GL_TIMEOUT_IGNORED );
    glDeleteSync ( emitSync );
    glUseProgram(drawProgramObject);
    
    // Load the VBO and vertex attributes
    SetupVertexAttributes ( particleVBOs[ curSrcIndex ] );
    glUniformMatrix4fv( MVP, 1, GL_FALSE,(float*) 
      TransformObj-&gt;TransformGetModelViewProjectionMatrix());
    
    glUniform1f ( drawTimeLoc, time );
    glUniform4f ( drawColorLoc, 1.0f, 1.0f, 1.0f, 1.0f );
    glUniform2f ( drawAccelerationLoc, 0.0f, ACCELERATION );
    
    glEnable ( GL_BLEND );
    glBlendFunc ( GL_SRC_ALPHA, GL_ONE );
    
    // Bind the texture 
    glActiveTexture ( GL_TEXTURE0 );  
    glBindTexture ( GL_TEXTURE_2D, textureId );
    
    // Set the sampler texture unit to 0
    glUniform1i ( samplerLoc, 0 );
    glDrawArrays ( GL_POINTS, 0, NUM_PARTICLES );
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec336"/>How it works...</h2></div></div></div><p>The transform feedback is a <a id="id951" class="indexterm"/>special stage in the OpenGL ES programmable pipeline. It exists right after the vertex shader, as shown in the following image. When the transform feedback is activated, it diverts the output from the vertex <a id="id952" class="indexterm"/>shader to the transform feedback. The transform feedback is registered with all the vertex output variables in which it needs to be captured. Data variables are captured in the special ping-pong VBO buffers:</p><div class="mediaobject"><img src="graphics/5527OT_12_10.jpg" alt="How it works..."/></div><p>During the initialization process, two vertex buffer objects are created and set with the necessary particle data in it. These VBOs are attached to the transformed feedback and swapped with each frame. In this manner, one VBO contains the input data and captures processed variables and vice versa.</p><p>Each time the transform feedback is executed, a corresponding fence is created to acknowledge the completion of the transform feedback. This fence is associated with a sync object, which waits in the rendering function for the fence. When the fence is signaled, the wait is finished and the rendering commands are executed to render the particle system.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note69"/>Note</h3><p>Particles are represented with <code class="literal">GL_POINTS</code>, where each point represents a tiny square. This command tells the GPU to draw each vertex as a square. The size of the point can be adjusted using <code class="literal">gl_PointSize</code>. Compared to the previous recipe, the sprite approach reduces the number of vertices required to represent a quad from four to one. A <a id="id953" class="indexterm"/>point sprite is a GPU built-in feature, where each point (representing a square) faces the camera. These can be textured with images without supplying texture coordinates explicitly, making it highly efficient for particle rendering.</p></div></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch12lvl2sec337"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to the <span class="emphasis"><em>Procedural texture shading with texture coordinates</em></span> recipe in <a class="link" href="ch06.html" title="Chapter 6. Working with Shaders">Chapter 6</a>, <span class="emphasis"><em>Working with Shaders</em></span></li><li class="listitem" style="list-style-type: disc">Refer to the<span class="emphasis"><em> Introduction</em></span> section in <a class="link" href="ch03.html" title="Chapter 3. New Features of OpenGL ES 3.0">Chapter 3</a>, <span class="emphasis"><em>New features of OpenGL ES 3.0</em></span></li><li class="listitem" style="list-style-type: disc">Refer to the<span class="emphasis"><em> Swizzling</em></span> recipe<span class="emphasis"><em> in </em></span><a class="link" href="apa.html" title="Appendix A. Supplementary Information on OpenGL ES 3.0">Appendix</a>, <span class="emphasis"><em>Supplementary Information on OpenGL ES 3.0</em></span></li></ul></div></div></div></body></html>