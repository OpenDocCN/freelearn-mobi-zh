<html><head></head><body>
		<div id="_idContainer091">
			<h1 id="_idParaDest-219" class="chapter-number"><a id="_idTextAnchor369"/><st c="0">12</st></h1>
			<h1 id="_idParaDest-220"><a id="_idTextAnchor370"/><st c="3">Being Smart with Apple Intelligence and ML</st></h1>
			<p><st c="46">The launch of ChatGPT in November 2022 wasn’t the first appearance of an </st><strong class="bold"><st c="120">Artificial Intelligence</st></strong><st c="143"> (</st><strong class="bold"><st c="145">AI</st></strong><st c="147">) tool, but it was the one that put the AI in </st><span class="No-Break"><st c="194">the spotlight.</st></span></p>
			<p><st c="208">Some may argue that Apple entered the AI world later than others. </st><st c="275">Perhaps, but what’s certain is that iOS has machine-learning capabilities for both users </st><span class="No-Break"><st c="364">and developers.</st></span></p>
			<p><st c="379">Machine learning opens up new capabilities in almost every area we can think of – from search, statistics, and insights to understanding images and sounds. </st><st c="536">There are even apps that are based on AI and machine </st><span class="No-Break"><st c="589">learning capabilities.</st></span></p>
			<p><st c="611">Currently, most of these capabilities are server-based. </st><st c="668">Still, the ongoing improvements in mobile phones’ </st><strong class="bold"><st c="718">System On Chip</st></strong><st c="732"> (</st><strong class="bold"><st c="734">SoC</st></strong><st c="737">) performance allow them to perform predictions on-device, which opens up </st><span class="No-Break"><st c="812">new opportunities.</st></span></p>
			<p><st c="830">In this chapter, we will do </st><span class="No-Break"><st c="859">the following:</st></span></p>
			<ul>
				<li><st c="873">Cover the basics of AI and machine learning, learn the different terms, how machine learning works, and what it means to train </st><span class="No-Break"><st c="1001">a model</st></span></li>
				<li><st c="1008">Explore built-in machine learning frameworks such as </st><strong class="bold"><st c="1062">Natural Language Processing</st></strong><st c="1089"> (</st><strong class="bold"><st c="1091">NLP</st></strong><st c="1094">), vision, and </st><span class="No-Break"><st c="1110">sound analysis</st></span></li>
				<li><st c="1124">Add a semantic search to our Core </st><span class="No-Break"><st c="1159">Spotlight implementation</st></span></li>
				<li><st c="1183">Build and integrate a custom machine learning model using the Create </st><strong class="bold"><st c="1253">Machine Learning</st></strong><st c="1269"> (</st><strong class="bold"><st c="1271">ML</st></strong><st c="1273">) application and the Core </st><span class="No-Break"><st c="1301">ML framework</st></span></li>
			</ul>
			<p><st c="1313">Machine learning is a vast topic, and we’ve got much to cover, so let’s jump right in to understand </st><span class="No-Break"><st c="1414">the basics.</st></span></p>
			<h1 id="_idParaDest-221"><a id="_idTextAnchor371"/><st c="1425">Technical requirements</st></h1>
			<p><st c="1448">You must download Xcode version 16.0 or above for this chapter from Apple’s </st><span class="No-Break"><st c="1525">App Store.</st></span></p>
			<p><st c="1535">You’ll also need to run the latest version of macOS (Ventura or above). </st><st c="1608">Search for </st><strong class="source-inline"><st c="1619">Xcode</st></strong><st c="1624"> in the App Store and select and download the latest version. </st><st c="1686">Launch Xcode and follow any additional installation instructions that your system may prompt you with. </st><st c="1789">Once Xcode has fully launched, you’re ready </st><span class="No-Break"><st c="1833">to go.</st></span></p>
			<p><st c="1839">This chapter includes many code examples, some of which can be found in the following GitHub </st><span class="No-Break"><st c="1933">repository: </st></span><a href="https://github.com/PacktPublishing/Mastering-iOS-18-Development/tree/main/Chapter12"><span class="No-Break"><st c="1945">https://github.com/PacktPublishing/Mastering-iOS-18-Development/tree/main/Chapter12</st></span></a><span class="No-Break"><st c="2028">.</st></span></p>
			<p><st c="2029">Note that some examples in this chapter need to be run on a device, not </st><span class="No-Break"><st c="2102">the simulator.</st></span></p>
			<h1 id="_idParaDest-222"><a id="_idTextAnchor372"/><st c="2116">Going over the basics of AI and machine learning</st></h1>
			<p><st c="2165">Before we dive in, let’s acknowledge the complexity – AI and machine learning are two huge topics that are impossible to cover in one chapter or even </st><span class="No-Break"><st c="2316">one book.</st></span></p>
			<p><st c="2325">However, it is recommended that we</st><a id="_idIndexMarker707"/><st c="2360"> understand the basics if we want to implement some machine-learning </st><a id="_idIndexMarker708"/><st c="2429">capabilities in </st><span class="No-Break"><st c="2445">our projects.</st></span></p>
			<p><st c="2458">So, let’s start with understanding the difference between machine learning </st><span class="No-Break"><st c="2534">and AI.</st></span></p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor373"/><st c="2541">Learning the differences between AI and machine learning</st></h2>
			<p><st c="2598">AI is considered a rising topic in computer</st><a id="_idIndexMarker709"/><st c="2642"> science, and this trend has been accelerated</st><a id="_idIndexMarker710"/><st c="2687"> since the launch </st><span class="No-Break"><st c="2705">of </st></span><span class="No-Break"><strong class="bold"><st c="2708">ChatGPT</st></strong></span><span class="No-Break"><st c="2715">.</st></span></p>
			<p><st c="2716">Even though ML is not a new technology, many</st><a id="_idIndexMarker711"/><st c="2761"> people still need clarification about the difference between ML and AI. </st><st c="2834">It’s not that they are not related –they are. </st><st c="2880">Still, as iOS professional developers, it is essential to have a clear overview of the differences now that Apple has integrated AI deeply into </st><span class="No-Break"><st c="3024">its system.</st></span></p>
			<p><st c="3035">So, what is ML? </st><st c="3052">ML technology focuses on developing algorithms and statistical models to help computers perform tasks such as prediction and classification. </st><st c="3193">For example, a model can receive an image and reply whether it contains a cat, or a model can take some text and locate the verbs and </st><span class="No-Break"><st c="3327">the nouns.</st></span></p>
			<p><st c="3337">The ML model is an algorithm that performs its predictions and classifications. </st><st c="3418">In fact, a model can use several algorithms to perform its calculations. </st><st c="3491">For example, a vision model can use neural networks to</st><a id="_idIndexMarker712"/><st c="3545"> perform its image classification and a </st><strong class="bold"><st c="3585">YOLO</st></strong><st c="3589"> (</st><strong class="bold"><st c="3591">You Only Look Once</st></strong><st c="3609">) algorithm to perform </st><a id="_idIndexMarker713"/><st c="3633">real-time object detection. </st><st c="3661">Each algorithm has its strengths and weaknesses. </st><st c="3710">For </st><a id="_idIndexMarker714"/><st c="3714">example, the decision tree algorithm is easy to interpret but is prone to overfitting, while </st><strong class="bold"><st c="3807">KNN</st></strong><st c="3810"> (</st><strong class="bold"><st c="3812">K-Nearest Neighbors</st></strong><st c="3831">) is simple and intuitive but </st><a id="_idIndexMarker715"/><span class="No-Break"><st c="3862">computationally intensive.</st></span></p>
			<p><st c="3888">Conversely, AI is an array of technologies and methods that create a system capable of performing tasks similar to what humans </st><span class="No-Break"><st c="4016">usually do.</st></span></p>
			<p><st c="4027">One great </st><a id="_idIndexMarker716"/><st c="4038">example is </st><strong class="bold"><st c="4049">LLM</st></strong><st c="4052"> (</st><strong class="bold"><st c="4054">Large Language Model</st></strong><st c="4074">) services such as </st><strong class="bold"><st c="4094">ChatGPT</st></strong><st c="4101"> or </st><strong class="bold"><st c="4105">Gemini</st></strong><st c="4111">. Another example is the autonomous car driving projects that involve many</st><a id="_idIndexMarker717"/><st c="4185"> ML models, such as object detection </st><span class="No-Break"><st c="4222">and decision-making.</st></span></p>
			<p><st c="4242">We now understand that the ML model is one building block in AI. </st><st c="4308">Next, let’s dive into the ML model to understand how </st><span class="No-Break"><st c="4361">it works.</st></span></p>
			<h2 id="_idParaDest-224"><a id="_idTextAnchor374"/><st c="4370">Delving into the ML model</st></h2>
			<p><st c="4396">The </st><strong class="bold"><st c="4401">ML model</st></strong><st c="4409"> contains data to generate a prediction, classification, or decision. </st><st c="4479">The ML models we want to store on a device are relatively small. </st><st c="4544">However, models such as GPT can be hundreds </st><span class="No-Break"><st c="4588">of gigabytes.</st></span></p>
			<p><st c="4601">But what does </st><em class="italic"><st c="4616">data</st></em><st c="4620"> mean here? </st><st c="4632">How is it </st><a id="_idIndexMarker718"/><st c="4642">structured? </st><st c="4654">The answer to that question depends on a model’s algorithms. </st><st c="4715">For example, if the model uses a linear regression algorithm, the data structure is a 2D array, where the rows represent samples and the columns represent features. </st><st c="4880">A model with a </st><strong class="bold"><st c="4895">decision tree algorithm</st></strong><st c="4918"> contains a tree, where the leaves</st><a id="_idIndexMarker719"/><st c="4952"> represent the different decisions </st><span class="No-Break"><st c="4987">or predictions.</st></span></p>
			<p><st c="5002">Going forward in this chapter, we will refer to building and creating a model’s data as training. </st><st c="5101">Let’s discuss that </st><span class="No-Break"><st c="5120">important topic.</st></span></p>
			<h2 id="_idParaDest-225"><st c="5136">Traini</st><a id="_idTextAnchor375"/><a id="_idTextAnchor376"/><st c="5143">ng the model</st></h2>
			<p><st c="5156">Two distinct ML models can have the same algorithms and structures, but the data can differ because of the training </st><a id="_idIndexMarker720"/><st c="5273">process. </st><st c="5282">Using the training process, we teach a model to make accurate predictions and decisions based on the input data. </st><st c="5395">This process involves optimizing the model’s data (parameters) to accurately perform predictions on </st><span class="No-Break"><st c="5495">unseen data.</st></span></p>
			<p><st c="5507">There are several steps we need to do to train </st><span class="No-Break"><st c="5555">a model:</st></span></p>
			<ol>
				<li><strong class="bold"><st c="5563">Data collection</st></strong><st c="5579">: We need to prepare a relatively large dataset to train our model. </st><st c="5648">We must also preprocess the data by handling missing values, cleaning unrelated data items, and </st><span class="No-Break"><st c="5744">normalizing values.</st></span></li>
				<li><strong class="bold"><st c="5763">Split the data collection</st></strong><st c="5789">: Now that we have the dataset, we must divide it into training data, validation, and test sets. </st><st c="5887">We use each of these sets in a different </st><span class="No-Break"><st c="5928">training stage.</st></span></li>
				<li><strong class="bold"><st c="5943">Pick our ML algorithm</st></strong><st c="5965">: Each algorithm aims to solve a different problem. </st><st c="6018">For example, the logistic regression algorithm solves classification problems, and the linear regression algorithm solves </st><span class="No-Break"><st c="6140">regression problems.</st></span></li>
				<li><strong class="bold"><st c="6160">Forward pass</st></strong><st c="6173">: We pass the training data through the model to </st><span class="No-Break"><st c="6223">make predictions.</st></span></li>
				<li><strong class="bold"><st c="6240">Validation</st></strong><st c="6251">: We use validation datasets to assess the model’s performance and adjust the model based on </st><span class="No-Break"><st c="6345">the results.</st></span></li>
				<li><strong class="bold"><st c="6357">Testing</st></strong><st c="6365">: We use the test data to evaluate our model’s performance in real-time use cases with unseen </st><span class="No-Break"><st c="6460">input data.</st></span></li>
			</ol>
			<p><st c="6471">That was a schematic overview of the training process. </st><st c="6527">In practice, the process contains even more steps, such as calculating loss and optimizations. </st><st c="6622">However, the goal is to give you a glimpse into training so that you can understand the following topics. </st><st c="6728">And don’t worry – we will build and train an ML model </st><span class="No-Break"><st c="6782">together soon!</st></span></p>
			<p><st c="6796">Now that we know what an ML model is, let’s try to understand how it relates </st><span class="No-Break"><st c="6874">to iOS.</st></span></p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor377"/><st c="6881">Apple intelligence and ML</st></h1>
			<p><st c="6907">When ChatGPT gained popularity, many felt Apple had fallen behind in AI and ML. </st><st c="6988">This book is not the place to discuss that</st><a id="_idIndexMarker721"/><st c="7030"> question; suffice it to say that ML has since been</st><a id="_idIndexMarker722"/><st c="7081"> an integral part of iOS for years. </st><st c="7117">iOS uses ML to optimize our photos according to their content. </st><st c="7180">Keyboard predictions involve ML models, and even the way iOS preserves a battery is based </st><span class="No-Break"><st c="7270">on ML.</st></span></p>
			<p><st c="7276">All these features and capabilities are transparent to users and performed under the hood. </st><st c="7368">However, iOS 18 brought AI into the spotlight with many features, such as an improved Siri, an image playground, and </st><span class="No-Break"><st c="7485">writing tools.</st></span></p>
			<p><st c="7499">iOS 18 also provided some neat capabilities for us developers, but it especially brought the areas of ML and AI to our attention. </st><st c="7630">For example, semantic search is one of the new capabilities available to</st><a id="_idIndexMarker723"/><st c="7702"> developers using iOS </st><span class="No-Break"><st c="7724">ML features.</st></span></p>
			<p><st c="7736">Before we dive into </st><a id="_idIndexMarker724"/><st c="7757">Core ML and learn how to train our models, let’s start with the models that come with the iOS SDK, as there is a good chance that that is where we’ll find what we need quickly without training a </st><span class="No-Break"><st c="7952">new model.</st></span></p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor378"/><st c="7962">Exploring built-in ML frameworks</st></h1>
			<p><st c="7995">When we reviewed the</st><a id="_idIndexMarker725"/><st c="8016"> basics of AI and ML, we saw what it means to train a model – it’s a long and complex process. </st><st c="8111">This process requires us to prepare relatively big datasets, including the validation and test datasets. </st><st c="8216">Even after that, we have an ML model we need to fine-tune and include in our app while trying to reduce </st><span class="No-Break"><st c="8320">its size.</st></span></p>
			<p><st c="8329">However, don’t get me wrong. </st><st c="8359">There are cases where training our ML model is essential, but before we start the training process, it’s important to be familiar with what </st><span class="No-Break"><st c="8499">iOS offers.</st></span></p>
			<p><st c="8510">Working with ML frameworks in iOS isn’t new. </st><st c="8556">These frameworks were introduced years ago, some even in iOS 10 (2016). </st><st c="8628">However, few developers use them, perhaps because they believe they are complex </st><span class="No-Break"><st c="8708">to integrate.</st></span></p>
			<p><st c="8721">We’ll start with one of the most practical frameworks in the</st><a id="_idTextAnchor379"/><a id="_idTextAnchor380"/><st c="8782"> ML toolset – </st><span class="No-Break"><st c="8796">NLP.</st></span></p>
			<h2 id="_idParaDest-228"><a id="_idTextAnchor381"/><st c="8800">Interpreting text using NLP</st></h2>
			<p><st c="8828">Interpreting and understanding texts can </st><a id="_idIndexMarker726"/><st c="8870">provide significant value to many </st><a id="_idIndexMarker727"/><st c="8904">apps. </st><st c="8910">For example, NLP can help us understand strings such as search phrases, text inputs, or extracting information from an </st><span class="No-Break"><st c="9029">imported text.</st></span></p>
			<p><st c="9043">The iOS SDK has a built-in NLP framework </st><span class="No-Break"><st c="9085">called </st></span><span class="No-Break"><strong class="source-inline"><st c="9092">NaturalLanguage</st></strong></span><span class="No-Break"><st c="9107">:</st></span></p>
			<pre class="source-code"><st c="9109">
import NaturalLanguage</st></pre>			<p><st c="9132">The </st><strong class="source-inline"><st c="9137">NaturalLanguage</st></strong><st c="9152"> framework helps us interpret text efficiently on </st><span class="No-Break"><st c="9202">a device.</st></span></p>
			<p><st c="9211">We must first know how NLP works under the hood and its basic terms to understand how </st><span class="No-Break"><st c="9298">it works.</st></span></p>
			<p><st c="9307">The NLP model works </st><a id="_idIndexMarker728"/><st c="9328">by finding relationships</st><a id="_idIndexMarker729"/><st c="9352"> between different parts of texts. </st><st c="9387">Even though this task is complex, it’s interesting to see how </st><span class="No-Break"><st c="9449">it works.</st></span></p>
			<h3><st c="9458">Understanding how NLP works</st></h3>
			<p><st c="9486">The NLP process involves text </st><a id="_idIndexMarker730"/><st c="9517">processing and several algorithms to extract the necessary information. </st><st c="9589">There are three basic steps – </st><strong class="bold"><st c="9619">preprocessing</st></strong><st c="9632">, </st><strong class="bold"><st c="9634">feature extraction</st></strong><st c="9652">, and </st><strong class="bold"><st c="9658">modeling</st></strong><st c="9666">. Let’s go over them one</st><a id="_idTextAnchor382"/><a id="_idTextAnchor383"/> <span class="No-Break"><st c="9690">by one.</st></span></p>
			<h4><st c="9698">Preprocessing</st></h4>
			<p><st c="9712">In this step, the NLP model starts by </st><a id="_idIndexMarker731"/><st c="9751">cleaning the input, such as removing duplicates, splitting texts into words and sentences, converting text to lowercase, and performing stemming and lemmatization. </st><st c="9915">Take the following text as </st><span class="No-Break"><st c="9942">an example:</st></span></p>
			<pre class="source-code"><st c="9953">
"Running is fun! </st><st c="9971">I love to run."</st></pre>			<p><st c="9986">This will be preprocessed to someth</st><a id="_idTextAnchor384"/><a id="_idTextAnchor385"/><st c="10022">ing like </st><span class="No-Break"><st c="10032">the following:</st></span></p>
			<pre class="source-code"><st c="10046">
"run fun love run".</st></pre>			<p><st c="10066">In this example, the NLP removed stop words (such as </st><strong class="source-inline"><st c="10120">is</st></strong><st c="10122">) and lowercased the </st><span class="No-Break"><st c="10144">whole string.</st></span></p>
			<h4><st c="10157">Feature extraction</st></h4>
			<p><st c="10176">After the string has been preprocessed, we transform it into a feature set that we can use with the ML algorithm. </st><st c="10291">In most </st><a id="_idIndexMarker732"/><st c="10299">cases, this involves capturing different patterns and word frequencies. </st><st c="10371">For example, the string from the previous step, </st><strong class="source-inline"><st c="10419">run fun love run</st></strong><st c="10435">, can be transformed into </st><span class="No-Break"><st c="10461">the following:</st></span></p>
			<pre class="source-code"><st c="10475">
{
  "run": 2,
  "fun": 1,
  "love": 1
}</st></pre>			<p><st c="10509">In this example, the NLP model takes the input string and analyzes the frequency of each word. </st><st c="10605">This technique is called </st><strong class="bold"><st c="10630">Bag of Words</st></strong><st c="10642"> (</st><strong class="bold"><st c="10644">BoW</st></strong><st c="10647">), and the model uses it to determine the importance of the different </st><a id="_idIndexMarker733"/><st c="10718">words in the string. </st><st c="10739">Note there are many feature extraction techniques, and BoW is just an example. </st><st c="10818">We can select the model now that we have the feature </st><span class="No-Break"><st c="10871">extraction data.</st></span></p>
			<h4><st c="10887">Modeling</st></h4>
			<p><st c="10896">In the modeling step, we use string and </st><a id="_idIndexMarker734"/><st c="10937">feature extraction as input to the model algorithm. </st><st c="10989">NLP uses several algorithms to analyze the string – logistic regression, naïve Bayes, and a neural network. </st><st c="11097">The algorithm that the model selects depends on the task it needs </st><span class="No-Break"><st c="11163">to achieve.</st></span></p>
			<p><st c="11174">For example, if the NLP framework needs to perform sentiment analysis, it would use a neural network-based model. </st><st c="11289">Simple text processing tasks would use a rule-based </st><span class="No-Break"><st c="11341">system model.</st></span></p>
			<p><st c="11354">These three steps demonstrate how complex it is to interpret a simple text. </st><st c="11431">Fortunately, the </st><strong class="source-inline"><st c="11448">NaturalLanguage</st></strong><st c="11463"> framework performs all of these steps </st><span class="No-Break"><st c="11502">for us.</st></span></p>
			<p><st c="11509">Let’s see how to use the </st><strong class="source-inline"><st c="11535">NaturalLanguage</st></strong> <span class="No-Break"><st c="11550">framework API.</st></span></p>
			<h3><st c="11565">Using the NaturalLanguage API</st></h3>
			<p><st c="11595">Finally, we are going to write some code! </st><st c="11638">The </st><strong class="source-inline"><st c="11642">NaturalLanguage</st></strong><st c="11657"> framework has two primary uses – classification </st><a id="_idIndexMarker735"/><st c="11706">and word tagging. </st><st c="11724">Let’s start </st><span class="No-Break"><st c="11736">with classification.</st></span></p>
			<h4><st c="11756">Text classification</st></h4>
			<p><st c="11776">Using </st><strong class="bold"><st c="11783">text classification</st></strong><st c="11802">, we can analyze the</st><a id="_idIndexMarker736"/><st c="11822"> text sentiment to determine whether it is positive </st><span class="No-Break"><st c="11874">or negative.</st></span></p>
			<p><st c="11886">For example, let’s take </st><a id="_idTextAnchor386"/><a id="_idTextAnchor387"/><st c="11911">a look at the </st><span class="No-Break"><st c="11925">following text:</st></span></p>
			<pre class="source-code"><st c="11940">
The latest update made everything so much better. </st><st c="11991">Great job!</st></pre>			<p><st c="12001">To analyze the sentiment of this sentence using the </st><strong class="source-inline"><st c="12054">NaturalLanguage</st></strong><st c="12069"> framework, we’ll use the </st><span class="No-Break"><strong class="source-inline"><st c="12095">NLTagger</st></strong></span><span class="No-Break"><st c="12103"> class:</st></span></p>
			<pre class="source-code"><st c="12110">
let sentimentAnalyzer = </st><strong class="bold"><st c="12135">NLTagger</st></strong><st c="12143">(tagSchemes:
  [</st><strong class="bold"><st c="12158">.sentimentScore</st></strong><st c="12173">])
        sentimentAnalyzer.string = userInput
        let (sentiment, _) = </st><strong class="bold"><st c="12235">sentimentAnalyzer.tag</st></strong><st c="12256">(at:
          userInput.startIndex, unit: .paragraph, scheme:
          .sentimentScore)
        if let sentiment = sentiment, let score =
          Double(sentiment.rawValue) {
          // here we can use the analyzed score
        } else {
            print("Unable to analyze sentiment")
        }</st></pre>			<p><strong class="source-inline"><st c="12483">NLTagger</st></strong><st c="12492"> is the primary class we use to process texts in NLP. </st><st c="12546">When we initialize it, we pass the information we are interested in. </st><st c="12615">In our example, we passed </st><strong class="source-inline"><st c="12641">sentimentScore</st></strong><st c="12655"> – a scheme that helps us determine the </st><span class="No-Break"><st c="12695">text sentiment.</st></span></p>
			<p><st c="12710">Our next step is to set the text input and call the tag function while passing relevant parameters, such as range, unit type, and scheme we want it </st><span class="No-Break"><st c="12859">to analyze.</st></span></p>
			<p><st c="12870">The tag function performs the text analysis and returns a score between -1 and 1, where a negative score indicates a negative sentiment and a positive score indicates a </st><span class="No-Break"><st c="13040">positive sentiment.</st></span></p>
			<p><st c="13059">If we run this code on</st><a id="_idIndexMarker737"/><st c="13082"> our example sentence before the code example, we’ll get a score of 1.0 – an extremely </st><span class="No-Break"><st c="13169">positive text!</st></span></p>
			<p><st c="13183">Even though text classification is very easy to use, it is also very powerful. </st><st c="13263">We can use this capability to analyze user feedback/reviews, chatbots, and surveys and even adapt an interface, based on the user’s sentiments </st><span class="No-Break"><st c="13406">and emotions.</st></span></p>
			<p><st c="13419">We mentioned that text classification is all about understanding the text sentiment. </st><st c="13505">However, we can use NLP to analyze text using </st><span class="No-Break"><st c="13551">word tagging.</st></span></p>
			<h4><st c="13564">Word tagging</st></h4>
			<p><strong class="bold"><st c="13577">Word tagging</st></strong><st c="13590"> is the process of breaking a text into components and assigning tags to each phrase in the text, indicating its</st><a id="_idIndexMarker738"/> <span class="No-Break"><st c="13702">grammatical category.</st></span></p>
			<p><st c="13724">Let’s take the</st><a id="_idTextAnchor388"/><a id="_idTextAnchor389"/><a id="_idTextAnchor390"/><a id="_idTextAnchor391"/><st c="13739"> example of the </st><span class="No-Break"><st c="13755">following text:</st></span></p>
			<pre class="source-code"><st c="13770">
She enjoys reading books in the library</st></pre>			<p><st c="13810">If we try to break this sentence into grammatical categories, it will be something like</st><em class="italic"><st c="13898"> She</st></em><st c="13902"> (pronoun), </st><em class="italic"><st c="13914">enjoys</st></em><st c="13920"> (verb), </st><em class="italic"><st c="13929">reading</st></em><st c="13936"> (verb), </st><em class="italic"><st c="13945">books</st></em><st c="13950"> (noun), </st><em class="italic"><st c="13959">in</st></em><st c="13961"> (preposition), </st><em class="italic"><st c="13977">the</st></em><st c="13980"> (determiner), and </st><span class="No-Break"><em class="italic"><st c="13999">library</st></em></span><span class="No-Break"><st c="14006"> (noun).</st></span></p>
			<p><st c="14014">The different parts of the</st><a id="_idIndexMarker739"/><st c="14041"> text are called </st><strong class="bold"><st c="14058">tokens</st></strong><st c="14064">, and their grammatical category is </st><a id="_idIndexMarker740"/><st c="14100">called </st><span class="No-Break"><st c="14107">a </st></span><span class="No-Break"><strong class="bold"><st c="14109">tag</st></strong></span><span class="No-Break"><st c="14112">.</st></span></p>
			<p><st c="14113">The </st><strong class="source-inline"><st c="14118">NaturalLanguage</st></strong><st c="14133"> framework helps us perform tokenization and tag </st><span class="No-Break"><st c="14182">its tokens.</st></span></p>
			<p><st c="14193">Let’s look at the </st><span class="No-Break"><st c="14212">following code:</st></span></p>
			<pre class="source-code"><st c="14227">
let inputText = "She enjoys reading books in the library"
let tagger = NLTagger(tagSchemes: [.lexicalClass])
tagger.string = inputText
let options: NLTagger.Options =  [.omitPunctuation,
  .omitWhitespace]
tagger.enumerateTags(in:
  inputText.startIndex..&lt;inputText.endIndex, unit: .word,
  scheme: .lexicalClass, options: options) { tag,
  tokenRange in
    if tag == .verb {
       verb = String(inputText[tokenRange])
       return false
   }
   return true
}</st></pre>			<p><st c="14657">The preceding code example takes the same sentence as earlier, tokenizes it, and locates the first verb </st><span class="No-Break"><st c="14762">it finds.</st></span></p>
			<p><st c="14771">We start by initializing </st><strong class="source-inline"><st c="14797">NLTagger</st></strong><st c="14805">, similar to what we did in text classification. </st><st c="14854">However, we do that this time by passing </st><strong class="source-inline"><st c="14895">lexicalClass</st></strong><st c="14907"> as </st><span class="No-Break"><st c="14911">its scheme.</st></span></p>
			<p><st c="14922">Then, we provide the input</st><a id="_idIndexMarker741"/><st c="14949"> text and omit punctuation and whitespaces. </st><st c="14993">We do this because we want our text to be as clean as possible. </st><strong class="source-inline"><st c="15057">NLTagger</st></strong><st c="15065"> can catch extra whitespace characters and punctuation as </st><span class="No-Break"><st c="15123">additional tags.</st></span></p>
			<p><st c="15139">After we clean our text, we call the </st><strong class="source-inline"><st c="15177">enumerateTags</st></strong><st c="15190"> function. </st><st c="15201">This function iterates the words in the text within a given range and extracts the different tags. </st><st c="15300">We compare the tag type inside the passed closure and store it in an </st><span class="No-Break"><st c="15369">instance variable.</st></span></p>
			<p><st c="15387">In our example, we locate the first verb, which </st><span class="No-Break"><st c="15436">is </st></span><span class="No-Break"><strong class="source-inline"><st c="15439">enjoys</st></strong></span><span class="No-Break"><st c="15445">.</st></span></p>
			<p><st c="15446">Although word tagging and text classification are </st><strong class="source-inline"><st c="15497">NLTagger</st></strong><st c="15505">’s two primary use cases, they can also be used for additional cases, such as to identify a </st><span class="No-Break"><st c="15598">text’s language:</st></span></p>
			<pre class="source-code"><st c="15614">
let tagger = NLTagger(tagSchemes: [</st><strong class="bold"><st c="15650">.language</st></strong><st c="15659">])
tagger.string = inputText
if let language = tagger.dominantLanguage {
     identifiedLanguage =
       Locale.current.localizedString(forLanguageCode:
       language.rawValue) ?? </st><st c="15824">"Unknown"
        } else {
            identifiedLanguage = "Unknown"
        }</st></pre>			<p><st c="15875">In the preceding example, the </st><strong class="source-inline"><st c="15906">NLTagger</st></strong><st c="15914"> receives input text and extracts its language. </st><st c="15962">It can identify 50 different languages – impressive for an on-device </st><span class="No-Break"><st c="16031">NLP model!</st></span></p>
			<p><st c="16041">We can use language identification</st><a id="_idIndexMarker742"/><st c="16076"> to identify the user locale and offer to change an app’s preferred language, or we can send that information as analytics data to </st><span class="No-Break"><st c="16207">our servers.</st></span></p>
			<p><st c="16219">Another great example of NLP is </st><strong class="bold"><st c="16252">word embedding</st></strong><st c="16266">. This feature can help our application </st><span class="No-Break"><st c="16306">become smarter.</st></span></p>
			<p><st c="16321">Each word in the dictionary is related to other</st><a id="_idIndexMarker743"/><st c="16369"> words. </st><st c="16377">For example, </st><em class="italic"><st c="16390">house</st></em><st c="16395"> is related to </st><em class="italic"><st c="16410">building</st></em><st c="16418"> and </st><em class="italic"><st c="16423">apartment</st></em><st c="16432">, and </st><em class="italic"><st c="16438">cat</st></em><st c="16441"> is associated </st><span class="No-Break"><st c="16456">with </st></span><span class="No-Break"><em class="italic"><st c="16461">dog</st></em></span><span class="No-Break"><st c="16464">.</st></span></p>
			<p><st c="16465">We can easily find related words, using a class </st><span class="No-Break"><st c="16514">called </st></span><span class="No-Break"><strong class="source-inline"><st c="16521">NLEmbedding</st></strong></span><span class="No-Break"><st c="16532">:</st></span></p>
			<pre class="source-code"><st c="16534">
guard let embedding = </st><strong class="bold"><st c="16557">NLEmbedding.</st></strong><st c="16569">wordEmbedding(for:
  .english) else {
            neighborsText = "Failed to load word
              embedding."
            </st><st c="16655">return
        }
</st><strong class="bold"><st c="16664">let neighbors = embedding.neighbors(for:</st></strong>
<strong class="bold"><st c="16704">  embedding.vector(for: inputWord) ?? </st><st c="16741">[], maximumCount: 5)</st></strong><st c="16761">
if neighbors.isEmpty {
       neighborsText = "No neighbors found for
         '\(inputWord)'."
        </st><st c="16842">} else {
            neighborsText = neighbors.map { "\($0.0)
              (\($0.1))" }.joined(separator: ", ")
        }</st></pre>			<p><st c="16930">In the preceding example, </st><strong class="source-inline"><st c="16957">NLEmbedding</st></strong><st c="16968"> receives an input test, calculates its vector, and finds its closed neighbors. </st><st c="17048">If you ask yourself why this is practical, think of a search engine that can find related content even if it isn’t exactly what the user </st><span class="No-Break"><st c="17185">searched for.</st></span></p>
			<p><st c="17198">In this section, we analyzed text using the </st><strong class="source-inline"><st c="17243">NaturalLanguage</st></strong><st c="17258"> framework. </st><st c="17270">We’ve learned how NLP works, how to classify text, and extract additional information such as word tagging and even</st><a id="_idIndexMarker744"/><st c="17385"> word embedding. </st><st c="17402">However, iOS apps contain more than just text; they also include images. </st><st c="17475">Can we analyze images </st><span class="No-Break"><st c="17497">as well?</st></span></p>
			<h2 id="_idParaDest-229"><a id="_idTextAnchor392"/><st c="17505">Analyzing images using the Vision framework</st></h2>
			<p><st c="17549">Analyzing images is a fundamental </st><a id="_idIndexMarker745"/><st c="17584">topic in iOS apps. </st><st c="17603">There are many use cases for analyzing images, such as detecting barcodes, scanning documents, or </st><span class="No-Break"><st c="17701">image editing.</st></span></p>
			<p><st c="17715">To analyze images in iOS, we need to </st><a id="_idIndexMarker746"/><st c="17753">use Apple’s Vision framework. </st><st c="17783">Introduced in 2017 with the release of iOS 11, the Vision framework provides </st><a id="_idIndexMarker747"/><st c="17860">high-level functionality to perform various image </st><span class="No-Break"><st c="17910">analysis tasks.</st></span></p>
			<h3><st c="17925">Understanding how image analysis works</st></h3>
			<p><st c="17964">In a way, image analysis works similarly to text analysis, working with different steps that clean and prepare </st><a id="_idIndexMarker748"/><st c="18076">data before inserting it into </st><span class="No-Break"><st c="18106">a model.</st></span></p>
			<p><st c="18114">The image analysis works with a </st><strong class="bold"><st c="18147">CNN</st></strong><st c="18150"> (</st><strong class="bold"><st c="18152">Convolutional Neural Network</st></strong><st c="18180">), a neural network designed for </st><span class="No-Break"><st c="18214">visual data.</st></span></p>
			<p><st c="18226">Consider CNN as a series of filters that can help a model better understand an image. </st><st c="18313">CNN will perform a similar process if the </st><strong class="source-inline"><st c="18355">NaturalLanguage</st></strong><st c="18370"> model preprocessed the text, removing whitespace and </st><span class="No-Break"><st c="18424">duplicate words.</st></span></p>
			<p><st c="18440">First, the CNN scans an image to detect similar patterns, such as lines, edges, and textures. </st><st c="18535">It then filters out what it thinks are non-important features and shrinks the image to contain the most </st><span class="No-Break"><st c="18639">essential information.</st></span></p>
			<p><st c="18661">Now that we have a smaller and cleaner image, the CNN tries to decide what the image is – for example, “</st><em class="italic"><st c="18766">It’s </st></em><span class="No-Break"><em class="italic"><st c="18772">a cat.</st></em></span><span class="No-Break"><st c="18778">”</st></span></p>
			<p><st c="18780">Detecting patterns and edges, filtering them, and analyzing an image are complex techniques that require</st><a id="_idIndexMarker749"/><st c="18884"> extensive training. </st><st c="18905">Luckily, the Vision framework performs all the heavy lifting </st><span class="No-Break"><st c="18966">for us.</st></span></p>
			<p><st c="18973">Let’s see what it can do </st><span class="No-Break"><st c="18999">for us.</st></span></p>
			<h3><st c="19006">Exploring the Vision Framework’s capabilities</st></h3>
			<p><st c="19052">Since starting iOS 18, the </st><strong class="bold"><st c="19080">Vision framework API</st></strong><st c="19100"> has become extremely simple yet even </st><span class="No-Break"><st c="19138">more powerful.</st></span></p>
			<p><st c="19152">To understand how the</st><a id="_idIndexMarker750"/><st c="19174"> Vision framework API works, we need to remember that it is based on two types – request </st><span class="No-Break"><st c="19263">and observation.</st></span></p>
			<p><st c="19279">To perform an image analysis, we first create a </st><strong class="bold"><st c="19328">request</st></strong><st c="19335">. Then, we request the specific image and receive an </st><strong class="bold"><st c="19388">observation</st></strong><st c="19399"> containing the result (if we </st><span class="No-Break"><st c="19429">have any).</st></span></p>
			<p><st c="19439">Let’s take two popular use cases – detecting barcodes </st><span class="No-Break"><st c="19494">and faces.</st></span></p>
			<h4><st c="19504">Detecting barcodes</st></h4>
			<p><st c="19523">Look at the following code to</st><a id="_idIndexMarker751"/><st c="19553"> see barcode detecting </st><span class="No-Break"><st c="19576">in action:</st></span></p>
			<pre class="source-code"><st c="19586">
func analyze(url: URL) async {
    let request = </st><strong class="bold"><st c="19632">DetectBarcodesRequest()</st></strong><st c="19655">
    do {
       let barcodeObservations = </st><strong class="bold"><st c="19687">try await</st></strong>
<strong class="bold"><st c="19696">         request.perform(on: url)</st></strong><st c="19721">
       barcodeIdentifier =
         </st><strong class="bold"><st c="19742">barcodeObservations.first?.payloadString ??</st></strong><st c="19785"> ""
    } catch let error {
       print("error analyzing image –
         \(error.localizedDescription)")
        }
    }</st></pre>			<p><st c="19875">The preceding code block performs</st><a id="_idIndexMarker752"/><st c="19909"> barcode detection using the Vision framework. </st><st c="19956">First, we create </st><strong class="source-inline"><st c="19973">DetectBarcodesRequest</st></strong><st c="19994">, which represents a request to scan barcodes in a given </st><span class="No-Break"><st c="20051">image URL.</st></span></p>
			<p><st c="20061">Then, we call the request’s </st><strong class="source-inline"><st c="20090">perform</st></strong><st c="20097"> function, which returns an array of observations in the case of </st><span class="No-Break"><st c="20162">several barcodes.</st></span></p>
			<p><st c="20179">Next, we take the first observation payload and store it in a variable. </st><st c="20252">That payload represents the </st><span class="No-Break"><st c="20280">barcode identifier.</st></span></p>
			<p><st c="20299">Note that the scanning operation can be a heavy task, which is why it is an </st><span class="No-Break"><st c="20376">asynchronous function.</st></span></p>
			<p><st c="20398">Another interesting example of a Vision framework usage is detecting faces in an image – let’s see </st><span class="No-Break"><st c="20498">an example.</st></span></p>
			<h4><st c="20509">Detecting faces</st></h4>
			<p><st c="20525">Detecting faces works similarly to </st><a id="_idIndexMarker753"/><st c="20561">detecting barcodes. </st><st c="20581">Let’s see a </st><span class="No-Break"><st c="20593">code example:</st></span></p>
			<pre class="source-code"><st c="20606">
func analyze(url: URL) async {
        let request = </st><strong class="bold"><st c="20652">DetectFaceRectanglesRequest()</st></strong><st c="20681">
        do {
            let observations = try await
              request</st><strong class="bold"><st c="20723">.perform</st></strong><st c="20731">(on: url)
            if let </st><strong class="bold"><st c="20749">observation</st></strong><st c="20760"> = observations.first {
                rect = observation.boundingBox.cgRect
            }
        } catch let error {
            print(error.localizedDescription)
        }
    }</st></pre>			<p><st c="20881">The preceding code example looks almost identical to the previous barcode example. </st><st c="20965">First, we create the request. </st><st c="20995">However, this time, the request is from type </st><strong class="source-inline"><st c="21040">DetectFaceRectanglesRequest</st></strong><st c="21067">. Next, we perform the detection operation on the given image URL and retrieve an array of observations. </st><st c="21172">Each observation instance contains a rectangle of one</st><a id="_idIndexMarker754"/><st c="21225"> of the faces in the image. </st><st c="21253">If the image contains multiple faces, we’ll get one observation for </st><span class="No-Break"><st c="21321">each face.</st></span></p>
			<p><st c="21331">Face detection and barcodes are two common examples of Vision framework use cases. </st><st c="21415">However, the Vision framework is full of surprises and detection capabilities. </st><st c="21494">Let’s see what else we can do </st><span class="No-Break"><st c="21524">with it.</st></span></p>
			<h4><st c="21532">Exploring more detection capabilities</st></h4>
			<p><st c="21570">As mentioned, the Vision framework is full of machine-learning models capable of detecting almost anything we want. </st><st c="21687">Barcodes </st><a id="_idIndexMarker755"/><st c="21696">and faces are just the tip of </st><span class="No-Break"><st c="21726">the iceberg.</st></span></p>
			<p><st c="21738">Here’s a list of </st><span class="No-Break"><st c="21756">additional detectors:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="21777">Image aesthetics analysis</st></strong><st c="21803">: For analyzing an image from an </st><span class="No-Break"><st c="21837">aesthetic viewpoint</st></span></li>
				<li><strong class="bold"><st c="21856">Saliency analysis</st></strong><st c="21874">: For finding the most important object in </st><span class="No-Break"><st c="21918">an image</st></span></li>
				<li><strong class="bold"><st c="21926">Object tracking</st></strong><st c="21942">: For tracking an object’s movement across a sequence </st><span class="No-Break"><st c="21997">of images</st></span></li>
				<li><strong class="bold"><st c="22006">Body detection</st></strong><st c="22021">: Similar to face detection, for locating arms, humans, eyes, a mouth, and a nose </st><span class="No-Break"><st c="22104">in images</st></span></li>
				<li><strong class="bold"><st c="22113">Body and hand pose</st></strong><st c="22132">: For locating arms in an image as well as detecting </st><span class="No-Break"><st c="22186">their pose.</st></span></li>
				<li><strong class="bold"><st c="22197">Text detection</st></strong><st c="22212">: For detecting text in </st><span class="No-Break"><st c="22237">an image</st></span></li>
				<li><strong class="bold"><st c="22245">Animal detection</st></strong><st c="22262">: For detecting cats and dogs in an image as well as </st><span class="No-Break"><st c="22316">their pose</st></span></li>
				<li><strong class="bold"><st c="22326">Background removal and object extraction</st></strong><st c="22367">: For removing the background and extracting objects </st><span class="No-Break"><st c="22421">from images</st></span></li>
			</ul>
			<p><st c="22432">The list of the different request types looks impressive, which it is. </st><st c="22504">Reviewing the requests reflects how powerful the Vision framework has become. </st><st c="22582">We can see capabilities usually reserved for </st><a id="_idIndexMarker756"/><st c="22627">high-end image editing applications, such as background removal or object extraction, now available with just three lines </st><span class="No-Break"><st c="22749">of code.</st></span></p>
			<p><st c="22757">This opens up new possibilities for unique features in our apps, such as working with a camera or prioritizing images based on </st><span class="No-Break"><st c="22885">their information.</st></span></p>
			<p><st c="22903">We’ve discussed analyzing text and images, which are considered the most common data sources we usually use. </st><st c="23013">The text and image analysis techniques are different but straightforward </st><span class="No-Break"><st c="23086">to implement.</st></span></p>
			<p><st c="23099">Now, let’s turn to a different type of source we can analyze – </st><span class="No-Break"><st c="23163">sound.</st></span></p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor393"/><st c="23169">Classifying audio using the Sound Analysis framework</st></h2>
			<p><st c="23222">Working with audio is not a popular </st><a id="_idIndexMarker757"/><st c="23259">expertise for many developers. </st><st c="23290">In fact, audio is considered to be a complex and unique </st><a id="_idIndexMarker758"/><st c="23346">world compared to what we developers are </st><span class="No-Break"><st c="23387">used to.</st></span></p>
			<p><st c="23395">To mitigate this, the iOS SDK also includes an analysis framework that can classify audio using </st><span class="No-Break"><st c="23492">ML models.</st></span></p>
			<p><st c="23502">Working with the </st><strong class="bold"><st c="23520">Sound Analysis framework</st></strong><st c="23544"> differs from the simplicity we are used to with the Vision framework. </st><st c="23615">But don’t worry – it is still simple </st><span class="No-Break"><st c="23652">to use.</st></span></p>
			<p><st c="23659">The Sound Analysis framework contains three </st><span class="No-Break"><st c="23704">different components:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="23725">SNAudioFileAnalyzer</st></strong><st c="23745">: The main class that coordinates the </st><span class="No-Break"><st c="23784">analysis work</st></span></li>
				<li><strong class="bold"><st c="23797">SNClassifySoundRequest</st></strong><st c="23820">: The sound </st><span class="No-Break"><st c="23833">detection request</st></span></li>
				<li><strong class="bold"><st c="23850">SNResultsObserving</st></strong><st c="23869">: A protocol we need to implement to observe the results from </st><span class="No-Break"><st c="23932">the analyzer</st></span></li>
			</ul>
			<p><st c="23944">To see these three components in action, take a look at the </st><span class="No-Break"><st c="24005">following code:</st></span></p>
			<pre class="source-code"><st c="24020">
func analyze(at url: URL) {
        do {
            let audioFileAnalyzer = try
              </st><strong class="bold"><st c="24082">SNAudioFileAnalyzer</st></strong><st c="24101">(url: url)
            let request = try
              </st><strong class="bold"><st c="24131">SNClassifySoundRequest</st></strong><st c="24153">(classifierIdentifier:
              .version1)
            let resultsObserver =
              </st><strong class="bold"><st c="24210">ClassificationResultsObserver</st></strong><st c="24239">()
            try audioFileAnalyzer.add(request,
              withObserver: resultsObserver)
            audioFileAnalyzer.analyze()
        } catch {
            print("Error: \(error.localizedDescription)")
        }
    }</st></pre>			<p><st c="24396">In this example, we first</st><a id="_idIndexMarker759"/><st c="24422"> create the </st><strong class="source-inline"><st c="24434">SNAudioFileAnalyzer</st></strong><st c="24453"> instance and initialize it with a URL to the audio file. </st><st c="24511">Then, we create a request for a classification sound request, passing </st><strong class="source-inline"><st c="24581">version1</st></strong><st c="24589"> as a </st><a id="_idIndexMarker760"/><st c="24595">parameter. </st><st c="24606">The </st><strong class="source-inline"><st c="24610">version1</st></strong><st c="24618"> parameter specifies the pre-trained classification version of the model. </st><st c="24692">At the time of writing, no additional versions </st><span class="No-Break"><st c="24739">are available.</st></span></p>
			<p><st c="24753">Then, we create the </st><strong class="source-inline"><st c="24774">resultsObserver</st></strong><st c="24789"> instance (which we’ll discuss briefly) and coordinate everything together, using the analyzer we </st><span class="No-Break"><st c="24887">created earlier.</st></span></p>
			<p><st c="24903">How do we get the results? </st><st c="24931">Unlike the Vision</st><a id="_idIndexMarker761"/><st c="24948"> framework, receiving the results can be streamlined. </st><st c="25002">The </st><strong class="source-inline"><st c="25006">ClassificationResultsObserver</st></strong><st c="25035"> is a custom class that conforms to </st><strong class="source-inline"><st c="25071">SNResultsObserving</st></strong><st c="25089">. Let’s look at the </st><span class="No-Break"><st c="25109">class implementation:</st></span></p>
			<pre class="source-code"><st c="25130">
class ClassificationResultsObserver: NSObject,
  SNResultsObserving {
    func request(_ request: SNRequest, didProduce result:
      SNResult) {
        guard let result = result as?
          </st><st c="25295">SNClassificationResult else { return }
        if let classification =
          result.classifications.first {
</st><strong class="bold"><st c="25389">            let result = classification.identifier</st></strong><st c="25427">
          }
    }
    func request(_ request: SNRequest, didFailWithError
      error: Error) { }
    func requestDidComplete(_ request: SNRequest) {}
}</st></pre>			<p><st c="25552">The </st><strong class="source-inline"><st c="25557">SNResultsObserving</st></strong><st c="25575"> protocol has three essential request methods – </st><strong class="source-inline"><st c="25623">didProduce</st></strong><st c="25633">, </st><strong class="source-inline"><st c="25635">didFailWithError</st></strong><st c="25651">, </st><span class="No-Break"><st c="25653">and </st></span><span class="No-Break"><strong class="source-inline"><st c="25657">requestDidComplete</st></strong></span><span class="No-Break"><st c="25675">.</st></span></p>
			<p><st c="25676">Great! </st><st c="25684">However, unfortunately, in this case, it seems like we need to go back in time and use the delegate pattern to </st><a id="_idIndexMarker762"/><st c="25795">observe results from the Sound </st><span class="No-Break"><st c="25826">Analysis framework.</st></span></p>
			<p><st c="25845">The result is a string describing the sound we passed to the analyzer. </st><st c="25917">The code example in this book’s GitHub </st><a id="_idIndexMarker763"/><st c="25956">repository shows a sound file with a baby crying. </st><st c="26006">In this case, the result would </st><span class="No-Break"><st c="26037">be </st></span><span class="No-Break"><strong class="source-inline"><st c="26040">baby_crying</st></strong></span><span class="No-Break"><st c="26051">.</st></span></p>
			<p><st c="26052">Apple has yet to officially publish the number of sound classes that the Sound Analysis framework can recognize. </st><st c="26166">However, in most cases, this should be enough for </st><span class="No-Break"><st c="26216">day-to-day usage.</st></span></p>
			<p><st c="26233">The Sound Analysis framework can be great for monitoring apps, adding </st><strong class="bold"><st c="26304">SDH</st></strong><st c="26307"> (</st><strong class="bold"><st c="26309">subtitles for the deaf or hard of hearing</st></strong><st c="26350">) to video captions, and </st><span class="No-Break"><st c="26376">analyzing videos.</st></span></p>
			<p><st c="26393">So far, we have discussed how to analyze </st><a id="_idIndexMarker764"/><st c="26435">different types of data – sound, images, and text. </st><st c="26486">However, ML is valuable in other areas, such as </st><span class="No-Break"><st c="26534">app search.</st></span></p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor394"/><st c="26545">Performing a semantic search with Core Spotlight</st></h2>
			<p><st c="26594">When we discussed NLP in the </st><em class="italic"><st c="26624">Interpreting text using NLP</st></em><st c="26651"> section, we said that one of the most common NLP </st><a id="_idIndexMarker765"/><st c="26701">use cases is analyzing a search</st><a id="_idIndexMarker766"/><st c="26732"> phrase to build intelligent </st><span class="No-Break"><st c="26761">search queries.</st></span></p>
			<p><st c="26776">Even though the </st><strong class="source-inline"><st c="26793">NaturalLanguage</st></strong><st c="26808"> Framework API is robust and straightforward, performing a semantic search is</st><a id="_idIndexMarker767"/><st c="26885"> considered a </st><span class="No-Break"><st c="26899">complex task.</st></span></p>
			<p><st c="26912">Starting iOS 18, the Core Spotlight framework supports a semantic search. </st><st c="26987">Before we dive into the details, let’s clarify the term </st><span class="No-Break"><strong class="bold"><st c="27043">semantic search</st></strong></span><span class="No-Break"><st c="27058">.</st></span></p>
			<h3><st c="27059">Understanding what semantic search is</st></h3>
			<p><st c="27097">Let’s think together about how</st><a id="_idIndexMarker768"/><st c="27128"> search queries work in a standard app, and we’ll do that using </st><span class="No-Break"><st c="27192">an example.</st></span></p>
			<p><st c="27203">Imagine that we have a course catalog app where a user can search for a particular course, and let’s say we have the following list of courses in our local </st><span class="No-Break"><st c="27360">data store:</st></span></p>
			<ul>
				<li><st c="27371">Management </st><span class="No-Break"><st c="27383">for employees</st></span></li>
				<li><span class="No-Break"><st c="27396">Data science</st></span></li>
				<li><span class="No-Break"><st c="27409">Digital marketing</st></span></li>
				<li><st c="27427">ML </st><span class="No-Break"><st c="27431">and AI</st></span></li>
			</ul>
			<p><st c="27437">Our user wants to improve their leadership skills, so they search for a management course within this list </st><span class="No-Break"><st c="27545">of courses.</st></span></p>
			<p><st c="27556">The search query’s basic form is to match a specific phrase. </st><st c="27618">For example, if the user searches for </st><strong class="source-inline"><st c="27656">management</st></strong><st c="27666">, we filter only courses containing </st><em class="italic"><st c="27702">management</st></em><st c="27712">. We also need to ensure that the output query </st><span class="No-Break"><st c="27759">is case-insensitive.</st></span></p>
			<p><st c="27779">However, what if the user searches for </st><strong class="source-inline"><st c="27819">manager</st></strong><st c="27826">? In this case, our query returns no results, even though a typical user can search for </st><em class="italic"><st c="27914">manager</st></em><st c="27921"> if they want a course </st><span class="No-Break"><st c="27944">about management.</st></span></p>
			<p><st c="27961">In this case, we can use the </st><strong class="source-inline"><st c="27991">NaturalLanguage</st></strong><st c="28006"> framework to try and perform lemmatization of the search phrase. </st><strong class="bold"><st c="28072">Lemmatization</st></strong><st c="28085"> is a technique that reduces words to their basic form. </st><st c="28141">So, the basic </st><a id="_idIndexMarker769"/><st c="28155">form of </st><em class="italic"><st c="28163">manager</st></em> <span class="No-Break"><st c="28170">is </st></span><span class="No-Break"><em class="italic"><st c="28174">manage</st></em></span><span class="No-Break"><st c="28180">.</st></span></p>
			<p><st c="28181">However, if we want to match the search phrase </st><em class="italic"><st c="28229">manage</st></em><st c="28235">, we also need all our records with the word </st><em class="italic"><st c="28280">management</st></em><st c="28290"> to contain the word </st><em class="italic"><st c="28311">manage</st></em><st c="28317"> so that we can filter the results accordingly. </st><st c="28365">It means we must maintain the basic form for each word in </st><span class="No-Break"><st c="28423">each record.</st></span></p>
			<p><st c="28435">But things can get </st><a id="_idIndexMarker770"/><st c="28455">even more complex than that. </st><st c="28484">What if the user searches for a management course using the phrase </st><em class="italic"><st c="28551">leadership</st></em><st c="28561">? In this case, we will have to index our records with embedded words, as we learned in the </st><em class="italic"><st c="28653">Word tagging</st></em><st c="28665"> section of </st><span class="No-Break"><st c="28677">this chapter.</st></span></p>
			<p><st c="28690">The conclusion is that basic search is easy. </st><st c="28736">However, semantic search, which is much more effective, is also much </st><span class="No-Break"><st c="28805">more complex.</st></span></p>
			<p><st c="28818">As mentioned, semantic search is built on top of the </st><strong class="bold"><st c="28872">Core Spotlight framework</st></strong><st c="28896">, starting with iOS 18. </st><st c="28920">The Core Spotlight</st><a id="_idIndexMarker771"/><st c="28938"> framework is not new – it was introduced in 2015 as part of iOS 9 and helps developers index app content and make it searchable, using the Spotlight feature </st><span class="No-Break"><st c="29096">in iOS.</st></span></p>
			<p><st c="29103">This chapter does not cover using the Core Spotlight framework. </st><st c="29168">However, we will briefly review the Core Spotlight principles to understand how to enable semantic search. </st><span class="No-Break"><st c="29275">Let’s begin.</st></span></p>
			<h3><st c="29287">Exploring the Core Spotlight framework</st></h3>
			<p><st c="29326">The Spotlight framework</st><a id="_idIndexMarker772"/><st c="29350"> indexes local data and retrieves it by </st><span class="No-Break"><st c="29390">performing queries.</st></span></p>
			<p><st c="29409">The Core Spotlight framework has three primary parts – creating searchable items, indexing, and querying. </st><st c="29516">Let’s go through the parts one </st><span class="No-Break"><st c="29547">by one.</st></span></p>
			<h4><st c="29554">Creating searchable items</st></h4>
			<p><st c="29580">Let’s say we have instances of a </st><a id="_idIndexMarker773"/><st c="29614">book structure in our local storage and want to implement Core Spotlight to allow users to search </st><span class="No-Break"><st c="29712">for books.</st></span></p>
			<p><st c="29722">First, we need to map all our </st><strong class="source-inline"><st c="29753">Book</st></strong><st c="29757"> instances </st><span class="No-Break"><st c="29768">to </st></span><span class="No-Break"><strong class="source-inline"><st c="29771">CSSearchableItem</st></strong></span><span class="No-Break"><st c="29787">:</st></span></p>
			<pre class="source-code"><st c="29789">
let searchableItems: [</st><strong class="bold"><st c="29812">CSSearchableItem</st></strong><st c="29829">] = books.map { book
  in
            let attributeSet =
              CSSearchableItemAttributeSet(contentType:
              .text)
            attributeSet.title = book.title
            attributeSet.contentDescription = book.author
            </st><strong class="bold"><st c="30000">let item = CSSearchableItem(uniqueIdentifier:</st></strong>
<strong class="bold"><st c="30045">              book.id, domainIdentifier: "books",</st></strong>
<strong class="bold"><st c="30081">              attributeSet: attributeSet)</st></strong><st c="30109">
            return item
        }</st></pre>			<p><st c="30123">In the preceding code example, we</st><a id="_idIndexMarker774"/><st c="30157"> took an array of </st><strong class="source-inline"><st c="30175">Book</st></strong><st c="30179"> and mapped it to an array of </st><strong class="source-inline"><st c="30209">CSSearchableItem</st></strong><st c="30225">. We do that by creating a </st><strong class="source-inline"><st c="30252">CSSearchableItemAttributeSet</st></strong><st c="30280"> – an item that contains general information about the searchable item. </st><st c="30352">Then, we initialize a new </st><strong class="source-inline"><st c="30378">CSSearchableItem</st></strong><st c="30394">, passing our </st><strong class="source-inline"><st c="30408">CSSearchableItemAttributeSet</st></strong><st c="30436"> and providing a unique identifier that can help us retrieve the </st><strong class="source-inline"><st c="30501">Book</st></strong><st c="30505"> record </st><span class="No-Break"><st c="30513">when needed.</st></span></p>
			<h4><st c="30525">Indexing</st></h4>
			<p><st c="30534">Now that we have an </st><a id="_idIndexMarker775"/><st c="30555">array of </st><strong class="source-inline"><st c="30564">CSSearchableItem</st></strong><st c="30580">, we need to index the array items for the Core Spotlight framework. </st><st c="30649">We do that by </st><span class="No-Break"><st c="30663">creating </st></span><span class="No-Break"><strong class="source-inline"><st c="30672">CSSearchableIndex</st></strong></span><span class="No-Break"><st c="30689">:</st></span></p>
			<pre class="source-code"><st c="30691">
let index = </st><strong class="bold"><st c="30704">CSSearchableIndex</st></strong><st c="30721">(name: "SpotlightSearchIndex")
        index.</st><strong class="bold"><st c="30759">indexSearchableItems</st></strong><st c="30780">(searchableItems) { error
          in
            if let error = error {
                print("Indexing error:
                  \(error.localizedDescription)")
            } else {
                print("Books successfully indexed!")
            }
        }</st></pre>			<p><st c="30937">In the preceding example, we created a new </st><strong class="source-inline"><st c="30981">CSSearchableIndex</st></strong><st c="30998"> and called the </st><strong class="source-inline"><st c="31014">indexSearchableItems</st></strong><st c="31034"> function, with the array of </st><strong class="source-inline"><st c="31063">CSSearchableItem</st></strong><st c="31079"> that we made in the previous step. </st><st c="31115">Note that</st><a id="_idIndexMarker776"/><st c="31124"> this is an asynchronous operation and is considered to be </st><span class="No-Break"><st c="31183">quite intensive.</st></span></p>
			<h4><st c="31199">Querying</st></h4>
			<p><st c="31208">Now that we have an index, we </st><a id="_idIndexMarker777"/><st c="31239">can perform a query to retrieve data based on a </st><span class="No-Break"><st c="31287">search phrase:</st></span></p>
			<pre class="source-code"><st c="31301">
let searchContext = CSUserQueryContext()
        searchContext.fetchAttributes = ["title"]
        searchContext.enableRankedResults = true
        var items: [CSSearchableItem] = []
        let query = CSUserQuery(userQueryString: query,
          userQueryContext: searchContext)
        do {
            for try await element in query.responses {
                switch(element) {
                case .item(let item):
                    items.append(item.item)
                    break
                case .suggestion(let suggestion):
                    // handle suggestions.
                    </st><st c="31717">break
                @unknown default:
                    break
                }
            }
            self.searchResults = items
        } catch let error {
            print(error.localizedDescription)
        }</st></pre>			<p><st c="31833">In the preceding example, we create a search context containing various query information. </st><st c="31925">Based on the</st><a id="_idIndexMarker778"/><st c="31937"> search context and the search phrase, we initialize an item of </st><strong class="source-inline"><st c="32001">CSUserQuery</st></strong><st c="32012"> and fetch the search results by calling its </st><span class="No-Break"><strong class="source-inline"><st c="32057">responses</st></strong></span><span class="No-Break"><st c="32066"> getter.</st></span></p>
			<p><st c="32074">The results are an array of </st><strong class="source-inline"><st c="32103">CSSearchableItem</st></strong><st c="32119">, and we can retrieve the original item by using the unique identifier for </st><span class="No-Break"><st c="32194">each record.</st></span></p>
			<p><st c="32206">Now that we know how to implement search using the Core Spotlight framework, let’s see how to implement a </st><span class="No-Break"><st c="32313">semantic search.</st></span></p>
			<h3><st c="32329">Implementing semantic search</st></h3>
			<p><st c="32358">Adding semantic search </st><a id="_idIndexMarker779"/><st c="32382">capabilities to an existing Core Spotlight search is simple. </st><st c="32443">All we need to do is load the ML model once using the following </st><span class="No-Break"><st c="32507">static function:</st></span></p>
			<pre class="source-code"><st c="32523">
CSUserQuery.prepare()</st></pre>			<p><st c="32545">The </st><strong class="source-inline"><st c="32550">prepare</st></strong><st c="32557"> function prepares the Core Spotlight framework to load its ML models for </st><span class="No-Break"><st c="32631">semantic search.</st></span></p>
			<p><st c="32647">If the search index has a protection level due to privacy concerns, we also need to call the </st><span class="No-Break"><strong class="source-inline"><st c="32741">prepreProtectionClasses</st></strong></span><span class="No-Break"><st c="32764"> function:</st></span></p>
			<pre class="source-code"><st c="32774">
CSUserQuery.prepareProtectionClasses([.completeUnlessOpen])</st></pre>			<p><st c="32834">This function prepares the search for indexes marked with the </st><strong class="source-inline"><st c="32897">completeUnlessOpen</st></strong> <span class="No-Break"><st c="32915">protection level.</st></span></p>
			<p class="callout-heading"><st c="32933">What are protection levels?</st></p>
			<p class="callout"><st c="32961">The term </st><strong class="bold"><st c="32971">protection level</st></strong><st c="32987"> refers to the </st><a id="_idIndexMarker780"/><st c="33002">accessibility level where users have specific resources, considering the device’s </st><a id="_idIndexMarker781"/><st c="33084">security conditions. </st><st c="33105">There are three primary </st><span class="No-Break"><st c="33129">protection levels:</st></span></p>
			<p class="callout"><st c="33147">- </st><strong class="source-inline"><st c="33150">NSFileProtectionNone</st></strong><st c="33170">: The index is always accessible, even when the device </st><span class="No-Break"><st c="33226">is locked</st></span></p>
			<p class="callout"><st c="33235">- </st><strong class="source-inline"><st c="33238">NSFileProtectionCompleteUntilFirstUserAuthentication</st></strong><st c="33290">: Once the user is authenticated for the first time after a device restart, the index </st><span class="No-Break"><st c="33377">is accessible</st></span></p>
			<p class="callout"><st c="33390">- </st><strong class="source-inline"><st c="33393">NSFileProtectionComplete</st></strong><st c="33417">: The index is accessible only when the device </st><span class="No-Break"><st c="33465">is unlocked</st></span></p>
			<p><st c="33476">Remember that preparing the ML models costs time and memory, so it’s better to call the </st><strong class="source-inline"><st c="33565">prepare</st></strong><st c="33572"> function only immediately before the search </st><span class="No-Break"><st c="33617">user interface.</st></span></p>
			<p><st c="33632">We have discussed various </st><a id="_idIndexMarker782"/><st c="33659">built-in ML models, and we can see that they cover many use cases where we can use ML capabilities with our projects. </st><st c="33777">However, there are cases where the iOS SDK doesn’t provide the exact ML solution we need. </st><st c="33867">Luckily, we can integrate our models using the </st><span class="No-Break"><st c="33914">CoreML framework.</st></span></p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor395"/><st c="33931">Integrating custom models using CoreML</st></h1>
			<p><st c="33970">Generally, ML models are trained to perform a specific task – recognizing a sentence’s sentiment, detecting humans, or analyzing sounds are all examples of different tasks done using various models. </st><st c="34170">This </st><a id="_idIndexMarker783"/><st c="34175">means that even though the potential of the existing models is enormous, we are still limited in what we </st><span class="No-Break"><st c="34280">can do.</st></span></p>
			<p><st c="34287">This is where the </st><strong class="bold"><st c="34306">CoreML framework</st></strong><st c="34322"> enters the </st><a id="_idIndexMarker784"/><st c="34334">picture. </st><st c="34343">Using CoreML, we can integrate ML models that are not part </st><a id="_idIndexMarker785"/><st c="34402">of the iOS SDK, and we can even train our own models and add more </st><span class="No-Break"><st c="34468">intelligent capabilities.</st></span></p>
			<p><st c="34493">It’s best to explain how to do this by using an example, such as detecting </st><span class="No-Break"><st c="34569">spam messages.</st></span></p>
			<p><st c="34583">Imaging we are developing a messaging app. </st><st c="34627">One of the most popular messaging app features is the ability to detect spam to improve the user experience and </st><span class="No-Break"><st c="34739">increase retention.</st></span></p>
			<p><st c="34758">We must create an ML model to classify messages as spam to implement a </st><span class="No-Break"><st c="34830">spam detector.</st></span></p>
			<p><st c="34844">To achieve this, we can use a desktop application called Create ML, which is part of the Xcode suite. </st><st c="34947">Let’s begin by learning more about </st><span class="No-Break"><st c="34982">Create ML!</st></span></p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor396"/><st c="34992">Getting to know the Create ML application</st></h2>
			<p><strong class="bold"><st c="35034">Create ML</st></strong><st c="35044"> was introduced in 2018 as </st><a id="_idIndexMarker786"/><st c="35071">part of Apple’s ongoing effort to make ML more accessible to developers. </st><st c="35144">We can build, train, and deploy ML models in various areas using </st><span class="No-Break"><st c="35209">Create ML.</st></span></p>
			<p><st c="35219">To open Create ML, follow </st><span class="No-Break"><st c="35246">these steps:</st></span></p>
			<ol>
				<li><span class="No-Break"><st c="35258">Open Xcode.</st></span></li>
				<li><st c="35270">Right-click on the Xcode icon on </st><span class="No-Break"><st c="35304">the dock.</st></span></li>
				<li><st c="35313">Select </st><strong class="bold"><st c="35321">Open Developer Tool</st></strong><st c="35340"> | </st><span class="No-Break"><strong class="bold"><st c="35343">Create ML</st></strong></span><span class="No-Break"><st c="35352">.</st></span></li>
			</ol>
			<p><st c="35353">Another way to open </st><strong class="source-inline"><st c="35374">Create ML</st></strong><st c="35383"> is by searching for it in Spotlight on your Mac and </st><span class="No-Break"><st c="35436">selecting it.</st></span></p>
			<p><st c="35449">After opening it and clicking on the </st><strong class="bold"><st c="35487">New Document</st></strong><st c="35499"> button, we get the following screen (</st><span class="No-Break"><em class="italic"><st c="35537">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="35547">.1</st></em></span><span class="No-Break"><st c="35549">):</st></span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B21795_12_1.jpg" alt="Figure 12.1: The Create ML template picker"/><st c="35552"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="36033">Figure 12.1: The Create ML template picker</st></p>
			<p><span class="No-Break"><em class="italic"><st c="36075">Figure 12</st></em></span><em class="italic"><st c="36085">.1</st></em><st c="36087"> shows the Create ML template picker screen. </st><st c="36132">Each template represents a different configuration for our model, and each is designed to handle a different type of data. </st><st c="36255">For example, the </st><strong class="bold"><st c="36272">Image Classification</st></strong><st c="36292"> template is designed to handle images. </st><st c="36332">Since we want to classify </st><a id="_idIndexMarker787"/><st c="36358">text messages, we will pick the </st><strong class="bold"><st c="36390">Text Classification</st></strong><st c="36409"> template and click the </st><span class="No-Break"><strong class="bold"><st c="36433">Next</st></strong></span><span class="No-Break"><st c="36437"> button.</st></span></p>
			<p><st c="36445">This will take us to the project details screen (</st><span class="No-Break"><em class="italic"><st c="36495">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="36505">.2</st></em></span><span class="No-Break"><st c="36507">):</st></span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B21795_12_2.jpg" alt="Figure 12.2: The project details form"/><st c="36510"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="36940">Figure 12.2: The project details form</st></p>
			<p><st c="36977">In the project details form, we will fill in some general information about our ML model, such as the name, author, license, and description, and then </st><span class="No-Break"><st c="37129">click </st></span><span class="No-Break"><strong class="bold"><st c="37135">Next</st></strong></span><span class="No-Break"><st c="37139">.</st></span></p>
			<p><st c="37140">Our next screen is the project window (</st><span class="No-Break"><em class="italic"><st c="37180">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="37190">.3</st></em></span><span class="No-Break"><st c="37192">):</st></span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B21795_12_3.jpg" alt="Figure 12.3: The SpamClassifier project window"/><st c="37195"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="37585">Figure 12.3: The SpamClassifier project window</st></p>
			<p><st c="37631">In </st><span class="No-Break"><em class="italic"><st c="37635">Figure 12</st></em></span><em class="italic"><st c="37644">.3</st></em><st c="37646">, we can see the </st><strong class="source-inline"><st c="37663">SpamClassifier</st></strong><st c="37677"> project window. </st><st c="37694">The project window is the main window </st><a id="_idIndexMarker788"/><st c="37732">where we will build our model. </st><st c="37763">Let’s go over the different </st><span class="No-Break"><st c="37791">window components:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="37809">Left pane</st></strong><st c="37819">: The left pane lists the project’s different sources – the ML model and its data sources, used for training </st><span class="No-Break"><st c="37929">and testing</st></span></li>
				<li><strong class="bold"><st c="37940">Settings tab</st></strong><st c="37953">: The </st><strong class="bold"><st c="37960">Settings</st></strong><st c="37968"> tab is where we define the different data sources for the various phases and general </st><span class="No-Break"><st c="38054">training parameters</st></span></li>
				<li><strong class="bold"><st c="38073">Training tab</st></strong><st c="38086">: The </st><strong class="bold"><st c="38093">Training</st></strong><st c="38101"> tab shows the progress of the </st><span class="No-Break"><st c="38132">training operation</st></span></li>
				<li><strong class="bold"><st c="38150">Evaluation tab</st></strong><st c="38165">: The </st><strong class="bold"><st c="38172">Evaluation</st></strong><st c="38182"> tab shows the performance of our model in the </st><span class="No-Break"><st c="38229">different phases</st></span></li>
				<li><strong class="bold"><st c="38245">Preview tab</st></strong><st c="38257">: We can </st><em class="italic"><st c="38267">play</st></em><st c="38271"> with our ML model and experience it in the </st><span class="No-Break"><strong class="bold"><st c="38315">Preview</st></strong></span><span class="No-Break"><st c="38322"> tab</st></span></li>
				<li><strong class="bold"><st c="38326">Output tab</st></strong><st c="38337">: The </st><strong class="bold"><st c="38344">Output</st></strong><st c="38350"> tab is the place where we can deploy </st><span class="No-Break"><st c="38388">our model</st></span></li>
			</ul>
			<p><st c="38397">The list of the components reflects the phases we must go through when we build </st><span class="No-Break"><st c="38478">our model.</st></span></p>
			<p><st c="38488">Now that we know what Create ML is, let’s start building </st><span class="No-Break"><st c="38546">our model.</st></span></p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor397"/><st c="38556">Building our Spam Classifier model</st></h2>
			<p><st c="38591">Our Spam Classifier </st><a id="_idIndexMarker789"/><st c="38612">model-building process is based on three </st><a id="_idIndexMarker790"/><st c="38653">data sources – training, validation, and testing data. </st><st c="38708">These three data sources are something we covered earlier in this chapter in the </st><em class="italic"><st c="38789">Training the </st></em><span class="No-Break"><em class="italic"><st c="38802">model</st></em></span><span class="No-Break"><st c="38807"> section.</st></span></p>
			<p><st c="38816">First, let’s take a look at how we will prepare </st><span class="No-Break"><st c="38865">our data.</st></span></p>
			<h3><st c="38874">Preparing our data</st></h3>
			<p><st c="38893">Since we are building a </st><a id="_idIndexMarker791"/><st c="38918">Spam Classifier model, we must prepare a dataset containing both spam and non-spam messages. </st><st c="39011">The text classification template requires our dataset to be in the form of a CSV file with two columns – </st><strong class="source-inline"><st c="39116">text</st></strong><st c="39120"> and </st><strong class="source-inline"><st c="39125">label</st></strong><st c="39130">. In our case, the </st><strong class="source-inline"><st c="39149">text</st></strong><st c="39153"> column represents the content of the SMS message, and the </st><strong class="source-inline"><st c="39212">label</st></strong><st c="39217"> column is the classification – </st><strong class="source-inline"><st c="39249">true</st></strong><st c="39253"> for a spam message and </st><strong class="source-inline"><st c="39277">false</st></strong> <span class="No-Break"><st c="39282">for non-spam.</st></span></p>
			<p><st c="39296">The ratio between the spam and the non-spam messages needs to reflect the real-world distribution. </st><st c="39396">In our case, we have a dataset file with 300,000 records, where 10% of them are spam messages and 90% are </st><span class="No-Break"><st c="39502">non-spam messages.</st></span></p>
			<p><st c="39520">To set the </st><strong class="bold"><st c="39532">training dataset</st></strong><st c="39548">, we can drag the</st><a id="_idIndexMarker792"/><st c="39565"> CSV file into the </st><strong class="bold"><st c="39584">Training Data</st></strong><st c="39597"> box (</st><span class="No-Break"><em class="italic"><st c="39603">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="39613">.4</st></em></span><span class="No-Break"><st c="39615">):</st></span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B21795_12_4.jpg" alt="Figure 12.4: Training Data with 300,000 records"/><st c="39618"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="39986">Figure 12.4: Training Data with 300,000 records</st></p>
			<p><span class="No-Break"><em class="italic"><st c="40033">Figure 12</st></em></span><em class="italic"><st c="40043">.4</st></em><st c="40045"> shows the </st><strong class="bold"><st c="40056">Settings</st></strong><st c="40064"> tab, with the training data now containing 300,000 records with 2 classes. </st><st c="40140">The classes are </st><strong class="source-inline"><st c="40156">true</st></strong><st c="40160"> and </st><strong class="source-inline"><st c="40165">false</st></strong><st c="40170">, as stated earlier. </st><st c="40191">In addition, we also have a new </st><a id="_idIndexMarker793"/><st c="40223">data source in the left pane – the file we imported as the </st><span class="No-Break"><st c="40282">training dataset.</st></span></p>
			<p><st c="40299">We can handle the </st><strong class="bold"><st c="40318">validation data</st></strong><st c="40333"> now that</st><a id="_idIndexMarker794"/><st c="40342"> we have the training data. </st><st c="40370">As a reminder, as part of the training process, we will use the validation data to tune the model. </st><st c="40469">We can provide our own validation data, but Create ML allows us to split it from the training dataset we’ve </st><span class="No-Break"><st c="40577">just supplied.</st></span></p>
			<p><st c="40591">The third dataset is the </st><strong class="bold"><st c="40617">testing data</st></strong><st c="40629">. We use the testing data to see how the model classifies unseen text. </st><st c="40700">We can add the</st><a id="_idIndexMarker795"/><st c="40714"> testing dataset later in the </st><span class="No-Break"><st c="40744">evaluation step.</st></span></p>
			<p><st c="40760">Apart from choosing the different datasets, we can also set the number of iterations our training will go through and the </st><span class="No-Break"><st c="40883">model algorithm.</st></span></p>
			<p><st c="40899">With each iteration, the training process can tune itself by reviewing the errors from the previous iteration and adjusting its parameters (like weights in a neural network). </st><st c="41075">Our intuition may say that the more iterations we have, the more our model will be smarter. </st><st c="41167">However, this is not so. </st><st c="41192">First, at some point, having another iteration stops improving the model and only consumes computational resources. </st><st c="41308">But the real problem is what we call overfitting. </st><strong class="bold"><st c="41358">Overfitting</st></strong><st c="41369"> is when an ML model learns the</st><a id="_idIndexMarker796"/><st c="41400"> training data too well, including its noise. </st><st c="41446">In this case, there will be issues with analyzing </st><span class="No-Break"><st c="41496">unseen data.</st></span></p>
			<p><st c="41508">Another parameter is the model algorithm (</st><span class="No-Break"><em class="italic"><st c="41551">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="41561">.5</st></em></span><span class="No-Break"><st c="41563">):</st></span></p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B21795_12_5.jpg" alt="Figure 12.5: Choosing the model algorithm"/><st c="41566"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="41788">Figure 12.5: Choosing the model algorithm</st></p>
			<p><span class="No-Break"><em class="italic"><st c="41829">Figure 12</st></em></span><em class="italic"><st c="41839">.5</st></em><st c="41841"> shows the </st><a id="_idIndexMarker797"/><st c="41852">pop-up menu where we can choose the model learning algorithm from five different options. </st><st c="41942">The algorithm overview is not in this chapter’s scope, but in short, different algorithms are suitable for different needs and consume other resources. </st><st c="42094">For example, the </st><strong class="bold"><st c="42111">BERT</st></strong><st c="42115"> algorithm is ideal for semantic understanding, and</st><a id="_idIndexMarker798"/><st c="42166"> the </st><strong class="bold"><st c="42171">Conditional Random Field</st></strong><st c="42195"> is great for </st><a id="_idIndexMarker799"/><st c="42209">sequence labeling. </st><st c="42228">In our case, we will choose the </st><strong class="bold"><st c="42260">Maximum Entropy</st></strong><st c="42275"> algorithm, which is excellent </st><span class="No-Break"><st c="42306">for classification.</st></span></p>
			<p><st c="42325">Now that we have all our datasets ready, we can click the </st><strong class="bold"><st c="42384">Train</st></strong><st c="42389"> button in the top-left corner and start </st><span class="No-Break"><st c="42430">our training.</st></span></p>
			<h3><st c="42443">Performing the training</st></h3>
			<p><st c="42467">Now, we have arrived at the </st><a id="_idIndexMarker800"/><st c="42496">main dish – the training phase. </st><st c="42528">In the training phase, the Create ML app goes over the training dataset using the algorithm we defined in the </st><em class="italic"><st c="42638">Preparing our data</st></em><st c="42656"> section. </st><st c="42666">Let’s try to describe </st><span class="No-Break"><st c="42688">that process:</st></span></p>
			<ul>
				<li><st c="42701">In each iteration, the model </st><em class="italic"><st c="42731">verifies itself</st></em><st c="42746"> using the validation dataset. </st><st c="42777">Remember that the validation dataset can be distinct. </st><st c="42831">However, by default, it is a subset of the </st><span class="No-Break"><st c="42874">training dataset.</st></span></li>
				<li><st c="42891">The </st><em class="italic"><st c="42896">duration</st></em><st c="42904"> of the training phase is derived from three major factors – the dataset size, the number of iterations, and the </st><span class="No-Break"><st c="43017">chosen algorithm.</st></span></li>
				<li><st c="43034">The model doesn’t have to perform the number of iterations we defined in the </st><strong class="bold"><st c="43112">Settings</st></strong><st c="43120"> tab. </st><st c="43126">If the validation accuracy reaches a high level, </st><em class="italic"><st c="43175">the training will stop earlier</st></em><st c="43205"> to save resources and </st><span class="No-Break"><st c="43228">avoid overfitting.</st></span></li>
			</ul>
			<p><st c="43246">At the end of the training</st><a id="_idIndexMarker801"/><st c="43273"> process, we’ll see the following graph (</st><span class="No-Break"><em class="italic"><st c="43314">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="43324">.6</st></em></span><span class="No-Break"><st c="43326">):</st></span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B21795_12_6.jpg" alt="Figure 12.6: The Training tab at the end of the training process"/><st c="43329"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="43660">Figure 12.6: The Training tab at the end of the training process</st></p>
			<p><span class="No-Break"><em class="italic"><st c="43724">Figure 12</st></em></span><em class="italic"><st c="43734">.6</st></em><st c="43736"> shows how well we did in our training phase. </st><st c="43782">We can see that we have reached a high accuracy after only two iterations. </st><st c="43857">In this case, it is because our training dataset is well-structured and reliable. </st><st c="43939">However, that won’t always be the case, so we need patience in </st><span class="No-Break"><st c="44002">this step.</st></span></p>
			<p><st c="44012">Now that our model has been trained, we need to test it. </st><st c="44070">To do that, we will use our test dataset as part of the evaluation step (</st><span class="No-Break"><em class="italic"><st c="44143">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="44153">.7</st></em></span><span class="No-Break"><st c="44155">):</st></span></p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B21795_12_7.jpg" alt="Figure 12.7: The model evaluation step"/><st c="44158"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="44654">Figure 12.7: The model evaluation step</st></p>
			<p><span class="No-Break"><em class="italic"><st c="44692">Figure 12</st></em></span><em class="italic"><st c="44702">.7</st></em><st c="44704"> shows the evaluation step and the different datasets used to train and validate the model. </st><st c="44796">We can also see that the testing data contains a dataset of 1,000 items. </st><st c="44869">The testing dataset </st><a id="_idIndexMarker802"/><st c="44889">structure is similar to the training and validation datasets. </st><st c="44951">Tapping on the </st><strong class="bold"><st c="44966">Test</st></strong><st c="44970"> button runs the classification on all the 1,000 items in the dataset and measures their classification accuracy. </st><st c="45084">Let’s see the test result (</st><span class="No-Break"><em class="italic"><st c="45111">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="45121">.8</st></em></span><span class="No-Break"><st c="45123">):</st></span></p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B21795_12_8.jpg" alt="Figure 12.8: The evaluation results"/><st c="45126"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="45602">Figure 12.8: The evaluation results</st></p>
			<p><span class="No-Break"><em class="italic"><st c="45637">Figure 12</st></em></span><em class="italic"><st c="45647">.8</st></em><st c="45649"> presents some</st><a id="_idIndexMarker803"/><st c="45663"> terms that we need to be familiar with if we want to understand </st><span class="No-Break"><st c="45728">the report:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="45739">Precision</st></strong><st c="45749">: Precision is the percentage of all messages that the model identified as </st><strong class="source-inline"><st c="45825">true</st></strong><st c="45829"> or </st><strong class="source-inline"><st c="45833">false</st></strong><st c="45838"> (depending on the specific class) and that were correct. </st><st c="45896">For example, 93% precision for the </st><strong class="source-inline"><st c="45931">false</st></strong><st c="45936"> class means that 93% of all the messages the model identifies as </st><strong class="source-inline"><st c="46002">false</st></strong><st c="46007"> were </st><span class="No-Break"><st c="46013">actually </st></span><span class="No-Break"><strong class="source-inline"><st c="46022">false</st></strong></span><span class="No-Break"><st c="46027">.</st></span></li>
				<li><strong class="bold"><st c="46028">Recall</st></strong><st c="46035">: Recall is the counterpart to precision. </st><st c="46078">A recall of 93% for the </st><strong class="source-inline"><st c="46102">true</st></strong><st c="46106"> class means that the model correctly identified 93% of all actual </st><span class="No-Break"><st c="46173">spam messages.</st></span></li>
				<li><strong class="bold"><st c="46187">F1 Score</st></strong><st c="46196">: The F1 score is the balance between precision </st><span class="No-Break"><st c="46245">and recall.</st></span></li>
			</ul>
			<p><st c="46256">The </st><strong class="bold"><st c="46261">F1 Score</st></strong><st c="46269"> involves more than just measuring a model’s accuracy. </st><st c="46324">It balances two important metrics – </st><strong class="bold"><st c="46360">precision</st></strong><st c="46369"> and </st><strong class="bold"><st c="46374">recall</st></strong><st c="46380"> – and reflects a better model performance measurement. </st><st c="46436">In our case, a score of 0.96 is considered a very </st><span class="No-Break"><st c="46486">high performance.</st></span></p>
			<p><st c="46503">Our next tab is </st><strong class="bold"><st c="46520">Preview</st></strong><st c="46527">, where we can play within a playground zone (</st><span class="No-Break"><em class="italic"><st c="46573">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="46583">.9</st></em></span><span class="No-Break"><st c="46585">):</st></span></p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B21795_12_9.jpg" alt="Figure 12.9: The Preview tab"/><st c="46588"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="46760">Figure 12.9: The Preview tab</st></p>
			<p><span class="No-Break"><em class="italic"><st c="46788">Figure 12</st></em></span><em class="italic"><st c="46798">.9</st></em><st c="46800"> shows our model’s </st><strong class="bold"><st c="46819">Preview</st></strong><st c="46826"> tab, with an example message that says, </st><strong class="bold"><st c="46867">Call now to get an invite</st></strong><st c="46892">. Our </st><a id="_idIndexMarker804"/><st c="46898">model identified this message as spam with a 92% confidence. </st><span class="No-Break"><st c="46959">Good job!</st></span></p>
			<p><st c="46968">Now, let’s see how we can deploy </st><span class="No-Break"><st c="47002">our model.</st></span></p>
			<h3><st c="47012">Deploying our model</st></h3>
			<p><st c="47032">There’s no point in </st><a id="_idIndexMarker805"/><st c="47053">having a great training process if we can’t deploy it in Xcode. </st><st c="47117">This is why we have the </st><strong class="bold"><st c="47141">Output</st></strong><st c="47147"> tab (</st><span class="No-Break"><em class="italic"><st c="47153">Figure 12</st></em></span><span class="No-Break"><em class="italic"><st c="47163">.10</st></em></span><span class="No-Break"><st c="47166">):</st></span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B21795_12_10.jpg" alt="Figure 12.10: The Create ML Output tab"/><st c="47169"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="47903">Figure 12.10: The Create ML Output tab</st></p>
			<p><st c="47941">The </st><strong class="bold"><st c="47946">Output</st></strong><st c="47952"> tab shows a summary of our model, including a new detail we haven’t seen until now – the </st><span class="No-Break"><st c="48042">model size.</st></span></p>
			<p><st c="48053">More importantly, the </st><strong class="bold"><st c="48076">Output</st></strong><st c="48082"> tab also contains the option to export a model or open it in Xcode. </st><st c="48151">Clicking the </st><strong class="bold"><st c="48164">Get</st></strong><st c="48167"> button</st><a id="_idIndexMarker806"/><st c="48174"> allows us to save the model locally in a file with an </st><span class="No-Break"><strong class="source-inline"><st c="48229">mlmodel</st></strong></span><span class="No-Break"><st c="48236"> extension.</st></span></p>
			<p><st c="48247">To use the </st><strong class="source-inline"><st c="48259">mlmodel</st></strong><st c="48266"> extension in our projects, we’ll need to use Core ML. </st><st c="48321">That’s our </st><span class="No-Break"><st c="48332">next topic.</st></span></p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor398"/><st c="48343">Using our model with Core ML</st></h2>
			<p><st c="48372">The </st><strong class="bold"><st c="48377">Core ML framework</st></strong><st c="48394">’s goal is to allow us</st><a id="_idIndexMarker807"/><st c="48417"> to integrate ML models into </st><span class="No-Break"><st c="48446">our projects.</st></span></p>
			<p><st c="48459">Our first step is to add the </st><strong class="source-inline"><st c="48489">mlmodel</st></strong><st c="48496"> file that we saved from the Create ML application to Xcode. </st><st c="48557">We can do that by dragging the file to the project navigator </st><span class="No-Break"><st c="48618">in Xcode.</st></span></p>
			<p><st c="48627">The main class in the Core ML framework we will use is </st><strong class="source-inline"><st c="48683">MLModel</st></strong><st c="48690">, which represents a ML model loaded into the system. </st><st c="48744">To</st><a id="_idIndexMarker808"/><st c="48746"> load our Spam Classifier model, we initialize the model in </st><span class="No-Break"><st c="48806">our code:</st></span></p>
			<pre class="source-code"><st c="48815">
class MessageClassifier {
    let model: MLModel
    init(configuration: MLModelConfiguration =
      MLModelConfiguration()) throws {
        </st><strong class="bold"><st c="48937">model = try SpamClassifier(configuration:</st></strong>
<strong class="bold"><st c="48978">          configuration).model</st></strong><st c="48999">
    }
}</st></pre>			<p><st c="49003">In the preceding code example, we created a new class, called </st><strong class="source-inline"><st c="49066">MessageClassifier</st></strong><st c="49083">, which encapsulates our ML integration with the Spam </st><span class="No-Break"><st c="49137">Classifier model.</st></span></p>
			<p><st c="49154">We then initiate the class, passing a new </st><strong class="source-inline"><st c="49197">MLModelConfiguration</st></strong><st c="49217">. This contains different options, but we can pass an empty instance at </st><span class="No-Break"><st c="49289">this stage.</st></span></p>
			<p><st c="49300">Our class also contains an </st><strong class="source-inline"><st c="49328">MLModel</st></strong><st c="49335"> instance. </st><st c="49346">To initiate the model instance, we use the </st><strong class="source-inline"><st c="49389">SpamClassifier</st></strong><st c="49403"> class, passing </st><span class="No-Break"><st c="49419">our configuration.</st></span></p>
			<p><st c="49437">But wait – where did the </st><strong class="source-inline"><st c="49463">SpamClassifier</st></strong><st c="49477"> class </st><span class="No-Break"><st c="49484">come from?</st></span></p>
			<p><st c="49494">When we added the Spam Classifier </st><strong class="source-inline"><st c="49529">mlmodel</st></strong><st c="49536"> file into our Xcode project, Core ML generated three interfaces – the </st><strong class="source-inline"><st c="49607">SpamClassifier</st></strong><st c="49621"> class, </st><strong class="source-inline"><st c="49629">SpamClassifierInput</st></strong><st c="49648">, </st><span class="No-Break"><st c="49650">and </st></span><span class="No-Break"><strong class="source-inline"><st c="49654">SpamClassifierOutput</st></strong></span><span class="No-Break"><st c="49674">.</st></span></p>
			<p><st c="49675">Now that we have our model, let’s write a function that can predict whether a message </st><span class="No-Break"><st c="49762">is spam:</st></span></p>
			<pre class="source-code"><st c="49770">
func prediction(text: String) throws -&gt; Bool {
        let input = SpamClassifierInput(text: text)
        if let result =  try? </st><st c="49883">model.prediction(from: input)
        {
            </st><strong class="bold"><st c="49915">let value = result.featureValue(for:</st></strong>
<strong class="bold"><st c="49951">              "label")!.stringValue</st></strong><st c="49973">
            return value == "true"
        }
        return false
    }</st></pre>			<p><st c="50013">In the preceding example, we created a </st><strong class="source-inline"><st c="50053">prediction</st></strong><st c="50063"> function that receives a text message as input and</st><a id="_idIndexMarker809"/><st c="50114"> returns </st><span class="No-Break"><st c="50123">a Boolean.</st></span></p>
			<p><st c="50133">It starts by creating a </st><strong class="source-inline"><st c="50158">SpamClassifierInput</st></strong><st c="50177"> instance with the text input. </st><st c="50208">Then, it generates a prediction result for this input by running the model’s </st><strong class="source-inline"><st c="50285">prediction()</st></strong><st c="50297"> function. </st><st c="50308">We then get the value from the feature, called </st><strong class="source-inline"><st c="50355">label</st></strong><st c="50360">, and compare it </st><span class="No-Break"><st c="50377">to </st></span><span class="No-Break"><strong class="source-inline"><st c="50380">true</st></strong></span><span class="No-Break"><st c="50384">.</st></span></p>
			<p><st c="50385">This code example demonstrates how to easily use a custom ML model in our </st><span class="No-Break"><st c="50460">Xcode projects.</st></span></p>
			<p><st c="50475">Now, let’s try to understand if using a custom ML in our Xcode is </st><span class="No-Break"><st c="50542">that simple.</st></span></p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor399"/><st c="50554">Where to go from here</st></h2>
			<p><st c="50576">The Core ML part of this book is unique. </st><st c="50618">In most cases, I have simplified complex topics to make them more accessible for developers. </st><st c="50711">However, I think the Core ML topic </st><span class="No-Break"><st c="50746">is different.</st></span></p>
			<p><st c="50759">ML is a broad topic, beyond the scope of this chapter. </st><st c="50815">Furthermore, it is a complex topic. </st><st c="50851">Training is more than just delivering datasets. </st><st c="50899">It is essential to understand the dataset mix between the different classes, pick the correct algorithm, and read the evaluation </st><span class="No-Break"><st c="51028">results carefully.</st></span></p>
			<p><st c="51046">And remember that the model we created is a custom. </st><st c="51099">This means that we don’t have any control over how its algorithm works and need to observe and fine-tune it </st><span class="No-Break"><st c="51207">over time.</st></span></p>
			<p><st c="51217">In summary, ML is a complex topic, and if we want to enter this area, we need to approach it more in-depth than reading </st><span class="No-Break"><st c="51338">15 pages.</st></span></p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor400"/><st c="51347">Summary</st></h1>
			<p><st c="51355">This was a long but fascinating chapter about one of the most exciting contemporary topics – ML </st><span class="No-Break"><st c="51452">and AI.</st></span></p>
			<p><st c="51459">We reviewed the basics of AI and ML, understanding what it means to train an ML model. </st><st c="51547">We explored the built-in ML models in the iOS SDK, including NLP, analyzing images using the Vision framework, and classifying audio with the Sound Analysis framework. </st><st c="51715">We learned how to add semantic search capabilities to the Core Spotlight framework, and if that wasn’t enough, we also learned how to create and integrate custom ML models into </st><span class="No-Break"><st c="51892">our projects.</st></span></p>
			<p><st c="51905">Now, we can add some intelligence features to </st><span class="No-Break"><st c="51952">our apps!</st></span></p>
			<p><st c="51961">Speaking of intelligence, our next chapter discusses how we can integrate Siri using App Intents. </st><st c="52060">The ML phase is not over </st><span class="No-Break"><st c="52085">just yet!</st></span></p>
		</div>
	<div id="charCountTotal" value="52094"/></body></html>