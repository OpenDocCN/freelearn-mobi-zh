- en: Using CoreML and Vision in Swift
  prefs: []
  type: TYPE_NORMAL
- en: The Swift programming language has come a long way since its first introduction,
    and in comparison to many other programming languages, it's still well within
    its infancy.
  prefs: []
  type: TYPE_NORMAL
- en: However, with this in mind, with every release of Swift and its place in the
    open source community, we've seen it grow from strength to strength over such
    a short period of time. We already covered server-side Swift back in [Chapter
    8](9ce1feb3-8fca-4656-91dc-796ba77c3d07.xhtml), *Server-Side Swift*, another evolution
    that was again fueled by the open source community.
  prefs: []
  type: TYPE_NORMAL
- en: Another fast-moving train is that of machine learning, once again driven by
    the power of the community, and recognized giants in the industry, such as TensorFlow,
    now support the Swift programming language.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we're going to look at Apple's offering for machine learning
    – CoreML – and how we can build an app using Swift to read and process machine
    learning models, giving us intelligent image recognition.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also take a look at Apple's Vision Framework and how it works alongside
    CoreML to allow us to process video being streamed to our devices in real time,
    recognizing objects on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Building an image capture app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CoreML models to detect objects in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a video capture app
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CoreML and the Vision Framework to detect objects in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files present in this chapter on GitHub at [https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11)
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/2NmP961](https://bit.ly/2NmP961)'
  prefs: []
  type: TYPE_NORMAL
- en: Building an image capture app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this first recipe, we're going to create an app that captures either an image
    from your camera roll or an image taken from your camera. This will set up our
    iOS app ready for us to incorporate CoreML to detect objects in our photos.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you'll need the latest version of Xcode available from the
    Mac App Store.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With Xcode open, let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new project in Xcode. Go to **File** | **New** | **Project** | **iOS
    App**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In `Main.storyboard`, add the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Add `UISegmentedControl` with two options (**Photo / Camera Roll** and **Live
    Camera**).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Next, add a `UILabel` view just underneath.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Add a `UIImageView` view beneath that.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, add a `UIButton` component.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Space these accordingly using AutoLayout constraints with `UIImageView` being
    the prominent object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/a5c4c4b3-3373-46d5-ac77-73cc7b895406.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Camera/photo app
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have this in place, let''s hook these up to our `ViewController.swift`
    file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Take note that in the preceding, we have two `IBOutlet` and one `IBAction` (we
    don't need an outlet for `UIButton`, we just care about its action).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, populate `IBAction` with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s create an extension of `UIViewController`. You can do this at the
    bottom of the `ViewController` class if you like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Our extension needs to conform to the `UIImagePickerControllerDelegate` and
    **`UINavigationControllerDelegate`** protocols. We can now go ahead and populate
    our extension with the following delegate method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we go any further, we''ll need to add a couple of lines to our `info.plist`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '9\. Add these in with the following string description: `Chapter 11 wants to
    detect cook Stuff`. This is an iOS security feature that will prompt the user
    when any app/code tries to access the camera, photo library, or location services.
    Failure to add this in could result in an app crash.'
  prefs: []
  type: TYPE_NORMAL
- en: For our app, we can add whatever we want, but for a production app, make sure
    the text you enter is useful and informative to the user. Apple will check this
    when reviewing your app and has been known to potentially block a release until
    this is resolved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and run your code, and then launch the app. One of the following things
    should happen:'
  prefs: []
  type: TYPE_NORMAL
- en: If you are running the app from the simulator, our `UIButton` press should present
    the photo picker (along with the default images supplied by the iOS simulator).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are running from a device, then you should be presented with the camera
    view, allowing you to capture a photo.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Either way, whether a photo was selected or a picture was taken, the resulting
    image should show in `UIImageView`!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s step through what we''ve just done. We''ll begin at `IBAction` and have
    a look at the `UIPickerView` view we''ve created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go through this one line at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: We instantiate an instance of `UIImagePickerController` – an available API that
    will allow us to choose an image based on a specific source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set the delegate as `self`, so we can harness any results or actions caused
    by `UIImagePickerController`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We set `allowEditing` to `false`, which is used to hide controls when the camera
    is our source.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this instance, we set the source type based on whether the camera is available
    or not (so it works well with the simulator).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we present our view controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at our delegate methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The first is pretty self-explanatory; `imagePickerControllerDidCancel` handles
    any instances where `UIImagePickerController` is canceled by the users. In our
    case, we just dismiss the instance returned – job done!
  prefs: []
  type: TYPE_NORMAL
- en: '`didFinishPickingMediaWithInfo` is where interesting things happen. Notice
    how we are given a dictionary of **info** in our response. Here, we have various
    segments of information. The one we are looking for is under the `UIImagePickerController.InfoKey.originalImage`
    key. This gives us a `UIImage` of what we''ve just selected, allowing us to assign
    this straight back to `UIImageView`.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we've got an app that allows us to take or choose a photo, we can apply
    it to some real work with the power of CoreML and object detection.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A quick note to mention: you''ll also have noticed that we were required to
    conform our extension to `UINavigationControllerDelegate`. This is required by
    iOS to allow `UIImageContoller` to be handled and presented correctly from its
    "presenting" stack (`ViewController` in our instance).'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on `UIImagePickerController`, refer to [https://developer.apple.com/documentation/uikit/uiimagepickercontroller](https://developer.apple.com/documentation/uikit/uiimagepickercontroller).
  prefs: []
  type: TYPE_NORMAL
- en: Using CoreML models to detect objects in images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we'll take the app we just built and incorporate the CoreML
    framework in order to detect objects in our images.
  prefs: []
  type: TYPE_NORMAL
- en: We'll also take a look at the generated CoreML models available for us to use
    and download directly from Apple's Developer portal.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this recipe, you'll need the latest version of Xcode available from the
    Mac App Store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, head on over to the Apple Developer portal at the following address:
    [https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you will find out a little bit more about the models available for us
    to download and use in our Xcode project.
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice there are options for image models and text models. For this recipe,
    we're going to be using image models, specifically one called Resnet50, which
    uses a residual neural network that attempts to identify and classify what it
    perceives to be the dominant object in an image.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the different types of machine learning models, see
    the links in the *See also* section at the end of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: From here, download the Resnet50.mlmodel (32-bit) model. If you are having trouble
    downloading the file, you can just take a copy from the sample project in our
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Once downloaded, add this to your Xcode project by simply dragging it into the
    file explorer tree in our previous app.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s make a start where we left off in our previous project:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With everything in place, head back into `ViewController.swift` and add the
    following global variable and addition to our `viewDidLoad()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Now, head on over to the sample project and obtain a file called `ImageHelpers.swift`;
    add this to our project. Once this has been added, we'll head on back over to
    our `didFinishPickingMediaWithInfo` delegate and expand on this a little further.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add in the following highlighted changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: With everything in place, run the app and select a photo. As long as you didn't
    point it at a blank wall, you should be seeing some interesting feedback.
  prefs: []
  type: TYPE_NORMAL
- en: With all that in place, let's break down the changes we just made to understand
    what just happened a little more.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing is to take a look at the following line we added in:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we added in a call to a helper method we took from our sample project.
    This helper contains the following two functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: These functions and what they do are a little out of the scope of this book,
    and this chapter in particular. However, at a very high level, the first function,
    `processImageData()`, takes an instance of `UIImage` and transforms this to `CVPixelBuffer`
    format.
  prefs: []
  type: TYPE_NORMAL
- en: This essentially returns the `UIImage` object back to its raw format that it
    was captured in (`UIImage` is merely a UIKit wrapper for our true raw image).
  prefs: []
  type: TYPE_NORMAL
- en: During this process, we need to flip the orientation too as with all captured
    images. This is almost certainly in landscape mode (and more often than not, you've
    taken a picture or selected a photo in portrait mode).
  prefs: []
  type: TYPE_NORMAL
- en: Another reason for performing this is that our ResNet50 model is trained to
    observe images at only 224 x 224\. So, we need to readjust the captured image
    to this size.
  prefs: []
  type: TYPE_NORMAL
- en: If you need more information on the model you have in your project, simply select
    the file in the file explorer and view the details in the main window. From here,
    the Predictions tab will give you all the details you need about the input file
    required.
  prefs: []
  type: TYPE_NORMAL
- en: So, with our helper function implemented, we receive a new `UIImage` object
    (modified to our new spec) and the image in `CVPixelBuffer` format, all ready
    to pass over to CoreML for processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, I've highlighted some areas of interest. First is our
    `prediction()` function call on our `model` object. Here, we pass in our image
    in the `CVPixelBuffer` format we got back from our helper method earlier. From
    this, wrapped in a `try` statement, CoreML will now attempt to detect an object
    in the photo. If successful, we'll exit our `guard` statement gracefully and be
    able to access the properties available in our `prediction` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take a look at the properties available in our ResNet50 model, you''ll
    see the various options we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The class label we've already seen, but the class label probability will return
    us a dictionary of the most likely category for our image with a value based around
    a confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: Each model will have its own set of properties based on its desired intention
    and how it's been built.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At the beginning of this section, we obtained a model that allowed us to detect
    objects in our images. Touching on this subject a little more, models are a set
    of data that has been trained to identify a pattern or characteristics of a certain
    description.
  prefs: []
  type: TYPE_NORMAL
- en: For example, we want a model that detects cats; so, we train our model by feeding
    it images of around 10,000 various pictures of cats. Our model training will identify
    features and shapes common to each other and categorize them accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: When we then feed our model an image of a cat, we hope that it is able to pick
    up those categorized features within our image and successfully identify the cat.
  prefs: []
  type: TYPE_NORMAL
- en: The more images you train with, the greater the performance; however, that still
    depends on the integrity of the images too. Training with the same image of a
    cat (just in a different pose) 1,000 times might give you the same results as
    if you take 10,000 images of the same cat (again in a different pose).
  prefs: []
  type: TYPE_NORMAL
- en: The same goes the other way too; if you train with 500,000 images of a panther
    and then 500,000 images of a kitten, it's just not going to work.
  prefs: []
  type: TYPE_NORMAL
- en: Away from CoreML, you are now able to train a model using Swift with TensorFlow.
    TensorFlow is a Google product that is leading the way in terms of machine learning
    and with an ever-growing community of developers behind it coupled with Swift's
    own open source community. Advancement in this particular technology is certainly
    looking bright.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information, please refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Apple CoreML documentation: [https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TensorFlow Swift: [https://www.tensorflow.org/swift/tutorials/model_training_walkthrough](https://www.tensorflow.org/swift/tutorials/model_training_walkthrough)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a video capture app
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, what we have seen so far of CoreML is pretty neat, to say the least. But
    taking a look back over this chapter so far, we have probably spent more time
    building our app to harness the power of CoreML than actually implementing it.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we're going to take our app a little further by streaming a
    live camera feed that in turn will allow us to intercept each frame and detect
    objects in real time.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this section, you'll need the latest version of Xcode available from the
    Mac App Store.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that for this section, you'll need to be connected to a real device
    for this to work. Currently, the iOS simulator does not have a way to emulate
    the front or back camera.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s begin:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Head over to our `ViewContoller.swift` file and make the following amendments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a function called `setupCaptureSession()` and we''ll start by adding
    in the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we are checking our device for an available camera, specifically
    `.builtInWideAngleCamera` at the back.If no device can be found, our guard will
    fail.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we initialize `AVCaptureDeviceInput` with our new `videoDevice` object.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, continuing in our function, add the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Essentially, here we are attaching our device to a capture session, allowing
    us to stream what the device input (camera) is processing programmatically straight
    into our code. Now we just to point this at our view so that we can see the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following additional code to our function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: With the code we've just added, we are essentially creating a visible layer
    from our current capture session. In order for us to process this on our screen,
    we need to assign this to a `rootLayer` (our `CALayer` variable we added earlier).
    While this seems a little overkill and we could just add this to the layer of
    our `UIImageView`, we're prepping for something we need to do in our next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, with our camera and device all set up, it''s time to set the camera
    rolling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Go ahead and run the app. Note again that this will only work on a real device
    and not a simulator. All going well, you should have a live stream from your camera.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The best way to explain this would be to think of the capture session as a wrapper
    or a configuration between the device's hardware and software. The camera hardware
    has a lot of options, so we configure our capture session to pick out what we
    want for our particular instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look back at this line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, you could control the enum bases on a UI toggle, allowing the user to
    specify which camera to use. You could even use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Re-configure the session and then `startRunning()` again. Essentially (albeit
    at a much more complex level), this is what happens when you switch from the front
    to the back camera when taking a photo.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the session captured, we can now stream the output directly to any view
    we like just like we did here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'But the fun comes when we want to manipulate the image that is being streamed,
    by capturing them one frame at a time. We do this by implementing the `AVCaptureVideoDataOutputSampleBufferDelegate`
    protocol, which allows us to override the following delegate methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Notice something familiar here... we're being given `sampleBuffer`, just like
    we got in `UIImagePickerDelegate`. The difference here is that this will be called
    with every frame, not just when one is selected.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Playing around with capture sessions and `AVCaptureOutputs` is an expensive
    operation. Always make sure you stop your session from running when it's not needed,
    and make sure your delegates are not unnecessarily processing data when they don't
    need to be.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note is that the initialization of a capture device can in
    some instances be slow, so make sure you have the appropriate UI to handle the
    potential blocking it may cause.
  prefs: []
  type: TYPE_NORMAL
- en: 'Final note: if you are struggling with memory leaks and high CPU times, take
    a look at a suite of tools called Instruments. Bundles of Xcode Instruments can
    offer a wide range of performance tracing tools that can really help you to get
    the most out of your Swift code.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For more information, refer to the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instruments overview: [https://help.apple.com/instruments/mac/current/#/dev7b09c84f5](https://help.apple.com/instruments/mac/current/#/dev7b09c84f5)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AVFoundation: [https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using CoreML and the Vision Framework to detect objects in real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've seen what CoreML can do in terms of object detection, but taking everything
    we've done so far into account, we can certainly go a step further. Apple's Vision
    Framework offers a unique set of detection tools from landmark detection and face
    detection in images to tracking recognition.
  prefs: []
  type: TYPE_NORMAL
- en: With the latter, tracking recognition, the Vision Framework allows us to take
    models built with CoreML and use them in conjunction with CoreML's object detection
    to identify and track the object in question.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll take everything we've learned so far, from how AVFoundation
    works to implementing CoreML, and build a real-time object detection app using
    a device camera.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For this section, you'll need the latest version of Xcode available from the
    Mac App Store.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, head on over to the Apple Developer portal at the following address:
    [https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/).'
  prefs: []
  type: TYPE_NORMAL
- en: Here, you will find out a little bit more about the models available for us
    to download and use in our Xcode project. You'll notice there are options for
    image models or text models. For this recipe, we're going to be using image models,
    specifically one called **YOLOv3**, which uses a residual neural network that
    attempts to identify and classify what it perceives to be the dominant object
    in the image.
  prefs: []
  type: TYPE_NORMAL
- en: For more information on the different types of machine learning models, see
    the links in the *See also* section at the end of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: From here, download the YOLOv3.mlmodel (32-bit) model. If you are having trouble
    downloading the file, you can just take a copy from the sample project in our
    GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: Once downloaded, add this to your Xcode project by simply dragging it into the
    file explorer tree in our previous app.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We''ll start by creating a new `UIViewController` for all our vision work,
    in Xcode:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to File | New | File.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Choose **Cocoa Touch Class**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name this `VisionViewController`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Make this a subclass of `UIViewController`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With that done, we can now head on over to our new `VisionViewController` and
    add in the following highlighted code. We''ll start by importing the Vision Framework:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll subclass our existing `ViewController` so that we can get the best
    of both worlds (without the need for copious amounts of code duplication):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With that done, we can now override some of our functions in `ViewContoller.swift`.
    We''ll start with `setupCaptureSession()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: When overriding from another class, always remember to call the base function
    first. In the case of the preceding code, this can be done by calling `super.setupCaptureSession()`
    as highlighted.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll notice some functions in the ViewControler.swift file that we''ve not
    yet created. Let''s go through these now one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll add a detection layer to our `rootLayer` that we created earlier.
    This `CALayer` will be used as the drawing plane for our detected object area:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the code, we create its bounds based on the height and width
    taken from our `bufferSize` property (which is being shared back over in our `ViewController`
    class).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we need to add some geometry to `detectionLayer()`. This will re-adjust
    and scale the detection layer based on the device''s current geometry:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let''s hook up our `startVision()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: With this comes a new function, `visionResults()`. Go ahead and create this
    function in `VisionViewController` too.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We could have simply used an extension in our original `ViewController` to house
    all these new functions, but we'd run the risk of overloading our view controller
    to the point where it could become too unmaintainable. Also, our logic and extension
    for `UIImagePicker` was in here, so the separation is nice.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this, let''s build out `visionResults()` function. We''ll do this a section
    at a time so it all makes sense:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: We start with some basic housekeeping; performing `CATransaction` locks in memory
    any changes we're going to make to `CALayer`, before we finally commit them for
    use. In this code, we'll be modifying `detectionLayer`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we''ll iterate around our `results` parameter to pull out anything that
    is of class type `VNRecognizedObjectObservation`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: From this, we'll continue to use Vision to obtain the `Rect` and position of
    the identified object(s) using `VNImageRectForNormalizedRect`. We can also grab
    some text information about the objects detected and use that too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we''ll gracefully close off any changes to `detectionLayer` and update
    the geometry to match the detected objects. You''ll notice there are two new functions
    we''ve just introduced:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'These again are helper functions, one to draw the rectangle of the detected
    object and the other to write the text. These functions are generic boilerplate
    sample code that can be obtained from Apple''s documentation. Feel free to have
    a play around with these to suit your needs. One thing I will mention: you''ll
    notice how we do all this again using layers rather than adding `UIView` and `UILabel`.
    This again is because UIKit is a wrapper around a lot of core functionality. But
    adding a UIKit component on top of another component is unnecessary and with what
    is already an intense program, this could be performed much more efficiently by
    updating and manipulating the layers directly on a UIKit object.'
  prefs: []
  type: TYPE_NORMAL
- en: These objects can be found in the sample project on GitHub; just copy them into
    your project (either in `VisionViewController` or your own helper file).
  prefs: []
  type: TYPE_NORMAL
- en: 'With our AV Foundation camera streaming in place and Vison and CoreML ready
    to do their magic, there is one final override we need to add to our `VisionViewController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Using the delegate for AV Foundation, we grab each frame again, converting this
    to `CVPixelBuffer` in order to create `VNImageRequestHander`. This now kicks off
    the requests in our `startVision()` function, stitching everything together nicely.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''re almost done; let''s finish off with some bits and pieces to tie all
    this together now:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Head on over to `ViewController.swift` and add the following `IBAction` and
    logic from `UISegmentedControl` that we created earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, create a function called `startLivePreview()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Remove `captureSession.startRunning()` from `setupCaptureSession()`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, in our `Main.storyboard` view controller, change the class from `ViewController`
    to `VisionViewController`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, go ahead and run the app. All going well, you should be live-detecting
    images with an overlay that looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c44f615f-9831-4031-8041-8e91c52658ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Vision detection
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, both Vision and CoreML have successfully detected my cell phone
    and its location in the image (all in real time).
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A high-level overview goes something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: Capture a real-time camera feed (using AV Foundation).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a trained CoreML model to detect whether the image contains an object (that
    it recognizes).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use Vision to detect the position of the object in the picture.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We covered the camera streaming elements in the previous recipe, but let's take
    a deeper look at how *steps 2* and *3* work.
  prefs: []
  type: TYPE_NORMAL
- en: Let's actually start with *step 3*. We saw in the last section how we use `VNImageRequestHander`
    to pass back `CVPixelBuffer` of each image frame. This now fires off calls in
    our `setupVision()` function, so let's take a closer look in there.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we grab our model from the apps bundle so that we can pass this over
    to Vision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Next, we head back to *step 2*, where we create an instance of `VNCoreMLModel()`,passing
    in our `localModel`. With this `visionModel`, we can now create our `VNCoreMLRequest`
    call, along with its completion handler, which will fire from requests that come
    in via our AV Foundation delegate.
  prefs: []
  type: TYPE_NORMAL
- en: This one simple request does the work of both the Vision Framework and CoreML
    – first detecting whether an object is found, then supplying us with the details
    on where that object is located inside the image.
  prefs: []
  type: TYPE_NORMAL
- en: This is where the bulk of our work is done. If you look again at our `visionResults()`
    function and all the helper functions within, these are merely just ways of parsing
    data that has come back, and in turn, decorating our view.
  prefs: []
  type: TYPE_NORMAL
- en: In our "results" from the `VNCoreMLRequest()` response, we take an instance
    of `VNRecognizedObjectObservation`, which in turn gives us two properties, a label
    (of what CoreML thinks it has found) along with a confidence score.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For more information on `CALayer`, refer to [https://developer.apple.com/documentation/quartzcore/calayer](https://developer.apple.com/documentation/quartzcore/calayer).
  prefs: []
  type: TYPE_NORMAL
