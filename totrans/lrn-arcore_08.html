<html><head></head><body>
        

                            
                    <h1 class="header-title">Recognizing the Environment</h1>
                
            
            
                
<p class="mce-root">Throughout this book, we have looked at the numerous ways of how our device, with the help of ARCore, can track the user, understand the user's world, and render an alternate reality. ARCore uses the device's sensors and camera as inputs to constantly update what it perceives as the user's real world. However, what if we wanted to do more for the user; perhaps identify a certain object, sign, or landmark? That would require a much more advanced set of tools. Even just 5 years ago, this would seem like an incredibly daunting task. With the advent of OpenAI, thanks to Mr. Musk, many other companies have started to open source and make their tools available. This has led to phenomenal explosive growth in these technologies, colloquially referred to as <strong>Machine Learning</strong> (<strong>ML</strong>), and broadened their accessibility to everyone. Fortunately, for those interested in developing AR apps, this is a good thing. We want all the help we can get when it comes to recognizing and understanding the user's environment.</p>
<p>For this chapter, we will introduce ML and explore how we can use it to create better AR apps for our users. In this chapter, we will cover the following topics:</p>
<ul>
<li>Introduction to ML</li>
<li>Deep reinforcement learning</li>
<li>Programming a neural network</li>
<li>Training a neural network</li>
<li>TensorFlow</li>
</ul>
<p>Machine Learning is a very advanced subject that can take years of study in order to master. However, for our purposes, we will learn some basic techniques, which the reader can extend on later, either through more learning or implementing their own solution.</p>
<p>If you already have an in-depth understanding of neural networks, convolutional neural networks, and TensorFlow, feel free to breeze over this chapter.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Introduction to ML</h1>
                
            
            
                
<p>Machine Learning is a term widely used to refer to artificial intelligence and related computer predictive analytical models. The name Machine Learning, while perhaps overly generalized, fits better than the term AI. However, Machine Learning is itself such a broad term that it perhaps needs some further explanation and clarification. A machine obviously refers to a computer, or other device and learning tends to denote an algorithm or model that will evolve or learn over time. However, this is often not the case in many Machine Learning models. Therefore, for our purposes, we will use the broader term of Machine Learning to refer to any tool or algorithm that can be trained to recognize the environment or parts of the environment in AR, thus allowing us, the developers, to better augment our user's world.</p>
<p>Data science and Machine Learning go hand in hand. Data science is all about making sense of data, extracting patterns, and making predictions. In essence, when you start writing Machine Learning models in order to recognize objects or the environment, you are really just analyzing data, which means you can also, very loosely, call yourself a data scientist.</p>
<p>Machine Learning is a big area and is only getting bigger every day, so let's break down the specific problems we would like ML to help us with:</p>
<ul>
<li><strong>Target detection</strong>: Targets have been used in AR for some time. It has been the primary tracking and reference point for many AR apps previous to ARCore.</li>
<li><strong>Image recognition</strong>: This spawns into a whole set of sub-applications, all of which we will deal with in detail later.</li>
<li><strong>Object detection</strong>: Being able to detect an object in 3D from point cloud data is no easy feat, but it has been done and is getting better.</li>
<li><strong>Face detection</strong>: Detecting a person's face in an image has been around for years and has been used to great effect in many apps.</li>
<li><strong>Person detection</strong>: Detecting people or motion has great possibilities. Think Kinect comes to AR.</li>
<li><strong>Hand</strong>/<strong>Gesture detection</strong>: Not to be confused with touch gestures. This is where we detect a user's hand motions or gestures in front of a device's camera.</li>
<li><strong>Pose detection on object</strong>: Related to object detection, but now we also detect the position and orientation of the object.</li>
<li><strong>Light source detection</strong>: Being able to place realistic lights in a scene to make virtual object rendering more realistic. We already looked at the importance of lighting in <a href="edd56812-fcba-4f66-aa3e-e9cf9ee7b637.xhtml" target="_blank">Chapter 7</a>, <em>Light Estimation</em>.</li>
<li><strong>Environment detection</strong>: Recognizing the environment a user has moved into has great application in mapping buildings or other locations where GPS is unavailable, which applies to most internal spaces.</li>
</ul>
<p>Each of those problems may require different tools and techniques to solve those issues. In ML, it's not always about using the tool but the final answer and what works. Think about this as you build any ML you need for your app. Try a variety of ML tools and techniques; differences in size and performance of ML models can be critical, and it's something you need to consider.</p>
<p>A Machine Learning algorithm walks into a restaurant.<br/>
The waiter asks, "What will you have?<br/>
The algorithm says, "What's everyone else having?"<br/>
                                                                                        - Unknown</p>
<p>In the following table is a summary of the current major ML providers and the types of AR problems they can be used to solve:</p>
<table>
<tbody>
<tr>
<td rowspan="2"><strong>Toolset</strong></td>
<td rowspan="2"><strong>Pros/Cons</strong></td>
<td colspan="7">
<div><strong>Machine Learning task</strong></div>
</td>
</tr>
<tr>
<td><strong>Targets/Image</strong></td>
<td><strong>Object/Pose</strong></td>
<td><strong>Face</strong></td>
<td><strong>Person</strong></td>
<td><strong>Hand</strong></td>
<td><strong>Light</strong></td>
<td><strong>Environment</strong></td>
</tr>
<tr>
<td>Vuforia</td>
<td>Mature and easy to use. Requires internet connectivity.</td>
<td>Yes</td>
<td>Yes/Paid</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>XZIMG</td>
<td>Face and image/target tracking supported for Unity and other platforms.</td>
<td>Yes</td>
<td/>
<td>Yes</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>ARToolkit</td>
<td>Mature OpenSource platform for image tacking and feature detection.</td>
<td>Yes</td>
<td/>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>EasyAR</td>
<td>Pro license gets object and feature tracking.</td>
<td>Yes</td>
<td>Yes/Paid</td>
<td/>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>Google Face Detection API</td>
<td>Low level Android API.</td>
<td/>
<td/>
<td>Yes</td>
<td/>
<td/>
<td/>
<td/>
</tr>
<tr>
<td>OpenCV</td>
<td>A mature low-level API for Android, commercial version ported to Unity. Still requires low level knowledge.</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Coming</td>
<td>Coming</td>
</tr>
<tr>
<td>Google TensorFlow</td>
<td>Still in its infancy but quickly becoming the platform standard for CNN. Low level and advanced ML knowledge required.</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>Yes</td>
<td>coming</td>
<td>coming</td>
</tr>
<tr>
<td>Google ARCore</td>
<td>Currently, identifies planes, feature points, and light.</td>
<td/>
<td/>
<td/>
<td/>
<td>Yes</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p>We only included the main players who have built an AR platform for a mobile ARCore-supported device. Web technologies were omitted from this due to their limitations, although many of the mentioned technologies require internet connectivity and support web platforms as well. If you quickly review the table, you can also clearly see two main contenders that have the potential to dominate the entire space; that's because these are both low-level technologies that often back larger platforms such as Vuforia. Both of these platforms now support mobile pretrained networks for fast recognition on mobile devices. This may not seem like a big deal yet, but after we get into training our own models, you will see why.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Linear regression explained</h1>
                
            
            
                
<p>Let's discuss the basic premise behind what Machine Learning is and what it attempts to accomplish. Take a look at the following chart that shows some fictional sales data for your next app:</p>
<div><br/>
<img src="img/a6bb518b-0fb4-485f-8be4-390fc27184bd.png"/></div>
<p>Chart of fictional sales data</p>
<p>Now, just looking at the chart, you can see that as the <em>x</em> values increase (perhaps days on sale), it appears that our sales also increase: <em>y</em> value (sales). By just eyeing the chart, we ourselves can make predictions by following the trend of the points. Try it; how many sales are for an <em>x</em> value (bottom axis) of 25? Give it a guess, and write it down. With your guess secured, we will use a technique called <strong>linear regression</strong> to find a good answer.</p>
<p class="mce-root">Linear regression has been around for years and is considered as the base for many statistical data analysis methods. It is the basis for many other Machine Learning algorithms used in data science and predictive analysis today. This technique works by finding a solution (a line, curve, or whatever) that best fits the points. From that solution, we can determine the future or previous events or occurrences. Since this method is so well established, you can just open up Excel and let it draw the linear regression solution right on the graph. The following is an example of the linear regression with a trend line and equation added to the chart:</p>
<div><img src="img/fb098124-343b-45ba-abe6-e30e4c25ec4b.png"/></div>
<p>Chart with linear regression trend line</p>
<p>Keep in mind that this example uses 2D points, but the same concepts equally apply to 3D as well. You just need to account for the extra dimension, which is not always a trivial thing but doable nonetheless.</p>
<p>Without getting into the nitty-gritty details of the math, just understand that the line is drawn in order to minimize the error between the line and the points, which is often referred to as the line of best fit or one that minimizes the error, which in this case, is expressed as an R squared value (<strong>R²</strong>). <strong>R²</strong> ranges in value from 1.0, a best possible fit, to 0.0, or shooting blanks in the dark. You can see that our <strong>R²</strong> is not perfect, but it is <strong>0.9125 </strong> out of 1 or 91.25% correct; it's not perfect but perhaps good enough.</p>
<p>Probability and statistics play heavily into Machine Learning of all forms. If you don't have a good statistics background, you can still get the statistics by choosing a third-party provider. The only exception is if you have issues with that technology; then, it helps to have some background on your side, which is probably not something you wanted to hear if you're already trying to catch up on your 3D math skills.</p>
<p>Take the example we just looked at and now think about the problem in 3D, and it's not a line but a 3D object we want to recognize or predict. Obviously, things can get complicated quite fast and computationally expensive using statistical models. Fortunately, there is a better way to do this using a technique that uses <strong>supervised learning</strong> that models the human brain, called <strong>neural networks</strong> (<strong>NN</strong>).</p>
<p>In the next section, we will go under the covers into supervised learning and explore some techniques that we can use to analyze data using <strong>deep learning</strong> (<strong>DL</strong>) with neural networks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Deep learning</h1>
                
            
            
                
<p>As we discussed, the more traditional predictive models such as linear regression don't scale well, because they always need to calculate the whole solution using all the available points or data. These types of techniques or models have no ability to remember, learn, and improve, and they are generally classified as supervised models. This has led to the evolution of more advanced learning models known as <strong>reinforcement learning</strong> (<strong>RL</strong>) techniques for solving ML problems. In fact, deep learning and deep reinforcement learning techniques now outclass statistical methods in performance and accuracy by several orders of magnitude. However, that wasn't always the case, and statistical methods are also improving just as dramatically everyday. It really is an exciting time to be getting into Machine Learning.</p>
<p class="mce-root CDPAlignLeft CDPAlign">The following diagram demonstrates the reinforcement learning process:</p>
<div><br/>
<img src="img/40c72b92-4041-4106-add0-16b98161e8f8.png" style="width:26.33em;height:20.58em;"/><br/>
<br/>
Reinforcement learning process</div>
<p>In the diagram, you can see that there is an <strong>Agent</strong> (assume computer) and the <strong>Environment</strong> (game or real world). The <strong>Agent</strong> acts on <strong>Observations</strong> from the <strong>Environment</strong>, and those actions may or may not be based on <strong>Rewards</strong>. An RL system using rewards is known as reinforcement learning. The learning method we will use in this chapter is called supervised learning since we are labeling or training to a specific output class. Unsupervised learning is a class of training that doesn't label data but just uses techniques to classify or group data.</p>
<p>There are three classes of training we typically identify: unsupervised learning, supervised learning, and reinforcement learning. Reinforcement learning uses a rewards-based system on top of supervised or unsupervised systems as an enhancement to learning. RL systems can learn this way with essentially no initial training. AlphaGo Zero, which uses a deep RL model, is currently making the news after being able to beat a trained version of itself from scratch, with no human intervention.</p>
<p class="mce-root">Part of the problem in defining all these ML concepts is that they often get woven together, where one learning algorithm or technique is layered on top of another, perhaps using RL with or without supervision. It is quite common, as we will see, to use multiple different layers of techniques to produce an accurate answer. This layering also has the benefit of being able to try multiple different approaches quickly or swap a technique out for something better later.</p>
<p>Deep learning is the term we use to describe this layering process. DL can be trained using any of the training methods we talked about. In any case, we need to stop talking in generalities and actually look at the DL process.</p>
<p>Deep reinforcement learning has become quite popular as of late with plenty of success from playing Atari games to beating earlier supervised trained versions of itself quickly. If this area of training interests you, ensure that you search for AlphaGo Zero.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Neural networks – the foundation of deep learning</h1>
                
            
            
                
<p>When we speak of DL, we generally think of one ML technique called neural networks. Neural networks were conceptualized by trying to model the human brain. At the core of a neural network is the neuron, called so because it represents a single human brain cell. The following is an image of a human and computer neuron:</p>
<div><img src="img/1ebe7ce1-750f-484d-bf5e-b567a5515012.jpg" style="width:33.58em;height:21.25em;"/><br/>
<br/>
Human and computer neuron</div>
<p>Just like the brain, where billions of neurons are connected in layers, we connect neurons in layers in a similar way. Each neuron is connected to all the other neurons' inputs and outputs in layers, where the first layer takes our input and the last layer or perhaps single neuron spits out our answer. The following is an example of what this typically looks like:</p>
<div><img src="img/3a6b9b74-f880-4efd-a7a9-ff00ee7db3a3.png" style="width:29.58em;height:22.92em;"/><br/>
<br/>
Neural network with layers</div>
<p>One thing we should clarify before going any further is that the layers we talk about in deep learning don't correspond to the layers in a neural network. Think of a neural network as being in one layer of the DL system.</p>
<p>Here, each circle in the diagram represents a single neuron. Each neuron fires when the sum of all its inputs passes some threshold or activation function. This process continues for all the neurons, and the final layer outputs the answer. Of course, this is a very simple example, but it is difficult to see the power of neural networks until you start programming with them. Therefore, in the next section, we will write a neural network, which we plan to use to recognize objects in the environment.</p>
<p>When you encounter neural networks for the first time, the assumption is that this can't possibly work. After all, how could a self-driving car recognize a person using just a bunch of interconnected neurons? The answer to that is how indeed. We are really only starting to understand how the neural networks do what they do and, often, what we find is that we need to go back to the drawing board. In this case, the drawing board is the human brain and some of the more recent advances in neural networks were results of further brain research.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Programming a neural network</h1>
                
            
            
                
<p>The best way to learn something is to do it, so in this section, we will write a simple neural network that we'll then train to perform various tasks. This network will have a set number of layers—input, hidden, and output—but we will allow for a number of neurons to be set in each layer. We will write this code in Unity so that we can use it in <a href="6a8f64fb-080f-47a2-9565-4099269831b1.xhtml" target="_blank">Chapter 10</a>, <a href="6a8f64fb-080f-47a2-9565-4099269831b1.xhtml" target="_blank"/><em>Mixing in Mixed Reality</em>.</p>
<p>Writing a neural network is an advanced example, which will require a discussion with math to properly explain. If you feel overwhelmed at any time, you can always open up the finished project and check the final results. Of course, if you have written a neural network earlier, then you may also want to skip this section.</p>
<p>For this example, we will create a new project from the source Unity template, so let's get started by opening Command Prompt:</p>
<ol>
<li>Create a new folder called <kbd>ARCore</kbd> off the root (<kbd>C:\</kbd> on Windows) folder using the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>mkdir ARCore</strong><br/><strong>cd ARCore</strong></pre>
<ol start="2">
<li>This set of commands creates a new folder and then navigates to it.</li>
<li>Execute the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>git clone https://github.com/google-ar/arcore-unity-sdk.git ARCoreML</strong></pre>
<ol start="4">
<li>This pulls the Unity ARCore template from GitHub into a new folder called <kbd>ARCoreML</kbd>.</li>
</ol>
<ol start="5">
<li>Open a new instance of Unity and click on Open on the Project page. This will open the select project folder dialog. Select the new folder you just pulled the template into, <kbd>ARCoreML</kbd>,<strong> </strong>to open the project. Wait as the project opens in the Unity editor.</li>
<li>Right-click on (<em>Ctrl</em> + Click on Mac) the <kbd>Assets</kbd> folder in the Project window. Select Create | Folder from the context menu. Name the new folder <kbd>Scripts</kbd>.</li>
<li>Open the <kbd>HelloAR</kbd> scene from the <kbd>Assets/GoogleARCore/Examples/HelloAR</kbd> folder by double-clicking on it in the Project window.</li>
<li>From the menu, select <strong>File</strong> | Build Settings. Ensure that Android is set for the target platform and the <kbd>HelloAR</kbd> scene is set as scene <kbd>0</kbd> in the build.</li>
<li>Connect your device and build and run. Just ensure that the example runs as you expected on your device.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Scripting the neural network</h1>
                
            
            
                
<p>With the new project set up, we can now start writing our scripts to build a neural network. Go back to Unity and perform the following steps:</p>
<ol>
<li>Open the <kbd>ARCoreML/Scripts</kbd> folder and then from the menu, select Assets | Create | C# Script. Name the script as <kbd>Neuron</kbd> and double-click to open it in your editor of choice.</li>
</ol>
<p>The code for this example was originally sourced from <a href="https://github.com/Blueteak/Unity-Neural-Network.git">https://github.com/Blueteak/Unity-Neural-Network.git</a>, which shows an excellent example of a simple and concise neural network with training explicitly developed for Unity. We will modify the original code for our needs, but feel free to check out and contribute to the original source if you are interested. This code is great for learning, but certainly, it's not something you may want to use in production. We will look at options for production-ready neural networks in the section on TensorFlow.</p>
<ol start="2">
<li>Delete all the code, leave the <kbd>using</kbd> statements, and then add the following:</li>
</ol>
<pre style="padding-left: 60px">using System.Linq; //add after other using's<br/><br/>public class Neuron<br/>{<br/>  private static readonly System.Random Random = new System.Random();<br/>  public List&lt;Synapse&gt; InputSynapses;<br/>  public List&lt;Synapse&gt; OutputSynapses;<br/>  public double Bias;<br/>  public double BiasDelta;<br/>  public double Gradient;<br/>  public double Value;<br/>}</pre>
<ol start="3">
<li>Note how this class does not inherit <kbd>MonoBehaviour</kbd> and thus will not be a game object, which means we will load this class in another script. Then, we create a placeholder for <kbd>Random</kbd>; we do this because we are using <kbd>System.Random</kbd> rather than <kbd>Unity.Random</kbd>. <kbd>Unity.Random</kbd> only supports generating a random <kbd>float</kbd>, but we need the precision of a <kbd>double</kbd>. The rest are just properties that we will discuss as we get to the relevant code sections.</li>
<li>Enter the following after the last property declaration but before the class's ending brace:</li>
</ol>
<pre style="padding-left: 60px">public static double GetRandom()<br/>{<br/> return 2 * Random.NextDouble() - 1;<br/>}</pre>
<ol start="5">
<li>We create this <kbd>static</kbd> helper method in order to generate <kbd>double</kbd> random numbers from <kbd>-1.0</kbd> to <kbd>1.0</kbd>. This allows for greater precision and assures that our values are always getting generated around <kbd>0</kbd>. Keeping values close to <kbd>0</kbd> avoids rounding errors and just generally makes things easier to calculate.</li>
<li>Next, enter the following code after the <kbd>static</kbd> method:</li>
</ol>
<pre style="padding-left: 60px">public Neuron()<br/>{<br/>  InputSynapses = new List&lt;Synapse&gt;();<br/>  OutputSynapses = new List&lt;Synapse&gt;();<br/>  Bias = GetRandom();<br/>}<br/><br/>public Neuron(IEnumerable&lt;Neuron&gt; inputNeurons) : this()<br/>{<br/>  foreach (var inputNeuron in inputNeurons)<br/>  {<br/>    var synapse = new Synapse(inputNeuron, this);<br/>    inputNeuron.OutputSynapses.Add(synapse);<br/>    InputSynapses.Add(synapse);<br/>  }<br/>}</pre>
<ol start="7">
<li>Here, we set up a base and single parameter constructors. The base constructor creates a <kbd>List&lt;Synapse&gt;</kbd> for the input and output connections to the neuron. A <kbd>Synapse</kbd> represents a connection. The other constructor calls the base (<kbd>this</kbd>) and takes an <kbd>IEnumerable&lt;Neuron&gt;</kbd> of neurons that it then connects back to. This way, networks can be built bottom up; we will see how this works when we get to the <kbd>NeuralNet</kbd> class.</li>
<li>Next, we will add the rest of the methods for the <kbd>Neuron</kbd> class:</li>
</ol>
<pre style="padding-left: 60px">public virtual double CalculateValue()<br/>{<br/>  return Value = Sigmoid.Output(InputSynapses.Sum(a =&gt; a.Weight *  <br/>                                a.InputNeuron.Value) + Bias);<br/>}<br/><br/>public double CalculateError(double target)<br/>{<br/>  return target - Value;<br/>}<br/><br/>public double CalculateGradient(double? target = null)<br/>{<br/>  if (target == null)<br/>    return Gradient = OutputSynapses.Sum(a =&gt;    <br/>    a.OutputNeuron.Gradient * a.Weight) * Sigmoid.Derivative(Value);<br/>    return Gradient = CalculateError(target.Value) * Sigmoid.Derivative(Value);<br/>}<br/><br/>public void UpdateWeights(double learnRate, double momentum)<br/>{<br/>  var prevDelta = BiasDelta;<br/>  BiasDelta = learnRate * Gradient;<br/>  Bias += BiasDelta + momentum * prevDelta;<br/>  foreach (var synapse in InputSynapses)<br/>  {<br/>    prevDelta = synapse.WeightDelta;<br/>    synapse.WeightDelta = learnRate * Gradient * synapse.InputNeuron.Value;<br/>    synapse.Weight += synapse.WeightDelta + momentum * prevDelta;<br/>  }<br/>}</pre>
<ol start="9">
<li>We added four methods here: <kbd>CalculateValue</kbd>, <kbd>CalculateError</kbd>, <kbd>CalculateGradient</kbd>, and <kbd>UpdateWeights</kbd>. <kbd>CalculateValue</kbd> is used to determine the neuron's output based on the activation function we defined in <kbd>Sigmoid</kbd>. We will get to <kbd>Sigmoid</kbd> shortly. The other methods are used to train the neuron. Training a neuron is something we will cover in the next section.</li>
<li>Stay in the same file, and add the following three new helper classes outside the <kbd>Neuron</kbd> class:</li>
</ol>
<pre style="padding-left: 60px"><strong>}</strong> // end of Neuron class definition<em><strong><br/></strong></em><br/>public class Synapse<br/>{<br/>  public Neuron InputNeuron;<br/>  public Neuron OutputNeuron;<br/>  public double Weight;<br/>  public double WeightDelta;<br/>  public Synapse(Neuron inputNeuron, Neuron outputNeuron)<br/>  {<br/>    InputNeuron = inputNeuron;<br/>    OutputNeuron = outputNeuron;<br/>    Weight = Neuron.GetRandom();<br/>  }<br/>}<br/><br/>public static class Sigmoid<br/>{<br/>  public static double Output(double x)<br/>  {<br/>    return x &lt; -45.0 ? 0.0 : x &gt; 45.0 ? 1.0 : 1.0 / (1.0 +    <br/>    Mathf.Exp((float)-x));<br/>  }<br/>  public static double Derivative(double x)<br/>  {<br/>    return x * (1 - x);<br/>  }<br/>}<br/>public class DataSet<br/>{<br/>  public double[] Values;<br/>  public double[] Targets;<br/>  public DataSet(double[] values, double[] targets)<br/>  {<br/>    Values = values;<br/>    Targets = targets;<br/>  }<br/>}</pre>
<ol start="11">
<li>The first class <kbd>Synapse</kbd>, as we already know, defines a connection between neurons. Next comes <kbd>Sigmoid</kbd>, which, conveniently enough, is just a wrapper class for the sigmoid activation function we use. Note that the values are getting capped at <kbd>-45.0</kbd> and <kbd>+45.0</kbd>. This limits the size of our network, but we can manually change that later. Then comes <kbd>DataSet</kbd>, which is just a holder for our training data.</li>
</ol>
<p>That completes the <kbd>Neuron</kbd> class. Create another script in Unity, and this time, call it <kbd>NeuralNet</kbd>; open it up in your editor of choice and perform the following steps:</p>
<ol>
<li>Delete the starter code again, but leave the <kbd>using</kbd>'s statements, and enter the following:</li>
</ol>
<pre style="padding-left: 60px">public class NeuralNet<br/>{<br/>  public double LearnRate;<br/>  public double Momentum;<br/>  public List&lt;Neuron&gt; InputLayer;<br/>  public List&lt;Neuron&gt; HiddenLayer;<br/>  public List&lt;Neuron&gt; OutputLayer;<br/><br/>}  //be sure to add ending brace</pre>
<ol start="2">
<li>Again, this is another set of public properties that define the <kbd>LearnRate</kbd> network and <kbd>Momentum</kbd>. Then, three <kbd>List&lt;Neuron&gt;</kbd> to hold the collection of neurons in the input, hidden (middle), and output layers. In this example, we use a single hidden layer, but more sophisticated networks often support several more layers. You guessed it, <kbd>LearnRate</kbd> and <kbd>Momentum</kbd> will be covered in the section on training.</li>
</ol>
<p>We generally prefer not to use properties with getters and setters in Unity. Why? Primarily because the Unity editor just plays better with public fields. Secondarily, game programming is all about performance, and it only makes sense to avoid the overhead of getters and setters where possible. Using a list is also a no-no, but it makes the code easier to understand in this case.</p>
<ol start="3">
<li>Next, let's add a constructor for our <kbd>NeuralNet</kbd>:</li>
</ol>
<pre style="padding-left: 60px">public NeuralNet(int inputSize, int hiddenSize, int outputSize, <br/>              double? learnRate = null, double? momentum = null)<br/>{<br/>  LearnRate = learnRate ?? .4;<br/>  Momentum = momentum ?? .9;<br/>  InputLayer = new List&lt;Neuron&gt;();<br/>  HiddenLayer = new List&lt;Neuron&gt;();<br/>  OutputLayer = new List&lt;Neuron&gt;();<br/>  for (var i = 0; i &lt; inputSize; i++){<br/>    InputLayer.Add(new Neuron());<br/>  }<br/>  <br/>  for (var i = 0; i &lt; hiddenSize; i++){<br/>    HiddenLayer.Add(new Neuron(InputLayer));<br/>  }<br/><br/>  for (var i = 0; i &lt; outputSize; i++){<br/>    OutputLayer.Add(new Neuron(HiddenLayer));<br/>  }<br/>}</pre>
<ol start="4">
<li>This constructor expects several inputs, including the number of neurons in the input, hidden, and output layers, in addition to a value for the <kbd>learnRate</kbd> and <kbd>momentum</kbd>. Inside the constructor, the properties are initialized based on the input values. Note how the first layer uses the default <kbd>Neuron</kbd> constructor, and the successive layers use the single parameter constructor with the previous layer as input. Remember from building the <kbd>Neuron</kbd> class that this is where all the synapse connections between the neuron layers are added.</li>
<li>Next, we will add a couple of methods for training:</li>
</ol>
<pre style="padding-left: 60px">public void Train(List&lt;DataSet&gt; dataSets, int numEpochs)<br/>{<br/>  for (var i = 0; i &lt; numEpochs; i++)<br/>  {<br/>    foreach (var dataSet in dataSets)<br/>    {<br/>      ForwardPropagate(dataSet.Values);<br/>      BackPropagate(dataSet.Targets);<br/>    }<br/>  }<br/>}<br/><br/>public void Train(List&lt;DataSet&gt; dataSets, double minimumError)<br/>{<br/>  var error = 1.0;<br/>  var numEpochs = 0;<br/>  while (error &gt; minimumError &amp;&amp; numEpochs &lt; int.MaxValue)<br/>  {<br/>    var errors = new List&lt;double&gt;();<br/>    foreach (var dataSet in dataSets)<br/>    {<br/>      ForwardPropagate(dataSet.Values);<br/>      BackPropagate(dataSet.Targets);<br/>      errors.Add(CalculateError(dataSet.Targets));<br/>    }<br/>    error = errors.Average();<br/>    numEpochs++;<br/>  }<br/>}</pre>
<ol start="6">
<li>Then, we will add methods to propagate the network forward and backward:</li>
</ol>
<pre style="padding-left: 60px">private void ForwardPropagate(params double[] inputs)<br/>{<br/>  var i = 0;<br/>  InputLayer.ForEach(a =&gt; a.Value = inputs[i++]);<br/>  HiddenLayer.ForEach(a =&gt; a.CalculateValue());<br/>  OutputLayer.ForEach(a =&gt; a.CalculateValue());<br/>}<br/><br/>private void BackPropagate(params double[] targets)<br/>{<br/>  var i = 0;<br/>  OutputLayer.ForEach(a =&gt; a.CalculateGradient(targets[i++]));<br/>  HiddenLayer.ForEach(a =&gt; a.CalculateGradient());<br/>  HiddenLayer.ForEach(a =&gt; a.UpdateWeights(LearnRate, Momentum));<br/>  OutputLayer.ForEach(a =&gt; a.UpdateWeights(LearnRate, Momentum));<br/>}</pre>
<ol start="7">
<li>Finally, add the following methods to compute the whole network and to calculate errors:</li>
</ol>
<pre style="padding-left: 60px">public double[] Compute(params double[] inputs)<br/>{<br/>  ForwardPropagate(inputs);<br/>  return OutputLayer.Select(a =&gt; a.Value).ToArray();<br/>}<br/><br/>private double CalculateError(params double[] targets)<br/>{<br/>  var i = 0;<br/>  return OutputLayer.Sum(a =&gt; Mathf.Abs((float)a.CalculateError(targets[i++])));<br/>}</pre>
<p>That completes the neural network code. We left a number of areas for discussion in the next section on training the neural network.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Training a neural network</h1>
                
            
            
                
<p>As you may have already summarized, a neural network is essentially useless until it is trained. Before we get into training, we should talk some more on how a neuron is activated. Open up the <kbd>Neuron</kbd> class again and take a look at the <kbd>CalculateValue</kbd> function. This method calculates the output based on its internal set of weights and is described by the following:</p>
<div><img class="fm-editor-equation" src="img/5bd05beb-4c40-411f-996d-e4c0c396ef43.png" style="width:6.83em;height:3.75em;"/></div>
<div><img class="fm-editor-equation" src="img/d0fefff0-1227-4e2b-b7ae-b3529be79bb6.png" style="width:12.08em;height:3.67em;"/></div>
<p>Here:</p>
<div><img class="fm-editor-equation" src="img/bbd0e139-c191-48c1-b1fb-32e5193d399a.png" style="width:12.25em;height:1.67em;"/></div>
<p>Also, keep the following in mind:</p>
<p><em>n</em> = total number of neurons connected as inputs<br/>
<em>I</em> = signaled input to the <kbd>Neuron</kbd> class</p>
<p><em>O</em> = calculated output</p>
<p><em>S</em> = the <kbd>sigmoid</kbd> function with a graph:</p>
<div><br/>
<img src="img/3cad5108-6d20-4609-81e8-6643e2a262ad.png"/><br/>
<br/>
Sigmoid function</div>
<p><strong>Sigmoid Function</strong> essentially distributes the weighted sum of values between 0 and 1 based on a curve (function) similar to the one shown in the preceding graph. We do this in order to evenly weigh the outputs of each of the neurons. Likewise, when we look to input data into a network, we also like to normalize the values between 0 and 1. If we didn't do this, one single neuron or input could bias our entire network. This is like hitting your thumb with a hammer and only being able to feel pain in your thumb for the next several seconds, Except that we don't want our network to respond to wild inputs like that. Instead, we want to mellow our network out with the <kbd>sigmoid</kbd> function.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Activating the warning</h1>
                
            
            
                
<p>Let's delay our discussion of training a bit further and put together a simple example to see how this works. Open up Unity again and perform the following steps:</p>
<ol>
<li>Create a new C# script called <kbd>EnvironmentScanner</kbd> in the <kbd>Assets/ARCoreML/Scripts</kbd> folder. Then, open the script in your editor.</li>
<li>Add the code, as shown, to the class definition:</li>
</ol>
<pre style="padding-left: 60px">[RequireComponent(typeof(AudioSource))]<br/>public class EnvironmentalScanner : MonoBehaviour  //before me</pre>
<ol start="3">
<li><kbd>RequireComponent</kbd> is a custom Unity attribute that forces a <kbd>GameObject</kbd> to require a specific class before this component can be added. In this example, we require an <kbd>AudioSource</kbd> component.</li>
<li>Enter the following new properties/fields and method to the class; don't delete anything:</li>
</ol>
<pre style="padding-left: 60px">public NeuralNet net;<br/>public List&lt;DataSet&gt; dataSets;<br/>    <br/>private float min = float.MaxValue;<br/>private float maxRange = float.MinValue;<br/>private float[] inputs;<br/>private double[] output;<br/>private double temp;<br/>private bool warning;<br/>private AudioSource audioSource;<br/>private double lastTimestamp;<br/><br/>public void Awake()<br/>{ <br/>    int numInputs, numHiddenLayers, numOutputs;<br/>    numInputs = 1; numHiddenLayers = 4; numOutputs = 1;<br/>    net = new NeuralNet(numInputs, numHiddenLayers, numOutputs);<br/>    dataSets = new List&lt;DataSet&gt;();<br/>}</pre>
<ol start="5">
<li>The <kbd>Awake</kbd> method is special in Unity in that it gets called when the object first wakes up or becomes active. <kbd>Awake</kbd> varies from <kbd>Start</kbd> in that it is called upon initialization of the object, whereas <kbd>Start</kbd> is called before the first frame an object is rendered. The difference is subtle and is typically only relevant when you are worried about object load time.<br/>
Next, we create a number of temporary input variables for setting the number of <strong>input</strong>, <strong>hidden</strong>, and <strong>output</strong> neurons. For this example, we will use one input, four hidden, and one output. These inputs are used to create <kbd>NeuralNet</kbd> in the next line, which is followed by the initialization of the <kbd>dataSets</kbd> list.</li>
<li>Next, let's modify the <kbd>Start</kbd> method to resemble the following:</li>
</ol>
<pre style="padding-left: 60px">void Start()<br/>{ <br/>  dataSets.Add(new DataSet(new double[]{ 1,.1,0.0}, new double[] { 0.0,1.0,1.0 } ));<br/>  net.Train(dataSets, .001);<br/>  audioSource = GetComponent&lt;AudioSource&gt;();<br/>}</pre>
<ol start="7">
<li>The first line inside <kbd>Start</kbd> creates a very simple <kbd>DataSet</kbd> with inputs and outputs. Since we are using a single input and output neuron, these inputs and outputs map 1 to 1 and thus produce the following chart:</li>
</ol>
<div><img src="img/ce4f1a06-3c30-4222-9461-9d280c01cb5f.png" style="width:36.08em;height:21.42em;"/></div>
<p>Chart of training inputs</p>
<ol start="8">
<li>Then, <kbd>net.Train</kbd> trains the neural network with a minimum error of <kbd>.001</kbd>. After that, it gets the required <kbd>AudioSource</kbd>, remembers the <kbd>RequireComponent</kbd> attribute, and sets it to a private <kbd>audioSource</kbd> field. We will use sound in order to warn the user when they get too close. Think about what it is that those points are describing as a function.</li>
<li>Finally, modify the <kbd>Update</kbd> method to include the following:</li>
</ol>
<pre style="padding-left: 60px">void Update()<br/>{<br/>  if (warning)<br/>  { <br/>    audioSource.Play();<br/>  }<br/>  else<br/>  {<br/>    audioSource.Stop();<br/>  }<br/>  // Do not update if ARCore is not tracking.<br/>  if (Frame.TrackingState != FrameTrackingState.Tracking)<br/>  {<br/>    return;<br/>  }<br/>  <br/>  min = float.MaxValue; <br/>  PointCloud pointCloud = Frame.PointCloud;<br/>  if (pointCloud.PointCount &gt; 0 &amp;&amp; pointCloud.Timestamp &gt; lastTimestamp)<br/>  {<br/>  lastTimestamp = pointCloud.Timestamp;<br/>  //find min<br/>    for (int i = 0; i &lt; pointCloud.PointCount; i++)<br/>    {<br/>      var rng = Mathf.Clamp01((pointCloud.GetPoint(i)- transform.parent.parent.transform.position).magnitude);<br/>      min = Mathf.Min(rng, min);<br/>    }<br/>    <br/>    //compute output<br/>    output = net.Compute(new double[] { (double)min });<br/>    if(output.Length &gt; 0)<br/>    {       <br/>      warning = output[0] &gt; .001;<br/>    }<br/>    else<br/>    {<br/>      warning = false;<br/>    }<br/>  }  <br/>}</pre>
<ol start="10">
<li>There is a lot going on here, so let's break it down. We first check whether the <kbd>warning</kbd> is <kbd>true</kbd>. If it is, we play a sound, otherwise we stop playing; <kbd>warning</kbd> will be our flag to indicate when our NN is signalling. Next, we ensure that the <kbd>Frame</kbd> is tracking, with the same code as we saw earlier. Then, we reset <kbd>min</kbd> and get the current point cloud from the <kbd>Frame</kbd>.<br/>
After that, we ensure that <kbd>pointCloud</kbd> has points, and it is the most recent. This is checked by testing the timestamp. Then, inside the <kbd>if</kbd> block, we calculate the current min by looping through all points. We then push this through our NN with <kbd>net.Compute</kbd>, the value of <kbd>min</kbd> (minimum point); this returns our signal or neuron output. In this particular case, we are testing for <kbd>.001</kbd> to determine whether the neuron is signalling an activation. This sets the warning to <kbd>true</kbd> or <kbd>false</kbd>.</li>
<li>Save the code and return to Unity; ensure that you see no compiler errors.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Adding the environmental scanner</h1>
                
            
            
                
<p>Now that we have a script that uses the component, let's add it to our scene as a new object. Return to the editor where we last left off and continue as follows:</p>
<ol>
<li>Open the <kbd>HelloAR</kbd> scene. From the menu, select File | Save as and save the scene as <kbd>Main</kbd> in the <kbd>Assets/ARCoreML</kbd> folder.</li>
<li>Find and select First Person Camera in the Hierarchy window. Remember that you can use the search panel.</li>
<li>Right-click (<em>Ctrl</em> + Click on Mac) on the First Person Camera and from the context menu, select Create Empty. Name the object as <kbd>Environmental Scanner</kbd>.</li>
<li>Select the new object and in the Inspector window, add a new <kbd>AudioSource</kbd> component.</li>
<li>Create a new folder called <kbd>Audio</kbd> in the <kbd>Assets/ARCoreML</kbd> path in the Project window.</li>
<li>Open the <kbd>Resources</kbd> folder from the downloaded code folder and copy the <kbd>tone-beep.wav</kbd> file to the new <kbd>Assets/ARCoreML/Audio</kbd> folder you just created.</li>
<li>Open up the <kbd>Environmental Scanner</kbd> object in the Inspector window and set the AudioSource properties, as shown in the following screenshot:</li>
</ol>
<div><img src="img/2e7a3aca-4c32-4a9f-a92d-4b3fd67d0d5c.png"/></div>
<p>Setting the AudioSource properties in the Inspector</p>
<ol start="8">
<li>With <kbd>Environmental Scanner</kbd> still selected, click on the Add Component button in the I<strong>nspector</strong> window. Add the <kbd>Environmental Scanner</kbd> script we wrote earlier.</li>
<li>Open the Build Settings dialog and ensure that you add the current scene (<kbd>Main</kbd>) to the build. Ensure that you remove any other scenes from the build.</li>
<li>Connect, build, and run. Move around the room. Now what happens when you get too close to objects? At what distance?</li>
</ol>
<p>Great, so we have effectively made a backup or warning beeper to let you know when you are getting too close to an object. Obviously, we could have just as easily written a simple threshold test ourselves to test when <kbd>min</kbd> is getting too close. However, this simple example gives us a good basis for understanding how training works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Backward propagation explained</h1>
                
            
            
                
<p>In this example, we are pretraining our model (supervised learning) to a simple function described by a set of inputs (1.0, 0.1, 0) and expected outputs of (0, 1.0, 1.0), which is represented by the graph/chart we saw earlier. In essence, we want our neural net to learn the function defined by those points and be able to output those results. We do this by calling <kbd>net.Train</kbd>, passing in <kbd>datasets</kbd> and the minimum expected error. This trains the network by backward propagating the error through each neuron of the network until a minimum error can be reached. Then, the training stops and the network declares itself ready.</p>
<p>Backward propagation works using a simple iterative optimization algorithm called <strong>gradient descent</strong>, which uses the minimum error to minimize each of the neuron input weights so that the global minimum error can be reached. To fully understand this, we will need to go into some differential calculus and derivatives. Instead, we will take a shortcut and just look at what the code is doing in the <kbd>Train</kbd> method of the <kbd>NeuralNet</kbd> class:</p>
<pre>public void Train(List&lt;DataSet&gt; dataSets, double minimumError)<br/>{<br/>  var error = 1.0;<br/>  var numEpochs = 0;<br/>  while (error &gt; minimumError &amp;&amp; numEpochs &lt; int.MaxValue)<br/>  { <br/>    var errors = new List&lt;double&gt;();<br/>    foreach (var dataSet in dataSets)<br/>    {<br/>      ForwardPropagate(dataSet.Values);<br/>      BackPropagate(dataSet.Targets);<br/>      errors.Add(CalculateError(dataSet.Targets));<br/>    }<br/>    error = errors.Average();<br/>    numEpochs++;<br/>  }<br/>}</pre>
<p>The code here is relatively straightforward. We set an <kbd>error</kbd> and <kbd>numEpochs</kbd>. Then, we start a <kbd>while</kbd> loop that ends when the <kbd>error</kbd> is greater than the <kbd>minimumError</kbd> (global) and the <kbd>numEpochs</kbd> is less than the maximum <kbd>int</kbd> value. Inside the loop, we then loop through each <kbd>dataSet</kbd> in <kbd>dataSets</kbd>. First, <kbd>ForwardPropagate</kbd> is used on the inputs of the dataset values to determine output. Then, <kbd>BackPropagate</kbd> is used on the dataset target value to adjust the weights on each of the neurons using gradient descent. Let's take a look inside the <kbd>BackPropagate</kbd> method:</p>
<pre>private void BackPropagate(params double[] targets)<br/>{<br/>    var i = 0;<br/>    OutputLayer.ForEach(a =&gt; a.CalculateGradient(targets[i++]));<br/>    HiddenLayer.ForEach(a =&gt; a.CalculateGradient());<br/>    HiddenLayer.ForEach(a =&gt; a.UpdateWeights(LearnRate, Momentum));<br/>    OutputLayer.ForEach(a =&gt; a.UpdateWeights(LearnRate, Momentum));<br/>}</pre>
<p>This method just elegantly loops through each layer of neurons using <kbd>ForEach</kbd> from <kbd>System.Linq</kbd>. First, it calculates the gradient in the output and hidden layers and then it adjusts the weights in reverse order: first the hidden and then the output. Next, we will dissect the <kbd>CalculateGradient</kbd> method:</p>
<pre>public double CalculateGradient(double? target = null)<br/>{<br/>  if (target == null)<br/>    return Gradient = OutputSynapses.Sum(a =&gt; a.OutputNeuron.Gradient * a.Weight) * Sigmoid.Derivative(Value);<br/><br/>  return Gradient = CalculateError(target.Value) * Sigmoid.Derivative(Value);<br/>}</pre>
<p>We can see that the <kbd>CalculateGradient</kbd> method takes a nullable <kbd>double</kbd> called <kbd>target</kbd>. If <kbd>target</kbd> is <kbd>null</kbd>, the <kbd>Gradient</kbd> is calculated by summing the previous gradient multiplied by the input weights. Otherwise, the <kbd>Gradient</kbd> is calculated by multiplying the error by the derivative of the <kbd>Sigmoid</kbd>. Remember that, sigmoid was our activation function, which is essentially what we are trying to minimize. If you recall from calculus, we can take the derivative of a function in order to determine its minimum or maximum value. In fact, in order to use the gradient descent method for backward propagation, your activation function has to be differentiable.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Gradient descent explained</h1>
                
            
            
                
<p>Gradient descent uses the partial derivative of the loss or error function in order to propagate the updates back to the neuron weights. Our cost function in this example is the sigmoid function, which relates back to our activation function. In order to find the gradient for the output neuron, we need to derive the partial derivative of the sigmoid function. The following graph shows how the gradient descent method walks down the derivative in order to find the minimum:</p>
<div><img src="img/f3899ca3-835e-4d3e-8e7f-fd1c5a9044fb.png" style="width:27.67em;height:17.17em;"/><br/>
<br/>
Gradient descent algorithm visualized</div>
<p>If you plan to spend anymore time studying neural networks, deep learning, or machine learning, you will certainly study the mathematics of gradient descent and backward propagation in more depth. However, it is unlikely that you will get further exposure to the basic concepts of programming a neural network, so this chapter will be a good future reference.</p>
<p>Let's take a look at the <kbd>CalculateError</kbd> function, which simply subtracts the neuron's output value from what its value should have been:</p>
<pre>public double CalculateError(double target)<br/>{<br/>    return target - Value;<br/>}</pre>
<p>Then, scroll to the <kbd>UpdateWeights</kbd> method, as shown in the following code:</p>
<pre>public void UpdateWeights(double learnRate, double momentum)<br/>{<br/>    var prevDelta = BiasDelta;<br/>    BiasDelta = learnRate * Gradient;<br/>    Bias += BiasDelta + momentum * prevDelta;<br/><br/>    foreach (var synapse in InputSynapses)<br/>    {<br/>        prevDelta = synapse.WeightDelta;<br/>        synapse.WeightDelta = learnRate * Gradient *         <br/>                               synapse.InputNeuron.Value;<br/>        synapse.Weight += synapse.WeightDelta + momentum * prevDelta;<br/>    }<br/>}</pre>
<p><kbd>UpdateWeights</kbd> then adjusts each of the neurons' weights based on <kbd>learnRate</kbd> and <kbd>momentum</kbd>; <kbd>learnRate</kbd> and <kbd>momentum</kbd> set the speed at which the NN will learn. We often want to control the learning rate of the algorithm to prevent overfitting and falling into a local minimum or maximum. After that, the code is relatively straightforward, with it looping through the synapse connections and updating the weights with a new value. The <kbd>Bias</kbd> is used to control the intercept of the sigmoid activation function, thus allowing the neuron to adjust its initial activation function. We can see how the <kbd>Bias</kbd> can alter the activation function in the following graph:</p>
<div><br/>
<img src="img/0380b555-7ffa-4da2-89cc-f0ab40f7a22a.png" style="width:42.25em;height:24.75em;"/><br/>
<br/>
Effect of Bias on the sigmoid activation function</div>
<p>Adjusting the <kbd>Bias</kbd> allows for the neuron to start firing or activating at a value other than 0, as indicated in the preceding graph. Thus, if the value of <kbd>Bias</kbd> is 2, then the neuron will start activating at -2, as shown in the graph.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Defining the network architecture</h1>
                
            
            
                
<p>We just learned how to write and use a simple neural network to warn a user when they are getting too close to an object. As you look through the code, appreciate that most of these values are internally adjusted as part of training. When using a neural network, it is important to understand these basic principals:</p>
<ul>
<li><strong>Activation function</strong>: If you are not using sigmoid, then you will also need to find the partial derivative of your activation function in order to use gradient descent with backward propagation.</li>
<li><strong># Input neurons</strong>: This will not only set the complexity of the network, but it will also determine the number of hidden or middle layer of neurons.</li>
<li><strong># Output neurons</strong>: How many outputs or ways do you need your network to classify?</li>
<li><strong># Hidden layers/neurons</strong>: As a good rule of thumb, you want to use the average of the input and output neurons, or just <em>input+output/2</em>. We will apply this rule in our next example.</li>
<li><strong>Training method</strong>: Our neural network supports two methods of training: minimum error or by epoch or number of iterations. Our preference will be to use minimum error, as this quantifies our model better.</li>
</ul>
<p class="mce-root">Included in the source code download for this chapter is a working example in an asset package of our simple neural network being used as an environment or object recognizer. Jump back to Unity and perform the following steps to set up this example:</p>
<p>Ensure that you save your existing project or download a new ARCore template before beginning. The asset import will overwrite your existing files, so you should make a backup before continuing if you want to keep any of your earlier work.</p>
<ol>
<li>From the menu, select Assets | Import Package | Custom Package. Use the file dialog to navigate to the <kbd>Code/Chapter_8</kbd> folder of the book's downloaded source code and import <kbd>Chapter_8_Final.unitypackage</kbd>.</li>
</ol>
<ol start="2">
<li>Open the Main scene from the <kbd>Assets/ARCoreML</kbd> folder.</li>
<li>Open the Build Settings dialog and ensure that the Main scene is added to the build and is active.</li>
<li>Connect, build, and run. Now when you run the app, you will see two buttons at the top of the interface: one that says Train 0 and one that says Train 1.</li>
<li>Face your device on an area you want the NN to recognize. Ensure that ARCore is identifying plenty of blue points on the screen, and then press the Train 1 button; this will signal to the network that you want it to identify this feature set.</li>
<li>Face the device on an area that you don't want the NN to recognize and press the Train 0 button; this will reinforce to the network that you do not want it to recognize this area.</li>
<li>While staying in place, continue this process. Point your device at the same area you want recognized repeatedly and press Train 1. Likewise, do this for areas you don't want recognized, but ensure that you press the Train 0 button. After you train 10 or so times, you should start hearing the warning beep, identifying when the NN has recognized your area.</li>
<li>If you start hearing the warning tones, that will be an indicator that your NN is starting to learn. Continue to spin around in the place, training the network, making sure to correct the network by pressing the appropriate button. You will likely have to do this several times (perhaps 20 to 50 times or so) before you note that the NN recognizes the area you want.</li>
</ol>
<p>Ensure that when you are training the network, you can see plenty of blue points. If you don't see any points, you will essentially be training with null data.</p>
<ol start="9">
<li>Finally, when your network is fully trained, you should be able to spin slowly around the room and hear when your device recognizes your region of choice.</li>
</ol>
<p>Using our simple NN, we were able to build an object/feature recognizer that we could train to recognize specific features, places, or objects. This example is quite simple and not very robust or accurate. However, considering the limited training dataset, it does a good job of being able to recognize features on the fly. Open up the <kbd>Environmental Scanner</kbd> script, and we will take a look at how the network is configured:</p>
<ol>
<li>Scroll down to the <kbd>Awake</kbd> method and take a look at how the network is created:</li>
</ol>
<pre style="padding-left: 60px">public void Awake()<br/>{ <br/>  int numInputs, numHiddenLayers, numOutputs;<br/>  numInputs = 25; numHiddenLayers = 13; numOutputs = 1;<br/>  net = new NeuralNet(numInputs, numHiddenLayers, numOutputs);<br/>  dataSets = new List&lt;DataSet&gt;();<br/>  normInputs = new double[numInputs];<br/>}</pre>
<ol start="2">
<li>Note that this time we are creating an input layer of <kbd>25</kbd> neurons and output of <kbd>1</kbd>. If we stick to the general rule for our hidden layer being the average of the input and output, that equates to <kbd>13</kbd> [(<kbd>25</kbd>+<kbd>1</kbd>)/2=<kbd>13</kbd>].</li>
<li>We removed the initial NN setup and training from <kbd>Start</kbd> and moved it to the bottom in a new method called <kbd>Train</kbd>:</li>
</ol>
<pre style="padding-left: 60px">private void Train()<br/>{ <br/>  net.Train(dataSets, 100);<br/>  trained = dataSets.Count &gt; 10;<br/>}</pre>
<ol start="4">
<li>This time, we are using a different form of training called <strong>epoch</strong>. We use this form of training when we are not actually sure what the expected error is or it needs to change, as in this case. Think about this—when we start training our network with a very limited dataset, our error rates will be high due to our lack of data. This will mean that we will never be able to train our network to a minimum error. It, therefore, makes more sense to just run our training algorithm for a set number of iterations or epochs for every training cycle.</li>
<li>Just preceding <kbd>Train</kbd> is <kbd>TrainNetwork</kbd>, and it's shown as follows:</li>
</ol>
<pre style="padding-left: 60px">public void TrainNetwork(float expected)<br/>{<br/>  this.expected = expected;<br/>  training = true;<br/>}<strong><br/></strong></pre>
<ol start="6">
<li><kbd>TrainNetwork</kbd> is a public method that we use to signal to the <kbd>Environmental Scanner</kbd> to initiate a training cycle with the expected outcome. This allows us to wire up event handlers on the UI buttons to call this method with an expected value. When you press Train 0, <kbd>TrainNetwork</kbd> is passed <kbd>0.0</kbd>, and after the Train 1 button is pressed, <kbd>1.0</kbd> is passed.</li>
<li>Scroll up to the <kbd>Update</kbd> method and look at the following section of code:</li>
</ol>
<pre style="padding-left: 60px">if (training)<br/>{ <br/>  dataSets.Add(new DataSet(normInputs, new double[] { expected }));<br/>  training = false;<br/>  Train();<br/>}</pre>
<ol start="8">
<li>This is the block of code that checks the <kbd>training</kbd> flag. If it is set, it collects the normalized inputs and adds them to <kbd>dataSets</kbd> with the expected outcome. We then turn the flag off and call <kbd>Train</kbd>.</li>
<li>Scroll up to the following block of code, and you can see how we are normalizing the training <kbd>inputs</kbd>:</li>
</ol>
<pre style="padding-left: 60px">for (int i = 0; i &lt; normInputs.Length; i++)<br/>{<br/>  if (i &lt; pointCloud.PointCount)<br/>  {<br/>    //normalize the inputs<br/>    normInputs[i] = inputs[i] / max;<br/>  }<br/>  else<br/>  {<br/>    normInputs[i] = 0;<br/>  }<br/>}</pre>
<ol start="10">
<li>Here, we are normalizing the <kbd>inputs</kbd>. An <kbd>input</kbd> represents the distance or magnitude between an identified point and the camera (user). Normalizing is scaling or converting your data to values in the range <kbd>0</kbd> to <kbd>1</kbd>. We do this, in this case, by finding the maximum distance of each point and then using that to divide into all the other inputs. The test in the loop to check whether <kbd>i</kbd> is less than the <kbd>PointCount</kbd> is to ensure that we always set a value for each input neuron.</li>
</ol>
<p>The rest of the code is similar to what we wrote earlier and not worth going over again.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">The network view of the world</h1>
                
            
            
                
<p>So what exactly is going on here, what is it that the network is identifying? Essentially, we are flattening our 3D view of the world into a 2D line or curve. A typical example of how this line may look normalized is as follows:</p>
<div><img src="img/5d6fd76d-b118-432a-b113-c6c2248e895a.png"/><br/>
<br/>
Normalized input points</div>
<p>Those inputs represent the normalized view the neural network is training for, or perhaps, against. If you trained the network to recognize that line, then the warning sound should go off when it detects the said line. Of course, the more points you add, the better your recognizer may or may not work. We will leave it up to you to further test the network on your own.</p>
<p>Neural networks were quite popular with game and graphic developers in the late 1990s and early 2000s. NNs showed some success in various AI scenarios, driving games especially, but at the end, other purpose-built techniques won out, that is, until quite recently with the advent of new techniques such as convolutional NNs. These new successes have led to massive surges in deep learning techniques and platforms.</p>
<p>This simple NN can be extended to recognize other simple functions or patterns you wanted. However, it will work poorly if we try to use it for any of the other recognition tasks we identified earlier as critical for AR. Therefore, in the next section, we will look at how ML solves our recognition problems with a new platform developed by Google, called TensorFlow.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p class="mce-root">Work through the following exercises on your own:</p>
<ol>
<li>Explain the difference between unsupervised learning, supervised learning, and reinforcement learning. This is more of a thought exercise, but it will be beneficial to really understand the difference.</li>
<li>Modify the original NN example to warn you when objects are detected past a certain distance.</li>
<li class="mce-root">What happens in the second example if you order the inputs by length? Does it still work?</li>
<li class="mce-root">Add an additional output neuron to the network in the second example. You will also need a new training button and will need to modify the <kbd>TrainNetwork</kbd> function to take two <kbd>inputs</kbd>.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">TensorFlow</h1>
                
            
            
                
<p>There is a new kid on the block called <strong>TensorFlow</strong>, also developed by Google, that is making impressive waves in ML. TensorFlow is a full ML platform that is actually more than just an execution engine with a bunch of built-in tools. What is even more impressive is that you can train advanced neural nets, convolutional neural networks, capsule networks, or whatever else you need on massive datasets offline. Then, you take those trained networks and put them on a mobile device in what is called a <strong>MobileNet</strong> to quickly recognize and classify complex objects. We will take a break from ARCore in this section and look at the upcoming power of TensorFlow.</p>
<p>TensorFlow is an advanced ML resource and toolkit that will be worth your time, learning more about whether you need to do any advanced recognition tasks. Keep in mind, though, that this tool requires advanced knowledge in math and a working knowledge of Python.</p>
<p>We will run the TensorFlow example for Android, not just to get a grasp of the power of the tool but also to understand what is possible. With Google building TensorFlow and ARCore though, we can only assume that new integrated tools will be built in the future. For now, though, let's open Command Prompt or shell and get started:</p>
<ol>
<li>Run the following command from your user folder or root:</li>
</ol>
<pre style="padding-left: 60px"><strong>mkdir TensorFlow</strong><br/><strong>cd TensorFlow</strong></pre>
<ol start="2">
<li>Create the <kbd>TensorFlow</kbd> directory and navigate to it. Then, type the following command:</li>
</ol>
<pre class="prettyprint notranslate" style="padding-left: 60px"><strong>git clone https://github.com/tensorflow/tensorflow</strong></pre>
<ol start="3">
<li>Open Android Studio. From the Welcome screen, select Open an existing Android Studio project.</li>
<li>
<p>Use the dialog and navigate to, select the <kbd>TensorFlow/tensorflow/examples/android</kbd> folder, and click on OK.</p>
</li>
<li>
<p>If it asks you to do a Gradle Sync, click on OK.</p>
</li>
<li>Open the <kbd>build.gradle</kbd> file from the Project side panel under the Gradle Scripts and set the <kbd>nativeBuildSystem</kbd> variable to <kbd>none</kbd>, as shown here:<br/></li>
</ol>
<pre style="padding-left: 60px">def nativeBuildSystem = 'none'</pre>
<ol start="7">
<li>Connect your device and click on the Run button, the green arrow icon on top. Follow any necessary build steps and let the apps push to your device.</li>
<li>When the build is completed, Studio will have pushed four apps to your device: <strong>TFClassify</strong>, <strong>TFDetect</strong>, <strong>TFSpeech</strong>, and <strong>TFStylize</strong>. Play around with each of these examples and observe the power of some networks running on your device.</li>
</ol>
<p>The following is an example of the TFDetect app running and correctly classifying a dog and person with very high accuracy:</p>
<div><img src="img/ad019856-28a7-4f43-84eb-82a4379fe961.png" style="width:25.00em;height:30.25em;"/><br/>
<br/>
TFDetect correctly classifying a dog and person</div>
<p>Unfortunately, the components needed to run TensorFlow with ARCore are not quite ready yet, so at the time of writing, we couldn't complete a full example. However, the future of ML for AR apps will most certainly be with TensorFlow or some other third-party solution, piggybacking on top of TensorFlow. Google has years of experience in AI/ML, from developing self-driving cars to the Google Home. It has put those years of knowledge into TensorFlow and made it accessible to the world. You would have to be a fool not to spend any time learning TensorFlow if you plan to build your own ML for object/feature recognition.</p>
<p>We had planned to build an example with a trained MobileNet running in ARCore. Unfortunately, the pieces were not quite ready yet, and it made for a far too complicated example. Right around the time that this book is published, we will likely see more tools developed to make integrating TensorFlow into ARCore easier.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we took a proverbial dive into the deep end—or the deep learning end—of the pool. We started by talking about the importance of ML and what applications we can use it for in AR. Then, we looked at how ML can use various methods of learning from unsupervised, supervised, and reinforcement learning in order to teach an ML agent to learn. We then looked at a specific example of learning ML algorithms, called neural networks and often referred to as deep learning. This led us to build a simple neural network that you can also use to learn the intricacies of neural networks on your own. NNs are very complex and not very intuitive, and it is helpful to understand their basic structure well. We then trained this network on a very simple dataset to notify the user if they get too close to an object. This led to a further discussion of how NNs train with back propagation using the gradient descent algorithm. After that, we looked at an enhanced example that allows you to train the network to recognize an area or object. Finally, we looked at the current king of ML, TensorFlow, and looked at a quick example of what is possible and what is coming soon.</p>
<p>In the next chapter, we get back to building a practical example with ARCore. We will build a simple design app that lets the user virtually decorate their living space.</p>


            

            
        
    </body></html>