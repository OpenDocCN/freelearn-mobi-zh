- en: '*Chapter 10*: Making Smarter Apps with Core ML'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over the past few years, machine learning has gained in popularity. However,
    it has never been easy to implement in mobile applications—that is, until Apple
    released the **Core ML** framework as part of iOS 11\. Core ML is Apple's solution
    to all of the problems that developers at the company have run into themselves
    while implementing machine learning for iOS. As a result, Core ML should have
    the fastest, most efficient implementations for working with sophisticated machine
    learning models, through an interface that is as simple and flexible as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn what machine learning is, how it works, and
    how you can use trained machine learning models in your apps. You will also learn
    how you can use Apple''s **Vision framework** to analyze images, and you''ll see
    how it integrates with Core ML for powerful image detection. Lastly, you''ll learn
    how to use the new **Create ML** tool to train your models, how to deploy your
    models to the cloud, and how to encrypt them for security. You will learn about
    these topics in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding machine learning and Core ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Combining Core ML and computer vision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training your own models with Create ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating models remotely with Model Deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encrypting Core ML models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to train and use your Core ML models
    to make the apps you build more intelligent and compelling.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The code bundle for this chapter includes three starter projects called `TextAnalyzer`,
    `ImageAnalyzer`, and `TextAnalyzerCloud`. It also includes a playground file named
    `Create ML.playground`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found here: [https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition/tree/master/Chapter%2010%20-%20Core%20ML](https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition/tree/master/Chapter%2010%20-%20Core%20ML).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding machine learning and Core ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine learning and Core ML go hand in hand, but they're not quite the same.
    Machine learning is all about teaching a machine how it can recognize, analyze,
    or apply certain things. The result of all this teaching is a trained model that
    can be used by Core ML to analyze specific inputs and produce an output based
    on the rules that were established during the training phase.
  prefs: []
  type: TYPE_NORMAL
- en: Before you learn about Core ML, it's good to obtain some knowledge about machine
    learning to make sure you're familiar with some of the terms that are used, and
    so you know what machine learning is.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding what machine learning is
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A lot of developers will hear about machine learning, deep learning, or neural
    networks at some point in their careers. You may have already heard about these
    topics. If you have, you know that machine learning is a complex field that requires
    particular domain knowledge. However, machine learning is becoming more prominent
    and popular by the day, and it is used to improve many different types of applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, machine learning can be used to predict what type of content
    a particular user might like to see in a music app based on the music that they
    already have in their library, or to automatically tag faces in photos to connect
    them to people in the user''s contact list. It can even be used to predict costs
    for specific products or services based on past data. While this might sound like
    magic, the flow for creating machine learning experiences like these can be split
    roughly into two phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using inference to obtain a result from the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Large amounts of high-quality data must be collected to perform the first step.
    If you're going to train a model that should recognize cats, you will need a large
    number of pictures of cats. You must also collect images that do not contain cats.
    Each image must then be appropriately tagged to indicate whether the image includes
    a cat or not.
  prefs: []
  type: TYPE_NORMAL
- en: If your dataset only contains images of cats that face towards the camera, the
    chances are that your model will not be able to recognize cats from a sideways
    point of view. If your dataset does contain cats from many different sides, but
    you only collected images for a single breed or with a solid white background,
    your model might still have a tough time recognizing all cats. Obtaining quality
    training data is not easy, yet it's essential.
  prefs: []
  type: TYPE_NORMAL
- en: During the training phase of a model, you must provide a set of inputs that
    are of the highest quality possible. The smallest mistake could render your entire
    dataset worthless. Collecting big amounts of high-quality data to train a model
    is a tedious task. It also takes a lot of time. Certain complex models could take
    a couple of hours to crunch all the data and train themselves.
  prefs: []
  type: TYPE_NORMAL
- en: A trained model comes in several types. Each type of model is suitable for a
    different kind of task. For instance, if you are working on a model that can classify
    specific email messages as spam, your model might be a so-called **support vector
    machine**. If you're training a model that recognizes cats in pictures, you are
    likely training a **neural network**.
  prefs: []
  type: TYPE_NORMAL
- en: Each model comes with its pros and cons, and each model is created and used
    differently. Understanding all these different models, their implications, and
    how to train them is extremely hard, and you could likely write a book on each
    kind of model.
  prefs: []
  type: TYPE_NORMAL
- en: In part, this is why Core ML is so great. Core ML enables you to make use of
    pre-trained models in your own apps. On top of this, Core ML standardizes the
    interface that you use in your own code. This means that you can use complex models
    without even realizing it. Let's learn more about Core ML, shall we?
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Core ML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the complex nature of machine learning and using trained models, Apple
    has built Core ML to make incorporating a trained model as straightforward as
    possible. On top of this, another goal was to ensure that whenever you implement
    machine learning using Core ML, your implementation is as fast and energy-efficient
    as possible. Since Apple has been enhancing iOS with machine learning for a couple
    of years now, they have loads of experience of implementing complex models in
    apps.
  prefs: []
  type: TYPE_NORMAL
- en: If you have ever researched machine learning, you might have come across cloud-based
    solutions. Typically, you send a bunch of data to a cloud-based solution, and
    the result is passed back as a response to your request. Core ML is very different,
    since the trained model lives on the device, instead of in the cloud. This means
    that your user's data never has to leave the device, which is very good for your
    user's privacy. Also, having your trained model on the device means that no internet
    connection is required to use Core ML, which saves both time and precious data.
    And since there is no potential bottleneck regarding response latency, Core ML
    is capable of calculating results in real time.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous section, you learned that there are several types of trained
    models. Each type of model is used slightly differently, so if you were to implement
    machine learning in your app manually, you would have to write different wrappers
    around each of the different models your app uses. Core ML makes sure that you
    can use each type of model without even being aware of this in your app; they
    all share the same programming interface. A Core ML model is domain-agnostic.
  prefs: []
  type: TYPE_NORMAL
- en: To be domain-agnostic, all trained models that you use with Core ML must be
    in a particular format. Since machine learning already has a vibrant community
    with several popular formats, Apple has made sure that the most popular models
    can be easily converted to Apple's own `.mlmodel` format. Let's see how to obtain
    `.mlmodel` files for you to use in your own apps.
  prefs: []
  type: TYPE_NORMAL
- en: Obtaining Core ML models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are two ways to obtain a model for you to use in your apps. The simplest
    way is to find an existing `.mlmodel` file. You can find several ready-to-use
    `.mlmodel` files on Apple's machine learning website, at [https://developer.apple.com/machine-learning/](https://developer.apple.com/machine-learning/).
    This website contains several of the most popular models. At the time of writing,
    most of these models are focused on recognizing the dominant objects in an image,
    and chances are that you have different needs for your app.
  prefs: []
  type: TYPE_NORMAL
- en: If you're looking for something that hasn't already been converted by Apple,
    you can try to look in several places online for a pre-converted `.mlmodel` file,
    or you can convert an existing model you have found online. Apple has created
    converters for several popular machine learning formats, such as `.mlmodel` file
    are written in Python, and they ship as part of Xcode. If your needs do not fit
    the converters that Apple provides, you can extend the **toolchain**, since the
    conversion tools are open source. This means that everybody can add their own
    converters or tweak existing converters.
  prefs: []
  type: TYPE_NORMAL
- en: Converting Core ML models using Apple's tools can usually be done with a couple
    of lines of Python. Writing a good conversion script does typically involve a
    little bit of domain knowledge in the area of machine learning, because you'll
    need to make sure that the converted model works just as well as the original
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Once you have obtained a Core ML model for your app, either by converting one
    or finding an existing one, you're ready to add it to your project and begin using
    it. Let's see how to do this next.
  prefs: []
  type: TYPE_NORMAL
- en: Using a Core ML model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applications can utilize Core ML for many different purposes. One of these purposes
    is text analysis. You can use a trained model to detect whether a particular piece
    of text has a positive or negative sentiment. To implement a feature like this,
    you can use a trained and converted Core ML model.
  prefs: []
  type: TYPE_NORMAL
- en: The code bundle for this chapter includes a project named `@IBAction`, named
    `analyze()`. The project folder also contains a file called `SentimentPolarity.mlmodel`.
    This file is a trained Core ML model that analyzes the sentiment associated with
    a certain text. Drag this file into Xcode to add the Core ML model to your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding the model to your project, you can click it in the **Project Navigator**
    to see more information about the model, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Model metadata'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.01_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.1 – Model metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that this model was provided by **Vadym Markov** under the **MIT**
    license. If you click the **Predictions** tab (see the preceding screenshot),
    you can find out which **Input** and **Output** you can expect this model to work
    with:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Input and output of the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.02_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – Input and output of the model
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see in this case; the `[String: Double]` type. This means that we should
    feed this model a dictionary of word counts. If you add this model to Xcode, the
    center section that lists the **Model Class** might notify you that the model
    is not part of any targets yet. If this is the case, fix it as you have done previously,
    by adding this model to your app target in the **Utilities** sidebar on the right
    side of the window.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that your model has been implemented, it's time to take it for a spin. First,
    implement a method that extracts the word count from any given string. You can
    implement this using the `NLTokenizer` object from the new `NaturalLanguage` framework.
  prefs: []
  type: TYPE_NORMAL
- en: '`NLTokenizer` is a text analysis class that is used to split a string into
    words, sentences, paragraphs, or even whole documents. In this example, the tokenizer
    is set up to detect individual words. Implement the word count method as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Add an import to the `ViewController.swift` file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now add the following method to the same file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The previous code iterates over all the words that the tokenizer has recognized
    and stores it in a dictionary of the `[String: Double]` type. You might wonder
    why a `Double` type is used for the word count, rather than an `Int` type, since
    the word counts won''t have to deal with decimals. This is true, but the `SentimentPolarity`
    model requires its input to be a dictionary of the `[String: Double]` type, so
    you must prepare the data accordingly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have the code to prepare the input data for the `SentimentPolarity`
    model, let''s see how you can use this model to analyze the user''s input. Add
    the following implementation for the `analyze()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You might be surprised that this method is so short, but that's how simple Core
    ML is! First, we retrieve the `wordCount` using the method we implemented earlier.
    Then, an instance of the Core ML model is created. When you added the `SentimentPolarity`
    model to the app target, Xcode generated a class interface that abstracted away
    all complexities involving the model. Because the model is now a simple class,
    you can obtain a prediction for the sentiment of the text by calling `prediction(input:)`
    on the model instance.
  prefs: []
  type: TYPE_NORMAL
- en: The `prediction` method returns an object that contains the processed prediction
    in the `classLabel` property, as well as an overview of all available predictions
    and how certain the model is about each option in the `classProbability` property.
    You can use this property if you want to be a bit more transparent to the user
    about the different options that the model suggested and how certain it was about
    these options.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see a couple of examples to demonstrate how it works. First, launch
    the app. Now write `I love rainbows` in the text area and press `I am sad on cloudy
    days`. The result now is **Your text is rated: Neg**. This time, the sentiment
    of your sentence is negative! You can try out your own ideas to see how the model
    behaves in different scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: In the last section of this chapter, you will learn how you can use `Create
    ML` to train your own natural language model to analyze texts that use domain-specific
    language relevant to your own app.
  prefs: []
  type: TYPE_NORMAL
- en: Using Core ML to perform text analysis was quite simple. Now let's see how you
    can use computer vision together with Core ML to determine the type of object
    that exists in a particular picture.
  prefs: []
  type: TYPE_NORMAL
- en: Combining Core ML and computer vision
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you're developing an app that works with photos or live camera footage,
    there are several things you might like to do using computer vision. For instance,
    it could be desirable to detect faces in an image. Or, maybe you would want to
    identify certain rectangular areas of photographs, such as traffic signs. You
    could also be looking for something more sophisticated, such as detecting the
    dominant object in a picture.
  prefs: []
  type: TYPE_NORMAL
- en: To work with computer vision in your apps, Apple has created the **Vision**
    framework. You can combine Vision and Core ML to perform some pretty sophisticated
    image recognition. Before you implement a sample app that uses dominant object
    recognition, let's take a quick look at the Vision framework, so you have an idea
    of what it's capable of and when you might like to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Vision framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Vision framework is capable of many different tasks that revolve around
    computer vision. It is built upon several powerful deep learning techniques that
    enable state-of-the-art facial recognition, text recognition, barcode detection,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: When you use Vision for facial recognition, you get much more information than
    just the location of a face in an image. The framework can recognize several facial
    landmarks, such as eyes, noses, or mouths. All of this is possible due to the
    extensive use of deep learning behind the scenes at Apple.
  prefs: []
  type: TYPE_NORMAL
- en: 'For most tasks, using Vision consists of the following three stages:'
  prefs: []
  type: TYPE_NORMAL
- en: You create a request that specifies what you want; for instance, a `VNDetectFaceLandmarksRequest`
    request to detect facial features.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You set up a handler that can analyze the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting observation contains the information you need.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following code sample illustrates how you might find facial landmarks in
    an image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: For something as complex as detecting the contour of a face or the exact location
    of an eye, the code is quite simple. You set up a `handler` and a `request`. Next,
    the `handler` is asked to `perform` one or more requests. This means that you
    can run several requests on a single image.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to enabling computer vision tasks like this, the Vision framework
    also tightly integrates with Core ML. Let's see just how tight this integration
    is, by adding an image classifier to the augmented-reality gallery app you have
    been working on!
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an image classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code bundle for this section contains an app called **ImageAnalyzer**. This
    app uses an image picker to allow a user to select an image from their photo library
    to use it as an input for the image classifier you will implement. Open the project
    and explore it for a little bit to see what it does and how it works. Use the
    starter project if you want to follow along with the rest of this section.
  prefs: []
  type: TYPE_NORMAL
- en: To add an image classifier, you need to have a Core ML model that can classify
    images. On Apple's machine learning website ([https://developer.apple.com/machine-learning/build-run-models/](https://developer.apple.com/machine-learning/build-run-models/)),
    there are several models available that can do image classification. An excellent
    lightweight model you can use is the **MobileNetV2** model; go ahead and download
    it from the machine learning page. Once you have downloaded the model, drag the
    model into Xcode to add it to the **ImageAnalyzer** project. Make sure to add
    it to your app target so that Xcode can generate the class interface for the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'After adding the model to Xcode, you can open it to examine the **Model Predictions**
    tab. The parameters tell you the different types of inputs and outputs the model
    will expect and provide. In the case of **MobileNetV2**, the input should be an
    image that is **224** points wide and **224** points high, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Input and output of the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.03_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.3 – Input and output of the model
  prefs: []
  type: TYPE_NORMAL
- en: After generating the model, the code to use the model is very similar to the
    code used to detect facial features with Vision earlier. The most significant
    difference is that the type of request that is used is a special `VNCore MLRequest`.
    This type of request takes the Core ML model you want to use, in addition to a
    completion handler.
  prefs: []
  type: TYPE_NORMAL
- en: When combining Core ML and Vision, Vision will take care of image scaling and
    converting the image to a type that is compatible with the Core ML model. You
    should make sure that the input image has the correct orientation. If your image
    is rotated in an unexpected orientation, Core ML might not be able to analyze
    it correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s import the `Vision` framework. Add this statement at the top
    of the `ViewController` class in the **ImageAnalyzer** project:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, add the following implementation for `analyzeImage(_:)` to the `ViewController`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The previous method takes a `UIImage` and converts it to a `CGImage`. Also,
    a `VNCore MLModel` is created, based on the `MobileNetV2` model. This particular
    model class wraps the Core ML model, so it works seamlessly with `Vision`. The
    request is very similar to the request you have seen before. In the `completionHandler`,
    the results array and first prediction of the image classifications are extracted
    and shown to the user. Every prediction made by the classifier will have a label
    that is stored in the identifier and a confidence rating with a value between
    `0` and `1` stored in the `confidence` property. Note that the value of the description
    label is set on the main thread to avoid crashes.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have already implemented two different types of Core ML models that were
    trained for general purposes. Sometimes, these models won''t be specific enough
    for your purposes. For instance, take a look at the following screenshot, where
    a machine learning model labels a certain landscape with only 32% confidence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Photo analysis result'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.04_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Photo analysis result
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn how to train models for purposes that are
    specific to you and your apps by using Create ML.
  prefs: []
  type: TYPE_NORMAL
- en: Training your own models with Create ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As part of Xcode 10 and Apple's version of macOS, **Mojave**, they have shipped
    a tool that you can use to train your own machine learning models by adding specializations
    to existing models. This means that you can train your own natural language model
    that places certain texts in categories that you define. Or, you can train a model
    that recognizes certain product names or terms in a text that are specific to
    your application's domain.
  prefs: []
  type: TYPE_NORMAL
- en: If you're building a news app, you might want to train a Core ML model that
    can automatically categorize the articles in the app. You can then use this model
    to keep track of the articles your users read, and present articles that are most
    likely to fit their interests on a dedicated page in your app.
  prefs: []
  type: TYPE_NORMAL
- en: In this segment, you will learn how to train natural language models and how
    you can train an image recognition model based on the Vision framework. In doing
    so, you will find that creating a large and optimized training set is crucial
    when you want to train a machine learning model.
  prefs: []
  type: TYPE_NORMAL
- en: In the code bundle for this chapter, you will find a Playground called **Create
    ML**. This playground contains all the resources used for training natural language
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Natural Language model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Natural Language framework has excellent features to analyze text with.
    Bundled with the power of machine learning models, you can perform some powerful
    operations on text. Apple has spent a lot of time training several models with
    vast amounts of data to ensure that the Natural Language framework can detect
    names, places, and more.
  prefs: []
  type: TYPE_NORMAL
- en: However, sometimes you might want to add your own analysis tools. To facilitate
    this, the Natural Language framework works well with Core ML and Apple's new **Create
    ML** framework. With **Create ML**, you can easily and quickly create your own
    machine learning models that you can use in your apps straight away.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use several different types of training for a Natural Language model.
    In this section, you will learn about two different models:'
  prefs: []
  type: TYPE_NORMAL
- en: A text classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A word tagger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **text classifier** will classify a particular piece of text with a label.
    This is similar to the sentiment analysis you have implemented in the **TextAnalyzer**
    sample app. An example of an entry in your training data would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: This is a sample of a news article headline that belongs in a category labeled
    `Tech`. When you feed a large number of samples like this to your model, you could
    end up with a classifier that can apply labels to news articles based on their
    headlines. Of course, this assumes that the headlines are specific enough and
    contain enough information to train the classifier properly. In reality, you will
    find that short sentences like these will not make the best models. The sample
    Playground contains a JSON file with training data that attempts to separate news
    articles into the two categories of politics and tech. Let's see how the model
    can be trained so you can then see for yourself how accurate the model is.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code trains and stores the custom Core ML model. In the playground
    file, open the `Labeller`. Check the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Training the entire model requires only a couple of lines of code. All you need
    to do is obtain your training data, create the classifier, and save it somewhere
    on your machine. You can even do some quick testing to see whether your model
    works well, from right inside the playground.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the preceding code uses a `try!` statement. This is done to keep the
    code sample brief and simple. In your own apps, you should always strive for proper
    error handling to avoid surprising crashes.
  prefs: []
  type: TYPE_NORMAL
- en: The string passed to the `URL(fileURLWithPath:)` initializer represents the
    location where your model will be stored. Make sure to specify the full path here,
    so, for instance, use `/Users/yourUser/Desktop/TextClassifier.mlmodel`, and not
    `~/Desktop/TextClassifier.mlmodel`. Make sure to substitute `yourUser` with your
    own username or folder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following lines of code test two different headlines to see if the model
    correctly labels them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If you're happy with the results of your model, you can grab the trained model
    from the place where you saved it, and immediately add it to your Xcode project.
    From there, you can use the model like you would use any other model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see another example of a model from the Natural Language framework.
    In this case, the model should label every word in a text to classify it as a
    certain type of word. For instance, you could train the model to recognize certain
    brand names, product names, or other words that have special meanings to your
    app. An example of some training data that you could use to train a model like
    this is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'By collecting many samples that include the words that you want to label, your
    model will be able to not only match tags based on the word itself, but even on
    the surrounding words. Essentially, the model would be aware of the context in
    which each word is used to then determine the correct tag. Once you have collected
    enough sample data, you can train the model in a similar way as the classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The amount of code to train the model hasn't changed. The only difference is
    that the previous model was based on the `MLTextClassifier` class, and the current
    model is based on `MLWordTagger`. Again, you can immediately use the trained model
    to make some predictions that you can then use to validate whether the model was
    trained properly. Providing good data and testing often are the keys to building
    a great Core ML model.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to text analysis models, Create ML can also help you to train your
    own image recognition models. Let's see how this works next.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Vision model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the **ImageAnalyzer** sample app, you saw that picking an image of a certain
    car would be classified as a sports car with a pretty low confidence score. You
    can train your own vision model that specializes in recognizing certain cars.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting good training data for image classifiers is tough, because you have
    to make sure that you gather many pictures of your subjects from all sides and
    in many different environments. For instance, if all your car images feature cars
    that are next to trees, or on the road, the model might end up classifying anything
    with trees or a road next to it as a car. The only way to obtain a perfect training
    set is to experiment, tweak, and test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Training a Vision model works slightly differently from training a Natural
    Language model. You can''t use a JSON file to feed your test data to the classifier.
    So, instead, you should create folders that contain your images where the folder
    name is the label you want to apply to each image inside that folder. The following
    screenshot is an example of a training set that contains two kinds of labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – Training set of images'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.05_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – Training set of images
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have collected your set of training data, you can store it anywhere
    on your computer—for instance, on the desktop. You will then pass the path for
    your training data to your model training code as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Again, you only need a couple of lines of code to train a model. That's how
    powerful Create ML is. If you want, you can quickly test your image classifier
    by dropping the `.mlmodel` file in the `MobileNetV2` classifier that you used
    before.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the simple ways of training models, there are certain parameters
    that you can pass to the different Create ML classifiers. If you have trouble
    training your models properly, you could tweak some of the parameters that are
    used by Create ML. For instance, you could apply more iterations to your training
    set, so the model gains a deeper understanding of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned before in this chapter, machine learning is a subject that could
    span several books on its own, and even though Create ML makes training models
    straightforward and simple, it's not easy to train a robust model without any
    prior machine learning experience.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have learned how to use your own trained data, in the next section,
    we are going to learn how to update your models from the cloud, without the need
    to update the app itself.
  prefs: []
  type: TYPE_NORMAL
- en: Updating models remotely with Model Deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the new features of iOS 14 for machine learning is the ability to keep
    collections of your models in the cloud, giving you the power to update them at
    any time without the need to update the app itself.
  prefs: []
  type: TYPE_NORMAL
- en: We are going to use a project, available in the code bundle of this book, in
    order to demonstrate this new feature. The project's name is **TextAnalyzerCloud**.
    It is the same project that we used before, but this time, the model will be on
    the cloud (with a local copy as a fallback).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two steps involved in order to use Model Deployment in our apps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the Core ML API to retrieve collections of models.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Prepare and deploy the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's implement these steps in the next subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Core ML API to retrieve collections of models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start by learning how to retrieve models that are stored in the cloud
    into your app. Open the `ViewController` class. At this point, the class just
    contains an `analyze` method that counts the words inside a `textView` and makes
    a prediction if a model exists. The class also contains some methods to display
    error and success messages to the user. Note that we have also defined the following
    property: `var model: SentimentPolarity?`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `analyze` method, we are going to download a model from the cloud, and
    in case of failure, we will use a local modal as a fallback. Let''s modify the
    method to achieve this. Update the implementation of the `analyze` method, and
    add the following code where it says `//add code`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s review the preceding code blocks (the following numbers refer to the
    comments in the preceding code):'
  prefs: []
  type: TYPE_NORMAL
- en: First, in `//1`, we are accessing the new Core ML API to retrieve a collection
    of models from our account on the Apple servers. We do that by using the `MLModelCollection.beginAccessing`
    method with an identifier for the collection (that has to match the one in the
    cloud) – in our case, we used `SentimentPolarityCollection`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, in `//2`, we are checking the result of `beginAccessing`. If it is successful
    and we get a collection of models, we search for a specific model with an identifier
    of `SentimentPolarity` and we extract the `modelURL` from it. If we get any errors
    (such as there being no network connection), we call the `handleCollectionError`
    method to handle it properly (in our case, we inform the user with a modal).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have a model URL, in `//3`, we try to load it. We haven't implemented
    the `loadSentimentClassifier` method yet, but we will do it shortly. Just take
    into account that this method will try to load a model with a given remote URL,
    and it will wrap it in a `Result<SentimentPolarity, Error>` enum (to handle errors
    properly).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the last part, under comment `//4`, we inspect the `Result` from `//3`. If
    we obtained a model, we store it in the `model` property variable. We store it
    so we don't need to download the model over and over again. After storing the
    model, we use it to analyze the text. On the other hand, if we obtained an error,
    we display a message to the user to inform them about it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now let''s add the `loadSentimentClassifier` method so the class compiles.
    Add the following method to the `ViewController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This method receives an optional model URL as a param; that is, the URL of
    our model stored in Apple Servers. It is an optional value because when we try
    to fetch it, it can fail (for example, if the user doesn''t have an internet connection).
    Inside the method, we handle two possibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: If the URL is not nil, we use it to initialize the model with `SentimentPolarity(contentsOf:)`
    and return it inside a `Result`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the URL is nil, we try to initialize the model with a local version and
    the default configuration with `SentimentPolarity(configuration: .init())`. Again,
    we return it inside `Result`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this method implemented, we have all the code necessary to load a model
    from the network and use it in our app. However, we still need to perform two
    important steps to complete the process: Prepare the model to be uploaded to the
    Apple servers in the proper format, and deploy the model to the cloud.'
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and deploying the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we created the methods to retrieve a model from the
    Apple servers and into our app. Now, we are going to prepare our local model to
    be deployed into the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the project explorer, click on the file named `SentimentPolarity.mlmodel`.
    Now, go to the **Utilities** tab. You will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Model Utilities tab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.06_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Model Utilities tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Create Model Archive**. This new option in iOS 14 will help us to
    deploy our model onto the Apple servers in the cloud. When you click it, this
    popup will appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Generate Model Archive'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.07_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.7 – Generate Model Archive
  prefs: []
  type: TYPE_NORMAL
- en: 'For now, leave the **Encrypt Model** checkbox unchecked and click **Continue**
    (we will explore this option later in the chapter). After clicking **Continue**,
    Xcode will generate an archive of the model and will display this modal:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – The Model Archive Generated dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.08_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.8 – The Model Archive Generated dialog
  prefs: []
  type: TYPE_NORMAL
- en: 'You can click on the blue arrow to the right of the first option in the preceding
    screenshot and it will take you to the exact location where the archive of your
    model is located. You will need to remember this location to upload the archive
    to the Apple servers. You will see a file with the`.mlarchive` extension, as in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Location of the archived model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.09_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.9 – Location of the archived model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now click on the blue arrow next to the second option that reads **You can
    upload the Model Archive on the Core ML Model Deployment dashboard**. It will
    open your web browser at the following page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.10 – Core ML Model Collections page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.10_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.10 – Core ML Model Collections page
  prefs: []
  type: TYPE_NORMAL
- en: 'This is your dashboard for managing your model collections on the Apple servers.
    What we need to do now is to create a new collection with a reference to our model
    inside, and upload the model archive we just created. Let''s do this; click on
    the blue plus (**+**) icon next to **Model Collections**, and fill in the form
    that appears with the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.11 – Create a Model Collection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.11_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.11 – Create a Model Collection
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review the input fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '`//1`: `MLModelCollection.beginAccessing(identifier: "SentimentPolarityCollection")`),
    we used the identifier `SentimentPolarityCollection`. Use the same one here (otherwise,
    you will not be able to download the collection).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Description**: Use this field to create a description that will help you
    to recognize this collection later on. Take into account that if you work in a
    team, it will need to be useful to the other developers too.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SentimentPolarity` (under comment `//2`: `modelURL = collection.entries["SentimentPolarity"]`).
    Again, these identifiers have to match each other. You have the possibility to
    add more model identifiers by pressing the **Model ID** blue button, but in our
    case, we have just one model inside our collection.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, you can click the blue **Create** button, and you will land on the
    following model collection page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Model collection page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.12_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.12 – Model collection page
  prefs: []
  type: TYPE_NORMAL
- en: 'From this page, you can finally deploy or archive the model into its reference
    on the cloud. Click on the blue plus (**+**) button next to **Deployments**, and
    fill in the fields as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.13 – Model deployment properties'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.13_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.13 – Model deployment properties
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s review the fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Deployment ID**: You can specify any text here that describes why you are
    deploying this model. It is just a descriptive field; it doesn''t need to match
    anything.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.mlarchive` file we created before in Xcode when archiving the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Notice in the bottom part of the form that we can add **Additional Targeting
    Rules**. This is another new feature of iOS 14 that allows us to target our models
    based on device characteristics. For example, we can download certain models only
    to iPads, or for specific OS versions. To keep this example simple, we are not
    going to add any rules, but you should try it out in your apps!
  prefs: []
  type: TYPE_NORMAL
- en: 'After you upload the `.mlarchive` file, it should display as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Our first model deployed'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.14_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.14 – Our first model deployed
  prefs: []
  type: TYPE_NORMAL
- en: When the status is `analyze` method will give you a verdict.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have learned how to consume the Core ML API to fetch models
    from the cloud to keep your app models up to date. You also learned how to prepare
    your models and how to deploy them to the Apple servers. Now you are going to
    learn how to encrypt those models with a new iOS 14 feature to keep your model's
    data safe on users' devices.
  prefs: []
  type: TYPE_NORMAL
- en: Encrypting Core ML models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the new features of iOS 14 Core ML is the ability to encrypt your machine
    learning models on users' devices. Xcode 12 has a new tool that will help you
    to create a private key that you will deploy to the Apple servers. Your app will
    download that key and store it securely on the users' devices, and will use the
    key to decrypt the local (encrypted) model, load that decrypted version into memory
    (so it is not stored insecurely), and have it ready for use in your app.
  prefs: []
  type: TYPE_NORMAL
- en: 'The steps to create the key and deploy it to the Apple servers are very straightforward.
    First, you select your model in the project explorer; in our case, open the `SentimentPolarity.mlmodel`
    file. Then, click on the **Utilities** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.15 – Model encryption'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.15_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.15 – Model encryption
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, click on **Create Encryption Key**. In the popup that appears, select
    the proper development account for your app:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Selecting the development team for the encryption key'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.16_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.16 – Selecting the development team for the encryption key
  prefs: []
  type: TYPE_NORMAL
- en: 'This will generate a key and `.mlmodelkey` in your folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Generating an encryption key'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.17_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.17 – Generating an encryption key
  prefs: []
  type: TYPE_NORMAL
- en: Clicking the blue arrow will take you to the specific folder where this key
    is stored. You will need to remember the location if you want to deploy this key
    to the Apple servers later so your team can use it too. Click **OK** and close
    the popup.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now if you click on **Create Model Archive**, you will notice that the **Encrypt
    Model** checkbox is active this time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.18 – Generating a model archive with encryption'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.18_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.18 – Generating a model archive with encryption
  prefs: []
  type: TYPE_NORMAL
- en: When you click **Continue**, Xcode creates an encrypted archive this time. The
    steps that follow are exactly the same as the steps we learned in the *Prepare
    and Deploy the model* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, you can also tell Xcode to encrypt the bundled model (the local copy).
    To do this, after generating the encryption key (as we just did), you need to
    click on your project, go to **Build Phases**, and open the **Compile Sources**
    section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.19 – The Build Phases tab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.19_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.19 – The Build Phases tab
  prefs: []
  type: TYPE_NORMAL
- en: 'Now select the **SentimentPolarity.mlmodel** model and on the right side of
    its row, you can double-click to add a flag. Add the route to the encryption key
    in your project folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'It should look like this after you have added the flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.20 – Model with the encryption flag'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_10.20_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.20 – Model with the encryption flag
  prefs: []
  type: TYPE_NORMAL
- en: Now if you build the app, Xcode will generate an encrypted version of the model
    inside your app.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have learned how to encrypt your model locally (and how to encrypt an archive
    for the Apple servers). Let''s see now how you can load that model at runtime.
    There is a new class method in ML Models named `load` that will decrypt the model
    for you, downloading the encryption key from the Apple servers. Check out the
    following example code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, the `class func load` will try to download the encryption
    key from the Apple servers and will decrypt the model with it, storing it in memory.
    We assign that decrypted model to our variable model, and it is ready to use.
    We also handle the failure case, displaying an error.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you learned how to generate an encryption key, how to encrypt
    an archived model to upload to the Apple servers and also to encrypt the local
    copy of it, and finally how to load and decrypt the model for the app to use.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have seen how you can make use of the machine learning
    capabilities that iOS provides. You saw that adding a machine learning model to
    your app is extremely simple since you only have to drag it to Xcode and add it
    to your target app. You also learned how you can obtain models, and where to look
    to convert existing models to Core ML models. Creating a machine learning model
    is not simple, so it's great that Apple has made it so simple to implement machine
    learning by embedding trained models in your apps.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to Core ML, you also learned about the Vision and Natural Language
    frameworks. Vision combines the power of Core ML and smart image analysis to create
    a compelling framework that can perform a massive amount of work on images. Convenient
    requests, such as facial landmark detection, text analysis, and more are available
    out of the box without adding any machine learning models to your app. If you
    do find that you need more power in the form of custom models, you now know how
    to use Create ML to train, export, and use your own custom trained Core ML models.
    You learned that Create ML makes training models simple, but you also learned
    that the quality of your model is drastically impacted by the quality of your
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you learned how to deploy your Core ML models in the cloud in order
    to update them without the need to update the app, and how to encrypt and decrypt
    them to store your models safely on the user device.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how you can capture, manipulate, and use
    media files in your apps, including audio, photo, and video elements.
  prefs: []
  type: TYPE_NORMAL
