<html><head></head><body>
        

                            
                    <h1 class="header-title">Understanding the Environment</h1>
                
            
            
                
<p class="mce-root">Augmented reality applications are all about enhancing or augmenting the user's reality. In order to do this, we as AR app developers need a set of tools capable of understanding the user's environment. As we saw in the last chapter, ARCore uses <strong>visual-inertial odometry</strong> (<strong>VIO</strong>) to identify objects and features in the environment, which it can then use to obtain a pose of the device and track motion. However, this technology can also help us identify objects and their pose using the same toolkit. In this chapter, we will explore how we can use the ARCore API to better understand the user's environment. Here's a quick overview of the main topics we will cover in this chapter:</p>
<ul>
<li class="mce-root">Tracking the point cloud</li>
<li class="mce-root">Meshing and the environment</li>
<li class="mce-root">Interacting with the environment</li>
<li class="mce-root">Drawing with OpenGL ES</li>
<li>Shader programming</li>
</ul>
<p>If you have not downloaded the source code from GitHub, you will need to do so for this chapter. Of course, you will also need to have completed the setup and installation of Android covered in <a href="c5c4b444-3342-457a-b756-266772b70d06.xhtml" target="_blank">Chapter 2</a>, <em>ARCore on Android</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Tracking the point cloud</h1>
                
            
            
                
<p>As we discussed, motion tracking in ARCore is done by identifying and tracking recognizable features around the user. It then uses those points with the device's orientation and accelerometer sensors to keep its tracking updated. Without doing this, the ability to track accurately quickly falls apart. Additionally, we gain the benefit of now tracking multiple points that ARCore identifies as object points. Let's see an example of what these tracking points look like by starting up the sample ARCore Android app again. Follow the given steps to get started:</p>
<ol>
<li>Open Android Studio. If you haven't opened any other projects, then it should immediately load the Android ARCore sample project. If not, load the project in the <kbd>Android/arcore-android-sdk/samples/java_arcore_hello_ar</kbd> folder.</li>
<li>Open the <kbd>HelloArActivity.java</kbd> file and scroll down to the <kbd>OnDrawFrame</kbd> method, as shown in the following excerpt:</li>
</ol>
<div><img src="img/3134f94f-df62-41e5-bd03-25269bbb5463.png" style="font-family: 'Times New Roman', times, serif;font-size: 10pt;text-align: center;"/></div>
<p>Opening the HelloArActivity.java file in Android Studio</p>
<ol start="3">
<li><kbd>OnDrawFrame</kbd> is the render method, exactly like the <kbd>update</kbd> function we have seen in the web example. This method is called every frame, generally around 60 frames per second in the typical 3D app. We also call 60 fps as the frame rate. Frame rates will vary depending on how much your code performs each frame. Therefore, we want our <kbd>render</kbd> function and the code inside to be as fast as possible. We will talk more about performance and rendering in <a href="e7c0bdd1-e380-4498-af5a-fe9e627eb6cb.xhtml" target="_blank">Chapter 11</a>, <em>Performance Tips and Troubleshooting</em>.</li>
<li>The first line in this method, starting with <kbd>GLES20.glClear</kbd>, clears the render buffer and prepares for drawing.</li>
</ol>
<p>Depending on the 3D platform you are working with, you may or may not have to worry about specific details such as clearing render buffers. Unity, for instance, hides many of these details away from the developer, which can be good and bad. Just understand that all 3D platforms will generally follow the same principals.</p>
<ol start="5">
<li>Scroll down a bit to just inside the <kbd>try</kbd> block and add the following line:</li>
</ol>
<pre style="padding-left: 60px">Frame frame = mSession.update();</pre>
<ol start="6">
<li><kbd>Frame</kbd> represents the current AR view captured from the device's camera. We get access to an instance of <kbd>frame</kbd> by calling <kbd>mSession.update()</kbd>; <kbd>mSession</kbd>, which is initialized earlier, represents our ARCore session service.</li>
<li><kbd>Frame</kbd> also exposes a number of helper methods; scroll down until you see the following lines:</li>
</ol>
<pre style="padding-left: 60px">mPointCloud.update(frame.getPointCloud());<br/>mPointCloud.draw(frame.getPointCloudPose(), viewmtx, projmtx);</pre>
<ol start="8">
<li>Starting with <kbd>mPointCloud.update()</kbd>, this call gets the visible points in the current <kbd>frame</kbd>. Then, <kbd>mPointCloud.draw()</kbd> draws the points based on the cloud's pose, using the current view (<kbd>viewmtx</kbd>) and projection (<kbd>projmtx</kbd>) matrices.</li>
</ol>
<p>View and projection matrices represent the camera or combined scene view. With <kbd>three.js</kbd>, this was handled for us. Likewise, when we get to Unity, we won't need to worry about setting these matrices either.</p>
<ol start="9">
<li>Connect your device to your machine, either through USB or remotely. Then, build and run the app on your device. Pay particular attention to the drawing of the point cloud.</li>
</ol>
<p>Note how the number of points increases the longer you hold the device in one orientation. These points represent those identifiable and recognizable feature points used for tracking and interpreting the environment. Those are the same points that will help us identify objects or surfaces in the environment. In the next section, we will look at how surfaces are identified and rendered.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Meshing and the environment</h1>
                
            
            
                
<p>So, being able to identify features or corners of objects is really just the start of what we would like to know about the user's environment. What we really want to do is use those feature points to help us identify planes, surfaces, or known objects and their pose. ARCore identifies planes or surfaces automatically for us through a technique called <strong>meshing</strong>. We have already seen how meshing works numerous times in the advanced samples, when ARCore tracks surfaces. Now, before we get ahead of ourselves, let's picture what a point cloud and mesh look like in 3D, with the following diagram:</p>
<div><br/>
<img src="img/bb4166b5-8609-4c8f-b6c2-6a9ccd0bdfa6.jpg" style="width:43.92em;height:23.92em;"/><br/>
<br/>
Point cloud and mesh in 3D</div>
<p>If you pay attention to the diagram, you will see an inset figure showing a polygon and the ordered set of vertices that comprise it. Note how the order of points goes counterclockwise. Yes, the order in which we join points makes a difference to the way a surface is facing when a mesh is lit and shaded. When a scene is rendered we only see surfaces that face the camera. Surfaces pointing away from the camera are removed or back-face culled. The order in which we join points is called winding and isn't something you have to worry about unless you plan to create meshes manually.</p>
<p>Meshing is the process of taking a collection of feature points and constructing a mesh from it. The generated mesh is then often shaded and rendered into the scene. If we run the sample right now and watch, we will see the surfaces or plane meshes being generated and placed by ARCore. How about we open up the Android sample project again in Android Studio to see where this meshing occurs:</p>
<ol>
<li>Ensure that your code is open to where we left off last time. You should be looking at the lines with <kbd>mPointCloud</kbd>.</li>
<li>Scroll down just a little until you see this block of code:</li>
</ol>
<pre style="padding-left: 60px">if (messageSnackbar != null) {<br/>  for (Plane plane : session.getAllTrackables(Plane.class)) {<br/>    if (plane.getType() == com.google.ar.core.Plane.Type.HORIZONTAL_UPWARD_FACING<br/>        &amp;&amp; plane.getTrackingState() == TrackingState.TRACKING) {<br/>      hideLoadingMessage();<br/>      break;<br/>    }<br/>  }<br/>}</pre>
<ol start="3">
<li>This block of code just loops through the trackables of type <strong>Plane</strong> (a flat mesh) identified in the session. When it identifies a tracked plane, of the correct type, it hides the loading message and breaks out of the loop.</li>
</ol>
<ol start="4">
<li>Then, it renders any planes it identifies with this line:</li>
</ol>
<pre style="padding-left: 60px">planeRenderer.drawPlanes(<br/>    session.getAllTrackables(Plane.class), camera.getDisplayOrientedPose(), projmtx);</pre>
<ol start="5">
<li>The <kbd>planeRenderer</kbd> helper class is for drawing planes. It uses the <kbd>drawPlanes</kbd> method to render any of the identified planes the ARCore session has identified using the view and projection matrices. You will notice it passes all the planes in through a call to <kbd>getAllTrackables(Plane.class)</kbd>.</li>
<li>Put your cursor on <kbd>drawPlanes</kbd> and type <em>Ctrl </em>+ <em>B</em> (<em>command</em> + <em>B</em> on Mac) to go to the definition.</li>
<li>Now you should see the <kbd>drawPlanes</kbd> method in the <kbd>PlaneRenderer.java</kbd> file—don't panic. Yes, there is a lot of scary code here, which, thankfully, is already written for us. As an exercise, just scroll through and read the code. We don't have time to go through it in depth, but reading through this code will give you more insight into the rendering process.</li>
<li>From the menu, select Run - Run 'HelloArActivity'. Now, as the app runs, pay special attention to the way the surfaces are rendered and how you can interact with them.</li>
</ol>
<p>Okay, now we understand how surfaces are created and rendered. What we also need to understand is how we interact with those surfaces or other objects in the environment.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Interacting with the environment</h1>
                
            
            
                
<p>We know that ARCore will provide us with identified feature points and planes/surfaces it recognizes around the user. From those identified points or planes, we can attach virtual objects. Since ARCore keeps track of these points and planes for us, as the user moves objects, those that are attached to a plane remain fixed. Except, how do we determine where a user is trying to place an object? In order to do that, we use a technique called <strong>ray casting</strong>. Ray casting takes the point of touch in two dimensions and casts a ray into the scene. This ray is then tested against other objects in the scene for collisions. The following diagram shows how this works:</p>
<div><br/>
<img src="img/cfe88118-9755-4b72-831f-e7fe6063e945.png" style="width:41.00em;height:23.33em;"/><br/>
<br/>
Example of ray casting from device screen to 3D space</div>
<p>You, of course, have likely already seen this work countless times. Not only the sample app, but virtually every 3D application uses ray casting for object interaction and collision detection. Now that we understand how ray casting works, let's see how this looks in code:</p>
<ol>
<li>Open up Android Studio, the sample project, and the <kbd>HelloArActivity.java</kbd> file.</li>
<li>Scroll down to the following block of code:</li>
</ol>
<pre style="padding-left: 60px">MotionEvent tap = queuedSingleTaps.poll();<br/>if (tap != null &amp;&amp; camera.getTrackingState() == TrackingState.TRACKING) {<br/>  <strong>for (HitResult hit : frame.hitTest(tap)) {</strong><br/>    // Check if any plane was hit, and if it was hit inside the plane <br/>       polygon<br/>    Trackable trackable = hit.getTrackable();<br/>    // Creates an anchor if a plane or an oriented point was hit.<br/>    <strong>if ((trackable instanceof Plane &amp;&amp; ((Plane) trackable).isPoseInPolygon(hit.getHitPose()))</strong><br/><strong>        || (trackable instanceof Point</strong><br/><strong>            &amp;&amp; ((Point) trackable).getOrientationMode()</strong><br/><strong>                == OrientationMode.ESTIMATED_SURFACE_NORMAL)) {</strong><br/>      // Hits are sorted by depth. Consider only closest hit on a plane <br/>         or oriented point.<br/>      // Cap the number of objects created. This avoids overloading <br/>         both the<br/>      // rendering system and ARCore.<br/>      if (anchors.size() &gt;= 20) {<br/>        anchors.get(0).detach();<br/>        anchors.remove(0);<br/>      }<br/>      // Adding an Anchor tells ARCore that it should track this          <br/>        position in<br/>      // space. This anchor is created on the Plane to place the 3D <br/>         model<br/>      // in the correct position relative both to the world and to the <br/>         plane.<br/>      anchors.add(hit.createAnchor());<br/>      break;<br/>    }<br/>  }<br/>}</pre>
<ol start="3">
<li>Read through the comments and pay attention to the highlighted lines of code. The first highlighted line starts a loop based on the number of hits detected in the scene using <kbd>frame.hitTest(tap)</kbd>. That call is doing the ray casting to determine what objects may be hit by the tap. A <strong>tap</strong> represents the screen touch in 2D.<br/>
The next highlighted line is inside the <kbd>if</kbd> statement that checks which of the ARCore recognized planes are touched. If there is a hit, we first check that the number of <kbd>anchors</kbd> is less than 20, where each anchor represents an attachment point. Then we add a new <kbd>Anchor</kbd> to the collection of <kbd>anchors</kbd> <kbd>ArrayList</kbd>, with a reference to a new anchor using <kbd>hit.createAnchor</kbd> .</li>
<li>Scroll down some more to the following block of code in <kbd>onDrawFrame</kbd>:</li>
</ol>
<pre style="padding-left: 60px">// Visualize anchors created by touch.<br/>float scaleFactor = 1.0f;<br/><strong>for (Anchor anchor : anchors) {</strong><br/>  if (anchor.getTrackingState() != TrackingState.TRACKING) {<br/>    continue;<br/>  }<br/>  // Get the current pose of an Anchor in world space. The Anchor pose is updated<br/>  // during calls to session.update() as ARCore refines its estimate of the world.<br/>  <strong>anchor.getPose().toMatrix(anchorMatrix, 0);</strong><br/><br/>  // Update and draw the model and its shadow.<br/>  virtualObject.updateModelMatrix(anchorMatrix, scaleFactor);<br/>  virtualObjectShadow.updateModelMatrix(anchorMatrix, scaleFactor);<br/>  virtualObject.draw(viewmtx, projmtx, lightIntensity);<br/>  virtualObjectShadow.draw(viewmtx, projmtx, lightIntensity);</pre>
<ol start="5">
<li>Take a quick read through the code. The first highlighted line starts by looping through the <kbd>anchor</kbd> in the <kbd>anchors</kbd> list. We then check whether the anchor is being tracked; if it is, we get its pose in the second highlighted line. Then, we draw our <kbd>virtualObject</kbd> (Andy) in the last lines of code. Note that in this case, we are also drawing shadows.</li>
<li>Change the first line of code to match the following:</li>
</ol>
<pre style="padding-left: 60px">float scaleFactor = <strong>2.0f</strong>;</pre>
<ol start="7">
<li>This change will double the size of Andy. Run the app in your device and wait for some surfaces to appear. Then, touch the screen to drop Andy. He should now look double the size.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Touch for gesture detection</h1>
                
            
            
                
<p>So, that covers simple interactions. How about we add another gesture to allow the user to clear all the attachment points and thus remove the Andy robot from the scene. Follow along the given steps to add another touch gesture:</p>
<ol>
<li>Scroll to the following section of code:</li>
</ol>
<pre style="padding-left: 60px">// Set up tap listener.<br/>gestureDetector =<br/>    <strong>new GestureDetector(</strong><br/>        this,<br/>        new GestureDetector.SimpleOnGestureListener() {<br/>          @Override<br/>          public boolean onSingleTapUp(MotionEvent e) {<br/>            onSingleTap(e);<br/>            return true;<br/>          }<br/><br/>          @Override<br/>          public boolean onDown(MotionEvent e) {<br/>            return true;<br/>          }<br/>        });<br/><br/><strong>surfaceView.setOnTouchListener(</strong><br/>    new View.OnTouchListener() {<br/>      @Override<br/>      public boolean onTouch(View v, MotionEvent event) {<br/>        return gestureDetector.onTouchEvent(event);<br/>      }<br/>    });</pre>
<ol start="2">
<li>The preceding section of code is in the <kbd>onCreate</kbd> method of the <kbd>HelloArActivity</kbd>. It first sets up <kbd>gestureDetector</kbd> for interpreting the selected touch events. Then, we set a listener with <kbd>setOnTouchListener</kbd> in order to capture touch events and send them to the gesture detector. Just remember that the listener listens for the touch, and the gesture detector interprets the type of touch. So what we want to do is capture another form of gesture from the user.</li>
<li>Add the following code right after the highlighted section:</li>
</ol>
<pre style="padding-left: 60px"><strong>@Override<br/>public boolean onDown(MotionEvent e) { return true;}</strong> //after this section<em><strong><br/><br/></strong></em>@Override<em><strong><br/></strong></em>public void onLongPress(MotionEvent e) {<br/> onLongPressDown(e);<br/>}<br/></pre>
<ol start="4">
<li>That sends our event to a new method, <kbd>onLongPressDown</kbd>. Let's add this new method just below the other gesture handling method by adding the following code:</li>
</ol>
<pre style="padding-left: 60px"><strong>private void onSingleTap(MotionEvent e) {</strong><br/>    // Queue tap if there is space. Tap is lost if queue is full.<strong><br/>    mQueuedSingleTaps.offer(e);</strong><br/><strong>}  </strong>//after this block of code<strong><br/></strong>private void onLongPressDown(MotionEvent e) {<br/>    mTouches.clear();<br/>}</pre>
<ol start="5">
<li>All that happens inside <kbd>onLongPressDown</kbd> is the collection of <kbd>anchors</kbd>, <kbd>anchors</kbd> is cleared. By clearing the <kbd>anchors</kbd>, we clear the attachment points and thus any rendering of Andy.</li>
<li>Save the file, connect your device, and run the sample. Try placing a few big Andy's around the scene. Then, use the new long press gesture to remove them.</li>
</ol>
<p>Good, now we have a basic understanding of how we can interact with the environment. In the next section, we will cover some basics of OpenGL ES, the 3D rendering framework we are using for Android.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Drawing with OpenGL ES</h1>
                
            
            
                
<p>OpenGL ES or just GLES is the trimmed down mobile version of OpenGL. OpenGL is a low-level and powerful 2D and 3D drawing API similar to DirectX. Since it is a low-level library, it does require significant knowledge of 2D/3D maths. Again, for our purposes, we will avoid most of the nasty math and just modify some of the drawing code to change the way the sample app functions. What we will do is modify the sample app to change the way objects are drawn. Load up Android Studio with the sample project and let's get started:</p>
<ol>
<li>Scroll down to the bottom of <kbd>PointCloudRenderer.java</kbd> and look at the following section of code identified in the following screen excerpt:</li>
</ol>
<div><img src="img/3a74943b-eabf-4290-982a-0c1e5cd97cbf.png"/></div>
<p>PointCloudRenderer.java open on the draw method</p>
<ol start="2">
<li>Now the code is straightforward, but a lot of what is going on assumes that the developer has a good foundation in 3D maths and graphic rendering. We don't have time to go through every step, but, essentially, all that the code is doing is drawing the identified point cloud features (those blue points).</li>
</ol>
<p>When we get to the chapters on Unity, you may start wondering why someone would ever put themselves through the pain of writing an AR app with OpenGL ES. That's a good question. Rendering realistic 3D graphics is all about speed and performance. While Unity does an excellent job at rendering, it still is just another layer of software on top of OpenGL ES. This means that Unity would typically run slower than its native OpenGL ES counterpart. How much slower, really depends on what you are trying to do.</p>
<ol start="3">
<li>Take a look at the identified line in the following excerpt, as shown:</li>
</ol>
<pre style="padding-left: 60px">GLES20.glUniform4f(colorUniform, 31.0f / 255.0f, 188.0f / 255.0f, 210.0f / 255.0f, 1.0f);</pre>
<ol start="4">
<li>This line sets the color of the rendered point cloud. It does this by normalizing the RGB color values of <kbd>31.0</kbd>, <kbd>188.0</kbd>, and <kbd>210.0</kbd> by dividing them by <kbd>255.0</kbd>, thus creating a uniform or normalized color vector of values from 0 to 1. With the last value of <kbd>1.0</kbd> representing the alpha or transparency, where <kbd>1.0</kbd> means the color is <strong>opaque</strong> and <kbd>0.0</kbd> means it is <strong>transparent</strong>.</li>
<li>Let's experiment a little by changing that line of code to the following:</li>
</ol>
<pre style="padding-left: 60px">GLES20.glUniform4f(colorUniform, 255.0f / 255.0f, 255.0f / 255.0f, 255.0f / 255.0f, 1.0f); </pre>
<ol start="4"/>
<ol start="6">
<li>Next, we will change the size of points we draw so that they are clearly visible, by changing the following line of code:</li>
</ol>
<pre style="padding-left: 60px">GLES20.glUniform1f(pointSizeUniform, <strong>25.0f</strong>);</pre>
<ol start="7">
<li>Save the file, connect up your device, and then deploy and run the app. As the app runs, note the color of the points now. Is it what you expected?</li>
</ol>
<p>Now, we can clearly see how and where the feature points are being identified. However, we still don't get a lot of information from the point data. What if we color the points based on their distance to the viewer? This will allow us to visualize our environment point cloud with some depth information. Doing this in a low-level API such as OpenGL ES to manually subset points by color will require substantial code changes. Fortunately, we can even go lower and write a program called a <strong>shader</strong> to change the color of the point just before we draw it. We will take a dive in to shader programming in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Shader programming</h1>
                
            
            
                
<p>Shader programming is probably one of the most difficult and low-level development tasks you can do as a graphics programmer. It requires an excellent knowledge of 3D math and the graphics rendering process. Also, writing good shaders is a skill that can take years to master. So why are we covering this in a book that covers fundamentals? Simply put, coding a good shader may be difficult, but it is also extremely rewarding, and it's a skillset that is essential to any serious 3D programmer.</p>
<p>We will be using shaders throughout the rest of this book for many things. If, at this point, you are starting to feel overwhelmed, then take a break and study some 3D math or jump ahead a chapter. Sometimes, you just need time for things to sink in before you get that eureka moment.</p>
<p>A shader program runs directly on the <strong>graphic processing unit</strong> (<strong>GPU</strong>) of the device or computer. If the device doesn't have a GPU, then the program is executed on the CPU, which is a much slower process. After all, the GPU has been optimized to run shader code and do it extremely well. In fact, virtually all 3D rendering done on the GPU runs the shader code. When we use Unity, a much higher-level game engine, we will still write our own shaders because of the power and flexibility it gives us.</p>
<p>So, what does a shader program look like? The following is an example of a shader written in the <strong>OpenGL Shading Language</strong> (<strong>GLSL</strong>):</p>
<pre>uniform mat4 u_ModelViewProjection;<br/>uniform vec4 u_Color;<br/>uniform float u_PointSize;<br/><br/>attribute vec4 a_Position;<br/><br/>varying vec4 v_Color;<br/><br/>void main() {<br/>   v_Color = u_Color;<br/>   gl_Position = u_ModelViewProjection * vec4(a_Position.xyz, 1.0);<br/>   gl_PointSize = u_PointSize;<br/>}</pre>
<p>This is the shader program we use for rendering our point cloud points or vertices. Specifically, this shader is responsible for rendering a single vertex for each call to <kbd>main</kbd>, and it's called a vertex shader. Later in the rendering process, after the 3D scene is flattened to a 2D image with the vertex shaders, we have the opportunity to run a fragment or pixel shader. A fragment shader is run for every pixel/fragment that needs to be rendered.</p>
<p>Shader programs come in a few variations, but since they all derive from a C language and share so many similar functions, switching from one language to another isn't as difficult as you think. We will, in fact, learn some basics of the GLSL and the form used in Unity called <strong>High Level Shading Language</strong> (<strong>HLSL</strong>), which has its roots in DirectX.</p>
<p>If you look in the <kbd>main</kbd> function, you will see we are setting three variables: <kbd>v_Color</kbd>, <kbd>gl_Position</kbd>, and <kbd>gl_PointSize</kbd>. Those variables are global and just determine the color, size, and position of the vertex. The first line sets the color to an input variable—<kbd>u_Color</kbd>. Then, the position is calculated by multiplying the <kbd>u_ModelViewProjection</kbd> matrix with a new vector representing the position. That operation converts our vertex from world space to screen space. Finally, we set the point size with another input—<kbd>u_PointSize</kbd>.</p>
<p>What we want to do is modify that shader program so that it colorizes the points based on the distance from the user. Before we do that, though, let's take a look at how the shader gets those inputs. Open up Android Studio to <kbd>PointCloudRenderer.java</kbd> and follow along:</p>
<ol>
<li>Scroll down to bottom of the <kbd>createOnGUIThread</kbd> method and look for the following lines:</li>
</ol>
<pre style="padding-left: 60px">positionAttribute = GLES20.glGetAttribLocation(programName, "a_Position");<br/>colorUniform = GLES20.glGetUniformLocation(programName, "u_Color");<br/>modelViewProjectionUniform = GLES20.glGetUniformLocation(programName, "u_ModelViewProjection");<br/>pointSizeUniform = GLES20.glGetUniformLocation(programName, "u_PointSize");</pre>
<ol start="2">
<li>Those lines of code set up our shader input positions. What we are doing here is determining the indexes we need for injecting data into the array buffer we pass to the shader. We need to add another input, so add the following line at the end of the preceding code snippet:</li>
</ol>
<pre style="padding-left: 60px">furthestPoint = GLES20.glGetUniformLocation(programName, "u_FurthestPoint");</pre>
<ol start="3">
<li>This line adds another input variable called <kbd>u_FurthestPoint</kbd>. We need to calculate the furthest point from the user (camera) in order to colorize the points on a gradient. Before we do that, go back to the top of the file and declare the following new variables under the line identified:</li>
</ol>
<pre style="padding-left: 60px"><strong>private int numPoints = 0;  </strong>//after this line<br/>private int furthestPoint;<br/>private float furthestPointLength;</pre>
<ol start="4">
<li>Remember that <kbd>furthestPoint</kbd> is an index to the variable and <kbd>furthestPointLength</kbd> will be used to hold the distance to the furthest point.</li>
<li>Scroll down to the <kbd>update</kbd> method and enter the following code after the identified line:</li>
</ol>
<pre style="padding-left: 60px"><strong>numPoints = lastPointCloud.getPoints().remaining() / FLOATS_PER_POINT; </strong> //after me<br/><br/>furthestPointLength = 1;<br/>if(numPoints &gt; 0) {<br/>    for(int i=0; i&lt;numPoints*FLOATS_PER_POINT;i= i+FLOATS_PER_POINT) {<br/>        float x = lastPointCloud.getPoints().get(i);<br/>        float y = lastPointCloud.getPoints().get(i+1);<br/>        float z = lastPointCloud.getPoints().get(i+2);<br/>        double len = Math.sqrt(x*x+y*y+z*z);<br/>        furthestPointLength = Math.max(furthestPointLength, (float)len);<br/>    } <br/> }<br/>}</pre>
<ol start="6">
<li>This code first sets our minimum distance (<kbd>1</kbd>) to <kbd>mFurthestPointLength</kbd>. Then, we check whether there are any observed points. If there are, we loop through the points in the point cloud. In the loop, we use the <kbd>get</kbd> method to index into the point buffer and extract the <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> of the points. This allows us to measure the length of the vector with <kbd>x</kbd>, <kbd>y</kbd>, and <kbd>z</kbd> of the point. You make recognize the equation as the Pythagorean theorem, but in 3 dimensions rather than the 2 you may be used to. We then check whether this new length (distance) is greater than the current furthest length with <kbd>Math.max</kbd>. Keep in mind that this code is run in the <kbd>update</kbd> method and thus executed every rendered frame.</li>
</ol>
<p>We calculate the distance between two points in 3D space using the following formulae:<br/>
<br/>
<img class="fm-editor-equation" src="img/266792db-feb1-4454-86bb-0a0ca4a6ecfd.png" style="width:29.92em;height:2.83em;"/><br/>
<br/>
Since our camera (user) is the origin, we can assume that one of our points is (0,0,0), which is equal to this:<br/>
<img class="fm-editor-equation" src="img/8e4e34c9-fcd7-40b0-94e6-901180fd4ed4.png" style="width:28.00em;height:2.83em;"/><br/>
<br/>
This becomes the following:<br/>
<br/>
<img class="fm-editor-equation" src="img/b0d0b8bd-d35b-4570-9293-3af04453ac62.png" style="width:14.42em;height:2.75em;"/></p>
<ol start="7">
<li>Scroll down to the <kbd>draw</kbd> method and add the following code beneath the identified line:</li>
</ol>
<pre style="padding-left: 60px"><strong>GLES20.glUniform1f(mPointSizeUniform, 25.0f);  </strong>//after me<br/><br/>GLES20.glUniform1f(furthestPoint, furthestPointLength);</pre>
<ol start="8">
<li>This call sets the <kbd>furthestPointLength</kbd> that we calculated in the <kbd>update</kbd> method to the shader program.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Editing the shader</h1>
                
            
            
                
<p>Okay, so that's all the Java code we need to write in order to calculate and set our new distance variable. Next, we want to open up the shader program and modify the code for our needs. Follow the given steps to modify the shader program:</p>
<ol>
<li class="mce-root">Open the <kbd>point_cloud_vertex.shader</kbd> file under the <kbd>res/raw</kbd> folder, as shown:</li>
</ol>
<p>&gt;<img src="img/680efd8c-2119-4b27-bfaf-adef5eae725f.png"/></p>
<p>Opening point_cloud_vertex.shader</p>
<ol start="2">
<li>Make the highlighted code changes, as follows:</li>
</ol>
<pre style="padding-left: 60px">uniform mat4 u_ModelViewProjection;<br/>uniform vec4 u_Color;<br/>uniform float u_PointSize;<br/><strong>uniform float u_FurthestPoint;</strong><br/><br/>attribute vec4 a_Position;<br/><br/>varying vec4 v_Color;<br/><br/>void main() {<br/>   <strong>float t = length(a_Position)/u_FurthestPoint;</strong><br/>   <strong>v_Color = vec4(t, 1.0-t,t,1.0);</strong><br/>   gl_Position = u_ModelViewProjection * vec4(a_Position.xyz, 1.0);<br/>   gl_PointSize = u_PointSize;<br/>}</pre>
<ol start="3">
<li>The first line of code is new. All we are doing is taking the length of the <kbd>a_Position</kbd> vector, determining its length or distance to the camera, and then normalizing that value between 0 and 1. The second line then creates a new <kbd>vec4</kbd> for color based on our calculations of the <kbd>t</kbd> variable. This new vector represents the color in the form <strong>red blue green alpha</strong> (<strong>RGBA</strong>), where alpha is set to a constant of <kbd>1.0</kbd>.</li>
<li>Save the file, connect your device, and build and run the app on your device. You should now see the cloud points colorized by distance to the camera, as follows:</li>
</ol>
<div><img src="img/03cccfa8-878f-4ce2-ae57-bd1f10574958.png" style="width:18.33em;height:35.33em;"/></div>
<p>Screenshot of colored point cloud points by depth</p>
<p>Imagine if we had to write Java code in order to do the same colorization of the points. We would certainly need a lot more code than what we wrote. Also, any Java code we used would certainly be much slower than a shader. Now, for our example, the app's performance is less critical, but when you develop a real AR app, you will want to squeeze all the performance you can; that's why our discussion and knowledge of shaders is so important.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p class="mce-root">The following exercises are meant to test your skills and the knowledge you just earned in order to build on the work we just completed. Complete the following exercises on your own:</p>
<ol>
<li>Change the color of the tracking line from blue to red, or another color.</li>
<li>Replace the straight line segments with a <kbd>SplineCurve</kbd>. Hint, you will need to track more than one previous position.</li>
<li>Make the cube and/or audio follow the user along the tracked path. Hint—you can use another <kbd>setInterval</kbd> timer function to move the box along the path every 1.1 seconds (1100 ms).</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>We began this chapter by first reviewing some concepts on environment tracking and exploring how ARCore keeps track of the environment. Then, we moved on to meshing and how it is used to generate planes and surfaces. From there, we moved on to interacting with the environment, where we saw how a touch gesture is interpreted and converted into a position in a 3D scene. After that, we learned some basics about OpenGL ES and how our point cloud is rendered. We then took a deep dive and introduced the low-level rendering process of shaders. With this, we then modified the point cloud vertex shader in order to colorize the points by distance.</p>
<p>Lighting is a critical element to the whole illusion of augmented reality. In the next chapter, we will dive back into Unity and learn about light estimation.</p>


            

            
        
    </body></html>