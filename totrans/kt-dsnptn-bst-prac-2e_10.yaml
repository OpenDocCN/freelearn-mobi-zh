- en: '*Chapter 8*: Designing for Concurrency'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Concurrent design patterns** help us to manage many tasks at once and structure
    their life cycle. By using these patterns efficiently, we can avoid problems such
    as resource leaks and deadlocks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll discuss concurrent design patterns and how they are
    implemented in **Kotlin**. To do this, we''ll be using the building blocks from
    previous chapters: coroutines, channels, flows, and concepts from **functional
    programming**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be covering the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deferred value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barrier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan out
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan in
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Racing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutex
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sidekick channel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After completing this chapter, you'll be able to work with asynchronous values
    efficiently, coordinate the work of different coroutines, and distribute and aggregate
    work, as well as have the tools needed to resolve any concurrency problems that
    may arise in the process.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to the technical requirements from the previous chapters, you will
    also need a **Gradle**-enabled Kotlin project to be able to add the required dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the source code used in this chapter on **GitHub** at the following
    location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Kotlin-Design-Patterns-and-Best-Practices/tree/main/Chapter08](https://github.com/PacktPublishing/Kotlin-Design-Patterns-and-Best-Practices/tree/main/Chapter08)'
  prefs: []
  type: TYPE_NORMAL
- en: Deferred Value
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the **Deferred Value** design pattern is to return a reference to
    a result of an asynchronous computation. A **Future** in **Java** and **Scala**,
    and a **Promise** in **JavaScript** are both implementations of the Deferred Value
    design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: We've already discussed `async()` function returns a type called `Deferred`,
    which is also an implementation of this design pattern.
  prefs: []
  type: TYPE_NORMAL
- en: Interestingly enough, the `Deferred` value itself is an implementation of both
    the **Proxy** design pattern that we've seen in [*Chapter 3*](B17816_03_ePub.xhtml#_idTextAnchor080),
    *Understanding Structural Patterns*, and the **State** design pattern from [*Chapter
    4*](B17816_04_ePub.xhtml#_idTextAnchor115), *Getting Familiar with Behavioral
    Patterns*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can create a new container for the result of an asynchronous computation
    using the `CompletableDeferred` constructor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To populate the `Deferred` value with a result, we use the `complete()` function,
    and if an error occurs in the process, we can use the `completeExceptionally()`
    function to pass the exception to the caller. To understand it better, let's write
    a function that returns an asynchronous result. Half of the time the result will
    contain `OK`, and the other half of the time it will contain an exception.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You can see that we return the `Deferred` value almost immediately, then we
    start an asynchronous computation using `launch` and simulate some computation
    using the `delay()` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the process is asynchronous, the results won''t be ready immediately.
    To wait for the results, we can use the `await()` function that we''ve already
    discussed in [*Chapter 6*](B17816_06_ePub.xhtml#_idTextAnchor164), *Threads and
    Coroutines*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s important to make sure that you always complete your `Deferred` value
    by calling either of the `complete()` or `completeExceptionally()` functions.
    Otherwise, your program may wait indefinitely for the results. It is also possible
    to cancel `deferred` if you''re no longer interested in its results. To do this,
    simply call `cancel()` on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: You'll rarely need to create your own deferred value. Usually, you would work
    with the one returned from the `async()` function.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's discuss how to wait for multiple asynchronous results at once.
  prefs: []
  type: TYPE_NORMAL
- en: Barrier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Barrier** design pattern provides us with the ability to wait for multiple
    concurrent tasks to complete before proceeding further. A common use case for
    this is composing objects from different sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, take the following class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s assume that the `catchphrase` data comes from one service and the `picture`
    data comes from another. We would like to fetch these two pieces of data concurrently:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The most basic way to implement concurrent fetching would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: But this solution has a major problem â€“ we don't start fetching the `picture`
    data until the `catchphrase` data was fetched. In other words, the code is unnecessarily
    *sequential*. Let's see how this can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: Using data classes as barriers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can slightly alter the previous code to achieve the concurrency we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Moving the `await` function into the invocation of the data class constructor
    allows us to start all of the coroutines at once and then wait for them to complete,
    just as we wanted.
  prefs: []
  type: TYPE_NORMAL
- en: 'The additional benefit of using data classes as barriers is the ability to
    *destructure* them easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: This works well if the type of data we receive from different asynchronous tasks
    is heterogeneous. In some cases, we receive the same types of data from different
    sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s ask `Michael` (our canary product owner), `Taylor` (our
    barista), and `Me` who our favorite movie character is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have three very similar objects that differ only in the contents of
    the asynchronous results they return.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, we can use a list to gather the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice the type of the list. It''s a collection of the `Deferred` elements
    of the `FavoriteCharacter` type. On such collections, there''s an `awaitAll()`
    function available that acts as a barrier as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: When working with a set of homogenous asynchronous results and you need all
    of them to complete before proceeding further, use `awaitAll()`.
  prefs: []
  type: TYPE_NORMAL
- en: The Barrier design pattern creates a rendezvous point for multiple asynchronous
    tasks. The next pattern will help us abstract the execution of those tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the **Scheduler** design pattern is to decouple *what* is being
    run from *how* it's being run and optimize the use of resources when doing so.
  prefs: []
  type: TYPE_NORMAL
- en: In Kotlin, **Dispatchers** are an implementation of the Scheduler design pattern
    that decouple the coroutine (that is, the *what*) from underlying thread pools
    (that is, the *how*).
  prefs: []
  type: TYPE_NORMAL
- en: We've already seen dispatchers briefly in [*Chapter 6*](B17816_06_ePub.xhtml#_idTextAnchor164),
    *Threads and Coroutines*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To remind you, the coroutine builders such as `launch()` and `async()` can
    specify which dispatcher to use. Here''s an example of how you specify it explicitly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The default dispatcher creates as many threads as you have CPUs in the underlying
    thread pool. Another dispatcher that is available to you is the **IO Dispatcher**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: The IO Dispatcher is used for potentially long-running or blocking operations
    and will create up to 64 threads for that purpose. Since our example code doesn't
    do much, the IO Dispatcher doesn't need to create many threads. That's why you'll
    see only a small number of workers used in this example.
  prefs: []
  type: TYPE_NORMAL
- en: Creating your own schedulers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We are not limited to the dispatchers Kotlin provides. We can also define dispatchers
    of our own.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of creating a dispatcher that would use a dedicated thread
    pool of `4` threads based on `ForkJoinPool`, which is efficient for *divide-and-conquer*
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: If you create your own dispatcher, make sure that you either release it with
    `close()` or reuse it, as creating a new dispatcher and holding to it is expensive
    in terms of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Pipeline** design pattern allows us to scale heterogeneous work, consisting
    of multiple steps of varying complexity across multiple CPUs, by breaking the
    work into smaller, concurrent pieces. Let's look at the following example to understand
    it better.
  prefs: []
  type: TYPE_NORMAL
- en: Back in [*Chapter 4*](B17816_04_ePub.xhtml#_idTextAnchor115), *Getting Familiar
    with Behavioral Patterns*, we wrote an HTML page parser. It was assumed that the
    HTML pages themselves were already fetched for us, though. What we would like
    to design now is a process that would create a possibly infinite stream of pages.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we would like to fetch news pages once in a while. For that, we''ll
    have a producer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: The `isActive` flag will be true as long as the coroutine is running and hasn't
    been canceled. It is a good practice to check this property in loops that may
    run for a long time so they can be stopped between iterations if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Each time we receive new titles, we send them downstream. Since tech news isn't
    updated very often, we can check for updates only once in a while by using `delay()`.
    In the actual code, the delay would probably be minutes, if not hours.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is creating a **Document Object Model** (**DOM**) out of those
    raw strings containing HTML. For that, we''ll have a second producer, with this
    one receiving a channel that connects it to the first one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `for` loop to iterate over the channel as long as it's still
    open. This is a very elegant way of consuming data from an asynchronous source
    without the need to define callbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll have a third function that receives the parsed documents and extracts
    the title out of each one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: We're looking for the headers, and so we use `getElementsByTagName("H1")`. For
    each header found, we turn it into its string representation.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on toward composing our coroutines into pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Composing a pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we''ve familiarized ourselves with the components of the pipeline,
    let''s see how we can combine multiple components together:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE152]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting pipeline will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: A pipeline is a great way to break a long process into smaller steps. Note that
    each resulting coroutine is a *pure function*, so it's also easy to test and reason
    about.
  prefs: []
  type: TYPE_NORMAL
- en: The entire pipeline could be stopped by calling `cancel()` on the first coroutine
    in line.
  prefs: []
  type: TYPE_NORMAL
- en: Fan Out
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The goal of the **Fan Out** design pattern is to distribute work between multiple
    concurrent processors, also known as *workers*. To understand it better, let''s
    look again at the previous section but consider the following problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '*What if the amount of work at the different steps in our pipeline is very
    different?*'
  prefs: []
  type: TYPE_NORMAL
- en: For example, it takes a lot more time to *fetch* the HTML content than to *parse*
    it. In such a case, we may want to distribute that heavy work between multiple
    coroutines. In the previous example, only a single coroutine was reading from
    each channel. But multiple coroutines can consume from a single channel too, thus
    dividing the work.
  prefs: []
  type: TYPE_NORMAL
- en: 'To simplify the problem we''re about to discuss, let''s have only one coroutine
    producing some results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE157]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE158]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE159]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE160]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE161]'
  prefs: []
  type: TYPE_PRE
- en: 'And we''ll have a function that creates a new coroutine that reads those results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE162]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE163]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE164]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE165]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: This function will generate a coroutine that is executed on the `Default` dispatcher.
    Each coroutine will listen to a channel and print every message it receives to
    the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start our producer. Remember that all the following pieces of code
    need to be wrapped in the `runBlocking` function, but for simplicity, we omitted
    that part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can create multiple workers that distribute the work between themselves
    by reading from the same channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE171]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE172]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE173]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now examine a part of the output of this program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE174]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE175]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE176]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE177]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE178]'
  prefs: []
  type: TYPE_PRE
- en: Note that no two workers receive the same message and the messages are not being
    printed in the order they were sent. The Fan Out design pattern allows us to efficiently
    distribute the work across a number of coroutines, threads, and CPUs.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's discuss an accompanying design pattern that often goes hand-in-hand
    with Fan Out.
  prefs: []
  type: TYPE_NORMAL
- en: Fan In
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of the **Fan In** design pattern is to combine results from multiple
    workers. This design pattern is helpful when our workers produce results and we
    need to gather them.
  prefs: []
  type: TYPE_NORMAL
- en: This design pattern is the opposite of the Fan Out design pattern we discussed
    in the previous section. Instead of multiple coroutines *reading* from the same
    channel, multiple coroutines can *write* their results to the same channel.
  prefs: []
  type: TYPE_NORMAL
- en: 'Combining the Fan Out and Fan In design patterns is a good base for **MapReduce**
    algorithms. To demonstrate this, we''ll slightly change the workers from the previous
    example, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE180]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE181]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE182]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE183]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE184]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE185]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE186]'
  prefs: []
  type: TYPE_PRE
- en: Now, once done, each worker sends the results of its calculation to `resultChannel`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that this pattern is different from the actor and producer builders we've
    seen before. Actors each have their own channels, while in this case, `resultChannel`
    is shared across all the workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'To collect the results from the workers, we''ll use the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE187]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE188]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE189]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE190]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE192]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE193]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE194]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE195]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s now clarify what this code does:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we create `resultChannel`, which all our workers will share.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, we supply it to each worker. We have ten workers in total. Each worker
    repeats the message it received twice and sends it on `resultChannel`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we consume the results from the channel in our main coroutine. This
    way, we accumulate results from multiple concurrent workers in the same place.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here''s a sample of the output from the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE198]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE199]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE200]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE202]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE203]'
  prefs: []
  type: TYPE_PRE
- en: Next, let's discuss another design pattern, which will help us improve the responsiveness
    of our code in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: Racing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Racing** is a design pattern that runs multiple jobs concurrently, picking
    the result that returns first as the *winner* and discarding others as *losers*.'
  prefs: []
  type: TYPE_NORMAL
- en: We can implement Racing in Kotlin using the `select()` function on channels.
  prefs: []
  type: TYPE_NORMAL
- en: Let's imagine you are building a weather application. For redundancy, you fetch
    the weather from two different sources, *Precise Weather* and *Weather Today*.
    We'll describe them as two producers that return their name and temperature.
  prefs: []
  type: TYPE_NORMAL
- en: If we have more than one producer, we can subscribe to their channels and take
    the first result that is available.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s declare the two weather producers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE204]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE205]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: Their logic is pretty much the same. Both wait for a random number of milliseconds
    and then return a temperature reading and the name of the source.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can listen to both channels simultaneously using the `select` expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE213]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE214]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE215]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE216]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE217]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE218]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE220]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE221]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE222]'
  prefs: []
  type: TYPE_PRE
- en: Using the `onReceive()` function allows us to listen to multiple channels simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Running this code multiple times will randomly print `(Precise Weather, +25c)`and
    `(Weather Today, +24c)`, as there is an equal chance for both of them to arrive
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Racing is a very useful concept when you are willing to sacrifice resources
    in order to get the most responsiveness from your system and we achieved that
    using Kotlin's `select` expression. Now, let's explore the `select` expression
    a little further to discover another concurrent design pattern that it implements.
  prefs: []
  type: TYPE_NORMAL
- en: Unbiased select
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When using the `select` clause, the order is important. Because it is inherently
    biased, if two events happen at the same time, it will select the first clause.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what that means in the following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll have only one producer this time, which sends over a channel which movie
    we should watch next:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE223]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE224]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE226]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE227]'
  prefs: []
  type: TYPE_PRE
- en: Since we defined a non-zero capacity on the channel, the value will be available
    as soon as this coroutine runs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s start the two producers and use a `select` expression to see which
    of the two movies will be selected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE228]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE229]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE232]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE233]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE234]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE235]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE236]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE237]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE238]'
  prefs: []
  type: TYPE_PRE
- en: 'No matter how many times you run this code, the winner will always be the same:
    `Quick&Angry 7`. This is because if both values are ready at the same time, the
    `select` clause will always pick the first channel available in the order they
    are declared.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s use `selectUnbiased` instead of the `select` clause:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE243]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE244]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this code now will sometimes produce `Quick&Angry 7` and sometimes
    produce `Revengers: Penultimatum`. Unlike the regular `select` clause, `selectUnbiased`
    doesn''t care about the order. If more than one result is available, it will pick
    one randomly.'
  prefs: []
  type: TYPE_NORMAL
- en: Mutex
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also known as **mutual exclusions**, **mutex** provides a means to protect a
    shared state that can be accessed by multiple coroutines at once.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start with the same old dreaded `counter` example, where multiple concurrent
    tasks try to update the same `counter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE245]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE248]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE249]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE250]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE251]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE252]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE253]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE254]'
  prefs: []
  type: TYPE_PRE
- en: As you've probably guessed, the result that is printed is less than 10,000 â€“
    *totally embarrassing!*
  prefs: []
  type: TYPE_NORMAL
- en: To solve this, we can introduce a locking mechanism that will allow only a single
    coroutine to interact with the variable at once, making the operation *atomic*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each coroutine will try to obtain the ownership of the `counter`. If another
    coroutine is updating the `counter`, our coroutine will wait patiently and then
    try to acquire the lock again. Once updated, it must release the lock so that
    other coroutines can proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE255]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE256]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE257]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE258]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE259]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE260]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE261]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE262]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE263]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE264]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE265]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, our example always prints the correct number: `10,000`.'
  prefs: []
  type: TYPE_NORMAL
- en: Mutex in Kotlin is different from the Java mutex. In Java, `lock()` on a mutex
    blocks the thread, until the lock can be acquired. A Kotlin mutex suspends the
    coroutine instead, providing better concurrency. Locks in Kotlin are cheaper.
  prefs: []
  type: TYPE_NORMAL
- en: This is good for simple cases. But what if the code within the critical section,
    that is, between `lock()` and `unlock()`, throws an exception?
  prefs: []
  type: TYPE_NORMAL
- en: 'We would have to wrap our code in `try...catch`, which is not very convenient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE266]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE267]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE268]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE269]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE270]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE271]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE272]'
  prefs: []
  type: TYPE_PRE
- en: However, if we omit the `finally` block, our lock will never be released and
    it will block all other coroutines from proceeding and creating a deadlock.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exactly for this purpose, Kotlin also introduces `withLock()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE273]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE274]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE275]'
  prefs: []
  type: TYPE_PRE
- en: Notice how much more concise this syntax is compared with the previous example.
  prefs: []
  type: TYPE_NORMAL
- en: Sidekick channel
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Sidekick channel** design pattern allows us to offload some work from
    our main worker to a *back worker*.
  prefs: []
  type: TYPE_NORMAL
- en: Up until now, we've only discussed the use of `select` as a *receiver*. But
    we can also use `select` to *send* items to another channel. Let's look at the
    following example.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll declare `batman` as an actor coroutine that processes 10 messages
    per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE276]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE277]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE278]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE279]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE280]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE281]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we''ll declare `robin` as another actor coroutine that is a bit slower
    and processes only four messages per second:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE282]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE283]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE284]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE285]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE286]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE287]'
  prefs: []
  type: TYPE_PRE
- en: So, we have a superhero and his sidekick as two actors. Since the superhero
    is more experienced, it usually takes him less time to beat the villain he's facing.
  prefs: []
  type: TYPE_NORMAL
- en: 'But in some cases, he still has his hands full, so a sidekick needs to step
    in. We''ll throw five villains at the pair with a few delays and see how they
    fare:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE288]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE289]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE290]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE291]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE292]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE293]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE294]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE295]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE296]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE297]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE298]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE299]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE300]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE301]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the type of parameter for `select` refers to what is *returned*
    from the block and not what is being *sent* to the channels. That's the reason
    we use `Pair<String, String>` here.
  prefs: []
  type: TYPE_NORMAL
- en: 'This code prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE302]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE303]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE304]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE305]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE306]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE307]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE308]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE309]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE310]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE311]'
  prefs: []
  type: TYPE_PRE
- en: Using a sidekick channel is a useful technique to provide fallback values. Consider
    using one in cases when you need to consume a consistent stream of data and cannot
    easily scale your consumers.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered various design patterns related to *concurrency*
    in Kotlin. Most of them are based on coroutines, channels, deferred values, or
    a combination of these building blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Deferred values are used as placeholders for asynchronous values. The Barrier
    design pattern allows multiple asynchronous tasks to rendezvous before proceeding
    further. The Scheduler design pattern decouples the code of tasks from the way
    they are executed at runtime.
  prefs: []
  type: TYPE_NORMAL
- en: The Pipeline, Fan In, and Fan Out design patterns help us distribute the work
    and collect the results. Mutex helps us to control the number of tasks that are
    being executed at the same time. The Racing design pattern allows us to improve
    the responsiveness of our application. Finally, the Sidekick Channel design pattern
    offloads work onto a backup task in case the main task is not able to process
    the incoming events quickly enough.
  prefs: []
  type: TYPE_NORMAL
- en: All of these patterns should help you to manage the concurrency of your application
    in an efficient and extensible manner. In the next chapter, we'll discuss Kotlin's
    idioms and best practices, as well as some of the anti-patterns that emerged with
    the language.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What does it mean when we say that the `select` expression in Kotlin is *biased*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When should you use a *mutex* instead of a *channel*?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which of the concurrent design patterns could help you implement a MapReduce
    or divide-and-conquer algorithm efficiently?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
