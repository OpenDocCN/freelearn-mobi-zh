- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Concurrency and Parallelism in Swift
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When I first started learning Objective-C, I already had a good understanding
    of concurrency and multitasking with my background in other languages, such as
    C and Java. This background made it very easy for me to create multithreaded applications
    using threads. Then, Apple changed everything when they released **Grand Central
    Dispatch** (**GCD**) with OS X 10.6 and iOS 4\. At first, I went into denial;
    there was no way GCD could manage my application's threads better than I could.
    Then, I entered the anger phase; GCD was hard to use and understand. Next was
    the bargaining phase; maybe I could use GCD with my threading code, so I could
    still control how the threading worked. Then, there was the depression phase;
    maybe GCD does handle threading better than I could. Finally, I entered the wow
    phase; this GCD thing is really easy to use and works amazingly well.
  prefs: []
  type: TYPE_NORMAL
- en: After using GCD and operation queues with Objective-C, I do not see a reason
    for using manual threads with Swift.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The basics of concurrency and parallelism
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use GCD to create and manage concurrent dispatch queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use GCD to create and manage serial dispatch queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use various GCD functions to add tasks to the dispatch queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use `Operation` and `OperationQueues` to add concurrency to our applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have not seen a lot of improvement with regards to concurrency in the Swift
    language over the course of Swift 5.x. It does appear that this will change in
    the future as concurrency improvements are one of the main goals of Swift 6\.
    Let's start off by looking at the difference between concurrency and parallelism,
    one that is important to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency and parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Concurrency** is the concept of multiple tasks starting, running, and completing
    within the same time period. This does not necessarily mean that the tasks are
    executing simultaneously. In fact, in order for tasks to be run simultaneously,
    our application needs to be running on a multicore or multiprocessor system. Concurrency
    allows us to share the processor or cores for multiple tasks; however, a single
    core can only execute one task at a given time.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Parallelism** is the concept of two or more tasks running simultaneously.
    Since each core of our processor can only execute one task at a time, the number
    of tasks executing simultaneously is limited to the number of cores within our
    processors and the number of processors that we have. As an example, if we have
    a four-core processor, then we are limited to running four tasks simultaneously.
    Today''s processors can execute tasks so quickly that it may appear that larger
    tasks are executing simultaneously. However, within the system, the larger tasks
    are actually taking turns executing subtasks on the cores.'
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand the difference between concurrency and parallelism, let's
    look at how a juggler juggles balls. If you watch a juggler, it seems they are
    catching and throwing multiple balls at any given time; however, a closer look
    reveals that they are, in fact, only catching and throwing one ball at a time.
    The other balls are in the air waiting to be caught and thrown. If we want to
    be able to catch and throw multiple balls simultaneously, we need to have multiple
    jugglers.
  prefs: []
  type: TYPE_NORMAL
- en: This example is really good because we can think of jugglers as the cores of
    a processor. A system with a single-core processor (one juggler), regardless of
    how it seems, can only execute one task (catch or throw one ball) at a time. If
    we want to execute more than one task at a time, we need to use a multicore processor
    (more than one juggler).
  prefs: []
  type: TYPE_NORMAL
- en: Back in the days when all of the processors were single-core, the only way to
    have a system that executed tasks simultaneously was to have multiple processors
    in the system. This also required specialized software to take advantage of the
    multiple processors. In today's world, just about every device has a processor
    that has multiple cores, and both iOS and macOS are designed to take advantage
    of these multiple cores to run tasks simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Traditionally, the way applications added concurrency was to create multiple
    threads; however, this model does not scale well to an arbitrary number of cores.
    The biggest problem with using threads was that our applications ran on a variety
    of systems (and processors), and in order to optimize our code, we needed to know
    how many cores/processors could be efficiently used at a given time, which is
    usually not known at the time of development.
  prefs: []
  type: TYPE_NORMAL
- en: To solve this problem, many operating systems, including iOS and macOS, started
    relying on asynchronous functions. These functions are often used to initiate
    tasks that could possibly take a long time to complete, such as making an HTTP
    request or writing data to disk. An asynchronous function typically starts a long-running
    task and then returns prior to the task's completion. Usually, this task runs
    in the background and uses a callback function (such as a closure in Swift) when
    the task completes.
  prefs: []
  type: TYPE_NORMAL
- en: 'These asynchronous functions work great for the tasks that the operating system
    provides them for, but what if we need to create our own asynchronous functions
    and do not want to manage the threads ourselves? For this, Apple provides a couple
    of technologies. In this chapter, we will be covering two of these: GCD and operation
    queues.'
  prefs: []
  type: TYPE_NORMAL
- en: GCD is a low-level, C-based API that allows specific tasks to be queued up for
    execution and schedules the execution on any of the available processor cores.
    Operation queues are similar to GCD; however, they are Foundation objects and
    are internally implemented using GCD.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin by looking at GCD.
  prefs: []
  type: TYPE_NORMAL
- en: Grand Central Dispatch (GCD)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to Swift 3, using GCD felt like writing low-level C code. The API was
    a little cumbersome and sometimes hard to understand because it did not use any
    of the Swift language design features. This all changed with Swift 3 because Apple
    took up the task of rewriting the API so it would meet the Swift 3 API guidelines.
  prefs: []
  type: TYPE_NORMAL
- en: GCD provides what is known as dispatch queues to manage submitted tasks. The
    queues manage these submitted tasks and execute them in a **First-In, First-Out**
    (**FIFO**) order. This ensures that the tasks are started in the order they were
    submitted.
  prefs: []
  type: TYPE_NORMAL
- en: A task is simply some work that our application needs to perform. For example,
    we can create tasks that perform simple calculations, read/write data to disk,
    make an HTTP request, or anything else that our application needs to do. We define
    these tasks by placing the code inside either a function or a closure and adding
    it to a dispatch queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'GCD provides three types of dispatch queues:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Serial queues**: Tasks in a serial queue (also known as a **private queue**)
    are executed one at a time in the order they were submitted. Each task is started
    only after the preceding task is completed. Serial queues are often used to synchronize
    access to specific resources because we are guaranteed that no two tasks in a
    serial queue will ever run simultaneously. Therefore, if the only way to access
    the specific resource is through the tasks in the serial queue, then no two tasks
    will attempt to access the resource at the same time or out of order.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent queues**: Tasks in a concurrent queue (also known as a **global
    dispatch queue**) execute concurrently; however, the tasks are still started in
    the order that they were added to the queue. The exact number of tasks that can
    be executed at any given instance is variable and is dependent on the system''s
    current conditions and resources. The decision of when to start a task is up to
    GCD and is not something that we can control within our application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Main dispatch queue**: The main dispatch queue is a globally available serial
    queue that executes tasks on the application''s main thread. Since tasks put into
    the main dispatch queue run on the main thread, it is usually called from a background
    queue when some background processing has finished and the user interface needs
    to be updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dispatch queues offer several advantages over traditional threads. The first
    and foremost advantage is that, with dispatch queues, the system handles the creation
    and management of threads rather than the application itself. The system can scale
    the number of threads dynamically, based on the overall available resources of
    the system and the current system conditions. This means that dispatch queues
    can manage the threads with greater efficiency than we could.
  prefs: []
  type: TYPE_NORMAL
- en: Another advantage of dispatch queues is that we are able to control the order
    in which the tasks are started. With serial queues, not only do we control the
    order in which tasks are started, but we also ensure that one task does not start
    before the preceding one is complete. With traditional threads, this can be very
    cumbersome and brittle to implement, but with dispatch queues, as we will see
    later in this chapter, it is quite easy.
  prefs: []
  type: TYPE_NORMAL
- en: Calculation types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before we look at how to use dispatch queues, let''s create a class that will
    help us to demonstrate how the various types of queues work. This class will contain
    two basic functions and we will name the class `DoCalculations`. The first function
    will simply perform some basic calculations and then return a value. Here is the
    code for this function, which is named `doCalc()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The other function, which we will name `performCalculation()`, accepts two
    parameters. One is an integer named `iterations` and the other is a string named
    `tag`. The `performCalculation()` function calls the `doCalc()` function repeatedly
    until it calls the function the same number of times as defined by the `iterations`
    parameter. We also use the `CFAbsoluteTimeGetCurrent()` function to calculate
    the elapsed time it took to perform all of the iterations, and then we print the
    elapsed time with the `tag` string to the console. This will let us know when
    the function completes and how long it took to complete it. Here is the code for
    this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: These functions will be used together to keep our queues busy, so we can see
    how they work. Let's begin by looking at how we would create a dispatch queue.
  prefs: []
  type: TYPE_NORMAL
- en: Creating queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We use the `DispatchQueue` initializer to create a new dispatch queue. The
    following code shows how we would create a new dispatch queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line would create a concurrent queue with a label of `cqueue.hoffman.jon`,
    while the second line would create a serial queue with a label of `squeue.hoffman.jon`.
    The `ispatchQueue` initializer takes the following parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`label`: This is a string label that is attached to the queue to uniquely identify
    it in debugging tools, such as instruments and crash reports. It is recommended
    that we use a reverse DNS naming convention. This parameter is optional and can
    be `nil`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`attributes`: This specifies the type of queue to make. This can be `DispatchQueue.Attributes.serial`,
    `DispatchQueue.Attributes.concurrent`, or `nil`. If this parameter is `nil`, a
    serial queue is created. You can use `.serial` or `.concurrent` as we showed in
    the sample code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some programming languages use the reverse DNS naming convention to name certain
    components. This convention is based on a registered domain name that is reversed.
    As an example, if we worked for a company that had a domain name of `mycompany.com`
    with a product called widget, the reverse DNS name would be `com.mycompany.widget`.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now look at how we can create and use concurrent queues.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and using a concurrent queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A concurrent queue will execute tasks in a FIFO order; however, the tasks will
    execute concurrently and finish in any order. Let''s see how we would create and
    use a concurrent queue. The following line will create the concurrent queue that
    we will be using for this section and will also create an instance of the `DoCalculations`
    type that will be used to test the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line will create a new dispatch queue that we will name `cqueue`,
    and the second line creates an instance of the `DoCalculations` type. Now, let''s
    see how we would use our concurrent queue by using the `performCalculation()`
    method from the `DoCalculations` type to perform some calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we created a closure, which represents our task and simply
    calls the `performCalculation()` function of the `DoCalculation` instance, requesting
    that it runs through 1,000 iterations of the `doCalc()` function. Finally, we
    use the `async(execute:)` method of our queue to execute it. This code will execute
    the task in a concurrent dispatch queue, which is separate from the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'While the preceding example works perfectly, we can actually shorten the code
    a little bit. The next example shows that we do not need to create a separate
    closure as we did in the preceding example. We can also submit the task to execute,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This shorthand version is how we usually submit small code blocks to our queues.
    If we have larger tasks or tasks that we need to submit multiple times, we will
    generally want to create a closure and submit the closure to the queue as we showed
    in the first example.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how a concurrent queue works by adding several items to the queue
    and looking at the order and time that they return. The following code will add
    three tasks to the queue. Each task will call the `performCalculation()` function
    with various iteration counts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember that the `performCalculation()` function will execute the calculation
    routine continuously until it is executed the number of times defined by the iteration
    count passed in. Therefore, the larger the iteration count we pass into the function,
    the longer it should take to execute. Let''s look at the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that each of the functions is called with a different value in the `tag`
    parameter. Since the `performCalculation()` function prints out the `tag` variable
    with the elapsed time, we can see the order in which the tasks complete and the
    time they took to execute. If we execute the preceding code, we should see results
    similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The elapsed time will vary from one run to the next and from system to system.
  prefs: []
  type: TYPE_NORMAL
- en: Since the queues function in a FIFO order, the task that had the tag of `async1`
    was executed first. However, as we can see from the results, it was the last task
    to finish. Since this is a concurrent queue, if it is possible (if the system
    has the available resources), the blocks of code will execute concurrently. This
    is why tasks with the tags of `async2` and `async3` completed prior to the task
    that had the `async1` tag, even though the execution of the `async1` task began
    before the other two.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's see how a serial queue executes tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Creating and using a serial queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A serial queue functions a little differently to a concurrent queue. A serial
    queue will only execute one task at a time and will wait for one task to complete
    before starting the next one. This queue, like the concurrent dispatch queue,
    follows the FIFO order. The following line of code will create a serial queue
    that we will be using for this section and will create an instance of the `DoCalculations`
    type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The first line will create a new serial dispatch queue that we name `squeue`,
    and the second line creates an instance of the `DoCalculations` type. Now, let''s
    see how we would use our serial queue by using the `performCalculation()` method
    from the `DoCalculations` type to perform some calculations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code, we created a closure, which represents our task, that
    simply calls the `performCalculation()` function of the `DoCalculation` instance,
    requesting that it runs through 1,000 iterations of the `doCalc()` function. Finally,
    we use the `async(execute:)` method of our queue to execute it. This code will
    execute the task in a serial dispatch queue, which is separate from the main thread.
    As we can see from this code, we use the serial queue exactly like we use the
    concurrent queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can shorten this code a little bit, just like we did with the concurrent
    queue. The following example shows how we would do this with a serial queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s see how the serial queue works by adding several items to the queue
    and looking at the order in which they complete. The following code will add three
    tasks, which will call the `performCalculation()` function with various iteration
    counts to the queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Just as we did in the concurrent queue example, we call the `performCalculation()`
    function with various iteration counts and different values in the `tag` parameter.
    Since the `performCalculation()` function prints out the `tag` string with the
    elapsed time, we can see the order in which the tasks complete and the time it
    takes to execute. If we execute this code, we should see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The elapsed time will vary from one run to the next and from system to system.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the concurrent queues, we can see that the tasks completed in the same
    order that they were submitted in, even though the `sync2` and `sync3` tasks took
    considerably less time to complete. This demonstrates that a serial queue only
    executes one task at a time and that the queue waits for each task to complete
    before starting the next one.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous examples, we used the `async` method to execute the code blocks.
    We could also use the `sync` method.
  prefs: []
  type: TYPE_NORMAL
- en: async versus sync
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous examples, we used the `async` method to execute the code blocks.
    When we use the `async` method, the call will not block the current thread. This
    means that the method returns and the code block is executed asynchronously.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than using the `async` method, we could use the `sync` method to execute
    the code blocks. The `sync` method will block the current thread, which means
    it will not return until the execution of the code has completed. Generally, we
    use the `async` method, but there are use cases where the `sync` method is useful.
    These use cases are usually when we have a separate thread and we want that thread
    to wait for some work to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Executing code on the main queue function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `DispatchQueue.main.async(execute:)` function will execute code on the application's
    main queue. We generally use this function when we want to update our code from
    another thread or queue.
  prefs: []
  type: TYPE_NORMAL
- en: The main queue is automatically created for the main thread when the application
    starts. This main queue is a serial queue; therefore, items in this queue are
    executed one at a time, in the order that they were submitted. We will generally
    want to avoid using this queue unless we have a need to update the user interface
    from a background thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code example shows how we would use this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: In the previous code, we assume that we have added a method to the `UIImage`
    type that will resize the image. In this code, we create a new serial queue and,
    in that queue, we resize an image. This is a good example of how to use a dispatch
    queue because we would not want to resize an image on the main queue since it
    would freeze the UI while the image is being resized. Once the image is resized,
    we then need to update `UIImageView` with the new image; however, all updates
    to the UI need to occur on the main thread. Therefore, we will use the `DispatchQueue.main.async`
    function to perform the update on the main queue.
  prefs: []
  type: TYPE_NORMAL
- en: There will be times when we need to execute tasks after a delay. If we were
    using a threading model, we would need to create a new thread, perform some sort
    of `delay` or `sleep` function, and execute our task. With GCD, we can use the
    `asyncAfter` function.
  prefs: []
  type: TYPE_NORMAL
- en: Using asyncAfter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `asyncAfter` function will execute a block of code asynchronously after
    a given delay. This is very useful when we need to pause the execution of our
    code. The following code sample shows how we would use the `asyncAfter` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we begin by creating a serial dispatch queue. We then create an
    instance of the `DispatchTime` type and calculate the time to execute the block
    of code based on the current time. We then use the `asyncAfter` function to execute
    the code block after the delay.
  prefs: []
  type: TYPE_NORMAL
- en: Now, that we have looked at GCD, let's look at operation queues.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Operation and OperationQueue types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Operation` and `OperationQueue` types, working together, provide us with
    an alternative to GCD for adding concurrency to our applications. Operation queues
    are part of the Foundation framework and function like dispatch queues as they
    are a higher level of abstraction over GCD.
  prefs: []
  type: TYPE_NORMAL
- en: We define the tasks (operations) that we wish to execute and then add the tasks
    to the operation queue. The operation queue will then handle the scheduling and
    execution of tasks. Operation queues are instances of the `OperationQueue` class
    and operations are instances of the `Operation` class.
  prefs: []
  type: TYPE_NORMAL
- en: An operation represents a single unit of work or a task. The `Operation` type
    is an abstract class that provides a thread-safe structure for modeling the state,
    priority, and dependencies. This class must be subclassed to perform any useful
    work; we will look at how to subclass this class in the *Subclassing the Operation
    class* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Apple provides a concrete implementation of the `Operation` type that we can
    use as-is for situations where it does not make sense to build a custom subclass.
    This subclass is `BlockOperation`.
  prefs: []
  type: TYPE_NORMAL
- en: More than one operation queue can exist at the same time, and, in fact, there
    is always at least one operation queue running. This operation queue is known
    as the **main queue**. The main queue is automatically created for the main thread
    when the application starts and is where all of the UI operations are performed.
  prefs: []
  type: TYPE_NORMAL
- en: One thing to keep in mind with operation queues is that they add additional
    overhead because they are Foundation objects. For the large majority of applications,
    this little extra overhead should not be an issue or even noticed; however, for
    some projects, such as games that need every last resource that they can get,
    this extra overhead might very well be an issue.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways that we can use the `Operation` and `OperationQueue`
    classes to add concurrency to our application. In this chapter, we will look at
    three of these ways. The first one we will look at is the use of the `BlockOperation`
    implementation of the `Operation` abstract class.
  prefs: []
  type: TYPE_NORMAL
- en: Using BlockOperation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will be using the same `DoCalculation` class that we used
    in the *Grand Central Dispatch (GCD)* section to keep our queues busy with work
    so that we can see how the `OperationQueue` class works.
  prefs: []
  type: TYPE_NORMAL
- en: The `BlockOperation` class is a concrete implementation of the `Operation` type
    that can manage the execution of one or more blocks. This class can be used to
    execute several tasks at once without the need to create separate operations for
    each task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we can use the `BlockOperation` class to add concurrency to
    our application. The following code shows how to add three tasks to an operation
    queue using a single `BlockOperation` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: In this code, we begin by creating an instance of the `DoCalculation` class
    and an instance of the `OperationQueue` class. Next, we create an instance of
    the `BlockOperation` class using the `init` constructor. This constructor takes
    a single parameter, which is a block of code that represents one of the tasks
    we want to execute in the queue. Next, we add two additional tasks using the `addExecutionBlock()`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: One of the differences between dispatch queues and operations is that, with
    dispatch queues, if resources are available, the tasks are executed as they are
    added to the queue. With operations, the individual tasks are not executed until
    the operation itself is submitted to an operation queue. This allows us to initiate
    all of the operations into a single block operation prior to executing them.
  prefs: []
  type: TYPE_NORMAL
- en: Once we add all of the tasks to the `BlockOperation` instance, we then add the
    operation to the `OperationQueue` instance that we created at the beginning of
    the code. At this point, the individual tasks within the operation start to execute.
  prefs: []
  type: TYPE_NORMAL
- en: This example shows how to use `BlockOperation` to queue up multiple tasks and
    then pass the tasks to the operation queue. The tasks are executed in a FIFO order;
    therefore, the first task that is added will be the first task executed. However,
    the tasks can be executed concurrently if we have the available resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The output from this code should look similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'What if we do not want the tasks to run concurrently? What if we wanted them
    to run serially like the serial dispatch queue? We can set a property in the operation
    queue that defines the number of tasks that can be run concurrently in the queue.
    The property is named `maxConcurrentOperationCount`, and is used like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: However, if we add this line to our previous example, it will not work as expected.
    To see why this is, we need to understand what the property actually defines.
    If we look at Apple's `OperationQueue` class reference, the definition of the
    property is *the maximum number of queued operations that can execute at the same
    time*.
  prefs: []
  type: TYPE_NORMAL
- en: What this tells us is that this property defines the number of operations (this
    is the keyword) that can be executed at the same time. The `BlockOperation` instance,
    which we added all of the tasks to, represents a single operation; therefore,
    no other `BlockOperation` added to the queue will execute until the first one
    is complete, but the individual tasks within the operation will execute concurrently.
    To run the tasks serially, we would need to create a separate instance of `BlockOperation`
    for each task.
  prefs: []
  type: TYPE_NORMAL
- en: Using an instance of the `BlockOperation` class is good if we have several tasks
    that we want to execute concurrently, but they will not start executing until
    we add the operation to an operation queue. Let's look at a simpler way of adding
    tasks to an operation queue using the `addOperationWithBlock()` method.
  prefs: []
  type: TYPE_NORMAL
- en: Using the addOperation() method of the operation queue
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `OperationQueue` class has a method named `addOperation()`, which makes
    it easy to add a block of code to the queue. This method automatically wraps the
    block of code in an operation object and then passes that operation to the queue.
    Let''s see how to use this method to add tasks to a queue:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: In the `BlockOperation` example earlier in this chapter, we added the tasks
    that we wished to execute into a `BlockOperation` instance. In this example, we
    are adding the tasks directly to the operation queue, and each task represents
    one complete operation. Once we create the instance of the operation queue, we
    then use the `addOperation()` method to add the tasks to the queue.
  prefs: []
  type: TYPE_NORMAL
- en: Also, in the `BlockOperation` example, the individual tasks did not execute
    until all of the tasks were added, and then that operation was added to the queue.
    This example is similar to the GCD example where the tasks began executing as
    soon as they were added to the operation queue.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we run the preceding code, the output should be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that the operations are executed concurrently. With this example,
    we can execute the tasks serially by using the `maxConcurrentOperationCount` property
    that we mentioned earlier. Let''s try this by initializing the `OperationQueue`
    instance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we run the example, the output should be similar to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we can see that each task waited for the previous task to complete
    before starting.
  prefs: []
  type: TYPE_NORMAL
- en: Using the `addOperation()` method to add tasks to the operation queue is generally
    easier than using the `BlockOperation` method; however, the tasks will begin as
    soon as they are added to the queue. This is usually the desired behavior, although
    there are use cases where we do not want the tasks executing until all operations
    are added to the queue, as we saw in the `BlockOperation` example.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at how we can subclass the `Operation` class to create an operation
    that we can add directly to an operation queue.
  prefs: []
  type: TYPE_NORMAL
- en: Subclassing the Operation class
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The previous two examples showed how to add small blocks of code to our operation
    queues. In these examples, we called the `performCalculations` method in the `DoCalculation`
    class to perform our tasks. These examples illustrate two really good ways to
    add concurrency for functionality that is already written, but what if, at design
    time, we want to design our `DoCalculation` class itself for concurrency? For
    this, we can subclass the `Operation` class.
  prefs: []
  type: TYPE_NORMAL
- en: The `Operation` abstract class provides a significant amount of infrastructure.
    This allows us to very easily create a subclass without a lot of work. We will
    need to provide at least an initialization method and a `main` method. The `main`
    method will be called when the queue begins executing the operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how to implement the `DoCalculation` class as a subclass of the
    `Operation` class; we will call this new class `MyOperation`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: We begin by defining that the `MyOperation` class is a subclass of the `Operation`
    class. Within the implementation of the class, we define two class constants,
    which represent the iteration count and the tag that the `performCalculations()`
    method uses. Keep in mind that when the operation queue begins executing the operation,
    it will call the `main()` method with no parameters; therefore, any parameters
    that we need to pass it must be passed through initializers.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, our initializer takes two parameters that are used to set the
    `iterations` and `tag` class constants. Then, the `main()` method, which the operation
    queue is going to call to begin the execution of the operation, simply calls the
    `performCalculation()` method.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now very easily add instances of our `MyOperation` class to an operation
    queue, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run this code, we will see the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: As we saw earlier, we can also execute the tasks serially by setting the `maxConcurrentOperationCount`
    property of the operation queue to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: If we know that we need to execute some functionality concurrently prior to
    writing the code, I would recommend subclassing the `Operation` class, as shown
    in this example, rather than using the previous examples. This gives us the cleanest
    implementation; however, there is nothing wrong with using the `BlockOperation`
    class or the `addOperation()` methods described earlier in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Before we consider adding concurrency to our application, we should make sure
    that we understand why we are adding it and ask ourselves whether it is necessary.
    While concurrency can make our application more responsive by offloading work
    from our main application thread to a background thread, it also adds extra overhead
    and complexity to our code. I have even seen numerous applications, in various
    languages, which actually ran better after we pulled out some of the concurrency
    code. This is because the concurrency was not well thought out or planned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the start of this chapter, we had a discussion about running tasks concurrently
    compared to running tasks in parallel. We also discussed the hardware limitations
    that restrict how many tasks can run in parallel on a given device. Having a good
    understanding of these concepts is very important to understanding how and when
    to add concurrency to our projects.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about GCD and operation queues, two different ways of implementing
    concurrency. While GCD is not limited to system-level applications, before we
    use it in our application, we should consider whether operation queues would be
    easier and more appropriate for our needs. In general, we should use the highest
    level of abstraction that meets our needs. This will usually point us to using
    operation queues; however, there really is nothing preventing us from using GCD,
    and it may be more appropriate for our needs.
  prefs: []
  type: TYPE_NORMAL
- en: We should always consider whether it is necessary to add concurrency to our
    applications. It is a good idea to think and talk about concurrency when we are
    discussing an application's expected behavior.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at some advanced topics and things to consider
    when we are creating our own custom types.
  prefs: []
  type: TYPE_NORMAL
