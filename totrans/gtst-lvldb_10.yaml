- en: Chapter 10. Tuning and Key Policies
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 调整和键策略
- en: LevelDB has two key architectural principles—immutability and speed in writing.
    The immutability is subtle but important to understand—data is never updated in
    LevelDB. Instead, it is marked as deleted or superseded by a new copy. From the
    application code, this may seem like a moot point, as you seem to be updating
    key values. However, it is vital to understanding the database structure and following
    behavior described.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: LevelDB有两个关键架构原则——不可变性和写入速度。不可变性虽然微妙但很重要——数据在LevelDB中永远不会被更新。相反，它被标记为已删除或被新的副本所取代。从应用程序代码来看，这似乎是一个无关紧要的问题，因为你似乎在更新键值。然而，理解数据库结构和遵循描述的行为至关重要。
- en: With your newly gained experience in LevelDB programming, we will study the
    implementation with an eye to tunable aspects. More details and file formats are
    explained in the code comments and the files in the `doc` folder of the LevelDB
    source.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在你获得LevelDB编程的新经验后，我们将从可调整的角度研究其实现。更多细节和文件格式在代码注释和LevelDB源代码的`doc`文件夹中的文件中解释。
- en: We will discuss the settings for tuning and the places where LevelDB gives you
    the ability to drop in your own classes. It can be used as we have done in our
    examples so far, as *out of the open-source box*, but there are also extension
    points and parameters that let you change its behavior. Some organizations take
    it even further, customizing the LevelDB source and then releasing their versions.
    The Riak and HyperDex servers are two significant NoSQL servers that have released
    their modifications to LevelDB as separate **forks**. We will discuss them briefly
    in the following tuning explanation.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论调整设置以及LevelDB允许你插入自定义类的地方。它可以像我们迄今为止的示例中那样使用，即*直接从开源箱中*使用，但也有一些扩展点和参数让你可以改变其行为。一些组织甚至更进一步，定制LevelDB源代码，然后发布他们的版本。Riak和HyperDex服务器是两个重要的NoSQL服务器，它们已经将他们的LevelDB修改作为独立的**分支**发布。我们将在接下来的调整说明中简要讨论它们。
- en: Understanding the Level in LevelDB
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解LevelDB中的Level
- en: As you can see in the following diagram, the main storage in LevelDB is a series
    of **levels** of *Sorted String Table* files. They currently have an `.sst` extension
    but that will change to `.ldb` in the near future, to avoid conflict with Microsoft.
    The files at each deeper level are up to ten times the size of the files in the
    previous level. The top level is an unsorted mix of records and sorting occurs
    as records are written to the next level.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在以下图中可以看到，LevelDB的主要存储是一个由**级别**组成的*排序字符串表*文件系列。它们目前有`.sst`扩展名，但不久将改为`.ldb`，以避免与微软冲突。每个更深层级的文件大小是前一个级别的文件大小的十倍。顶级是一个未排序的记录混合，排序发生在记录写入下一个级别时。
- en: Copying, sorting, and compaction is performed each time files from one level
    are amalgamated down to the next one. This **Write Amplification** is the single
    biggest performance trade-off in the LevelDB architecture. In a huge database,
    a given value may be written to disk up to eleven times over its lifetime as it
    is copied down the different levels. The big benefit of this approach is the speed
    of writing data without pausing for index updates. Conventional B-tree indexes
    also rewrite data as they balance trees.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 每次将一个级别的文件合并到下一个级别时，都会执行复制、排序和压缩。这种**写放大**是LevelDB架构中最大的性能权衡。在一个大型数据库中，某个值在其生命周期内可能被写入磁盘多达十一次，因为它在不同的级别之间被复制。这种方法的大优点是写入数据速度快，无需暂停进行索引更新。传统的B-tree索引在平衡树时也会重写数据。
- en: 'Data doesn''t just leap straight from your function call to the level tables.
    The first place the data goes from a `write()` is into a `memtable` which is a
    skip list structure. It is simultaneously written to a log file on disk that provides
    for data recovery if there is an application failure. When this log hits a 4 MB
    limit (governed by `write_buffer_limit)`, LevelDB starts writing a new log and
    flips over to the alternate `memtable`. There are only two of these structures
    maintained in memory. The one being written is referred to as imm as it is then
    regarded as immutable and a background thread copies it to a new `.sst` file at
    level 0:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并不会直接从你的函数调用跳转到level表。从`write()`操作开始，数据首先进入一个`memtable`，这是一个跳表结构。它同时被写入磁盘上的日志文件，以便在应用程序失败时提供数据恢复。当这个日志达到4MB的限制（由`write_buffer_limit`控制）时，LevelDB开始写入一个新的日志，并切换到备用的`memtable`。内存中只维护这两种结构。正在写入的那个被称为imm，因为它被视为不可变的，并且一个后台线程将其复制到一个新的`.sst`文件，位于0级：
- en: '![Understanding the Level in LevelDB](img/1015OS_10_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![理解LevelDB中的级别](img/1015OS_10_01.jpg)'
- en: LevelDB's data lifecycle of copying and sorting into bigger levels
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: LevelDB的数据生命周期：复制和排序到更大的级别
- en: To manage this collection of files in a persistent way, a **manifest** file
    is written that records the key ranges and levels for each of the `.sst` files
    in use. Remember that these are immutable files. The manifest has a record added
    each time a new `.sst` file is added by either writing out the current `memtable`
    or the compaction thread combining files and pushing them down a level. Also in
    the database directory is the plain text file CURRENT which just contains the
    name of the latest manifest file.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以持久的方式管理这些文件集合，会写入一个**manifest**文件，记录每个`.sst`文件中使用的键范围和级别。记住，这些是不可变文件。每当通过写入当前的`memtable`或压缩线程合并文件并将其推到下一级别时添加新的`.sst`文件，manifest都会添加一条记录。此外，在数据库目录中还有一个名为CURRENT的纯文本文件，它仅包含最新manifest文件的名称。
- en: Understanding that deleting is another form of write
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解删除是另一种形式的写入
- en: The explanation of immutable levels and writing above should enable you to understand
    how it can appear that we update a key—a newer value is written with the same
    key and trickles down the levels. Deleting a key similarly works from the top
    down. Unlike a tree index, we can't actually remove a key. Instead, what is written
    is a copy of the same key with a special marker to say it has been deleted.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 上述不可变级别和写入的解释应该能帮助你理解为什么我们看起来更新了一个键——使用相同键写入了一个新值，并逐级下渗。删除键的工作方式也是从上到下。与树索引不同，我们实际上不能删除一个键。相反，写入的是相同键的副本，并带有特殊标记以表明它已被删除。
- en: Understanding how reads work from the top down
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解从上到下读取的工作方式
- en: 'We''ve seen how writing values pushes from the top down, through the `memtable`
    into the level files. Reading could be said to pull from the top down and is a
    process of **maybe** and **elimination**. When a `Get` call looks for a key, the
    following steps occur until it is found, including finding a key with a delete
    marker, or there''s a definite no such key:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到写入值是如何从上到下，通过`memtable`推入级别文件的。读取可以说是从上到下拉取，是一个**可能**和**排除**的过程。当`Get`调用查找键时，以下步骤发生，直到找到为止，包括找到带有删除标记的键，或者确定没有这样的键：
- en: The current memtable skip list is scanned (can exit with found key).
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描当前的memtable跳表（可以找到键后退出）。
- en: The imm memtable skip list is scanned, if it is not empty. It will only have
    contents if in the process of being written to disk (can exit with found key).
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: imm memtable跳表如果非空，也会被扫描。它将只包含内容，如果它正在写入磁盘过程中（可以找到键后退出）。
- en: Manifest is checked to determine if the key is in a range that is known to exist
    in a level file (can say no).
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查Manifest以确定键是否在已知存在于级别文件中的范围内（可以说没有）。
- en: The candidate level file is opened if it is not already cached as open.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果候选级别文件尚未缓存为打开状态，则将其打开。
- en: If using a **filter policy**, described in detail later in this chapter, the
    filter is checked to see if the key is possibly in the level file (can say no).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用**filter policy**（将在本章后面详细描述），则检查过滤器以查看键是否可能在级别文件中（可以说没有）。
- en: The file's index is used to determine if the file contains a block with a key
    range including the key (can say no).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用文件的索引来确定文件是否包含一个包含键范围的块，该范围包括键（可以说没有）。
- en: If the block possibly containing the key is not in the block cache, read the
    block from the level file.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可能包含键的块不在块缓存中，则从级别文件中读取该块。
- en: Sequentially walk through the key-value pairs in the block to read the value,
    or determine that the key is not after all in that file (final found or no).
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 顺序遍历块中的键值对以读取值，或确定键最终不在该文件中（最终找到或没有）。
- en: Remember that keys and values are arbitrarily long so there's no way to calculate
    an offset to jump to the start of a given key's value, hence all this work of
    delving through the levels. The in-memory caches and filters help a lot. See `doc/table_format.txt`
    for more details on the layout and how the index points to blocks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 记住键和值可以是任意长度，因此无法计算一个偏移量以跳转到给定键值的起始位置，因此所有这些深入级别的努力。内存缓存和过滤器有很大帮助。有关布局和索引如何指向块的更多详细信息，请参阅`doc/table_format.txt`。
- en: Understanding how snapshots make reading predictable
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解快照如何使读取可预测
- en: One other subtle aspect of the immutable table architecture is the way that
    snapshots are used. The name is a bit misleading as it suggests something heavyweight
    which is a picture of the database. They do provide a way to effectively freeze
    your view of the database but at a low cost—they are basically just a special
    number.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 不变表架构的一个微妙方面是快照的使用方式。这个名字有点误导，因为它暗示这是一个重量级的数据库快照。但实际上，它们提供了一种以低成本冻结你对数据库视图的方法——它们基本上只是一个特殊的数字。
- en: The keys that are used inside the database are composed of your key, a flag
    value, and the snapshot number. The flag indicates if this is a data key or a
    delete key—the special keys that are added as a result of a `Delete`, operation
    as we saw earlier. Simultaneous reading while writing is protected by the snapshot
    number, effectively isolating the keys being iterated from any deletes or rewrites
    of those key values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据库内部使用的键由你的键、一个标志值和快照号组成。标志表示这是一个数据键还是一个删除键——正如我们之前看到的，这是由于`Delete`操作添加的特殊键。同时读写操作通过快照号得到保护，有效地隔离了正在迭代的键与任何删除或重写这些键值的操作。
- en: 'You can think of the snapshot as affecting any individual `Get` or `Iterator`
    reads, specifying it via the `ReadOptions` structure:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将快照视为影响任何单个`Get`或`Iterator`读取，通过`ReadOptions`结构指定它：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Although `GetSnapshot` returns an object that should be deleted to help with
    database state, its behavior is just as if you passed in the snapshot number to
    form the key used in the reads. Each new write or batch of writes will increment
    the current number and so the actual keys being searched for in your reads with
    the snapshot will not see the later ones.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`GetSnapshot`返回的对象应该被删除以帮助数据库状态，但其行为就像你传递了快照号来形成用于读取的关键一样。每次新的写入或写入批次都会增加当前数字，因此实际在快照中搜索的键将不会看到后续的键。
- en: Using snapshots is only a transitory activity within the current open session
    of the database. They are represented under the hood in an opaque way which means
    there is no safe way to persist a snapshot to disk and continue using it in a
    later run of your program.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用快照只是数据库当前打开会话中的一个临时活动。它们在底层以不可见的方式表示，这意味着没有安全的方法将快照持久化到磁盘并在程序后续运行中使用它。
- en: Understanding how Bloom filters help guesses
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解布隆过滤器如何帮助猜测
- en: Failing to find something is usually slower than finding it—when do you give
    up? In most applications, you will not have every possible key value stored in
    the database. One of the biggest optimizations in LevelDB is the use of a filter
    policy to decide if a given key is present in a level.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 找不到某物通常比找到它要慢——你何时放弃？在大多数应用程序中，你不会在数据库中存储每个可能的关键值。LevelDB中最大的优化之一是使用过滤器策略来决定给定的键是否存在于某个级别。
- en: We know from the manifest file which level file contains a key range for our
    key. If you are using a filter, the filter data is cached for each open file so
    it provides a quick answer as to the key presence in that table, without reading
    the index and scanning blocks. The default filter provided for you to use is a
    Bloom filter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从清单文件中知道哪个级别文件包含我们的键的范围。如果你使用过滤器，过滤器数据会为每个打开的文件进行缓存，因此它提供了一个快速回答，即该键是否存在于该表中，而不需要读取索引和扫描块。为你提供的默认过滤器是一个布隆过滤器。
- en: '![Understanding how Bloom filters help guesses](img/1015OS_10_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![理解布隆过滤器如何帮助猜测](img/1015OS_10_02.jpg)'
- en: A Bloom filter in operation from Jason Davies' online demonstrator
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 来自Jason Davies在线演示器的操作中的布隆过滤器
- en: The preceding figure shows a snapshot of having entered seven values into the
    animated demonstrator at [http://www.jasondavies.com/bloomfilter/](http://www.jasondavies.com/bloomfilter/),
    which is a good way to understand how they work. If the site is still in operation,
    go and play with entering some values and watch the bit vector changes with different
    values, then resume reading this chapter. I had my "aha" moment with the benefit
    of his site, after a few attempts of reading papers and looking at static diagrams.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了在[http://www.jasondavies.com/bloomfilter/](http://www.jasondavies.com/bloomfilter/)的动态演示器中输入了七个值后的快照，这是一个理解它们工作原理的好方法。如果该网站仍在运行，请去尝试输入一些值并观察不同值下的位向量变化，然后继续阅读本章。我在阅读论文和查看静态图表的几次尝试后，在他的网站上有了“啊哈”的顿悟。
- en: Hopefully, you've just seen a great demo of filters in action and the following
    will make a lot more sense. Bloom filters work with one simple insight—a bunch
    of simple, quick hash functions can be used in combination to decrease their chance
    of collision. The combined hash functions all write their results to the same
    bitmask. Calculating three simple hashes is much faster in general than trying
    to calculate a perfect hash. The filter doesn't work like a hash table—it fails
    to handle collisions because that's the job of the other LevelDB data structures
    getting to the actual key.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 希望你已经看到了过滤器在实际操作中的精彩演示，以下内容将使你更容易理解。布隆过滤器利用一个简单的洞察——通过组合使用一些简单、快速的哈希函数可以减少它们发生冲突的机会。组合的哈希函数将它们的结果写入相同的位掩码。通常计算三个简单的哈希值比尝试计算一个完美的哈希值要快得多。过滤器不像哈希表那样工作——它无法处理冲突，因为这是其他LevelDB数据结构的任务，它们到达实际键。
- en: Tip
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Hash functions aim to provide a small value that maps to a much larger key.
    Depending on your data, there may be colliding values. A bad hash is where too
    many of the original strings generate the same hash value. If you are completely
    new to the idea of hash values, just imagine taking the lowercase first letter
    of your key as the hash. This would be a perfect hash if you just had twenty names,
    starting with different letters. It would be a disaster if they were all Smith.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 哈希函数旨在提供一个小的值，该值映射到一个更大的键。根据你的数据，可能会有冲突的值。一个糟糕的哈希是当太多的原始字符串生成相同的哈希值。如果你对哈希值的概念完全陌生，只需想象将你的键的第一个小写字母作为哈希值。如果你只有二十个以不同字母开头的名字，这将是一个完美的哈希。如果它们都是Smith，这将是一场灾难。
- en: A Bloom filter guarantees no **false negatives**. If it says a key is not there,
    it is absolutely not there. But, if it says the key is present, there's only a
    chance it is present—another key may have had the same series of hashes. Deciding
    to use a filter is a classic trade-off gaining performance at the cost of more
    space on disk, storing the filter data. This is further refined by changing the
    bits-per-key or even the filter algorithm—more bits usually yields better performance
    at the cost of more space.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 布隆过滤器保证没有**假阴性**。如果它说一个键不存在，那么它绝对不存在。但是，如果它说键存在，那么它可能存在——另一个键可能有相同的哈希序列。决定使用过滤器是一个经典的权衡，以牺牲更多的磁盘空间为代价来提高性能，存储过滤器数据。这可以通过更改每键位数或甚至过滤器算法来进一步细化——更多的位数通常以牺牲更多空间为代价带来更好的性能。
- en: If you know your keys will almost always be in the database, there is no point
    using Bloom filters!
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道你的键几乎总是存在于数据库中，那么使用布隆过滤器是没有意义的！
- en: Tuning using Bloom filters or alternatives
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用布隆过滤器或替代方案进行调整
- en: LevelDB doesn't care what kind of filter you use, if any. It provides a single
    hook for you to specify a `FilterPolicy` object. You can subclass that interface
    to provide any filter you like. It is not mandatory to use a filter but you will
    usually improve performance at least by using the default one from `NewBloomFilterPolicy`.
    However, if you have a custom comparator which ignores areas of the key or treats
    them out of order, you can't use the default filter policy. You might also want
    a custom policy if your keys contained a lot of information and only a small part
    of them was mostly unique.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: LevelDB不关心你使用什么类型的过滤器，如果有的话。它提供了一个钩子供你指定一个`FilterPolicy`对象。你可以通过继承该接口来提供任何你喜欢的过滤器。使用过滤器不是强制性的，但通常至少通过使用`NewBloomFilterPolicy`提供的默认过滤器来提高性能。然而，如果你有一个自定义的比较器，它忽略了键的某些区域或以错误的顺序处理它们，那么你不能使用默认的过滤器策略。如果你的键包含大量信息，而其中只有一小部分是唯一的，你也可能需要一个自定义策略。
- en: Your custom filter might still use the Bloom algorithm or could be your own.
    There is no assumption about the data stored on the disk by the filter—LevelDB
    just stores and retrieves the bytes the filter object provides, at the end of
    each level file.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你的自定义过滤器可能仍然使用布隆算法，或者可能是你自己的。过滤器存储在磁盘上的数据没有假设——LevelDB只是存储和检索过滤器对象提供的字节，在每个级别文件的末尾。
- en: If you are using the standard filter, there's a tuning opportunity as it requires
    you to specify how many bits are used per key. The recommended value is 10 bits
    per key, which is the memory impact of the filter cached for that particular file.
    If you have a database with a lot of sparse keys, you might use more bits to improve
    accuracy and avoid index scans.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用标准过滤器，那么有一个调整的机会，因为它需要你指定每个键使用多少位。建议的值是每个键10位，这是为该特定文件缓存的过滤器内存影响。如果你有一个包含大量稀疏键的数据库，你可能需要使用更多的位来提高准确性并避免索引扫描。
- en: Basho's Riak server uses the Erlang-wrapper **eleveldb** which has a LevelDB
    clone. It's available at [http://github.com/basho/leveldb](http://github.com/basho/leveldb)
    and it includes an improved Bloom filter, as well as other changes that are more
    suited to their server environment. They claim their filter takes up less space
    on disk and has an 0.05 percent false positive rate, compared to the 1 percent
    false positive rate in the standard Google version (at the 10 bits per key mentioned
    earlier). A 1 percent false positive rate means that, when the filter says a key
    is there, 1 out of 100 times you will walk through the SSTable and find that key
    is not really there. Their filter can be copied and used as a drop-in replacement
    for the standard one.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Basho的Riak服务器使用Erlang包装器 **eleveldb**，它是一个LevelDB克隆。它可在 [http://github.com/basho/leveldb](http://github.com/basho/leveldb)
    上找到，并包括改进的布隆过滤器以及其他更适合其服务器环境的更改。他们声称他们的过滤器在磁盘上占用的空间更少，并且错误正率为0.05%，相比之下，标准Google版本（如前所述，每键10位）的错误正率为1%。1%的错误正率意味着，当过滤器说一个键存在时，100次中有1次你将遍历SSTable并发现该键实际上并不存在。他们的过滤器可以被复制并用作标准过滤器的直接替换。
- en: Using settings that affect performance
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用影响性能的设置
- en: 'The following settings are documented in `include/options.h` with significant
    comments and are all set in the `LevelDB::Options` structure passed into `Open`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 以下设置在 `include/options.h` 中有文档说明，并带有重要的注释，所有这些都在 `LevelDB::Options` 结构中设置，该结构传递给
    `Open`。
- en: '`write_buffer_size` defaults to 4 MB and much larger values will improve write
    performance, as used on Riak, but can result in blocking when the memtable is
    written to disk. Remember there are only two memtable buffers so stalling will
    occur if `imm` is still being written and the current buffer fills.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`write_buffer_size` 默认为4 MB，更大的值将提高写入性能，如Riak所使用，但写入内存表到磁盘时可能会导致阻塞。记住，只有两个内存表缓冲区，如果`imm`仍在写入且当前缓冲区已满，则将发生停滞。'
- en: '`max_open_files` defaults to 1000 and will be adequate for most databases.
    If you have a massive database on a server, this could be increased as it would
    allow more level files to be cached open and avoid the cost of opening them and
    reading in their index and filter blocks.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_open_files` 默认为1000，对于大多数数据库来说将足够。如果你在服务器上有一个庞大的数据库，则可以增加这个值，因为它将允许更多的级别文件被缓存打开，从而避免打开它们和读取其索引和过滤器块的成本。'
- en: '`block_cache` is a pointer to cache that takes the object created by `NewLRUCache`,
    and defaults to 8 MB, see the following discussion.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_cache` 是指向由 `NewLRUCache` 创建的缓存的指针，默认为8 MB，请参阅以下讨论。'
- en: '`block_size` user data per block, default 4 KB, affects the indexing of the
    level tables with one index entry per block. Leave this alone unless you have
    many keys that are much larger than 4 KB. It also is used for the flushing of
    I/O, so picking a much larger size may leave a very active database vulnerable
    to an OS crash losing data.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_size` 每块的用户数据，默认为4 KB，影响级别表的索引，每个块有一个索引条目。除非你有许多大于4 KB的键，否则不要更改此设置。它还用于I/O刷新，因此选择一个更大的大小可能会使非常活跃的数据库容易受到操作系统崩溃丢失数据的影响。'
- en: '`block_restart_interval` defaults to 16, leaves alone unless you have a lot
    of sequential keys with minimal changes. It is the checkpointing interval at which
    a new entire key is written rather than just the trailing changes.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`block_restart_interval` 默认为16，除非你有大量具有最小变化的连续键，否则不要更改。这是检查点间隔，在此间隔内会写入整个新键而不是仅写入尾部更改。'
- en: '`filter_policy` defaults to NULL, use `NewBloomFilterPolicy` to create a policy
    unless using a replacement such as the Riak one discussed earlier. Using a filter
    policy costs storage and uses a bit of memory but optimizes key lookups if there''s
    a reasonable chance of keys not being in tables.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`filter_policy` 默认为NULL，除非使用前面讨论过的Riak等替换方案，否则使用 `NewBloomFilterPolicy` 创建策略。使用过滤器策略会占用存储空间并使用一些内存，但如果存在键不在表中的合理可能性，则可以优化键查找。'
- en: Tuning and structuring data by scenario
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 根据场景调整和结构化数据
- en: The following scenarios provide context for the settings and key design techniques
    we've discussed here and in earlier chapters.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以下场景为我们在这里和前几章中讨论的设置和键设计技术提供了上下文。
- en: Choosing to structure data according to update rate
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据更新率选择数据结构
- en: As we discussed in [Chapter 8](ch08.html "Chapter 8. Richer Keys and Data Structures"),
    *Richer Keys and Data Structures*, you can decide to move some values into separate
    keys rather than keeping them in a main record. As you should now understand,
    if the main record is very static, it will tend to migrate to a level table and
    then sit there, while new key values are pushed down from the top for your regularly
    updated data. This warehousing approach will work even better if the main records
    are indexed with an ascending identifier, as their level tables won't require
    resorting.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第8章](ch08.html "第8章。更丰富的键和数据结构")中讨论的，*更丰富的键和数据结构*，你可以决定将一些值移动到单独的键中，而不是将它们保留在主记录中。正如你现在应该理解的那样，如果主记录非常静态，它将倾向于迁移到层级表，然后在那里停留，而新的键值将从顶部向下推送，用于你经常更新的数据。如果主记录使用升序标识符进行索引，这种仓库方法将工作得更好，因为它们的层级表不需要回退。
- en: There's an optimization in the compaction process which simply copies such tables
    down into the larger one when merging, if no resorting is required. You can also
    call the `CompactRange` function to force compaction for a given key range.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在压缩过程中有一个优化，即在合并时，如果不需要回退，只需简单地将这些表复制到较大的表中。你也可以调用`CompactRange`函数来强制对给定键范围进行压缩。
- en: Caching choices for key performance based on expected access
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于预期访问的键性能缓存选择
- en: Caching is a complicated process. One interesting additional option you can
    apply in the `ReadOptions` is to **bypass** the cache, by setting the `fill_cache`
    flag to false. For example, imagine you have a database open and some user action
    requires you to go off and read a number of keys somewhat out of the flow of most
    of the user-driven actions. Their use of the database to-date may have nicely
    loaded the cache with records being heavily reused. Creating an iterator with
    `fill_cache=false` will avoid flushing the current cache.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是一个复杂的过程。在`ReadOptions`中你可以应用一个有趣的附加选项，通过将`fill_cache`标志设置为false来**绕过**缓存。例如，想象一下你有一个数据库已打开，某些用户操作需要你离开并读取一些与大多数用户驱动操作流程不太相关的键。他们到目前为止对数据库的使用可能已经很好地加载了缓存，记录被大量重用。使用`fill_cache=false`创建的迭代器将避免刷新当前缓存。
- en: 'The other consideration is to use a larger cache size. The cache is an object
    that is created either using a standard call or your own factory if you subclassed
    theirs:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个考虑因素是使用更大的缓存大小。缓存是一个对象，它可以通过标准调用或如果你对其进行了子类化，使用你自己的工厂来创建：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The cache is a read cache—it will only help if you are doing a lot of reading
    and its size should be based on the volumes of data being read. Otherwise, you're
    wasting memory.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存是一个读取缓存——只有在你进行大量读取时才会有所帮助，其大小应该基于读取的数据量。否则，你就是在浪费内存。
- en: Using multiple databases depending on role
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 根据角色使用多个数据库
- en: The Riak server achieves great database throughput by using 7 to 64 LevelDB
    databases per platform, partly to improve write performance. You can also use
    different databases as an opportunity to tune settings differently depending on
    role. Imagine that you have a very dynamic audit trail—it could use a small cache
    and avoid the overhead of the filter policy, being optimized for writing. To optimize
    for robustness, you could reduce its `write_buffer_size` or dramatically increase
    the size to get high throughput. However, experience reports suggest that choosing
    to segment your use across databases is a late optimization unless you have radically
    different user profiles. The natural unpredictability of much user behavior will
    often be best served by having a single database cache data and react by building
    the levels as they accumulate data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Riak服务器通过在每个平台上使用7到64个LevelDB数据库来实现极高的数据库吞吐量，部分原因是为了提高写入性能。你也可以利用不同的数据库作为机会，根据角色调整不同的设置。想象一下，你有一个非常动态的审计跟踪——它可以使用一个小缓存，避免过滤策略的开销，优化于写入。为了优化健壮性，你可以减少其`write_buffer_size`或显著增加其大小以获得高吞吐量。然而，经验报告表明，除非你有截然不同的用户画像，否则选择在数据库之间分割使用是一种后期优化。许多用户行为的自然不可预测性通常最好通过拥有一个单独的数据库来缓存数据，并在积累数据时构建层级来实现。
- en: Reconsidering policies for generating keys
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新考虑生成键的策略
- en: We have just discussed how the desire to avoid table updates may lead to using
    different keys and explained how stable key ranges allow for optimal compaction.
    There are a few points we may want to consider about how keys are generated that
    can affect the level tables.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚讨论了避免表更新的愿望如何导致使用不同的键，并解释了如何通过稳定的键范围来实现最佳压缩。关于键的生成方式，有一些可能需要考虑的点，这些点会影响层级表。
- en: The simplest scenario to consider is the kind of bulk loading of data we used
    in the name and address databases. When `Sample06` moved to using multiple keys,
    we loaded them with a single loop creating two differently prefixed keys. That
    causes a lot of key overlapping and consequential sorting in the compaction from
    level 0 to level 1\. If there's such a once-off load of many records, like our
    50,000 line sample, consider using two passes through the data being loaded. A
    separate pass for each prefix means the keys we generate will already be grouped
    by prefix and reduces sorting at compaction time..
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑的最简单场景是我们用于名称和地址数据库的批量加载数据。当`Sample06`转向使用多个键时，我们通过一个循环创建两个不同前缀的键来加载它们。这导致了很多键重叠，并在从0级到1级的压缩过程中产生了大量的排序。如果存在一次性加载大量记录的情况，比如我们的50,000行样本，考虑使用两次数据加载。为每个前缀使用单独的遍历意味着我们生成的键将已经按前缀分组，这减少了压缩时的排序。
- en: Two-pass loading may not always be easy if you are generating unique ID suffixes
    such as the `nameId` we added to make names unique. However, even with such unique
    primary keys, you can still loop through the database and generate the secondary
    keys in a later pass. This is a lot more processing for data loading but could
    be a good trade-off as a single hit compared to many read operations later.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在生成唯一的ID后缀，如我们添加的`nameId`以使名称唯一，两次遍历加载可能并不总是容易。然而，即使有这种唯一的键，你仍然可以在后续遍历中通过数据库循环生成二级键。这为数据加载增加了更多的处理，但与后续的多次读取操作相比，可能是一个好的权衡。
- en: Remember that key values in the level tables are stored as trailing deltas,
    skipping the common prefix. You should be careful to avoid adding suffixes that
    might break this. If there is a common value that you had considered adding as
    a key suffix, see if it makes sense to make it a prefix instead. This would normally
    require some application logic change but might yield major table improvements.
    This kind of complicated change would only be useful if you have extreme performance
    requirements but is mentioned for your consideration.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，级别表中的键值以尾随增量形式存储，跳过了公共前缀。你应该小心避免添加可能会破坏这一点的后缀。如果你考虑将某个共同值添加为键后缀，看看是否将其作为前缀更合理。这通常需要一些应用程序逻辑更改，但可能会带来重大的表改进。这种复杂的变化只有在你有极端的性能要求时才有用，但这里提到以供你考虑。
- en: Another thing to consider to take advantage of the key deltas is if your keys
    have a common value field. If there is something in the value that doesn't change
    for many keys, it will be duplicated for each key. If you moved it into the key
    rather than the value side of the record, you might be able to get benefits from
    key compression.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可以考虑的是，如果你的键有一个共同的值字段，以利用键增量。如果值中有许多键不变的内容，它将为每个键重复。如果你将它移动到记录的键而不是值一侧，你可能会从键压缩中获得好处。
- en: Finally, remember that LevelDB's Bloom filters and key range behavior make it
    very good at determining if a key value is not in the database. If you have binary
    flags, consider whether you can invert their behavior and store a key to indicate
    the opposite, so your normal searches would be if the flag key was missing.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请记住LevelDB的Bloom过滤器和键范围行为使其非常擅长确定一个键值是否不在数据库中。如果你有二进制标志，考虑是否可以反转它们的行为，并存储一个键来表示相反的情况，这样你的正常搜索就会是如果标志键缺失。
- en: Summary
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: We have learned a lot more about the structures in memory and on disk that give
    LevelDB its name and behavior. Putting these in context of the API that we have
    been programming throughout the book gave you a more informed way to structure
    your programs and think about your key policies. You also learned about different
    settings that can affect performance and memory use, which might lead you to use
    multiple databases with varied settings.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对内存和磁盘上的结构有了更多的了解，这些结构赋予了LevelDB其名称和行为。将这些内容置于我们整本书中编程的API的上下文中，为你提供了更明智的方式来构建程序和考虑你的键策略。你还了解了一些可能影响性能和内存使用的不同设置，这可能会让你使用具有不同设置的多个数据库。
- en: Rounding off the LevelDB ecosystem, we will leave the native code world behind
    and end with an appendix reviewing some of the more common scripting language
    wrappers that let you use LevelDB without compilation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 总结LevelDB生态系统，我们将告别原生代码世界，并以一个附录结束，回顾一些更常见的脚本语言包装器，这些包装器允许你在不编译的情况下使用LevelDB。
