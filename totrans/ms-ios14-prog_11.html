<html><head></head><body>
		<div id="_idContainer116">
			<h1 id="_idParaDest-188"><em class="italic"><a id="_idTextAnchor366"/>Chapter 11</em>: Adding Media to Your App</h1>
			<p>A lot of the apps that people use every day make use of media in some way. Some apps show photos and videos in a user's feed. Other apps focus on playing audio or video, while there are also apps that allow users to record media and share it with their peers. You can probably name at least a couple of very well-known apps that make use of such media in one way or the other.</p>
			<p>Because media has such a significant presence in people's daily lives, it's good to know how you can integrate media into your own apps. iOS has excellent support for media playback and offers several different ways to create and consume different types of media. Some ways provide less flexibility but are more straightforward to implement. Others are more complex but provide significant power to you as a developer.</p>
			<p>In this chapter, you will learn about several ways to play and record media on iOS. You will learn how to play and record video, play audio, and take pictures, and you'll even learn how to apply filters to images with Apple's Core Image framework. This chapter covers the following topics:</p>
			<ul>
				<li>Playing audio and video</li>
				<li>Recording video and taking pictures </li>
				<li>Manipulating photos with Core Image</li>
			</ul>
			<p>By the end of this chapter, you will have a great foundation that you can build on to create engaging experiences for your users, allowing them to not only view content but also to create their own content in your app.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor367"/>Technical requirements</h1>
			<p>The code bundle for this chapter includes two starter projects called <strong class="source-inline">Captured_start</strong> and <strong class="source-inline">MediaPlayback_start</strong>. You can find them in the code bundle repository:</p>
			<p><a href="https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition">https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition</a></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor368"/><a id="_idTextAnchor369"/><a id="_idTextAnchor370"/>Playing audio and video</h1>
			<p>To make playing<a id="_idIndexMarker519"/> audio and video files as simple and straightforward as can <a id="_idIndexMarker520"/>be, Apple has created the <strong class="source-inline">AVFoundation</strong> framework. This framework contains a lot of helper classes that provide very low-level control over how iOS plays audio and video files. You can use <strong class="source-inline">AVFoundation</strong> to build a rich, custom media player with as many features as you need for your purpose.</p>
			<p>If you're looking for a simpler way to integrate media into your app, the <strong class="source-inline">AVKit</strong> framework might be what you need. <strong class="source-inline">AVKit</strong> contains several helpers that build upon the <strong class="source-inline">AVFoundation</strong> components to provide an excellent default player that supports many features, such as subtitles, AirPlay, and more.</p>
			<p>In this section, you will learn how to implement a simple video player with <strong class="source-inline">AVPlayerViewController</strong> from the <strong class="source-inline">AVKit</strong> framework. You will also implement a more complex audio player with <strong class="source-inline">AVFoundation</strong> components that play audio in the background and display, on the lock screen, the audio track currently being played.</p>
			<p>To follow along with the examples, you should open the <strong class="source-inline">MediaPlayback_start</strong> project in this chapter's code bundle. The starter app contains a straightforward interface with a tab bar and two pages. You will implement a video player on one page, and the audio player on the other page. The audio page comes with so<a id="_idTextAnchor371"/>me predefined controls and actions that you will implement later<a id="_idTextAnchor372"/>.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor373"/>Creating a simple video player</h2>
			<p>The first thing you <a id="_idIndexMarker521"/>need to do to implement a video player is to obtain a video file. You can use any video that is encoded in the <strong class="source-inline">h.264</strong> format. A good sample video is the <strong class="bold">Big Buck Bunny</strong> sample movie that was created by the Blender Foundation. You can find this video at the following UR<a id="_idTextAnchor374"/><a id="_idTextAnchor375"/>L: <a href="http://bbb3d.renderfarming.net/download.html">http://bbb3d.renderfarming.net/download.html</a>. If you want to use this video to practice with, make sure to download the 2D version of the video.</p>
			<p>As stated before, you will implement the video player using <strong class="source-inline">AVPlayerViewController</strong>. This view controller provides a convenient wrapper around several components from <strong class="source-inline">AVFoundation</strong>, and also provides default video controls, so you don't have to build your entire video player from scratch, as you will do for the audio player late<a id="_idTextAnchor376"/>r.</p>
			<p><strong class="source-inline">AVPlayerViewController</strong> is highly configurable, which means that you can choose whether the player supports AirPlay, shows playback controls, whether it should be full screen when a video plays, and more. For a complete list of configurable options, you can refer to Apple's <strong class="source-inline">AVPlayerViewController</strong> documentation.</p>
			<p>Once you have found your test video, you should add it to the <strong class="source-inline">MediaPlayback</strong> project and ensure that the video is added to the app target. You can follow these steps:</p>
			<ol>
				<li>Click on your project.</li>
				<li>Click on your target.</li>
				<li>Select <strong class="bold">Build Phases</strong>.</li>
				<li>Expand <strong class="bold">Copy Bundle Resources</strong>.</li>
				<li>Click <strong class="bold">+</strong> and select your file.</li>
			</ol>
			<p>After doing this, open <strong class="source-inline">VideoViewController.swift</strong> and add the following line to import <strong class="source-inline">AVKit</strong>:</p>
			<p class="source-code">import AVKit</p>
			<p>You should also add a<a id="_idIndexMarker522"/> property to <strong class="source-inline">VideoViewController</strong> to hold on to your video player instance. Add the following line to the <strong class="source-inline">VideoViewController</strong> class to do this:</p>
			<p class="source-code">let playerController = AVPlayerViewController()</p>
			<p>Since <strong class="source-inline">AVPlayerViewController</strong> is a <strong class="source-inline">UIViewController</strong> subclass, you should add it to <strong class="source-inline">VideoViewController</strong> as a child view controller. Doing this will make sure that <strong class="source-inline">VideoViewController</strong> forwards any view controller life cycle events, such as <strong class="source-inline">viewDidLoad()</strong>, along with any changes in trait collections and more to the video player. To do this, add the following code to the <strong class="source-inline">viewDidLoad()</strong> method in <strong class="source-inline">VideoViewControl<a id="_idTextAnchor377"/>ler</strong>:</p>
			<p class="source-code">// 1</p>
			<p class="source-code">addChild(playerController)</p>
			<p class="source-code">playerController.didMove(toParent: self)</p>
			<p class="source-code">// 2</p>
			<p class="source-code">view.addSubview(playerController.view)</p>
			<p class="source-code">let playerView = playerController.view!</p>
			<p class="source-code">playerView.translatesAutoresizingMaskIntoConstraints = false</p>
			<p class="source-code">NSLayoutConstraint</p>
			<p class="source-code">  .activate([</p>
			<p class="source-code">    playerView.widthAnchor.constraint(equalTo: view.widthAnchor, constant: -20),</p>
			<p class="source-code">    playerView.heightAnchor.constraint(equalTo: playerView.widthAnchor, multiplier: 9/16),</p>
			<p class="source-code">    playerView.centerXAnchor.constraint(equalTo: view.centerXAnchor),</p>
			<p class="source-code">    playerView.centerYAnchor.constraint(equalTo: view.centerYAnchor)</p>
			<p class="source-code">])</p>
			<p>The previous code <a id="_idIndexMarker523"/>snippet adds the video player to the video view controller as a child view controller. When you add a view controller as a child view controller, you must always call <strong class="source-inline">didMove(toParent:)</strong> on the child controller to make sure that it knows that it has been added as a child view controller to another view controller. After adding the video player as a child view controller, the video player's view is added as a subview for the video view controller, and some constraints are set up to position the player view.</p>
			<p>This is all you need to do to create an instance of the video player and make it appear in your view controller. The last step is to obtain a reference to your video file, create an <strong class="source-inline">AVPlayer</strong> that has a reference to the video file, and assign it to the player. Add the following code to do this:</p>
			<p class="source-code">let url = Bundle.main.url(forResource: "samplevideo", withExtension: "mp4")!</p>
			<p class="source-code">playerController.player = AVPlayer(url: url)</p>
			<p>The preceding code looks for a video file called <strong class="source-inline">samplevideo.mp4</strong> and obtains a URL for that file. It then creates an instance of <strong class="source-inline">AVPlayer</strong> that points to that video file and assigns it to the video player. The <strong class="source-inline">AVPlayer</strong> object is responsible for playing the video file. The <strong class="source-inline">AVPlayerViewController</strong> instance uses the <strong class="source-inline">AVPlayer</strong> instance to play the video and manages the actual playback of the video internally.</p>
			<p>If you run your app after adding the player this way, you will find that the video plays perfectly well, and that you have access to all the controls you might need. This is a great demonstration of how simple it is to add basic media integration to your app. The next step is a little more complex. You will directly use an <strong class="source-inline">AVAudioPlayer</strong> instance to play an audio file that is controlled through several custom media controls. The player will even play audio <a id="_idIndexMarker524"/>in the background and integrate with the lock screen to show information about the current file. In other words, you will<a id="_idTextAnchor378"/> build a simple audio player that does everything a user would expect it to do.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">When launching in the simulator, <strong class="source-inline">AVKit</strong> and large movie files can take some time to load up. Try it on a real de<a id="_idTextAnchor379"/>vice.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor380"/>Creating an audio player</h2>
			<p>Before you can <a id="_idIndexMarker525"/>implement your audio player, you will need to obtain some <strong class="source-inline">.mp3</strong> files you wish to use in your player. If you don't have any audio files on your computer, you can get some files from The Free Music Archive website, available at <a href="https://freemusicarchive.org/">https://freemusicarchive.org/</a>about, to obtain a couple of free songs that you would like to use for playback. Make sure to add them to the <strong class="source-inline">MediaPlayer</strong> Xcode project and ensure that they are included in the app t<a id="_idTextAnchor381"/>arget.</p>
			<p>You will build the audio player using the following steps:</p>
			<ol>
				<li value="1">Implement the necessary controls to start and stop the player and navigate to the next and previous songs.</li>
				<li>Implement the time scrubber.</li>
				<li>Read the file's metadata and show it to the user.</li>
			</ol>
			<p>The user interface, outlets, and actions are already set up, so make sure to familiarize yourself with the existing<a id="_idIndexMarker526"/> code before following along with the implementatio<a id="_idTextAnchor382"/>n of the audio <a id="_idTextAnchor383"/>player.</p>
			<h3>Implementing basic audio controls</h3>
			<p>Before you <a id="_idIndexMarker527"/>implement the audio player code, you will need to do a little bit of housekeeping. To be able to play audio, you need a list of the files that the player will play. In addition to this list, you also need to keep track of what song the user is currently playing, so you can determine the next and previous songs. Lastly, you also need to have the audio player itself. Instead of using a pre-built component, you will build your own audio player using an <strong class="source-inline">AVAudioPlayer</strong> object. <strong class="source-inline">AVAudioPlayer</strong> is perfect for implementing a simple audio player that plays a couple of local <strong class="source-inline">.mp3</strong> files. It offers some convenient helper methods to easily adjust the player's volume, seek to a specific timestamp in the song, and more.</p>
			<p>Define the following properties in <strong class="source-inline">AudioViewController.swift</strong>:</p>
			<p class="source-code">let files = ["one", "two", "three"]</p>
			<p class="source-code">var currentTrack = 0</p>
			<p class="source-code">var audioPlayer: AVAudioPlayer!</p>
			<p>Also, don't forget to add the import:</p>
			<p class="source-code">import AVKit</p>
			<p>Make sure to replace the files array with the filenames that you use for your own audio files. <strong class="source-inline">audioPlayer</strong> does not have a value yet at this point. You will set up the audio player next.</p>
			<p>Before you can play audio, you need to obtain a reference to a media file and provide this reference to an <strong class="source-inline">AVAudioPlayer</strong> object. Any time you want to load a new media file, you will have to create a new instance of the audio player, since you can't change the current file once a file is playing. Add the following helper method to <strong class="source-inline">AudioViewController</strong> to load the current track and create an <strong class="source-inline">AVAudioPlayer</strong> instance:</p>
			<p class="source-code">func loadTrack() {</p>
			<p class="source-code">  let url = Bundle.main.url(forResource: files[currentTrack], withExtension: "mp3")!</p>
			<p class="source-code">  audioPlayer = try! AVAudioPlayer(contentsOf: url)</p>
			<p class="source-code">  audioPlayer.delegate = self</p>
			<p class="source-code">}</p>
			<p>This method reads the filename for the current track and retrieves the local URL for it. This URL is then used to create and set the <strong class="source-inline">audioPlayer</strong> property on <strong class="source-inline">AudioViewController</strong>. The view controller is also assigned as the delegate for the audio player. You won't implement any of the delegate methods just yet, but you can add the following <a id="_idIndexMarker528"/>extension to make <strong class="source-inline">AudioViewController</strong> conform to the <strong class="source-inline">AVAudioPlayerDelegate</strong> protocol to ensure your code compiles:</p>
			<p class="source-code"><strong class="bold">extension</strong> AudioViewController: AVAudioPlayerDelegate {</p>
			<p class="source-code">}</p>
			<p>Now, let's call <strong class="source-inline">loadTrack()</strong> on <strong class="source-inline">viewDidLoad()</strong> to instantiate <strong class="source-inline">audioPlayer</strong> and load the first song. Add the following method to <strong class="source-inline">AudioViewController</strong>:</p>
			<p class="source-code">override func viewDidLoad() {</p>
			<p class="source-code">  super.viewDidLoad()</p>
			<p class="source-code">  loadTrack()</p>
			<p class="source-code">}</p>
			<p>You will implement one of the <strong class="source-inline">AVAudioPlayerDelegate</strong> methods when you add support for navigating to the next and previous tracks.</p>
			<p>Add the following two methods to the audio view controller to add support for playing and pausing the current audio file:</p>
			<p class="source-code">func startPlayback() {</p>
			<p class="source-code">  audioPlayer.play()</p>
			<p class="source-code">  playPause.setTitle("Pause", for: .normal)</p>
			<p class="source-code">}</p>
			<p class="source-code">func pausePlayback() {</p>
			<p class="source-code">  audioPlayer.pause()</p>
			<p class="source-code">  playPause.setTitle("Play", for: .normal)</p>
			<p class="source-code">}</p>
			<p>These methods are relatively straightforward. They call the audio player's <strong class="source-inline">play()</strong> and <strong class="source-inline">pause()</strong> methods and update the button's label, so it reflects the current player state. Add the following implementation for <strong class="source-inline">playPauseTapped()</strong> so that the play and pause methods get called when the user taps the play/paus<a id="_idTextAnchor384"/>e button:</p>
			<p class="source-code">@IBAction func playPauseTapped() {</p>
			<p class="source-code">  if audioPlayer.isPlaying {</p>
			<p class="source-code">    pausePlayback()</p>
			<p class="source-code">  } else {</p>
			<p class="source-code">    startPlayback()</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>If you run the app <a id="_idIndexMarker529"/>now, you can tap the play/pause button to start and stop the currently playing file. Make sure your device is not in silent mode, because the audio for your app is muted when the device is in silent mode. You will learn how to fix this when you implement the ability to play audio in the background. The next step is to add support for playing the next and previous tracks. Add the following two implementations to <strong class="source-inline">AudioViewController</strong> to do this:</p>
			<p class="source-code">@IBAction func nextTapped() {</p>
			<p class="source-code">  currentTrack += 1</p>
			<p class="source-code">  if currentTrack &gt;= files.count {</p>
			<p class="source-code">    currentTrack = 0</p>
			<p class="source-code">  }</p>
			<p class="source-code">  loadTrack()</p>
			<p class="source-code">  audioPlayer.play()</p>
			<p class="source-code">}</p>
			<p class="source-code">@IBAction func previousTapped() {</p>
			<p class="source-code">  currentTrack -= 1</p>
			<p class="source-code">  if currentTrack &lt; 0 {</p>
			<p class="source-code">    currentTrack = files.count - 1</p>
			<p class="source-code">  }</p>
			<p class="source-code">  loadTrack()</p>
			<p class="source-code">  audioPlayer.play()</p>
			<p class="source-code">}</p>
			<p>The preceding<a id="_idIndexMarker530"/> code adjusts the current track index, loads the new track, and immediately plays it. Note that every time the user taps on the next or previous button, a fresh audio player has to be created by calling <strong class="source-inline">loadTrack()</strong>. If you run the app now, you can play audio, pause it, and skip to the next or previous tracks.</p>
			<p>When you allow a full song to play, it will not yet advance to the next song afterward. To implement this, you need to add an implementation for the <strong class="source-inline">audioPlayerDidFinishPlaying(_:successfully:)</strong> method from <strong class="source-inline">AVAudioPlayerDelegate</strong>. Add the following implementation to call <strong class="source-inline">nextTapped()</strong>, so the next song automatically plays when the current song finishes:</p>
			<p class="source-code">func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {</p>
			<p class="source-code">  nextTapped()</p>
			<p class="source-code">}</p>
			<p>Now that the first features are implemented, the next step is to implement the time scrubber that shows the current song's progress and allows the user to adjust the playhead'<a id="_idTextAnchor385"/><a id="_idTextAnchor386"/><a id="_idTextAnchor387"/>s position.</p>
			<h3>Implementing the time scrubber</h3>
			<p>The user interface <a id="_idIndexMarker531"/>for the audio player app already contains a scrubber that is hooked up to the following three actions in the view controller:</p>
			<ul>
				<li><strong class="source-inline">sliderDragStart()</strong></li>
				<li><strong class="source-inline">sliderDragEnd() </strong></li>
				<li><strong class="source-inline">sliderChanged()</strong></li>
			</ul>
			<p>When an audio file is playing, the scrubber should automatically update to reflect the current position in the song. However, when a user starts dragging the scrubber, it should not update its position until the user has chosen the scrubber's new position. When the user is done dragging the scrubber, it should adjust itself based on the song's progress again. Any time the value for the slider changes, the audio player should adjust the playhead, so the song's progress matches that of the scrubber.</p>
			<p>Unfortunately, the <strong class="source-inline">AVAudioPlayer</strong> object does not expose any delegate methods to observe the progress of the current audio file. To update the scrubber regularly, you can implement a timer that updates the scrubber to the audio player's current position every second. Add the following property to <strong class="source-inline">AudioViewController</strong>, so you can hold on to the timer after you have created it:</p>
			<p class="source-code">var timer: Timer?</p>
			<p>Also, add the following two methods to <strong class="source-inline">AudioViewController</strong> as a convenient way to start the timer when the user starts dragging the scrubber, or when a file starts playing, and stop it when a user stops dragging the scrubber or to preserve resources when the playback is paused:</p>
			<p class="source-code">func startTimer() {</p>
			<p class="source-code">  timer = Timer.scheduledTimer(withTimeInterval: 1, repeats: true) { [unowned self] timer in</p>
			<p class="source-code">    self.slider.value = Float(self.audioPlayer.currentTime / self.audioPlayer.duration)</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p class="source-code">func stopTimer() {</p>
			<p class="source-code">  timer?.in<a id="_idTextAnchor388"/>validate()</p>
			<p class="source-code">}</p>
			<p>Add a call to <strong class="source-inline">startTimer()</strong> in the <strong class="source-inline">startPlayback()</strong> method and a call to <strong class="source-inline">stopTimer()</strong> in the <strong class="source-inline">pausePlayback()</strong> method. If you run the app after doing this, the scrubber will immediately begin updating its position when a song starts playing. However, scrubbing does not work yet. Add the following implementations for the scrubber <a id="_idIndexMarker532"/>actions to enable manual scrubbing:</p>
			<p class="source-code">@IBAction func sliderDragStart() {</p>
			<p class="source-code">  stopTimer()</p>
			<p class="source-code">}</p>
			<p class="source-code">@IBAction func sliderDragEnd() {</p>
			<p class="source-code">  startTimer()</p>
			<p class="source-code">}</p>
			<p class="source-code">@IBAction func sliderChanged() {</p>
			<p class="source-code">  audioPlayer.currentTime = Double(slider.value) * audioPlayer.duration</p>
			<p class="source-code">}</p>
			<p>The preceding methods are relatively simple, but they provide a very powerful feature that immediately makes your homemade audio player feel like an audio player you might use every day. The final step for implementing the <a id="_idTextAnchor389"/>audio player's functionality is to display metadata about the <a id="_idTextAnchor390"/>current song.</p>
			<h3>Displaying song metadata</h3>
			<p>Most <strong class="source-inline">.mp3</strong> files contain<a id="_idIndexMarker533"/> metadata in the form of ID3 tags. These metadata tags are used by applications such as iTunes to extract information about a song and display it to the user, as well as to categorize a music library or filter it. You can gain access to an audio file's metadata through code by loading the audio file into an <strong class="source-inline">AVPlayerItem</strong> object and extracting the metadata for its internal <strong class="source-inline">AVAsset</strong> instance. An <strong class="source-inline">AVAsset</strong> object contains information about a media item, such as its type, location, and more. When you load a file using an <strong class="source-inline">AVPlayerItem</strong> object, it will automatically create a corresponding <strong class="source-inline">AVAsset</strong> o<a id="_idTextAnchor391"/>bject for you.</p>
			<p>A single asset can contain loads of metadata in the metadata dictionary. Luckily, Apple has captured all of the valid ID3 metadata tags in the <strong class="source-inline">AVMetadataIdentifier</strong> object, so once you have extracted the metadata for an <strong class="source-inline">AVAsset</strong> instance, you can loop over all of its metadata to filter out the data you need. The following method does this, and sets the extracted values on the <strong class="source-inline">titleLabel</strong> variable of <strong class="source-inline">AudioViewController</strong>, as shown here:</p>
			<p class="source-code">func showMetadataForURL(_ url: URL) {</p>
			<p class="source-code">  let mediaItem = AVPlayerItem(url: url)</p>
			<p class="source-code">  let metadata = mediaItem.asset.metadata</p>
			<p class="source-code">  var information = [String]()</p>
			<p class="source-code">  for item in metadata {</p>
			<p class="source-code">    guard let identifier = item.identifier else { continue }</p>
			<p class="source-code">    switch identifier {</p>
			<p class="source-code">      case .id3MetadataTitleDescription, .id3MetadataBand:</p>
			<p class="source-code">        information.append(item.value?.description ?? "")</p>
			<p class="source-code">      default:</p>
			<p class="source-code">        break</p>
			<p class="source-code">    }</p>
			<p class="source-code">  }</p>
			<p class="source-code">  let trackTitle = information.joined(separator: " - ")</p>
			<p class="source-code">  titleLabel.text = trackTitle</p>
			<p class="source-code">}</p>
			<p>Make sure to add a call to this method from <strong class="source-inline">loadTrack()</strong>, and pass the audio file's URL that you obtain in <strong class="source-inline">loadTrack()</strong> to <strong class="source-inline">showMetadataForURL(_:)</strong>. If you run your app now, your basic functionality should be all there. The metadata should be shown correctly, the scrubber should work, and you should be able to skip songs or pause the playback.</p>
			<p>Even though your media player seems to be pretty much done at this point, did you notice that the music pauses when you send the app to the background? To make your app feel more like a real audio player, you should implement background audio playback and make<a id="_idIndexMarker534"/> sure that the currently playing song is presented on the user's lock screen, similar to how the native music app for iOS works. This is precisely the functionality you<a id="_idTextAnchor392"/><a id="_idTextAnchor393"/><a id="_idTextAnchor394"/> will add next.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor395"/>Playing media in the background</h2>
			<p>On iOS, playing <a id="_idIndexMarker535"/>audio in the background requires special permissions that you can enable in your app's <strong class="bold">Capabilities</strong> tab. If you enable the <strong class="bold">Background Modes</strong> capability, you can select the <strong class="bold">Audio, AirPlay, and Picture in Picture</strong> option to make your app eligible for playing audio in the background. The following screenshot shows the enabled capability for playing audio in the background:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/Figure_11.1_B14717.jpg" alt="Figure 11.1 − Background Modes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 − Background Modes</p>
			<p>If you want to add proper support for background audio playback, there are three features you need to implement:</p>
			<ul>
				<li>Set up an audio session, so audio continues playing in the background. </li>
				<li>Submit metadata to the "now playing" info center.</li>
				<li>Respond to playback actions from remote sources, such as the lock screen.</li>
			</ul>
			<p>You can set up the<a id="_idIndexMarker536"/> audio session for your app with just two lines of code. When you create an audio session, iOS will treat the audio played by your app slightly differently; for instance, your songs will play even if the device is set to silent. It also makes sure that your audio is played when your app is in the background, if you have the proper capabilities set up. Add the following code to <strong class="source-inline">viewDidLoad()</strong> to set up an audio session for the app:</p>
			<p class="source-code">try? AVAudioSession.sharedInstance().setCategory(.playback, mode: .default, options: [.allowAirPlay])</p>
			<p class="source-code">try? AVAudioSession.sharedInstance().setActive(t<a id="_idTextAnchor396"/>rue, options: [])</p>
			<p>The second feature to add is to supply information about the currently playing track. All information about the currently playing media file should be passed to the <strong class="source-inline">MPNowPlayingInfoCenter</strong> object. This object is part of the <strong class="source-inline">MediaPlayer</strong> framework and is responsible for showing the user information about the currently playing media file on the lock screen and in the command center. Before you pass information to the "now playing" info center, make sure to import the <strong class="source-inline">MediaPlayer</strong> framework at the top of the <strong class="source-inline">AudioViewController.swift</strong> file:</p>
			<p class="source-code">import MediaPlayer</p>
			<p>Next, add the following line of code to <strong class="source-inline">viewDidLoad()</strong>:</p>
			<p class="source-code">NotificationCenter.default.addObserver(self, selector: #selector(updateNowPlaying), name: UIApplication.didEnterBackgroundNotification, object: nil)</p>
			<p>In the documentation for <strong class="source-inline">MPNowPlayingInfoCenter</strong>, Apple states that you should always pass the most recent "now playing" information to the info center when the app goes to the background. To do this, the audio view controller should listen to the <strong class="source-inline">UIApplication.didEnterBackgroundNotification</strong> notification, so it can respond to the app<a id="_idIndexMarker537"/> going to the background. Add the following implementation for the <strong class="source-inline">updateNowPlaying()</strong> method to <strong class="source-inline">AudioVideoController</strong>:</p>
			<p class="source-code">@objc func updateNowPlaying() {</p>
			<p class="source-code">  var nowPlayingInfo = [String: Any]()</p>
			<p class="source-code">  nowPlayingInfo[MPMediaItemPropertyTitle] = titleLabel.text ?? "untitled"</p>
			<p class="source-code">  nowPlayingInfo[MPNowPlayingInfoPropertyElapsedPlaybackTime] = audioPlayer.currentTime</p>
			<p class="source-code">  nowPlayingInfo[MPMediaItemPropertyPlaybackDuration] = audioPlayer.duration</p>
			<p class="source-code">  MPNowPlayingInfoCenter.default().nowPlayingInfo = nowPlayingInfo</p>
			<p class="source-code">}</p>
			<p>The preceding code configures a dictionary with metadata about the currently playing file and passes it to the "now playing" info center. This method is called automatically when the app goes to the background, but you should also update the "now playing" information when a new song begins playing. Add a call to <strong class="source-inline">updateNowPlaying()</strong> in the <strong class="source-inline">loadTrack()</strong> method to make sure the "now playing" information is updated whenever a new<a id="_idTextAnchor397"/> track is loaded.</p>
			<p>The next and final step is to respond to remote commands. When the user taps the play/pause button, next button, or previous button on the lock screen, this is sent to your app as a remote command. You should explicitly define the handlers that should be called by iOS when a remote command occurs. Add the following method to <strong class="source-inline">AudioViewController</strong> to<a id="_idIndexMarker538"/> add support for remote commands:</p>
			<p class="source-code">func configureRemoteCommands() {</p>
			<p class="source-code">  let commandCenter = MPRemoteCommandCenter.shared()</p>
			<p class="source-code">  commandCenter.playCommand.addTarget { [unowned self] event in</p>
			<p class="source-code">    guard self.audioPlayer.isPlaying == false else { return .commandFailed }</p>
			<p class="source-code">    self.startPlayback()</p>
			<p class="source-code">    return .success</p>
			<p class="source-code">  }</p>
			<p class="source-code">  commandCenter.pauseCommand.addTarget { [unowned self] event in</p>
			<p class="source-code">    guard self.audioPlayer.isPlaying else { return .commandFailed }</p>
			<p class="source-code">    self.pausePlayback()</p>
			<p class="source-code">    return .success</p>
			<p class="source-code">  }</p>
			<p class="source-code">  commandCenter.nextTrackCommand.addTarget { [unowned self] event in</p>
			<p class="source-code">    self.nextTapped()</p>
			<p class="source-code">    return .success</p>
			<p class="source-code">  }</p>
			<p class="source-code">  commandCenter.previousTrackCommand.addTarget { [unowned self] event in</p>
			<p class="source-code">    self.previousTapped()</p>
			<p class="source-code">    return .success</p>
			<p class="source-code">  }</p>
			<p class="source-code">  UIApplication.shared.beginReceivingRemoteControlEvents()</p>
			<p class="source-code">}</p>
			<p>The preceding code obtains a reference to the remote command center and registers several handlers. It also calls <strong class="source-inline">beginReceivingRemoteControlEvents()</strong> on the application object <a id="_idIndexMarker539"/>to make sure it receives remote commands. Add a call to <strong class="source-inline">configureRemoteCommands()</strong> in <strong class="source-inline">viewDidLoad()</strong> to make sure that the app begins receiving remote commands as soon as the audio player is configured. As an exercise to practice, try implementing the commands to control the time scrubber and <strong class="source-inline">+15</strong> and <strong class="source-inline">-15</strong> from the lo<a id="_idTextAnchor398"/>ck screen yourself.</p>
			<p>Try to run your app and send it to the background. You should be able to control media playback from both the control center and the lock screen. The visible metadata should correctly update when you skip to the next or previous song, and the scrubber should accurately represent the current position of playback in the song. </p>
			<p>At this point, you have implemented a reasonably complete audio player that has pretty sophisticated behaviors. The next step in your exploration o<a id="_idTextAnchor399"/>f media on iOS is to discover how you can take picture<a id="_idTextAnchor400"/>s and record video.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor401"/>Recording video and taking pictures</h1>
			<p>In addition to <a id="_idIndexMarker540"/>playing existing media, you can also make apps that allow users to create<a id="_idIndexMarker541"/> their own content. In this section, you will learn how you can use a built-in component to enable users to take a picture. You will also learn how you can use a raw video feed to record a video. If you want to follow along with the samples in this section, make sure to grab the starter project for <strong class="source-inline">Captured</strong> from this chapter's code bundle.</p>
			<p>The starter project contains a couple of view controllers and some connected outlets and actions. Note that there is a <strong class="source-inline">UIViewController</strong> extension in the project, too.</p>
			<p>This extension includes a helper method that makes displaying an alert to the user a little bit simpler. This extension will be used to show an alert that informs the user when their photo or video is stored in the camera roll.</p>
			<p>Since a user's camera and photo library are considered very privacy-sensitive, you need to make sure that you add the following privacy-related keys to the app's <strong class="source-inline">Info.plist</strong>:</p>
			<ul>
				<li><strong class="bold">Privacy - Camera Usage Description</strong>: This property is required in order to access the camera so that you can take pictures and record video.</li>
				<li><strong class="bold">Privacy - Microphone Usage Description</strong>: You must add this property so that your videos record audio, as well as images.</li>
				<li><strong class="bold">Privacy - Photo Library Additions Usage Description</strong>: This property allows you to write photos to the user's photo library.</li>
			</ul>
			<p>Make sure to provide a good description for the privacy keys, so the user knows why you need access to their camera, microphone, and photo library. The better your description is, the more likely the user is to allow your app to access the associated privacy-sensitive information. After adding the keys, you are ready to see how you can take a picture using the built-in <strong class="source-inline">UIImagePickerControlle<a id="_idTextAnchor402"/><a id="_idTextAnchor403"/><a id="_idTextAnchor404"/>r</strong> component of UIKit.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor405"/>Taking and storing a picture</h2>
			<p>When you need a<a id="_idIndexMarker542"/> user to supply an image, they can do this by either selecting an image <a id="_idIndexMarker543"/>from their photo library or by taking a picture with the camera. <strong class="source-inline">UIImagePickerController</strong> supports both ways of picking an image. In this section, you will learn how you can allow users to take an image using the camera. Changing the example to allow users to select an image from their photo library should be straightforward, as long as you remember to add the <strong class="bold">Privacy - Photo Library Usage Description</strong> key to <strong class="source-inline">Info.plist</strong>.</p>
			<p>Add the following implementation for <strong class="source-inline">viewDidLoad()</strong> to the <strong class="source-inline">ImageViewController</strong> class:</p>
			<p class="source-code">override func viewDidLoad() {</p>
			<p class="source-code">  super.viewDidLoad()</p>
			<p class="source-code">  let imagePicker = UIImagePickerController()</p>
			<p class="source-code">  imagePicker.sourceType = .camera</p>
			<p class="source-code">  imagePicker.delegate = self</p>
			<p class="source-code">  present(imagePicker, animated: true, completion: nil)</p>
			<p class="source-code">}</p>
			<p>The previous<a id="_idIndexMarker544"/> implementation<a id="_idIndexMarker545"/> creates an instance of the <strong class="source-inline">UIImagePickerController</strong> object and configures it so that it uses the camera as the image source and presents it to the user. Note that the view controller is set as a delegate for the image picker.</p>
			<p>When the user has taken a picture, the image picker will notify its delegate about this so that it can extract the image and use it. In this case, the image should be given the <strong class="source-inline">selectedImage</strong> label in the view controller so that it can be shown in the image view, and saved when the user taps on the save button, and the <strong class="source-inline">saveImage()</strong> method is called as a result.</p>
			<p>Add the following extension to make <strong class="source-inline">ImageViewController</strong> conform to <strong class="source-inline">UIImagePickerControllerDelegate</strong>:</p>
			<p class="source-code">extension ImageViewController: UIImagePickerControllerDelegate, UINavigationControllerDelegate {</p>
			<p class="source-code">  func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {</p>
			<p class="source-code">    picker.dismiss(animated: true, completion: nil)</p>
			<p class="source-code">    guard let image = info[.originalImage] as? UIImage else { return }</p>
			<p class="source-code">    selectedImage = image</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>Note that this extension also makes the image view controller conform to <strong class="source-inline">UINavigationControllerDelegate</strong>. The delegate property on the image picker controller requires all delegates to conform to both <strong class="source-inline">UINavigationControllerDelegate</strong> and <strong class="source-inline">UIImagePickerControllerDelegate</strong>.</p>
			<p>When the user has taken a picture with the camera, <strong class="source-inline">imagePickerController(_: didFinishPickingMediaWithInfo)</strong> is called to notify the delegate about the photo that the user took. The first thing that the preceding code does is dismiss the picker, as it's<a id="_idIndexMarker546"/> no longer needed. The picture that the user just took is stored in<a id="_idIndexMarker547"/> the <strong class="source-inline">info</strong> dictionary as the original image. When the image is extracted from the dictionary, it is set as <strong class="source-inline">selectedImage</strong>.</p>
			<p>To store the image, add the following implementation of <strong class="source-inline">saveImage()</strong>:</p>
			<p class="source-code">@IBAction func saveImage() {</p>
			<p class="source-code">  guard let image = selectedImage else { return }</p>
			<p class="source-code">  UIImageWriteToSavedPhotosAlbum(image, self, #selector(didSaveImage(_:withError:contextInfo:)), nil)</p>
			<p class="source-code">}</p>
			<p class="source-code">@objc func didSaveImage(_ image: UIImage, withError error: Error?, contextInfo: UnsafeRawPointer) {</p>
			<p class="source-code">  guard error == nil else { return }</p>
			<p class="source-code">  presentAlertWithTitle("Success", message: "Image was saved succesfully")</p>
			<p class="source-code">}</p>
			<p>The preceding code calls <strong class="source-inline">UIImageWriteToSavedPhotosAlbum(_:_:_:_)</strong> to store the image in the user's photo library. When the save operation completes, the <strong class="source-inline">didSaveImage(_:withError:contextInfo:)</strong> method will be called. If this method does not receive any errors, then the photo was successfully stored in the photo library and an alert is shown.</p>
			<p>Allowing the user to take a picture by implementing <strong class="source-inline">UIImagePickerController</strong> is relatively straightforward, and it is a great way to implement a camera feature in your app without too<a id="_idIndexMarker548"/> much effort. Sometimes, you need more advanced access to the <a id="_idIndexMarker549"/>camera. In these cases, you can use <strong class="source-inline">AVFoundation</strong> to gain access to the raw video feed from the <a id="_idTextAnchor406"/><a id="_idTextAnchor407"/><a id="_idTextAnchor408"/>camera, as you will see next.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor409"/>Recording and storing video</h2>
			<p>In the previous <a id="_idIndexMarker550"/>section, you used <strong class="source-inline">AVFoundation</strong> to build a simple audio player <a id="_idIndexMarker551"/>app. You will now use <strong class="source-inline">AVFoundation</strong> again, except instead of playing video or audio, you will now record video and store it in the user's photo library. When using <strong class="source-inline">AVFoundation</strong> to record a video feed, you do so with an <strong class="source-inline">AVCaptureSession</strong> object. A capture session is responsible for taking the input from one or more <strong class="source-inline">AVCaptureDeviceInput</strong> objects and writing it to an <strong class="source-inline">AVCaptureOutput</strong> subclass.</p>
			<p>The following diagram shows the objects that are involved with recording media through <strong class="source-inline">AVCaptureSession</strong>:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/Figure_11.2_B14717.jpg" alt="Figure 11.2 − AVCaptureSession entities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 − AVCaptureSession entities</p>
			<p>To get started on implementing the video recorder, make sure to import <strong class="source-inline">AVFoundation</strong> in <strong class="source-inline">RecordVideoViewController.swift</strong>. Also, add the following properties to the <strong class="source-inline">RecordVideoViewController</strong> class:</p>
			<p class="source-code">let videoCaptureSession = AVCaptureSession()</p>
			<p class="source-code">let videoOutput = AVCaptureMovieFileOutput()</p>
			<p class="source-code">var previewLayer:  AVCaptureVideoPreviewLayer?</p>
			<p>Most of the preceding properties should look familiar because they were also shown in the screenshot that outlined the components that are involved with <strong class="source-inline">AVCaptureSession</strong>. Note that <strong class="source-inline">AVCaptureMovieFileOutput</strong> is a subclass of <strong class="source-inline">AVCaptureOutput</strong>, specialized in capturing video. The preview layer will be used to render the video feed at runtime and present it to the user so that they can see what they <a id="_idTextAnchor410"/>are capturing with the camera.</p>
			<p>The next step is<a id="_idIndexMarker552"/> to set up the <strong class="source-inline">AVCaptureDevice</strong> objects for the camera and<a id="_idIndexMarker553"/> microphone and associate them with <strong class="source-inline">AVCaptureSession</strong>. Add the following code to the <strong class="source-inline">viewDidLoad()</strong> method:</p>
			<p class="source-code">override func viewDidLoad() {</p>
			<p class="source-code">  super.viewDidLoad()</p>
			<p class="source-code">  // 1</p>
			<p class="source-code">  guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),</p>
			<p class="source-code">  let microphone = AVCaptureDevice.default(.builtInMicrophone, for: .audio, position: .unspecified) else { return }</p>
			<p class="source-code">  // 2</p>
			<p class="source-code">  do {</p>
			<p class="source-code">    let cameraInput = try AVCaptureDeviceInput(device: camera)</p>
			<p class="source-code">    let microphoneInput = try AVCaptureDeviceInput(device: microphone)</p>
			<p class="source-code">    videoCaptureSession.addInput(cameraInput)</p>
			<p class="source-code">    videoCaptureSession.addInput(microphoneInput)</p>
			<p class="source-code">    videoCaptureSession.addOutput(videoOutput)</p>
			<p class="source-code">  } catch {</p>
			<p class="source-code">    print(error.localizedDescription)</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>The preceding code first obtains a reference to the camera and microphone that will be used to record the video and audio. The second step is to create the <strong class="source-inline">AVCaptureDeviceInput</strong> objects that are associated with the camera and microphone and associate them with the capture session. The video output is also added to the video<a id="_idIndexMarker554"/> capture session. If you examine the screenshot that you <a id="_idIndexMarker555"/>saw earlier and compare it with the preceding code snippet, you will find that all four components are present in this implementation.</p>
			<p>The next step is to provide the user with a view that shows the current camera feed so that they can see what they are recording. Add the following code to <strong class="source-inline">viewDidLoad()</strong> after the capture session setup code:</p>
			<p class="source-code">previewLayer = AVCaptureVideoPreviewLayer(session: videoCaptureSession)</p>
			<p class="source-code">previewLayer?.videoGravity = .resizeAspectFill</p>
			<p class="source-code">videoView.layer.addSublayer(previewLayer!)</p>
			<p class="source-code"><a id="_idTextAnchor411"/>videoCaptureSession.startRunning()</p>
			<p>The preceding code sets up the preview layer and associates it with the video capture session. The preview layer will directly use the capture session to render the camera feed. The capture session is then started. This does not mean that the recording session starts; rather, only that the capture session will begin processing the data from its camera and microphone inputs.</p>
			<p>The preview layer is added to the view at this point, but it doesn't cover the video view yet. Add the following implementation for <strong class="source-inline">viewDidLayoutSubviews()</strong> to <strong class="source-inline">RecordVideoViewController</strong> to set the preview layer's size and position, so it matches the size and position of <strong class="source-inline">videoView</strong>:</p>
			<p class="source-code">override func viewDidLayoutSubviews() {</p>
			<p class="source-code">  super.viewDidLayoutSubviews()</p>
			<p class="source-code">  previewLayer?.bounds.size = videoView.frame.size</p>
			<p class="source-code">  previewLayer?.position = CGPoint(x: videoView.frame.midX, y:videoView.frame.size.height / 2)</p>
			<p class="source-code">}</p>
			<p>Running the app now will already show you the camera feed. However, tapping the record button doesn't work yet, because you haven't yet implemented the <strong class="source-inline">startStopRecording()</strong> method. Add <a id="_idIndexMarker556"/>the following implementation for this<a id="_idIndexMarker557"/> method:</p>
			<p class="source-code">@IBAction func startStopRecording() {</p>
			<p class="source-code">  // 1</p>
			<p class="source-code">  if videoOutput.isRecording {</p>
			<p class="source-code">    videoOutput.stopRecording()</p>
			<p class="source-code">  } else {</p>
			<p class="source-code">    // 2</p>
			<p class="source-code">    guard let path = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else { return }</p>
			<p class="source-code">    let fileUrl = path.appendingPathComponent("recording.mov")</p>
			<p class="source-code">    // 3</p>
			<p class="source-code">    try? FileManager.default.removeItem(at: fileUrl)</p>
			<p class="source-code">    // 4</p>
			<p class="source-code">    videoOutput.startRecording(to: fil<a id="_idTextAnchor412"/>eUrl, recordingDelegate: self)</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>Let's go over the preceding snippet step by step to see what exactly is going on:</p>
			<ol>
				<li value="1">First, the <strong class="source-inline">isRecording</strong> property for the video output is checked. If a recording is currently active, the recording should be stopped.</li>
				<li>If no recording is currently active, a new path is created to store the video temporarily.</li>
				<li>Since the video <a id="_idIndexMarker558"/>output cannot overwrite an existing file, the <strong class="source-inline">FileManager</strong> object<a id="_idIndexMarker559"/> should attempt to remove any existing files at the temporary video file path.</li>
				<li>The video output will start recording to the temporary file. The view controller itself is passed as a delegate to be notified when the recording has begun and is stopped.</li>
			</ol>
			<p>Since <strong class="source-inline">RecordVideoViewController</strong> does not conform to <strong class="source-inline">AVCaptureFileOutputRecordingDelegate</strong> yet, you should add the following extension to add conformance to <strong class="source-inline">AVCaptureFileOutputRecordingDelegate</strong>:</p>
			<p class="source-code">extension RecordVideoViewController: AVCaptureFileOutputRecordingDelegate {</p>
			<p class="source-code">  // 1</p>
			<p class="source-code">  func fileOutput(_ output: AVCaptureFileOutput, didStartRecordingTo fileURL: URL, from connections: [AVCaptureConnection]) {</p>
			<p class="source-code">    startStopButton.setTitle("Stop Recording", for: .normal)</p>
			<p class="source-code">  }</p>
			<p class="source-code">  // 2</p>
			<p class="source-code">  func fileOutput(_ output: AVCaptureFileOutput, didFinishRecordingTo outputFileURL: URL, from connections: [AVCaptureConnection], error: Error?) {</p>
			<p class="source-code">    guard error == nil else { return }</p>
			<p class="source-code">    UISaveVideoAtPathToSavedPhotosAlbum(outputFileURL.path, self, #selector(didSaveVideo(at:withError:contextInfo:)), nil)</p>
			<p class="source-code">  }</p>
			<p class="source-code">  // 3</p>
			<p class="source-code">  @objc func didSaveVideo(at path: String, withError error: Error?, contextInfo: UnsafeRawPointer?) {</p>
			<p class="source-code">    guard error == nil else { return }</p>
			<p class="source-code">    presentAlertWithTitle("Success", message: "Video was saved succesfully")</p>
			<p class="source-code">    startStopButton.setTitle("Start Recording", for: .normal)</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>The preceding extension <a id="_idIndexMarker560"/>contains three methods. The first is a delegate method, called <a id="_idIndexMarker561"/>when the video output has begun recording. When the recording has started, the title of the <strong class="source-inline">startStopButton</strong> button is updated to reflect the current state. The second method is also a delegate method. This method is called when the recording has completed. If no errors occur, the video is stored at the temporary location you set up earlier. <strong class="source-inline">UISaveVideoAtPathToSavedPhotosAlbum(_:_:_:_:)</strong> is then called, to move the video from the temporary location to the user's photo library. This method is very similar to the <strong class="source-inline">UIImageWriteToSavedPhotosAlbum(_:_:_:_:)</strong> method that you used to store a picture. The third and final method in the extension is called when the video is stored in the user's photo library. When the video has been successfully stored, an alert is shown, and the title of the <strong class="source-inline">startStopButton</strong> button is updated again.</p>
			<p>You can now run the app and record some videos! Even though you have done a lot of manual work by implementing the video recording logic directly with <strong class="source-inline">AVCaptureSession</strong>, most of the hard work is done inside of the <strong class="source-inline">AVFoundation</strong> framework. One final<a id="_idIndexMarker562"/> media-related feature to explore is applying visual filters<a id="_idIndexMarker563"/> to images using <strong class="bold">Core Image</strong>. Applying filters to images is a very popular functionality in lots of apps and <a id="_idTextAnchor413"/><a id="_idTextAnchor414"/>it can make your photo app more appealing.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor415"/>Manipulating photos with Core Image</h1>
			<p>In this chapter, you<a id="_idIndexMarker564"/> have already seen that iOS has<a id="_idIndexMarker565"/> powerful capabilities for recording and playing media. In this section, you will learn how you can manipulate images with Core Image. The Core Image framework provides many different filters that you can use to process both images and videos. You will expand on the photo-taking capabilities that you implemented in the <strong class="source-inline">Captured</strong> app so that users can grayscale and crop images.</p>
			<p>Every Core Image filter you apply to images is an instance of the <strong class="source-inline">CIFilter</strong> class. You can create instances of filters as follows:</p>
			<p class="source-code">let filter = CIFilter(name: "CIPhotoEffectNoir")</p>
			<p>The <strong class="source-inline">name</strong> parameter in the filter's initializer is expected to be a string that refers to a specific filter. You can refer to Apple's documentation on Core Image and the Core Image Filter Reference guide to see an overview of al<a id="_idTextAnchor416"/>l the filters that you can use in your apps.</p>
			<p>Every filter has a certain set of parameters that you need to set on the <strong class="source-inline">CIFilter</strong> instance to use the filter; for instance, a grayscale filter requires you to provide an input image. Other filters might take an intensity, location, or other properties. The best way to see how you can apply a filter to an image is through an example. Add the following implementation for <strong class="source-inline">applyGrayScale()</strong> to <strong class="source-inline">ImageViewController.swift</strong> to implement<a id="_idIndexMarker566"/> a<a id="_idIndexMarker567"/> grayscale filter:</p>
			<p class="source-code">@IBAction func applyGrayScale() {</p>
			<p class="source-code">  // 1</p>
			<p class="source-code">  guard let cgImage = selectedImage?.cgImage,</p>
			<p class="source-code">  // 2</p>
			<p class="source-code">  let initialOrientation = selectedImage?.imageOrientation,</p>
			<p class="source-code">  // 3</p>
			<p class="source-code">  let filter = CIFilter(name: "CIPhotoEffectNoir") else { return }</p>
			<p class="source-code">  // 4</p>
			<p class="source-code">  let sourceImage = CIImage(cgImage: cgImage)</p>
			<p class="source-code">  filter.setValue(sourceImage, forKey: kCIInputImageKey)</p>
			<p class="source-code">  // 5</p>
			<p class="source-code">  let context = CIContext(options: nil)</p>
			<p class="source-code">  guard let outputImage = filter.outputImage, let cgImageOut = context.createCGImage(outputImage, from: outputImage.extent) else { return }</p>
			<p class="source-code">  // 6</p>
			<p class="source-code">  selectedImage = UIImage(cgImage: cgImageOut, scale: 1, orientation: initialOrientation)</p>
			<p class="source-code">}</p>
			<p>The preceding code has a lot of small, interesting details, highlighted with numbered comments. Let's go over the comments one by one to see how the grayscale filter is applied:</p>
			<ol>
				<li value="1">The <strong class="source-inline">UIImage</strong> instance that is stored in <strong class="source-inline">selectedImage</strong> is converted into a <strong class="source-inline">CGImage</strong> instance. Strictly speaking, this conversion isn't required, but it does make applying other filters to the <strong class="source-inline">UIImage</strong> instance later a bit easier.</li>
				<li>One downside of using <strong class="source-inline">CGImage</strong>, instead of <strong class="source-inline">UIImage</strong>, is that the orientation information that is stored in the image is lost. To make sure the final image maintains its orientation, the initial orientation is stored.</li>
				<li>This ste<a id="_idTextAnchor417"/>p creates an instance of the grayscale filter.</li>
				<li>Since Core Image does not directly support <strong class="source-inline">CGImage</strong> instances, the <strong class="source-inline">CGImage</strong> instance is converted into a <strong class="source-inline">CIImage</strong> instance that can be used with Core Image. The <strong class="source-inline">CIImage</strong> instance is then assigned as the input image for the grayscale<a id="_idIndexMarker568"/> filter, by<a id="_idIndexMarker569"/> calling <strong class="source-inline">setValue(_:forKey:)</strong> on the filter.</li>
				<li>The fifth step extracts the new image from the filter and uses a <strong class="source-inline">CIContext</strong> object to export the <strong class="source-inline">CIImage</strong> output to a <strong class="source-inline">CGImage</strong> instance.</li>
				<li>The sixth and final step is to create a new <strong class="source-inline">UIImage</strong> instance, based on the <strong class="source-inline">CGImage</strong> output. The initial orientation is passed to the new <strong class="source-inline">UIImage</strong> instance to make sure it has the same orientation as the original image.</li>
			</ol>
			<p>Even though there are a lot of steps involved, and you need to convert between different image types quite a bit, applying the filter is relatively simple. Most of the preceding code takes care of switching between image types, while the filter itself is set up in just a couple of lines. Try running the app now and taking a picture. The initial picture will be in full color. After you apply the grayscale filter, the image is automatically replaced with a grayscale version of the image, as shown in the <a id="_idTextAnchor418"/>following screenshot:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/Figure_11.3_B14717.jpg" alt="Figure 11.3 − Grayscale&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 − Grayscale</p>
			<p>The next filter you will implement is a crop filter. The crop filter will crop the image so that it's a square, rather than a portrait or landscape picture. The process for implementing the <a id="_idIndexMarker570"/>crop filter is mostly the same as for the grayscale<a id="_idIndexMarker571"/> filter, except for the values that need to be passed to the crop filter. Add the following implementation for <strong class="source-inline">cropSquare()</strong> to implement the crop filter:</p>
			<p class="source-code">@IBAction func cropSquare() {</p>
			<p class="source-code">  let context = CIContext(options: nil)</p>
			<p class="source-code">  guard let cgImage = selectedImage?.cgImage, let initialOrientation = selectedImage?.imageOrientation, let filter = CIFilter(name: "CICrop") else { return }</p>
			<p class="source-code">  let size = CGFloat(min(cgImage.width, cgImage.height))</p>
			<p class="source-code">  let center = CGPoint(x: cgImage.width / 2, y: cgImage.height / 2)</p>
			<p class="source-code">  let origin = CGPoint(x: center.x - size / 2, y: center.y - size / 2)</p>
			<p class="source-code">  let cropRect = CGRect(origin: origin, size: CGSize(width: size, height: size))</p>
			<p class="source-code">  let sourceImage = CIImage(cgImage: cgImage)</p>
			<p class="source-code">  filter.setValue(sourceImage, forKey: kCIInputImageKey)</p>
			<p class="source-code">  filter.setValue(CIVector(cgRect: cropRect), forKey: "inputRectangle")</p>
			<p class="source-code">  guard let outputImage = filter.outputImage, let cgImageOut = context.createCGImage(outputImage, from: outputImage.extent) else { return }</p>
			<p class="source-code">  selectedImage = UIImage(cgImage: cgImageOut, scale: 1, orientation: initialOrientation)</p>
			<p class="source-code">  }</p>
			<p class="source-code">}</p>
			<p>The preceding <a id="_idIndexMarker572"/>code performs several calculations to figure<a id="_idIndexMarker573"/> out the best way to crop the image into a square. The <strong class="source-inline">CGRect</strong> instance specifies the crop coordinates and size, which are then used to create a <strong class="source-inline">CIVector</strong> object. This object is then passed to the filter as the value for the <strong class="source-inline">inputRectangle</strong> key. Apart from specifying the crop values, the process of applying the filter is <a id="_idTextAnchor419"/>identical, so the code should look familiar to you.</p>
			<p>If you run the app now and tap the crop button, the image will be cropped, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/Figure_11.4_B14717.jpg" alt="Figure 11.4 − Cropping the image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 − Cropping the image</p>
			<p>There are many more filters available in Core Image, which you can play around with to build pretty advanced filters. You can even apply multiple filters to a single image to create elaborate<a id="_idIndexMarker574"/> effects for the pictures in your apps. Because <a id="_idIndexMarker575"/>all filters work in very similar ways, it's relatively easy to apply any filter to your images once you understand how the general process of applying a filter works. You can always use the code from the preceding examples if you ne<a id="_idTextAnchor420"/>ed a reminder about how to apply Core Image filters.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor421"/>Summary</h1>
			<p>In this chapter, you have learned a lot about media in iOS. You saw how you can implement a video player with just a couple of lines of code. After that, you learned how to use <strong class="source-inline">AVFoundation</strong> directly to build an audio player that supports features such as stopping and resuming playback, skipping songs, and scrubbing forward or backward in a song. You even learned how you can keep playing audio when the app goes to the background or when the phone is set to silent mode. To apply the finishing touches to the audio player, you learned how you can use the <strong class="source-inline">MediaPlayer</strong> framework to show the currently playing file on the user's lock screen, and how to respond to control events that are sent to the app remotely.</p>
			<p>After implementing media playback, you learned how you can build apps that help users to create media. You saw that <strong class="source-inline">UIImagePickerController</strong> provides a quick and simple interface to allow users to take a picture with the camera. You also learned how you can use <strong class="source-inline">AVFoundation</strong> and an <strong class="source-inline">AVCaptureSession</strong> object to implement a custom video recording experience. To wrap it all up, you learned about the Core Image framework, and how you can use it to apply filters to images.</p>
			<p>In the next chapter, you will learn everything you need to know about location services and how to use Core Location in your apps. Depending on the use case of your app, handling the user location properly can be a critical task for your app to be successful. Examples are well known by now: food delivery apps, map apps, sport tracker apps, and so on.</p>
		</div>
	</body></html>