<html><head></head><body>
		<div><h1 id="_idParaDest-188"><em class="italic"><a id="_idTextAnchor366"/>Chapter 11</em>: Adding Media to Your App</h1>
			<p>A lot of the apps that people use every day make use of media in some way. Some apps show photos and videos in a user's feed. Other apps focus on playing audio or video, while there are also apps that allow users to record media and share it with their peers. You can probably name at least a couple of very well-known apps that make use of such media in one way or the other.</p>
			<p>Because media has such a significant presence in people's daily lives, it's good to know how you can integrate media into your own apps. iOS has excellent support for media playback and offers several different ways to create and consume different types of media. Some ways provide less flexibility but are more straightforward to implement. Others are more complex but provide significant power to you as a developer.</p>
			<p>In this chapter, you will learn about several ways to play and record media on iOS. You will learn how to play and record video, play audio, and take pictures, and you'll even learn how to apply filters to images with Apple's Core Image framework. This chapter covers the following topics:</p>
			<ul>
				<li>Playing audio and video</li>
				<li>Recording video and taking pictures </li>
				<li>Manipulating photos with Core Image</li>
			</ul>
			<p>By the end of this chapter, you will have a great foundation that you can build on to create engaging experiences for your users, allowing them to not only view content but also to create their own content in your app.</p>
			<h1 id="_idParaDest-189"><a id="_idTextAnchor367"/>Technical requirements</h1>
			<p>The code bundle for this chapter includes two starter projects called <code>Captured_start</code> and <code>MediaPlayback_start</code>. You can find them in the code bundle repository:</p>
			<p><a href="https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition">https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition</a></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor368"/><a id="_idTextAnchor369"/><a id="_idTextAnchor370"/>Playing audio and video</h1>
			<p>To make playing<a id="_idIndexMarker519"/> audio and video files as simple and straightforward as can <a id="_idIndexMarker520"/>be, Apple has created the <code>AVFoundation</code> framework. This framework contains a lot of helper classes that provide very low-level control over how iOS plays audio and video files. You can use <code>AVFoundation</code> to build a rich, custom media player with as many features as you need for your purpose.</p>
			<p>If you're looking for a simpler way to integrate media into your app, the <code>AVKit</code> framework might be what you need. <code>AVKit</code> contains several helpers that build upon the <code>AVFoundation</code> components to provide an excellent default player that supports many features, such as subtitles, AirPlay, and more.</p>
			<p>In this section, you will learn how to implement a simple video player with <code>AVPlayerViewController</code> from the <code>AVKit</code> framework. You will also implement a more complex audio player with <code>AVFoundation</code> components that play audio in the background and display, on the lock screen, the audio track currently being played.</p>
			<p>To follow along with the examples, you should open the <code>MediaPlayback_start</code> project in this chapter's code bundle. The starter app contains a straightforward interface with a tab bar and two pages. You will implement a video player on one page, and the audio player on the other page. The audio page comes with so<a id="_idTextAnchor371"/>me predefined controls and actions that you will implement later<a id="_idTextAnchor372"/>.</p>
			<h2 id="_idParaDest-191"><a id="_idTextAnchor373"/>Creating a simple video player</h2>
			<p>The first thing you <a id="_idIndexMarker521"/>need to do to implement a video player is to obtain a video file. You can use any video that is encoded in the <code>h.264</code> format. A good sample video is the <strong class="bold">Big Buck Bunny</strong> sample movie that was created by the Blender Foundation. You can find this video at the following UR<a id="_idTextAnchor374"/><a id="_idTextAnchor375"/>L: <a href="http://bbb3d.renderfarming.net/download.html">http://bbb3d.renderfarming.net/download.html</a>. If you want to use this video to practice with, make sure to download the 2D version of the video.</p>
			<p>As stated before, you will implement the video player using <code>AVPlayerViewController</code>. This view controller provides a convenient wrapper around several components from <code>AVFoundation</code>, and also provides default video controls, so you don't have to build your entire video player from scratch, as you will do for the audio player late<a id="_idTextAnchor376"/>r.</p>
			<p><code>AVPlayerViewController</code> is highly configurable, which means that you can choose whether the player supports AirPlay, shows playback controls, whether it should be full screen when a video plays, and more. For a complete list of configurable options, you can refer to Apple's <code>AVPlayerViewController</code> documentation.</p>
			<p>Once you have found your test video, you should add it to the <code>MediaPlayback</code> project and ensure that the video is added to the app target. You can follow these steps:</p>
			<ol>
				<li>Click on your project.</li>
				<li>Click on your target.</li>
				<li>Select <strong class="bold">Build Phases</strong>.</li>
				<li>Expand <strong class="bold">Copy Bundle Resources</strong>.</li>
				<li>Click <strong class="bold">+</strong> and select your file.</li>
			</ol>
			<p>After doing this, open <code>VideoViewController.swift</code> and add the following line to import <code>AVKit</code>:</p>
			<pre>import AVKit</pre>
			<p>You should also add a<a id="_idIndexMarker522"/> property to <code>VideoViewController</code> to hold on to your video player instance. Add the following line to the <code>VideoViewController</code> class to do this:</p>
			<pre>let playerController = AVPlayerViewController()</pre>
			<p>Since <code>AVPlayerViewController</code> is a <code>UIViewController</code> subclass, you should add it to <code>VideoViewController</code> as a child view controller. Doing this will make sure that <code>VideoViewController</code> forwards any view controller life cycle events, such as <code>viewDidLoad()</code>, along with any changes in trait collections and more to the video player. To do this, add the following code to the <code>viewDidLoad()</code> method in <code>VideoViewControl<a id="_idTextAnchor377"/>ler</code>:</p>
			<pre>// 1
addChild(playerController)
playerController.didMove(toParent: self)
// 2
view.addSubview(playerController.view)
let playerView = playerController.view!
playerView.translatesAutoresizingMaskIntoConstraints = false
NSLayoutConstraint
  .activate([
    playerView.widthAnchor.constraint(equalTo: view.widthAnchor, constant: -20),
    playerView.heightAnchor.constraint(equalTo: playerView.widthAnchor, multiplier: 9/16),
    playerView.centerXAnchor.constraint(equalTo: view.centerXAnchor),
    playerView.centerYAnchor.constraint(equalTo: view.centerYAnchor)
])</pre>
			<p>The previous code <a id="_idIndexMarker523"/>snippet adds the video player to the video view controller as a child view controller. When you add a view controller as a child view controller, you must always call <code>didMove(toParent:)</code> on the child controller to make sure that it knows that it has been added as a child view controller to another view controller. After adding the video player as a child view controller, the video player's view is added as a subview for the video view controller, and some constraints are set up to position the player view.</p>
			<p>This is all you need to do to create an instance of the video player and make it appear in your view controller. The last step is to obtain a reference to your video file, create an <code>AVPlayer</code> that has a reference to the video file, and assign it to the player. Add the following code to do this:</p>
			<pre>let url = Bundle.main.url(forResource: "samplevideo", withExtension: "mp4")!
playerController.player = AVPlayer(url: url)</pre>
			<p>The preceding code looks for a video file called <code>samplevideo.mp4</code> and obtains a URL for that file. It then creates an instance of <code>AVPlayer</code> that points to that video file and assigns it to the video player. The <code>AVPlayer</code> object is responsible for playing the video file. The <code>AVPlayerViewController</code> instance uses the <code>AVPlayer</code> instance to play the video and manages the actual playback of the video internally.</p>
			<p>If you run your app after adding the player this way, you will find that the video plays perfectly well, and that you have access to all the controls you might need. This is a great demonstration of how simple it is to add basic media integration to your app. The next step is a little more complex. You will directly use an <code>AVAudioPlayer</code> instance to play an audio file that is controlled through several custom media controls. The player will even play audio <a id="_idIndexMarker524"/>in the background and integrate with the lock screen to show information about the current file. In other words, you will<a id="_idTextAnchor378"/> build a simple audio player that does everything a user would expect it to do.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">When launching in the simulator, <code>AVKit</code> and large movie files can take some time to load up. Try it on a real de<a id="_idTextAnchor379"/>vice.</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor380"/>Creating an audio player</h2>
			<p>Before you can <a id="_idIndexMarker525"/>implement your audio player, you will need to obtain some <code>.mp3</code> files you wish to use in your player. If you don't have any audio files on your computer, you can get some files from The Free Music Archive website, available at <a href="https://freemusicarchive.org/">https://freemusicarchive.org/</a>about, to obtain a couple of free songs that you would like to use for playback. Make sure to add them to the <code>MediaPlayer</code> Xcode project and ensure that they are included in the app t<a id="_idTextAnchor381"/>arget.</p>
			<p>You will build the audio player using the following steps:</p>
			<ol>
				<li value="1">Implement the necessary controls to start and stop the player and navigate to the next and previous songs.</li>
				<li>Implement the time scrubber.</li>
				<li>Read the file's metadata and show it to the user.</li>
			</ol>
			<p>The user interface, outlets, and actions are already set up, so make sure to familiarize yourself with the existing<a id="_idIndexMarker526"/> code before following along with the implementatio<a id="_idTextAnchor382"/>n of the audio <a id="_idTextAnchor383"/>player.</p>
			<h3>Implementing basic audio controls</h3>
			<p>Before you <a id="_idIndexMarker527"/>implement the audio player code, you will need to do a little bit of housekeeping. To be able to play audio, you need a list of the files that the player will play. In addition to this list, you also need to keep track of what song the user is currently playing, so you can determine the next and previous songs. Lastly, you also need to have the audio player itself. Instead of using a pre-built component, you will build your own audio player using an <code>AVAudioPlayer</code> object. <code>AVAudioPlayer</code> is perfect for implementing a simple audio player that plays a couple of local <code>.mp3</code> files. It offers some convenient helper methods to easily adjust the player's volume, seek to a specific timestamp in the song, and more.</p>
			<p>Define the following properties in <code>AudioViewController.swift</code>:</p>
			<pre>let files = ["one", "two", "three"]
var currentTrack = 0
var audioPlayer: AVAudioPlayer!</pre>
			<p>Also, don't forget to add the import:</p>
			<pre>import AVKit</pre>
			<p>Make sure to replace the files array with the filenames that you use for your own audio files. <code>audioPlayer</code> does not have a value yet at this point. You will set up the audio player next.</p>
			<p>Before you can play audio, you need to obtain a reference to a media file and provide this reference to an <code>AVAudioPlayer</code> object. Any time you want to load a new media file, you will have to create a new instance of the audio player, since you can't change the current file once a file is playing. Add the following helper method to <code>AudioViewController</code> to load the current track and create an <code>AVAudioPlayer</code> instance:</p>
			<pre>func loadTrack() {
  let url = Bundle.main.url(forResource: files[currentTrack], withExtension: "mp3")!
  audioPlayer = try! AVAudioPlayer(contentsOf: url)
  audioPlayer.delegate = self
}</pre>
			<p>This method reads the filename for the current track and retrieves the local URL for it. This URL is then used to create and set the <code>audioPlayer</code> property on <code>AudioViewController</code>. The view controller is also assigned as the delegate for the audio player. You won't implement any of the delegate methods just yet, but you can add the following <a id="_idIndexMarker528"/>extension to make <code>AudioViewController</code> conform to the <code>AVAudioPlayerDelegate</code> protocol to ensure your code compiles:</p>
			<pre><strong class="bold">extension</strong> AudioViewController: AVAudioPlayerDelegate {
}</pre>
			<p>Now, let's call <code>loadTrack()</code> on <code>viewDidLoad()</code> to instantiate <code>audioPlayer</code> and load the first song. Add the following method to <code>AudioViewController</code>:</p>
			<pre>override func viewDidLoad() {
  super.viewDidLoad()
  loadTrack()
}</pre>
			<p>You will implement one of the <code>AVAudioPlayerDelegate</code> methods when you add support for navigating to the next and previous tracks.</p>
			<p>Add the following two methods to the audio view controller to add support for playing and pausing the current audio file:</p>
			<pre>func startPlayback() {
  audioPlayer.play()
  playPause.setTitle("Pause", for: .normal)
}
func pausePlayback() {
  audioPlayer.pause()
  playPause.setTitle("Play", for: .normal)
}</pre>
			<p>These methods are relatively straightforward. They call the audio player's <code>play()</code> and <code>pause()</code> methods and update the button's label, so it reflects the current player state. Add the following implementation for <code>playPauseTapped()</code> so that the play and pause methods get called when the user taps the play/paus<a id="_idTextAnchor384"/>e button:</p>
			<pre>@IBAction func playPauseTapped() {
  if audioPlayer.isPlaying {
    pausePlayback()
  } else {
    startPlayback()
  }
}</pre>
			<p>If you run the app <a id="_idIndexMarker529"/>now, you can tap the play/pause button to start and stop the currently playing file. Make sure your device is not in silent mode, because the audio for your app is muted when the device is in silent mode. You will learn how to fix this when you implement the ability to play audio in the background. The next step is to add support for playing the next and previous tracks. Add the following two implementations to <code>AudioViewController</code> to do this:</p>
			<pre>@IBAction func nextTapped() {
  currentTrack += 1
  if currentTrack &gt;= files.count {
    currentTrack = 0
  }
  loadTrack()
  audioPlayer.play()
}
@IBAction func previousTapped() {
  currentTrack -= 1
  if currentTrack &lt; 0 {
    currentTrack = files.count - 1
  }
  loadTrack()
  audioPlayer.play()
}</pre>
			<p>The preceding<a id="_idIndexMarker530"/> code adjusts the current track index, loads the new track, and immediately plays it. Note that every time the user taps on the next or previous button, a fresh audio player has to be created by calling <code>loadTrack()</code>. If you run the app now, you can play audio, pause it, and skip to the next or previous tracks.</p>
			<p>When you allow a full song to play, it will not yet advance to the next song afterward. To implement this, you need to add an implementation for the <code>audioPlayerDidFinishPlaying(_:successfully:)</code> method from <code>AVAudioPlayerDelegate</code>. Add the following implementation to call <code>nextTapped()</code>, so the next song automatically plays when the current song finishes:</p>
			<pre>func audioPlayerDidFinishPlaying(_ player: AVAudioPlayer, successfully flag: Bool) {
  nextTapped()
}</pre>
			<p>Now that the first features are implemented, the next step is to implement the time scrubber that shows the current song's progress and allows the user to adjust the playhead'<a id="_idTextAnchor385"/><a id="_idTextAnchor386"/><a id="_idTextAnchor387"/>s position.</p>
			<h3>Implementing the time scrubber</h3>
			<p>The user interface <a id="_idIndexMarker531"/>for the audio player app already contains a scrubber that is hooked up to the following three actions in the view controller:</p>
			<ul>
				<li><code>sliderDragStart()</code></li>
				<li><code>sliderDragEnd() </code></li>
				<li><code>sliderChanged()</code></li>
			</ul>
			<p>When an audio file is playing, the scrubber should automatically update to reflect the current position in the song. However, when a user starts dragging the scrubber, it should not update its position until the user has chosen the scrubber's new position. When the user is done dragging the scrubber, it should adjust itself based on the song's progress again. Any time the value for the slider changes, the audio player should adjust the playhead, so the song's progress matches that of the scrubber.</p>
			<p>Unfortunately, the <code>AVAudioPlayer</code> object does not expose any delegate methods to observe the progress of the current audio file. To update the scrubber regularly, you can implement a timer that updates the scrubber to the audio player's current position every second. Add the following property to <code>AudioViewController</code>, so you can hold on to the timer after you have created it:</p>
			<pre>var timer: Timer?</pre>
			<p>Also, add the following two methods to <code>AudioViewController</code> as a convenient way to start the timer when the user starts dragging the scrubber, or when a file starts playing, and stop it when a user stops dragging the scrubber or to preserve resources when the playback is paused:</p>
			<pre>func startTimer() {
  timer = Timer.scheduledTimer(withTimeInterval: 1, repeats: true) { [unowned self] timer in
    self.slider.value = Float(self.audioPlayer.currentTime / self.audioPlayer.duration)
  }
}
func stopTimer() {
  timer?.in<a id="_idTextAnchor388"/>validate()
}</pre>
			<p>Add a call to <code>startTimer()</code> in the <code>startPlayback()</code> method and a call to <code>stopTimer()</code> in the <code>pausePlayback()</code> method. If you run the app after doing this, the scrubber will immediately begin updating its position when a song starts playing. However, scrubbing does not work yet. Add the following implementations for the scrubber <a id="_idIndexMarker532"/>actions to enable manual scrubbing:</p>
			<pre>@IBAction func sliderDragStart() {
  stopTimer()
}
@IBAction func sliderDragEnd() {
  startTimer()
}
@IBAction func sliderChanged() {
  audioPlayer.currentTime = Double(slider.value) * audioPlayer.duration
}</pre>
			<p>The preceding methods are relatively simple, but they provide a very powerful feature that immediately makes your homemade audio player feel like an audio player you might use every day. The final step for implementing the <a id="_idTextAnchor389"/>audio player's functionality is to display metadata about the <a id="_idTextAnchor390"/>current song.</p>
			<h3>Displaying song metadata</h3>
			<p>Most <code>.mp3</code> files contain<a id="_idIndexMarker533"/> metadata in the form of ID3 tags. These metadata tags are used by applications such as iTunes to extract information about a song and display it to the user, as well as to categorize a music library or filter it. You can gain access to an audio file's metadata through code by loading the audio file into an <code>AVPlayerItem</code> object and extracting the metadata for its internal <code>AVAsset</code> instance. An <code>AVAsset</code> object contains information about a media item, such as its type, location, and more. When you load a file using an <code>AVPlayerItem</code> object, it will automatically create a corresponding <code>AVAsset</code> o<a id="_idTextAnchor391"/>bject for you.</p>
			<p>A single asset can contain loads of metadata in the metadata dictionary. Luckily, Apple has captured all of the valid ID3 metadata tags in the <code>AVMetadataIdentifier</code> object, so once you have extracted the metadata for an <code>AVAsset</code> instance, you can loop over all of its metadata to filter out the data you need. The following method does this, and sets the extracted values on the <code>titleLabel</code> variable of <code>AudioViewController</code>, as shown here:</p>
			<pre>func showMetadataForURL(_ url: URL) {
  let mediaItem = AVPlayerItem(url: url)
  let metadata = mediaItem.asset.metadata
  var information = [String]()
  for item in metadata {
    guard let identifier = item.identifier else { continue }
    switch identifier {
      case .id3MetadataTitleDescription, .id3MetadataBand:
        information.append(item.value?.description ?? "")
      default:
        break
    }
  }
  let trackTitle = information.joined(separator: " - ")
  titleLabel.text = trackTitle
}</pre>
			<p>Make sure to add a call to this method from <code>loadTrack()</code>, and pass the audio file's URL that you obtain in <code>loadTrack()</code> to <code>showMetadataForURL(_:)</code>. If you run your app now, your basic functionality should be all there. The metadata should be shown correctly, the scrubber should work, and you should be able to skip songs or pause the playback.</p>
			<p>Even though your media player seems to be pretty much done at this point, did you notice that the music pauses when you send the app to the background? To make your app feel more like a real audio player, you should implement background audio playback and make<a id="_idIndexMarker534"/> sure that the currently playing song is presented on the user's lock screen, similar to how the native music app for iOS works. This is precisely the functionality you<a id="_idTextAnchor392"/><a id="_idTextAnchor393"/><a id="_idTextAnchor394"/> will add next.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor395"/>Playing media in the background</h2>
			<p>On iOS, playing <a id="_idIndexMarker535"/>audio in the background requires special permissions that you can enable in your app's <strong class="bold">Capabilities</strong> tab. If you enable the <strong class="bold">Background Modes</strong> capability, you can select the <strong class="bold">Audio, AirPlay, and Picture in Picture</strong> option to make your app eligible for playing audio in the background. The following screenshot shows the enabled capability for playing audio in the background:</p>
			<div><div><img src="img/Figure_11.1_B14717.jpg" alt="Figure 11.1 − Background Modes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.1 − Background Modes</p>
			<p>If you want to add proper support for background audio playback, there are three features you need to implement:</p>
			<ul>
				<li>Set up an audio session, so audio continues playing in the background. </li>
				<li>Submit metadata to the "now playing" info center.</li>
				<li>Respond to playback actions from remote sources, such as the lock screen.</li>
			</ul>
			<p>You can set up the<a id="_idIndexMarker536"/> audio session for your app with just two lines of code. When you create an audio session, iOS will treat the audio played by your app slightly differently; for instance, your songs will play even if the device is set to silent. It also makes sure that your audio is played when your app is in the background, if you have the proper capabilities set up. Add the following code to <code>viewDidLoad()</code> to set up an audio session for the app:</p>
			<pre>try? AVAudioSession.sharedInstance().setCategory(.playback, mode: .default, options: [.allowAirPlay])
try? AVAudioSession.sharedInstance().setActive(t<a id="_idTextAnchor396"/>rue, options: [])</pre>
			<p>The second feature to add is to supply information about the currently playing track. All information about the currently playing media file should be passed to the <code>MPNowPlayingInfoCenter</code> object. This object is part of the <code>MediaPlayer</code> framework and is responsible for showing the user information about the currently playing media file on the lock screen and in the command center. Before you pass information to the "now playing" info center, make sure to import the <code>MediaPlayer</code> framework at the top of the <code>AudioViewController.swift</code> file:</p>
			<pre>import MediaPlayer</pre>
			<p>Next, add the following line of code to <code>viewDidLoad()</code>:</p>
			<pre>NotificationCenter.default.addObserver(self, selector: #selector(updateNowPlaying), name: UIApplication.didEnterBackgroundNotification, object: nil)</pre>
			<p>In the documentation for <code>MPNowPlayingInfoCenter</code>, Apple states that you should always pass the most recent "now playing" information to the info center when the app goes to the background. To do this, the audio view controller should listen to the <code>UIApplication.didEnterBackgroundNotification</code> notification, so it can respond to the app<a id="_idIndexMarker537"/> going to the background. Add the following implementation for the <code>updateNowPlaying()</code> method to <code>AudioVideoController</code>:</p>
			<pre>@objc func updateNowPlaying() {
  var nowPlayingInfo = [String: Any]()
  nowPlayingInfo[MPMediaItemPropertyTitle] = titleLabel.text ?? "untitled"
  nowPlayingInfo[MPNowPlayingInfoPropertyElapsedPlaybackTime] = audioPlayer.currentTime
  nowPlayingInfo[MPMediaItemPropertyPlaybackDuration] = audioPlayer.duration
  MPNowPlayingInfoCenter.default().nowPlayingInfo = nowPlayingInfo
}</pre>
			<p>The preceding code configures a dictionary with metadata about the currently playing file and passes it to the "now playing" info center. This method is called automatically when the app goes to the background, but you should also update the "now playing" information when a new song begins playing. Add a call to <code>updateNowPlaying()</code> in the <code>loadTrack()</code> method to make sure the "now playing" information is updated whenever a new<a id="_idTextAnchor397"/> track is loaded.</p>
			<p>The next and final step is to respond to remote commands. When the user taps the play/pause button, next button, or previous button on the lock screen, this is sent to your app as a remote command. You should explicitly define the handlers that should be called by iOS when a remote command occurs. Add the following method to <code>AudioViewController</code> to<a id="_idIndexMarker538"/> add support for remote commands:</p>
			<pre>func configureRemoteCommands() {
  let commandCenter = MPRemoteCommandCenter.shared()
  commandCenter.playCommand.addTarget { [unowned self] event in
    guard self.audioPlayer.isPlaying == false else { return .commandFailed }
    self.startPlayback()
    return .success
  }
  commandCenter.pauseCommand.addTarget { [unowned self] event in
    guard self.audioPlayer.isPlaying else { return .commandFailed }
    self.pausePlayback()
    return .success
  }
  commandCenter.nextTrackCommand.addTarget { [unowned self] event in
    self.nextTapped()
    return .success
  }
  commandCenter.previousTrackCommand.addTarget { [unowned self] event in
    self.previousTapped()
    return .success
  }
  UIApplication.shared.beginReceivingRemoteControlEvents()
}</pre>
			<p>The preceding code obtains a reference to the remote command center and registers several handlers. It also calls <code>beginReceivingRemoteControlEvents()</code> on the application object <a id="_idIndexMarker539"/>to make sure it receives remote commands. Add a call to <code>configureRemoteCommands()</code> in <code>viewDidLoad()</code> to make sure that the app begins receiving remote commands as soon as the audio player is configured. As an exercise to practice, try implementing the commands to control the time scrubber and <code>+15</code> and <code>-15</code> from the lo<a id="_idTextAnchor398"/>ck screen yourself.</p>
			<p>Try to run your app and send it to the background. You should be able to control media playback from both the control center and the lock screen. The visible metadata should correctly update when you skip to the next or previous song, and the scrubber should accurately represent the current position of playback in the song. </p>
			<p>At this point, you have implemented a reasonably complete audio player that has pretty sophisticated behaviors. The next step in your exploration o<a id="_idTextAnchor399"/>f media on iOS is to discover how you can take picture<a id="_idTextAnchor400"/>s and record video.</p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor401"/>Recording video and taking pictures</h1>
			<p>In addition to <a id="_idIndexMarker540"/>playing existing media, you can also make apps that allow users to create<a id="_idIndexMarker541"/> their own content. In this section, you will learn how you can use a built-in component to enable users to take a picture. You will also learn how you can use a raw video feed to record a video. If you want to follow along with the samples in this section, make sure to grab the starter project for <code>Captured</code> from this chapter's code bundle.</p>
			<p>The starter project contains a couple of view controllers and some connected outlets and actions. Note that there is a <code>UIViewController</code> extension in the project, too.</p>
			<p>This extension includes a helper method that makes displaying an alert to the user a little bit simpler. This extension will be used to show an alert that informs the user when their photo or video is stored in the camera roll.</p>
			<p>Since a user's camera and photo library are considered very privacy-sensitive, you need to make sure that you add the following privacy-related keys to the app's <code>Info.plist</code>:</p>
			<ul>
				<li><strong class="bold">Privacy - Camera Usage Description</strong>: This property is required in order to access the camera so that you can take pictures and record video.</li>
				<li><strong class="bold">Privacy - Microphone Usage Description</strong>: You must add this property so that your videos record audio, as well as images.</li>
				<li><strong class="bold">Privacy - Photo Library Additions Usage Description</strong>: This property allows you to write photos to the user's photo library.</li>
			</ul>
			<p>Make sure to provide a good description for the privacy keys, so the user knows why you need access to their camera, microphone, and photo library. The better your description is, the more likely the user is to allow your app to access the associated privacy-sensitive information. After adding the keys, you are ready to see how you can take a picture using the built-in <code>UIImagePickerControlle<a id="_idTextAnchor402"/><a id="_idTextAnchor403"/><a id="_idTextAnchor404"/>r</code> component of UIKit.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor405"/>Taking and storing a picture</h2>
			<p>When you need a<a id="_idIndexMarker542"/> user to supply an image, they can do this by either selecting an image <a id="_idIndexMarker543"/>from their photo library or by taking a picture with the camera. <code>UIImagePickerController</code> supports both ways of picking an image. In this section, you will learn how you can allow users to take an image using the camera. Changing the example to allow users to select an image from their photo library should be straightforward, as long as you remember to add the <code>Info.plist</code>.</p>
			<p>Add the following implementation for <code>viewDidLoad()</code> to the <code>ImageViewController</code> class:</p>
			<pre>override func viewDidLoad() {
  super.viewDidLoad()
  let imagePicker = UIImagePickerController()
  imagePicker.sourceType = .camera
  imagePicker.delegate = self
  present(imagePicker, animated: true, completion: nil)
}</pre>
			<p>The previous<a id="_idIndexMarker544"/> implementation<a id="_idIndexMarker545"/> creates an instance of the <code>UIImagePickerController</code> object and configures it so that it uses the camera as the image source and presents it to the user. Note that the view controller is set as a delegate for the image picker.</p>
			<p>When the user has taken a picture, the image picker will notify its delegate about this so that it can extract the image and use it. In this case, the image should be given the <code>selectedImage</code> label in the view controller so that it can be shown in the image view, and saved when the user taps on the save button, and the <code>saveImage()</code> method is called as a result.</p>
			<p>Add the following extension to make <code>ImageViewController</code> conform to <code>UIImagePickerControllerDelegate</code>:</p>
			<pre>extension ImageViewController: UIImagePickerControllerDelegate, UINavigationControllerDelegate {
  func imagePickerController(_ picker: UIImagePickerController, didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey : Any]) {
    picker.dismiss(animated: true, completion: nil)
    guard let image = info[.originalImage] as? UIImage else { return }
    selectedImage = image
  }
}</pre>
			<p>Note that this extension also makes the image view controller conform to <code>UINavigationControllerDelegate</code>. The delegate property on the image picker controller requires all delegates to conform to both <code>UINavigationControllerDelegate</code> and <code>UIImagePickerControllerDelegate</code>.</p>
			<p>When the user has taken a picture with the camera, <code>imagePickerController(_: didFinishPickingMediaWithInfo)</code> is called to notify the delegate about the photo that the user took. The first thing that the preceding code does is dismiss the picker, as it's<a id="_idIndexMarker546"/> no longer needed. The picture that the user just took is stored in<a id="_idIndexMarker547"/> the <code>info</code> dictionary as the original image. When the image is extracted from the dictionary, it is set as <code>selectedImage</code>.</p>
			<p>To store the image, add the following implementation of <code>saveImage()</code>:</p>
			<pre>@IBAction func saveImage() {
  guard let image = selectedImage else { return }
  UIImageWriteToSavedPhotosAlbum(image, self, #selector(didSaveImage(_:withError:contextInfo:)), nil)
}
@objc func didSaveImage(_ image: UIImage, withError error: Error?, contextInfo: UnsafeRawPointer) {
  guard error == nil else { return }
  presentAlertWithTitle("Success", message: "Image was saved succesfully")
}</pre>
			<p>The preceding code calls <code>UIImageWriteToSavedPhotosAlbum(_:_:_:_)</code> to store the image in the user's photo library. When the save operation completes, the <code>didSaveImage(_:withError:contextInfo:)</code> method will be called. If this method does not receive any errors, then the photo was successfully stored in the photo library and an alert is shown.</p>
			<p>Allowing the user to take a picture by implementing <code>UIImagePickerController</code> is relatively straightforward, and it is a great way to implement a camera feature in your app without too<a id="_idIndexMarker548"/> much effort. Sometimes, you need more advanced access to the <a id="_idIndexMarker549"/>camera. In these cases, you can use <code>AVFoundation</code> to gain access to the raw video feed from the <a id="_idTextAnchor406"/><a id="_idTextAnchor407"/><a id="_idTextAnchor408"/>camera, as you will see next.</p>
			<h2 id="_idParaDest-196"><a id="_idTextAnchor409"/>Recording and storing video</h2>
			<p>In the previous <a id="_idIndexMarker550"/>section, you used <code>AVFoundation</code> to build a simple audio player <a id="_idIndexMarker551"/>app. You will now use <code>AVFoundation</code> again, except instead of playing video or audio, you will now record video and store it in the user's photo library. When using <code>AVFoundation</code> to record a video feed, you do so with an <code>AVCaptureSession</code> object. A capture session is responsible for taking the input from one or more <code>AVCaptureDeviceInput</code> objects and writing it to an <code>AVCaptureOutput</code> subclass.</p>
			<p>The following diagram shows the objects that are involved with recording media through <code>AVCaptureSession</code>:</p>
			<div><div><img src="img/Figure_11.2_B14717.jpg" alt="Figure 11.2 − AVCaptureSession entities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.2 − AVCaptureSession entities</p>
			<p>To get started on implementing the video recorder, make sure to import <code>AVFoundation</code> in <code>RecordVideoViewController.swift</code>. Also, add the following properties to the <code>RecordVideoViewController</code> class:</p>
			<pre>let videoCaptureSession = AVCaptureSession()
let videoOutput = AVCaptureMovieFileOutput()
var previewLayer:  AVCaptureVideoPreviewLayer?</pre>
			<p>Most of the preceding properties should look familiar because they were also shown in the screenshot that outlined the components that are involved with <code>AVCaptureSession</code>. Note that <code>AVCaptureMovieFileOutput</code> is a subclass of <code>AVCaptureOutput</code>, specialized in capturing video. The preview layer will be used to render the video feed at runtime and present it to the user so that they can see what they <a id="_idTextAnchor410"/>are capturing with the camera.</p>
			<p>The next step is<a id="_idIndexMarker552"/> to set up the <code>AVCaptureDevice</code> objects for the camera and<a id="_idIndexMarker553"/> microphone and associate them with <code>AVCaptureSession</code>. Add the following code to the <code>viewDidLoad()</code> method:</p>
			<pre>override func viewDidLoad() {
  super.viewDidLoad()
  // 1
  guard let camera = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .back),
  let microphone = AVCaptureDevice.default(.builtInMicrophone, for: .audio, position: .unspecified) else { return }
  // 2
  do {
    let cameraInput = try AVCaptureDeviceInput(device: camera)
    let microphoneInput = try AVCaptureDeviceInput(device: microphone)
    videoCaptureSession.addInput(cameraInput)
    videoCaptureSession.addInput(microphoneInput)
    videoCaptureSession.addOutput(videoOutput)
  } catch {
    print(error.localizedDescription)
  }
}</pre>
			<p>The preceding code first obtains a reference to the camera and microphone that will be used to record the video and audio. The second step is to create the <code>AVCaptureDeviceInput</code> objects that are associated with the camera and microphone and associate them with the capture session. The video output is also added to the video<a id="_idIndexMarker554"/> capture session. If you examine the screenshot that you <a id="_idIndexMarker555"/>saw earlier and compare it with the preceding code snippet, you will find that all four components are present in this implementation.</p>
			<p>The next step is to provide the user with a view that shows the current camera feed so that they can see what they are recording. Add the following code to <code>viewDidLoad()</code> after the capture session setup code:</p>
			<pre>previewLayer = AVCaptureVideoPreviewLayer(session: videoCaptureSession)
previewLayer?.videoGravity = .resizeAspectFill
videoView.layer.addSublayer(previewLayer!)
<a id="_idTextAnchor411"/>videoCaptureSession.startRunning()</pre>
			<p>The preceding code sets up the preview layer and associates it with the video capture session. The preview layer will directly use the capture session to render the camera feed. The capture session is then started. This does not mean that the recording session starts; rather, only that the capture session will begin processing the data from its camera and microphone inputs.</p>
			<p>The preview layer is added to the view at this point, but it doesn't cover the video view yet. Add the following implementation for <code>viewDidLayoutSubviews()</code> to <code>RecordVideoViewController</code> to set the preview layer's size and position, so it matches the size and position of <code>videoView</code>:</p>
			<pre>override func viewDidLayoutSubviews() {
  super.viewDidLayoutSubviews()
  previewLayer?.bounds.size = videoView.frame.size
  previewLayer?.position = CGPoint(x: videoView.frame.midX, y:videoView.frame.size.height / 2)
}</pre>
			<p>Running the app now will already show you the camera feed. However, tapping the record button doesn't work yet, because you haven't yet implemented the <code>startStopRecording()</code> method. Add <a id="_idIndexMarker556"/>the following implementation for this<a id="_idIndexMarker557"/> method:</p>
			<pre>@IBAction func startStopRecording() {
  // 1
  if videoOutput.isRecording {
    videoOutput.stopRecording()
  } else {
    // 2
    guard let path = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask).first else { return }
    let fileUrl = path.appendingPathComponent("recording.mov")
    // 3
    try? FileManager.default.removeItem(at: fileUrl)
    // 4
    videoOutput.startRecording(to: fil<a id="_idTextAnchor412"/>eUrl, recordingDelegate: self)
  }
}</pre>
			<p>Let's go over the preceding snippet step by step to see what exactly is going on:</p>
			<ol>
				<li value="1">First, the <code>isRecording</code> property for the video output is checked. If a recording is currently active, the recording should be stopped.</li>
				<li>If no recording is currently active, a new path is created to store the video temporarily.</li>
				<li>Since the video <a id="_idIndexMarker558"/>output cannot overwrite an existing file, the <code>FileManager</code> object<a id="_idIndexMarker559"/> should attempt to remove any existing files at the temporary video file path.</li>
				<li>The video output will start recording to the temporary file. The view controller itself is passed as a delegate to be notified when the recording has begun and is stopped.</li>
			</ol>
			<p>Since <code>RecordVideoViewController</code> does not conform to <code>AVCaptureFileOutputRecordingDelegate</code> yet, you should add the following extension to add conformance to <code>AVCaptureFileOutputRecordingDelegate</code>:</p>
			<pre>extension RecordVideoViewController: AVCaptureFileOutputRecordingDelegate {
  // 1
  func fileOutput(_ output: AVCaptureFileOutput, didStartRecordingTo fileURL: URL, from connections: [AVCaptureConnection]) {
    startStopButton.setTitle("Stop Recording", for: .normal)
  }
  // 2
  func fileOutput(_ output: AVCaptureFileOutput, didFinishRecordingTo outputFileURL: URL, from connections: [AVCaptureConnection], error: Error?) {
    guard error == nil else { return }
    UISaveVideoAtPathToSavedPhotosAlbum(outputFileURL.path, self, #selector(didSaveVideo(at:withError:contextInfo:)), nil)
  }
  // 3
  @objc func didSaveVideo(at path: String, withError error: Error?, contextInfo: UnsafeRawPointer?) {
    guard error == nil else { return }
    presentAlertWithTitle("Success", message: "Video was saved succesfully")
    startStopButton.setTitle("Start Recording", for: .normal)
  }
}</pre>
			<p>The preceding extension <a id="_idIndexMarker560"/>contains three methods. The first is a delegate method, called <a id="_idIndexMarker561"/>when the video output has begun recording. When the recording has started, the title of the <code>startStopButton</code> button is updated to reflect the current state. The second method is also a delegate method. This method is called when the recording has completed. If no errors occur, the video is stored at the temporary location you set up earlier. <code>UISaveVideoAtPathToSavedPhotosAlbum(_:_:_:_:)</code> is then called, to move the video from the temporary location to the user's photo library. This method is very similar to the <code>UIImageWriteToSavedPhotosAlbum(_:_:_:_:)</code> method that you used to store a picture. The third and final method in the extension is called when the video is stored in the user's photo library. When the video has been successfully stored, an alert is shown, and the title of the <code>startStopButton</code> button is updated again.</p>
			<p>You can now run the app and record some videos! Even though you have done a lot of manual work by implementing the video recording logic directly with <code>AVCaptureSession</code>, most of the hard work is done inside of the <code>AVFoundation</code> framework. One final<a id="_idIndexMarker562"/> media-related feature to explore is applying visual filters<a id="_idIndexMarker563"/> to images using <strong class="bold">Core Image</strong>. Applying filters to images is a very popular functionality in lots of apps and <a id="_idTextAnchor413"/><a id="_idTextAnchor414"/>it can make your photo app more appealing.</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor415"/>Manipulating photos with Core Image</h1>
			<p>In this chapter, you<a id="_idIndexMarker564"/> have already seen that iOS has<a id="_idIndexMarker565"/> powerful capabilities for recording and playing media. In this section, you will learn how you can manipulate images with Core Image. The Core Image framework provides many different filters that you can use to process both images and videos. You will expand on the photo-taking capabilities that you implemented in the <code>Captured</code> app so that users can grayscale and crop images.</p>
			<p>Every Core Image filter you apply to images is an instance of the <code>CIFilter</code> class. You can create instances of filters as follows:</p>
			<pre>let filter = CIFilter(name: "CIPhotoEffectNoir")</pre>
			<p>The <code>name</code> parameter in the filter's initializer is expected to be a string that refers to a specific filter. You can refer to Apple's documentation on Core Image and the Core Image Filter Reference guide to see an overview of al<a id="_idTextAnchor416"/>l the filters that you can use in your apps.</p>
			<p>Every filter has a certain set of parameters that you need to set on the <code>CIFilter</code> instance to use the filter; for instance, a grayscale filter requires you to provide an input image. Other filters might take an intensity, location, or other properties. The best way to see how you can apply a filter to an image is through an example. Add the following implementation for <code>applyGrayScale()</code> to <code>ImageViewController.swift</code> to implement<a id="_idIndexMarker566"/> a<a id="_idIndexMarker567"/> grayscale filter:</p>
			<pre>@IBAction func applyGrayScale() {
  // 1
  guard let cgImage = selectedImage?.cgImage,
  // 2
  let initialOrientation = selectedImage?.imageOrientation,
  // 3
  let filter = CIFilter(name: "CIPhotoEffectNoir") else { return }
  // 4
  let sourceImage = CIImage(cgImage: cgImage)
  filter.setValue(sourceImage, forKey: kCIInputImageKey)
  // 5
  let context = CIContext(options: nil)
  guard let outputImage = filter.outputImage, let cgImageOut = context.createCGImage(outputImage, from: outputImage.extent) else { return }
  // 6
  selectedImage = UIImage(cgImage: cgImageOut, scale: 1, orientation: initialOrientation)
}</pre>
			<p>The preceding code has a lot of small, interesting details, highlighted with numbered comments. Let's go over the comments one by one to see how the grayscale filter is applied:</p>
			<ol>
				<li value="1">The <code>UIImage</code> instance that is stored in <code>selectedImage</code> is converted into a <code>CGImage</code> instance. Strictly speaking, this conversion isn't required, but it does make applying other filters to the <code>UIImage</code> instance later a bit easier.</li>
				<li>One downside of using <code>CGImage</code>, instead of <code>UIImage</code>, is that the orientation information that is stored in the image is lost. To make sure the final image maintains its orientation, the initial orientation is stored.</li>
				<li>This ste<a id="_idTextAnchor417"/>p creates an instance of the grayscale filter.</li>
				<li>Since Core Image does not directly support <code>CGImage</code> instances, the <code>CGImage</code> instance is converted into a <code>CIImage</code> instance that can be used with Core Image. The <code>CIImage</code> instance is then assigned as the input image for the grayscale<a id="_idIndexMarker568"/> filter, by<a id="_idIndexMarker569"/> calling <code>setValue(_:forKey:)</code> on the filter.</li>
				<li>The fifth step extracts the new image from the filter and uses a <code>CIContext</code> object to export the <code>CIImage</code> output to a <code>CGImage</code> instance.</li>
				<li>The sixth and final step is to create a new <code>UIImage</code> instance, based on the <code>CGImage</code> output. The initial orientation is passed to the new <code>UIImage</code> instance to make sure it has the same orientation as the original image.</li>
			</ol>
			<p>Even though there are a lot of steps involved, and you need to convert between different image types quite a bit, applying the filter is relatively simple. Most of the preceding code takes care of switching between image types, while the filter itself is set up in just a couple of lines. Try running the app now and taking a picture. The initial picture will be in full color. After you apply the grayscale filter, the image is automatically replaced with a grayscale version of the image, as shown in the <a id="_idTextAnchor418"/>following screenshot:</p>
			<div><div><img src="img/Figure_11.3_B14717.jpg" alt="Figure 11.3 − Grayscale&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.3 − Grayscale</p>
			<p>The next filter you will implement is a crop filter. The crop filter will crop the image so that it's a square, rather than a portrait or landscape picture. The process for implementing the <a id="_idIndexMarker570"/>crop filter is mostly the same as for the grayscale<a id="_idIndexMarker571"/> filter, except for the values that need to be passed to the crop filter. Add the following implementation for <code>cropSquare()</code> to implement the crop filter:</p>
			<pre>@IBAction func cropSquare() {
  let context = CIContext(options: nil)
  guard let cgImage = selectedImage?.cgImage, let initialOrientation = selectedImage?.imageOrientation, let filter = CIFilter(name: "CICrop") else { return }
  let size = CGFloat(min(cgImage.width, cgImage.height))
  let center = CGPoint(x: cgImage.width / 2, y: cgImage.height / 2)
  let origin = CGPoint(x: center.x - size / 2, y: center.y - size / 2)
  let cropRect = CGRect(origin: origin, size: CGSize(width: size, height: size))
  let sourceImage = CIImage(cgImage: cgImage)
  filter.setValue(sourceImage, forKey: kCIInputImageKey)
  filter.setValue(CIVector(cgRect: cropRect), forKey: "inputRectangle")
  guard let outputImage = filter.outputImage, let cgImageOut = context.createCGImage(outputImage, from: outputImage.extent) else { return }
  selectedImage = UIImage(cgImage: cgImageOut, scale: 1, orientation: initialOrientation)
  }
}</pre>
			<p>The preceding <a id="_idIndexMarker572"/>code performs several calculations to figure<a id="_idIndexMarker573"/> out the best way to crop the image into a square. The <code>CGRect</code> instance specifies the crop coordinates and size, which are then used to create a <code>CIVector</code> object. This object is then passed to the filter as the value for the <code>inputRectangle</code> key. Apart from specifying the crop values, the process of applying the filter is <a id="_idTextAnchor419"/>identical, so the code should look familiar to you.</p>
			<p>If you run the app now and tap the crop button, the image will be cropped, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_11.4_B14717.jpg" alt="Figure 11.4 − Cropping the image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 11.4 − Cropping the image</p>
			<p>There are many more filters available in Core Image, which you can play around with to build pretty advanced filters. You can even apply multiple filters to a single image to create elaborate<a id="_idIndexMarker574"/> effects for the pictures in your apps. Because <a id="_idIndexMarker575"/>all filters work in very similar ways, it's relatively easy to apply any filter to your images once you understand how the general process of applying a filter works. You can always use the code from the preceding examples if you ne<a id="_idTextAnchor420"/>ed a reminder about how to apply Core Image filters.</p>
			<h1 id="_idParaDest-198"><a id="_idTextAnchor421"/>Summary</h1>
			<p>In this chapter, you have learned a lot about media in iOS. You saw how you can implement a video player with just a couple of lines of code. After that, you learned how to use <code>AVFoundation</code> directly to build an audio player that supports features such as stopping and resuming playback, skipping songs, and scrubbing forward or backward in a song. You even learned how you can keep playing audio when the app goes to the background or when the phone is set to silent mode. To apply the finishing touches to the audio player, you learned how you can use the <code>MediaPlayer</code> framework to show the currently playing file on the user's lock screen, and how to respond to control events that are sent to the app remotely.</p>
			<p>After implementing media playback, you learned how you can build apps that help users to create media. You saw that <code>UIImagePickerController</code> provides a quick and simple interface to allow users to take a picture with the camera. You also learned how you can use <code>AVFoundation</code> and an <code>AVCaptureSession</code> object to implement a custom video recording experience. To wrap it all up, you learned about the Core Image framework, and how you can use it to apply filters to images.</p>
			<p>In the next chapter, you will learn everything you need to know about location services and how to use Core Location in your apps. Depending on the use case of your app, handling the user location properly can be a critical task for your app to be successful. Examples are well known by now: food delivery apps, map apps, sport tracker apps, and so on.</p>
		</div>
	</body></html>