<html><head></head><body><div class="chapter" title="Chapter&#xA0;9.&#xA0;Postscreen Processing and Image Effects"><div class="titlepage"><div><div><h1 class="title"><a id="ch09"/>Chapter 9. Postscreen Processing and Image Effects</h1></div></div></div><p>In this chapter, we will cover the following recipes:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Detecting scene edges with the Sobel operator</li><li class="listitem" style="list-style-type: disc">Making the scene blur with the Gaussian blur equation</li><li class="listitem" style="list-style-type: disc">Making a scene glow real time with the bloom effect</li><li class="listitem" style="list-style-type: disc">Painting the scene like a cartoon shading</li><li class="listitem" style="list-style-type: disc">Generating an embossed scene</li><li class="listitem" style="list-style-type: disc">Implementing grayscale and CMYK conversions</li><li class="listitem" style="list-style-type: disc">Implementing fisheye with barrel distortion</li><li class="listitem" style="list-style-type: disc">Implementing the binocular view with procedural texturing</li><li class="listitem" style="list-style-type: disc">Twirling the image</li><li class="listitem" style="list-style-type: disc">Sphere illusion with textured quadrilateral</li></ul></div><div class="section" title="Introduction"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec72"/>Introduction</h1></div></div></div><p>This chapter will unfold the endless possibilities of a scene and its image-based effects, which are widely used in the field of data visualization and after effects. Practically, objects are represented as a set of vertices in the 3D space. As the number of vertices go higher, the time complexity of the scene increases. Moreover, representing the object in terms of an image has a time complexity proportional to the number of fragments in the scene. Additionally, many effects can only be efficiently possible in the image space rather than implementing in the vertex space, such as blurring, blooming, cloud rendering, and so on.</p><p>The term<a id="id663" class="indexterm"/> post screen processing is a texel manipulation technique applied on an OpenGL ES scene once it's rendered. To be more specific, the scene is first rendered to an offscreen surface where effects are applied. Then, this manipulated offscreen texture is rendered back to the screen surface.</p><p>In post processing, the outcome of a given texel is affected by its surrounding texels. Such techniques cannot be applied on live scenes because the vertex and fragment shader works locally. This means a vertex shader is only aware of the current vertex and the fragment shader about the current fragment; they cannot use elements information of their neighbors. This limitation can be fixed easily by rendering the scene into a texture, which allows the fragment shader to read any texel information present in the texture. After the scene is rendered to a texture, the image/texture-based techniques are applied to the texture.</p><p>The image-based effects<a id="id664" class="indexterm"/> are applied to an image texture using the fragment shader. During the post-processing implementation, the rendered scene goes through a number of passes, depending on the complexity of the effect. At each pass, it saves the processed output in a texture and then passes it on to next pass as an input.</p><p>The post screen processing execution model for post processing can be majorly divided into four sections:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Creation of the framebuffer</strong></span>: The first<a id="id665" class="indexterm"/> stage requires creation of an offline texture to render the scene into it. This is achieved by creating the <span class="strong"><strong>Frame Buffer Objects</strong></span> (FBO). Depending on the requirements of the scene, various textures or buffers, such as color, stencil, and depth are attached to the FBO.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Render the scene to texture</strong></span>: By<a id="id666" class="indexterm"/> default, the OpenGL ES scene renders to a default framebuffer. As a prerequisite of post processing, this rendering must be diverted to an offline texture (the FBO texture) by binding the FBO handle to the current rendering pipeline. This ensures that rendering must happen on the FBO texture rather than the default framebuffer.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Apply texture effects</strong></span>: After the scene is rendered into the texture, it's like an image<a id="id667" class="indexterm"/> in the memory where various image effects can be applied. Depending on the post processing complexities, you may require multiple passes to process the desired effect. In the multipass post processing, we may require two or more FBO's in order to hold the intermediate processed result of the current pass in it and to be used in the next or later passes.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Render to the default framebuffer</strong></span>: Finally, the <a id="id668" class="indexterm"/>post processed textured scene is rendered back to the default framebuffer, which becomes visible on the scene. The following figure shows an edge detection example, in which various stages of the post screen processing are illustrated:<div class="mediaobject"><img src="graphics/5527OT_09_01.jpg" alt="Introduction"/></div></li></ul></div></div></div>
<div class="section" title="Detecting scene edges with the Sobel operator"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec73"/>Detecting scene edges with the Sobel operator</h1></div></div></div><p>Edge <a id="id669" class="indexterm"/>detection is an image-processing <a id="id670" class="indexterm"/>technique used to detect boundaries in an image. It is widely used in the field of computer vision, data visualization, and surface topology. For example, the pencil sketch effect of an image is nothing, but an application of edge detection algorithm. This recipe will demonstrate the edge detection technique using the Sobel operator or filter.</p><p>A Sobel filter measures the change in the gradient of an image in which it recognizes the regions of an image where the frequency of the color transition is higher. These higher transition regions shows sharp changes in the gradient that eventually correspond to the edges. The Sobel operator uses convolution kernels to detect the edge portions in the image. A convolution kernel is a matrix that contains predefined weights that formulate the calculation of the current pixel based on the neighboring pixels intensity and weights contained in the convolution matrix itself.</p><p>A Sobel filter uses two 3 x 3 convolution kernels for edge detection processing; one operates on the neighboring pixels in the horizontal direction to the current pixel. Similarly, the other operates on the vertical neighboring pixels. The following image shows two convolution kernels:</p><div class="mediaobject"><img src="graphics/5527OT_09_02.jpg" alt="Detecting scene edges with the Sobel operator"/></div><p>Now, we know very well that the Sobel filter approximates the gradient of an image. Therefore, the RGB information of the image must be brought to some gradient form and the best way is to calculate the brightness or luminance of the image. An RGB color represents a <a id="id671" class="indexterm"/>3D space of color in the R, G, and<a id="id672" class="indexterm"/> B direction. These colors must bring in 1D gradient space using the brightness information of the image. The brightness of an image is represented by gradient colors between white and black:</p><div class="mediaobject"><img src="graphics/5527OT_09_03.jpg" alt="Detecting scene edges with the Sobel operator"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec235"/>Getting ready</h2></div></div></div><p>Post-processing techniques heavily rely on texturing basics and FBO. Therefore, as a prerequisite for this chapter, you must understand these concepts. We have covered these concepts very well in <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span>. For more information, refer to the <span class="emphasis"><em>See also</em></span> subsection in this recipe.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note51"/>Note</h3><p>The texture filtering technique must be <code class="literal">GL_NEAREST</code> to detect more edges and darker appearances. Unlike the <code class="literal">GL_LINEAR</code> filtering, which uses the weighted average of the four surrounding pixels closest to texture coordinates, the <code class="literal">GL_NEAREST</code> filtering uses the pixel color that is closest to texture coordinates, therefore resulting gradients with higher chances of sharp changes in frequency.</p></div></div></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec236"/>How to do it...</h2></div></div></div><p>Perform the following step-by-step guidelines to understand the programming procedure. Make sure that you refer to the <span class="emphasis"><em>See also</em></span> section for dependencies before you read this section. This recipe reuses the FBO recipe from textures and renames the class from <code class="literal">DemoFBO</code> to <code class="literal">EdgeDetection</code>:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">In the constructor, load the <code class="literal">SimpleTexture</code> and <code class="literal">ObjLoader</code> class. The former class renders the polka dot pattern mesh and the latter class is used to render the FBO texture.</li><li class="listitem">In this class, create two variables called <code class="literal">DefaultFBO</code> and <code class="literal">FboId</code> to hold the handles of the default framebuffer and FBO respectively. Create two more variables: <code class="literal">textureId</code> and <code class="literal">depthTextureId</code> to hold the handles of the color texture and the depth texture in the FBO.</li><li class="listitem">Create<a id="id673" class="indexterm"/> the FBO in the <code class="literal">initModel()</code> with<a id="id674" class="indexterm"/> user-defined dimensions (width and height) as per the application requirement. This recipe uses the same dimension as the render buffer dimension. The framebuffer is created in the <code class="literal">GenerateFBO()</code> function, which creates a color buffer and a depth buffer to store the scene color and depth information:<div class="informalexample"><pre class="programlisting">   void EdgeDetection::GenerateFBO(){
   glGetRenderbufferParameteriv(GL_RENDERBUFFER,GL_RENDERBUFFER_WIDTH, &amp;TEXTURE_WIDTH);

   glGetRenderbufferParameteriv(GL_RENDERBUFFER,
   GL_RENDERBUFFER_HEIGHT, &amp;TEXTURE_HEIGHT);
   
   
   glGenFramebuffers(1, &amp;FboId); // Create FBO
   glBindFramebuffer(GL_FRAMEBUFFER, FboId);
    
   // Create color and depth buffer textureobject
    textureId = generateTexture(
    TEXTURE_WIDTH,TEXTURE_HEIGHT);
    depthTextureId = generateTexture(TEXTURE_WIDTH,TEXTURE_HEIGHT,true);
    
    // attach the texture to FBO color 
    // attachment point
    glFramebufferTexture2D(GL_FRAMEBUFFER,
    GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, textureId, 0);

   // attach the texture to FBO color 
   // attachment point
   glFramebufferTexture2D(GL_FRAMEBUFFER,GL_DEPTH_ATTACHMENT, GL_TEXTURE_2D,depthTextureId, 0);

   // check FBO status
   GLenum status = glCheckFramebufferStatus(
   GL_FRAMEBUFFER);
   if(status != GL_FRAMEBUFFER_COMPLETE){
   printf("Framebuffer creation fails: %d", status);
                }
   glBindFramebuffer(GL_FRAMEBUFFER, 0);
         }</pre></div></li><li class="listitem">Render <a id="id675" class="indexterm"/>the scene using the <code class="literal">RenderObj()</code> function. The scene is rendered to the perspective projection<a id="id676" class="indexterm"/> system using <code class="literal">SetUpPerspectiveProjection()</code>, which is called before <code class="literal">RenderObj()</code>. FBO must be bound before drawing a scene. This will render the color information of scenes to FBO's color texture and depth information to FBO's depth texture.</li><li class="listitem">Set the model-view matrix and draw a scene. Make sure to restore the default framebuffer at last after the scene is rendered to the FBO:<div class="informalexample"><pre class="programlisting">void EdgeDetection::RenderObj(){
    // Get the default Framebuffer
    glGetIntegerv(GL_FRAMEBUFFER_BINDING, &amp;DefaultFBO);
    
    // Bind Framebuffer object
    glBindFramebuffer(GL_FRAMEBUFFER,FboId);
    glViewport(0, 0, TEXTURE_WIDTH, TEXTURE_HEIGHT);
    glFramebufferTexture2D(GL_FRAMEBUFFER,GL_COLOR_ATTACHMENT0, 
    GL_TEXTURE_2D, textureId,0);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT,
    GL_TEXTURE_2D, depthTextureId, 0);
    
    glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT);
    objModel-&gt;Render();
    
    glBindFramebuffer(GL_FRAMEBUFFER, DefaultFBO);
}</pre></div></li><li class="listitem">Now, we are good to go with edge detection with the help of the <code class="literal">SimpleTexture</code> class. This class will take the saved texture from the FBO and apply the edge detection shader to it. For more information on how the <code class="literal">SimpleTexture</code> class works, refer to the <span class="emphasis"><em>Applying texture with the UV mapping</em></span> recipe in <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span>.</li><li class="listitem">The FBO texture is rendered to a quad of size two. This quad fits to the complete viewport. This is why the orthographic projection system must also be defined with the same dimensions:<div class="informalexample"><pre class="programlisting">  TransformObj-&gt;TransformSetMatrixMode( PROJECTION_MATRIX );
  TransformObj-&gt;TransformLoadIdentity();
  float span = 1.0;
  TransformObj-&gt;TransformOrtho(-span,span,-span,span,-span,span);</pre></div></li><li class="listitem">The <code class="literal">EdgeDetect()</code> function applies the <a id="id677" class="indexterm"/>Sobel filter using<a id="id678" class="indexterm"/> the <code class="literal">SimpleTexture</code> class. This sets the required <code class="literal">pixelSize</code> uniform in the edge detection shader:<div class="informalexample"><pre class="programlisting">void EdgeDetection::EdgeDetect(){
    glDisable(GL_DEPTH_TEST);
    glBindFramebuffer(GL_FRAMEBUFFER, DefaultFBO);
    glViewport(0, 0, TEXTURE_WIDTH, TEXTURE_HEIGHT);
    glActiveTexture (GL_TEXTURE0);
    glBindTexture(GL_TEXTURE_2D,textureId);
    
    program = ProgramManagerObj-&gt;Program
((char*)"EdgeDetection" );
    glUseProgram( program-&gt;ProgramID );
    GLint PIXELSIZE = ProgramManagerObj-&gt;ProgramGetUniformLocation
(program, (char*) "pixelSize");
    glUniform2f(PIXELSIZE, 1.0/TEXTURE_HEIGHT,
1.0/TEXTURE_WIDTH);
    textureQuad-&gt;Render();
}</pre></div></li><li class="listitem">Implement the following <code class="literal">EdgeDetectionFragment.glsl</code> fragment shader for edge detection. There is no change required in the vertex shader. Use <code class="literal">SimpleTexture::InitModel()</code> to load this shader:<div class="informalexample"><pre class="programlisting">#version 300 es
precision mediump float;
in vec2 TexCoord;
uniform vec2 pixelSize;
uniform sampler2D Tex1;
layout(location = 0) out vec4 outColor;
uniform float GradientThreshold;
float p00,p10,p20,p01,p21,p02,p12,p22,x,y,px,py,distance;
vec3 lum = vec3(0.2126, 0.7152, 0.0722);
void main(){
    x = pixelSize.x; y = pixelSize.y;
    p00 = dot(texture(Tex1, TexCoord+vec2(-x, y)).rgb, lum);
    p10 = dot(texture(Tex1, TexCoord+vec2(-x,0.)).rgb, lum);
    p20 = dot(texture(Tex1, TexCoord+vec2(-x,-y)).rgb, lum);
    p01 = dot(texture(Tex1, TexCoord+vec2(0., y)).rgb, lum);
    p21 = dot(texture(Tex1, TexCoord+vec2(0.,-y)).rgb, lum);
    p02 = dot(texture(Tex1, TexCoord+vec2( x, y)).rgb, lum);
    p12 = dot(texture(Tex1, TexCoord+vec2( x,0.)).rgb, lum);
    p22 = dot(texture(Tex1, TexCoord+vec2( x,-y)).rgb, lum);
    
// Apply Sobel Operator
    
    px = p00 + 1.0*p10 + p20 - (p02 + 1.0*p12 + p22);
    py = p00 + 1.0*p01 + p02 - (p20 + 1.0*p21 + p22);
    // Check frequency change with given threshold
    if ((distance = px*px+py*py) &gt; GradientThreshold ){
        outColor = vec4(0.0, 0.0, 0.0, 1.0);
    }else{ outColor = vec4(1.0); }
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec237"/>How it works...</h2></div></div></div><p>Edge detection is implemented in the <code class="literal">EdgeDetection</code> class. This<a id="id679" class="indexterm"/> class contains<a id="id680" class="indexterm"/> two objects of the <code class="literal">ObjLoader</code> and <code class="literal">SimpleTexture</code> class. The former class renders the 3D mesh and the latter renders the texture on the HUD. First, the scene is rendered to a frame buffer object. This allows you to capture the current scene in the texture form in the color buffer of the frame buffer object. This texture is then applied to the Sobel operator convolution filter, which detects edges. Finally, the process texture is rendered back to the HUD using the object of the <code class="literal">SimpleTexture</code> class.</p><p>Let's understand its functioning in detail. The <code class="literal">EdgeDetection</code> class first initializes the <code class="literal">ObjLoader</code> and <code class="literal">SimpleTexture</code> class objects in the constructor. In the <code class="literal">initModel()</code> function, it calls <code class="literal">GenerateFBO</code> to create an offline rendering buffer (FBO) with the same dimensions as the render buffer. In the render function, this FBO is attached to the drawing pipeline so that all drawing commands are diverted to our FBO, rather than going to the default buffer. The <code class="literal">ObjLoader</code> class renders the scene to this FBO's texture (with <code class="literal">textureId</code>). The graphics pipeline again binds back to the default framebuffer so that the output is visible on the screen. Now, the <code class="literal">SimpleTexture</code> class handles the remaining job of finding the scene edges through the <code class="literal">EdgeDetectionFragment.glsl</code> shader. This shader implements the Sobel operator and accepts a texture as an input. This texture must be the FBO's color texture (<code class="literal">textureId</code>). In the fragment shader program, each time a current fragment is processed, it retrieves a 3 x 3 fragment matrix around it. This matrix is then multiplied by the convolution kernel along the horizontal and vertical direction to result <code class="literal">px</code> and <code class="literal">py</code>. This result is used in calculating the intensity (<code class="literal">distance</code>) and compared with the given threshold (<code class="literal">GradientThreshold</code>). If the comparison is<a id="id681" class="indexterm"/> greater, then <a id="id682" class="indexterm"/>the fragment is colored black; otherwise, it's colored with white color:</p><div class="mediaobject"><img src="graphics/5527OT_09_04.jpg" alt="How it works..."/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec238"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to the <span class="emphasis"><em>Implementing render to texture with Frame Buffer Objects</em></span> recipe in <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Implementing grayscale and CMYK conversions</em></span></li><li class="listitem" style="list-style-type: disc">Refer to the <span class="emphasis"><em>Generating the polka dot pattern</em></span> recipe in <a class="link" href="ch06.html" title="Chapter 6. Working with Shaders">Chapter 6</a>, <span class="emphasis"><em>Working with Shaders</em></span></li></ul></div></div></div>
<div class="section" title="Making the scene blur with the Gaussian blur equation"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec74"/>Making the scene blur with the Gaussian blur equation</h1></div></div></div><p>The blur <a id="id683" class="indexterm"/>effect is an image processing<a id="id684" class="indexterm"/> technique that softens an image or makes it hazy. As a result, the image appears smoother like viewing it through a translucent mirror. It reduces the overall sharpness of the image by decreasing the image noise. It's used in many applications, such as blooming effect, depth-of-field, fuzzy glass, and heat haze effect.</p><p>The blurring effect in this recipe is implemented using the Gaussian blur equation. Like other image processing techniques, the Gaussian blur equation also makes use of the convolution filter to process image pixels. Bigger the size of the convolution filter, better and dense is the blur effect. The working principle of the Gaussian blur algorithm is very simple. Basically, each pixel's color is mixed with the neighboring pixel's color. This mixing is performed on the basis of a weight system. Closer pixels are given more weight as compared to farther ones.</p><p><span class="strong"><strong>The math behind the Gaussian blur equation</strong></span>:</p><p>The <a id="id685" class="indexterm"/>Gaussian blur equation makes use of the Gaussion function. The mathematical form of the equation and graphical representation of this function in one and two-dimensional space, as shown in the left-hand side of the following figure. This recipe uses the 2D form of this function, where <span class="emphasis"><em>σ</em></span> is the standard deviation of the distribution, <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> are the texel distance in the horizontal and vertical axis from the current texel on which the convolution filter works. The Gaussian function is very useful in making high frequency values smoother:</p><div class="mediaobject"><img src="graphics/5527OT_09_05.jpg" alt="Making the scene blur with the Gaussian blur equation"/></div><p><span class="strong"><strong>Working principle</strong></span>:</p><p>The<a id="id686" class="indexterm"/> Gaussian filter is applied on each and every texel. As a result, the change in its original value is based on the neighboring pixels. The number of the neighboring pixels depends on the size of the convolution kernel. For a 9 x 9 kernel, the number of computations required are <span class="emphasis"><em>9 * 9 = 81</em></span>. These can be reduced by performing the Gaussian blur in two passes, wherein the first pass is applied on each texel in the horizontal direction (<span class="emphasis"><em>s</em></span> axis), as shown in the upper-right corner image by label (1), and the second pass is applied in the vertical direction (<span class="emphasis"><em>t</em></span> axis) represented by<a id="id687" class="indexterm"/> label (2). This requires 18 computations and the result is the same as 81 calculations. The final output of the Gaussian blur is represented by label 3.</p><p>There are <a id="id688" class="indexterm"/>five steps required to implement the Gaussian blur:</p><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Filter size</strong></span>: This depends on many things, such as the processing time, image size, output quality, and so on. Bigger the filter size, more the processing time, and better the results. For this recipe, we will use the 9 x 9 convolution filter.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>FBO</strong></span>: This creates two FBO's, the first one with the color and depth information and the second one only with the color information.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Render to texture</strong></span>: This renders the scene to the first FBO's color texture.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Horizontal pass</strong></span>: This uses the color buffer of the first FBO and applies the horizontal Gaussian blur pass.</li><li class="listitem" style="list-style-type: disc"><span class="strong"><strong>Vertical pass</strong></span>: This reuses the first FBO's color buffer and applies the vertical pass.</li></ul></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec239"/>How to do it...</h2></div></div></div><p>This <a id="id689" class="indexterm"/>recipe makes use of the first recipe on <a id="id690" class="indexterm"/>edge detection. We renamed the class from <code class="literal">EdgeDetection</code> to <code class="literal">GaussianBlur</code>. The steps to understand the required changes are as follows:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a new vertex shader called <code class="literal">Vertex.glsl</code>, as shown in the following code. This vertex shader will be shared by horizontal and vertical Gaussian blur passes:<div class="informalexample"><pre class="programlisting">#version 300 es
// Vertex information
layout(location = 0) in vec3  VertexPosition;
layout(location = 1) in vec2  VertexTexCoord;

out vec2 TexCoord;
uniform mat4 ModelViewProjectionMatrix;
void main( void ) {
    TexCoord = VertexTexCoord;
    vec4 glPos = ModelViewProjectionMatrix *
    vec4(VertexPosition,1.0);
    vec2 Pos = sign(glPos.xy);
    gl_Position = ModelViewProjectionMatrix *
    vec4(VertexPosition,1.0);
}</pre></div></li><li class="listitem">Create a new fragment shader called <code class="literal">BlurHorizontal.glsl</code> and add the following code:<div class="informalexample"><pre class="programlisting">#version 300 es
precision mediump float;
in vec2 TexCoord; 
uniform vec2 pixelSize; 
uniform sampler2D Tex1;

layout(location = 0) out vec4 outColor;

uniform float PixOffset[5];   // Texel distance
uniform float Weight[5];      // Gaussian weights

void main(){
    vec4 sum = texture(Tex1, TexCoord) * Weight[0];
    for( int i = 1; i &lt; 5; i++ ){ // Loop 4 times
       sum+=texture( Tex1, TexCoord + vec2(PixOffset[i],0.0)
       * pixelSize.x) * Weight[i];
       sum += texture( Tex1, TexCoord - vec2(PixOffset[i],0.0) 
       * pixelSize.x) * Weight[i];
    }
    outColor = sum;
}</pre></div></li><li class="listitem">Similarly, create another new fragment <a id="id691" class="indexterm"/>shader <a id="id692" class="indexterm"/>called <code class="literal">BlurVertical.glsl</code>:<div class="informalexample"><pre class="programlisting">// Use same code from BlurHorizontal.glsl
void main(){
    vec4 sum = texture(Tex1, TexCoord) * Weight[0];
    for( int i = 1; i &lt; 5; i++ ){ // Loop 4 times
      sum+=texture( Tex1, TexCoord + vec2(0.0, PixOffset[i])
      * pixelSize.y) * Weight[i];
      sum += texture( Tex1, TexCoord - vec2(0.0, PixOffset[i])
      * pixelSize.y) * Weight[i];}
      outColor = sum;
}</pre></div></li><li class="listitem">Compile and link these shaders in the <code class="literal">SimpleTexture::InitModel()</code>.</li><li class="listitem">Calculate the Gaussian weight using <code class="literal">GaussianEquation()</code>. We assumed sigma (<span class="emphasis"><em>σ</em></span>) as 10.0. The parameter value contains the texel distance along the horizontal or vertical direction, and the σ is the variance or standard deviation of the Gaussian distribution:<div class="informalexample"><pre class="programlisting">float GaussianBlur::GaussianEquation(float value, float sigma){
return 1./(2.*PI*sigma)*exp(-(value*value)/(2*sigma));
}</pre></div></li><li class="listitem">Calculate the weights for the horizontal and vertical Gaussian fragment shader, as given in the <a id="id693" class="indexterm"/>following code using<a id="id694" class="indexterm"/> the <code class="literal">GaussianEquation</code> function:<div class="informalexample"><pre class="programlisting">   gWeight[0]  = GaussianBlur::GaussianEquation(0, sigma);
    sum         = gWeight[0]; // Weight for centered texel
    
    for(int i = 1; i&lt;FILTER_SIZE; i++){
        gWeight[i] = GaussianBlur::GaussianEquation(i, sigma);
        
       // Why multiplied by 2.0? because each weight
       // is applied in +ve and –ve direction from the 
       // centered texel in the fragment shader.
        sum += 2.0 * gWeight[i];
    }

    for(int i = 0; i&lt;FILTER_SIZE; i++){
        gWeight[i] = gWeight[i] / sum;
    }
    
    if (GAUSSIAN_WEIGHT_HOR &gt;= 0){
        glUniform1fv(GAUSSIAN_WEIGHT_HOR, 
        sizeof(gWeight)/sizeof(float), gWeight);
    }

    // Similarly, pass the weight to vertical Gaussian 
    // blur fragment shader corresponding weight 
    // variable GAUSSIAN_WEIGHT_VERT

    float pixOffset[FILTER_SIZE];
    // Calculate pixel offset 
    for(int i=0; i&lt;FILTER_SIZE; i++){ pixOffset[i] = float(i); }
    if (PIXEL_OFFSET_HOR &gt;= 0){
        glUniform1fv(PIXEL_OFFSET_HOR, sizeof(pixOffset)/
        sizeof(float), pixOffset);
    }</pre></div></li><li class="listitem">Create two FBO's within the <code class="literal">Gaussian::InitModel</code> with <code class="literal">GenerateBlurFBO1</code> (with the color and depth texture) and <code class="literal">GenerateBlurFBO2</code> (only the color buffer). These create two FBO's with the <code class="literal">blurFboId1</code> and <code class="literal">blurFboId2</code> handles respectively. The first FBO uses an additional buffer for depth because we want depth testing to be performed so that the correct image will be rendered to the color texture of this FBO.</li><li class="listitem">Render the scene with the perspective projection system to the first FBO (<code class="literal">blurFboId1</code> color texture). This will <a id="id695" class="indexterm"/>render the<a id="id696" class="indexterm"/> scene image to the color texture of this FBO:<div class="informalexample"><pre class="programlisting">void GaussianBlur::Render(){
   // Set up perspective projection
    SetUpPerspectiveProjection();
    
    RenderObj();
    // Set up orthographic project for HUD display
    SetUpOrthoProjection();
    RenderHorizontalBlur();
    RenderVerticalBlur();
}

void GaussianBlur::RenderObj(){
    // Get the current framebuffer handle
    glGetIntegerv(GL_FRAMEBUFFER_BINDING, &amp;CurrentFbo);
    
    // Bind Framebuffer 1
    glBindFramebuffer(GL_FRAMEBUFFER,blurFboId1);
    glViewport(0, 0, TEXTURE_WIDTH, TEXTURE_HEIGHT);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_-ATTACHMENT0, GL_TEXTURE_2D, textureId,0);
    glFramebufferTexture2D(GL_FRAMEBUFFER, GL_DEPTH_-ATTACHMENT, GL_TEXTURE_2D, depthTextureId, 0);
 
    glClear(GL_COLOR_BUFFER_BIT|GL_DEPTH_BUFFER_BIT);
    objModel-&gt;Render();
    
    glBindFramebuffer(GL_FRAMEBUFFER, CurrentFbo);
}</pre></div></li><li class="listitem">Now, set the second FBO (with the <code class="literal">blurFboId2</code> handle) as a render destination, reuse the color texture from the first FBO (which contains the scene image), and pass it on to the horizontal blur pass (pass 1) with the <code class="literal">RenderHorizontalBlur()</code> function. This will produce the horizontal blur scene image on the (<code class="literal">textureId2</code>) color buffer of the second FBO. Note that the project system should be orthographic before the second FBO is set:<div class="informalexample"><pre class="programlisting">void GaussianBlur::RenderHorizontalBlur(){
    glDisable(GL_DEPTH_TEST);

    // Bind Framebuffer 2
    glBindFramebuffer(GL_FRAMEBUFFER,blurFboId2);
    glViewport(0, 0, TEXTURE_WIDTH, TEXTURE_HEIGHT);
    glFramebufferTexture2D(GL_FRAMEBUFFER, 
     GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, textureId2, 0);
    glActiveTexture (GL_TEXTURE0);
    glBindTexture(GL_TEXTURE_2D, textureId);

    // Apply the shader for horizontal blur pass
    program = textureQuad-&gt;ApplyShader(HorizontalBlurShader);
    textureQuad-&gt;Render();
    TransformObj-&gt;TransformError();
}</pre></div></li><li class="listitem">Finally, use the default framebuffer <a id="id697" class="indexterm"/>and apply the pass 2 (vertical <a id="id698" class="indexterm"/>blur) using the <code class="literal">RenderVerticalBlur</code> function in the second FBO's texture (<code class="literal">textureId2</code>):<div class="informalexample"><pre class="programlisting"> void GaussianBlur::RenderVerticalBlur() {
    glDisable(GL_DEPTH_TEST);
 
 // Restore to old framebuffer 
    glBindFramebuffer(GL_FRAMEBUFFER, CurrentFbo);
    glViewport(0, 0, TEXTURE_WIDTH, TEXTURE_HEIGHT);
    glActiveTexture (GL_TEXTURE1);
    glBindTexture(GL_TEXTURE_2D,textureId2);

    // Apply the shader for horizontal blur pass
    program = textureQuad-&gt;ApplyShader(VerticalBlurShader);
    GLint PIXELSIZE = ProgramManagerObj-&gt;ProgramGetUniform-Location( program, (char *) "pixelSize" );
    glUniform2f(PIXELSIZE, 1.0/TEXTURE_HEIGHT,1.0/TEXTURE_WIDTH);

    textureQuad-&gt;Render();
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec240"/>How it works...</h2></div></div></div><p>The basic idea behind the Gaussian blur is to create a new texel of an image by taking a weighted average of the texels around it. Weights are applied using the Gaussian distribution function. For each texel, we need to create a square around the centered pixel. For instance, for a given texel, a square kernel of five texel contributes 25 texels weighted average to get the middle texel. Now, as the diameter of the kernel grows, the operation becomes expensive because it needs to read more texels to contribute. This expense is not linear fashioned because a 9 x 9 kernel requires 81 texels to read, which is almost four times the previous kernel.</p><p>Now, the Gaussian blur can be optimized to read less texels and yet achieve the same results. This can be done by dividing the kernel operation into two passes as the horizontal and vertical pass. In the former, only row-wise elements of the kernel are used for weighted average to calculate the middle texel of the row. Similarly, for the latter case, columnwise elements are considered. This way, it requires 18 (9 + 9) pixels to read instead of 81.</p><p>Now, let's understand the working of this <a id="id699" class="indexterm"/>recipe. The <a id="id700" class="indexterm"/>Gaussian blur is applied in two phases. Each phase works on one-dimensional row and column. The first phase is a horizontal pass, where texels in the horizontal direction are considered by the Gaussian kernel. This phase is called pass 1, which is performed using <code class="literal">BlurHorizontal.glsl</code>. Similarly, the second phase for pass 2 is carried within the <code class="literal">BlurVertical.glsl</code> fragment shader. Both these fragment shaders share a common vertex shader called <code class="literal">Vertex.glsl</code> and these shaders are managed by the <code class="literal">SimpleTexture</code> class.</p><p>When the <code class="literal">GaussianBlur</code> class is initialized, it creates two FBO's. The first FBO requires the color and depth information to render the scene. However, the second FBO does not require any depth texture information because it works on the first FBO's color texture, which is already taken the depth of the scene into consideration.</p><p>The scene is rendered to the color texture of the first FBO. This color texture is shared with the <code class="literal">SimpleTexture</code> class where the first pass (horizontal blur) is applied to it. During the second pass, the second FBO is used and provided with the horizontal, blurred color texture (from the first FBO) as an input. This texture (horizontal blurred) processes the vertical blur shader and stores the processed texture in the color buffer of the second FBO. Finally, the scene is attached to the default framebuffer, and the color buffer from the second FBO is rendered on the screen:</p><div class="mediaobject"><img src="graphics/5527OT_09_06.jpg" alt="How it works..."/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec241"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Detecting the scene edges with the Sobel operator</em></span></li></ul></div></div></div>
<div class="section" title="Making a scene glow real time with the bloom effect"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec75"/>Making a scene glow real time with the bloom effect</h1></div></div></div><p>Blooming is a very useful post screen processing technique that makes a real-time scene glow. With<a id="id701" class="indexterm"/> this effect, certain parts of the scene appear highly brighter and give an illusion of emitting scattered light in the atmosphere. This technique is widely used in gaming and cinematic effects.</p><p>The working principle<a id="id702" class="indexterm"/> of the bloom effect is very simple. The following image shows a pictorial representation of the working model, which is used in the current recipe. First, the scene is rendered to an offline framebuffer or texture (label <span class="strong"><strong>1</strong></span>), where its texture is used as an input in the next stage that detects the bright portions in the scene and writes in a new texture (label <span class="strong"><strong>2</strong></span>). This texture is then passed on to the horizontal (label <span class="strong"><strong>3</strong></span>) and vertical blur (label <span class="strong"><strong>4</strong></span>), which applies the Gaussian blurring effect to make it blurred and scattered a bit. This output (label <span class="strong"><strong>4</strong></span>) is then finally combined on top of the original rendered scene (label <span class="strong"><strong>1</strong></span>), which produces a glow-like effect:</p><div class="mediaobject"><img src="graphics/5527OT_09_07.jpg" alt="Making a scene glow real time with the bloom effect"/></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec242"/>How to do it...</h2></div></div></div><p>This recipe reuses our previous recipe on Gaussian blur. We rename the class from <code class="literal">GaussianBlur</code> to <code class="literal">Bloom</code>. Here are the steps to implement this recipe:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a new fragment shader called <code class="literal">Bloom.glsl</code>. This fragment shader need to be compiled and linked within the <code class="literal">SimpleTexture</code> class. This shader is responsible for locating the bright portions of the scene:<div class="informalexample"><pre class="programlisting">   in vec2 TexCoord;
   uniform sampler2D Tex1;
   layout(location = 0) out vec4 outColor;
   void main() {
   vec4 val = texture(Tex1, TexCoord);
   float brightness = 0.212*val.r + 0.715*val.g + 0.072*val.b;
   brightness&gt;0.6 ? outColor=vec4(1.) : outColor=vec4(0.); 
}</pre></div></li><li class="listitem">There is <a id="id703" class="indexterm"/>no change required in the <code class="literal">BlurHorizontal.glsl</code>. However, in the <code class="literal">BlurVertical.glsl</code>, add the following code. This code is responsible for mixing the blurred bright portions of the scene with the original scene (unchanged) preserved in the <code class="literal">RenderTex</code> texture:<div class="informalexample"><pre class="programlisting">void main(){
     vec4 scene = texture(RenderTex, TexCoord);
     vec4 sum = texture(Tex1, TexCoord) * Weight[0];
     for( int i = 1; i &lt; 5; i++ ){
     sum+=texture(Tex1,TexCoord+vec2(0.0,PixOffset[i]) 
    *pixelSize.y)*Weight[i];
    sum+=texture(Tex1,TexCoord-vec2(0.0,PixOffset[i]) 
    *pixelSize.y)*Weight[i];
 }
     outColor = sum + scene;
}</pre></div></li><li class="listitem">Create three FBO's in <code class="literal">Bloom::InitModel</code> with <code class="literal">GenerateSceneFBO()</code> (using the color and depth texture), <code class="literal">GenerateBloomFBO()</code> (using only the color buffer), and <code class="literal">GenerateBlurFBO2()</code> (using only the color buffer). These functions will create three FBO's with the <code class="literal">SceneFbo</code>, <code class="literal">BloomFbo</code>, and <code class="literal">BlurFbo</code> handles respectively.</li><li class="listitem">Render the bloom recipe under <code class="literal">Bloom::Render()</code>. In this function, render the scene with the perspective projection system, process the textures under the orthographic projection system, and store the handle of the default framebuffer.</li><li class="listitem">Render different phases for the bloom effect using <code class="literal">RenderObj()</code>, <code class="literal">RenderBloom()</code>, <code class="literal">RenderHorizontalBlur()</code>, and <code class="literal">RenderVerticalBlur()</code>. All of these functions accept four arguments. The first argument (<code class="literal">BindTexture</code>) specifies the input color texture/buffer, the second argument (<code class="literal">Framebuffer</code>) specifies the handle of the framebuffer to which the scene should be attached, the <a id="id704" class="indexterm"/>third argument (<code class="literal">ColorBuf</code>), and the fourth argument (<code class="literal">DepthBuf</code>) specifies the color and depth buffer to which the scene writes. If any of the argument is not required, send <code class="literal">NULL</code> as an argument:<div class="informalexample"><pre class="programlisting">void Bloom::Render(){
   // Perspective projection
   SetUpPerspectiveProjection(); 
   glGetIntegerv(GL_FRAMEBUFFER_BINDING, &amp;DefaultFrameBuffer);

   // Render scene in first FBO called SceneFBO
   RenderObj(NULL, SceneFbo, SceneTexture, DepthTexture); 

   // Orthographic projection
   SetUpOrthoProjection(); 

   // Render Bloom pass  
   RenderBloom(SceneTexture, BloomFbo, BloomTexture, NULL);
  
   // Render Horizontal pass
   RenderHorizontalBlur(BloomTexture,
   BlurFbo, BlurTexture, NULL);
   // Render Vertical pass
   RenderVerticalBlur(BlurTexture,
   DefaultFrameBuffer,NULL,NULL);
}</pre></div></li><li class="listitem">The <code class="literal">RenderObj()</code> will render the scene to the <code class="literal">SceneFbo</code> framebuffer in the <code class="literal">SceneTexture</code> and <code class="literal">DepthTexture</code>.</li><li class="listitem">Similarly, the <code class="literal">RenderBloom()</code> uses <code class="literal">SceneTexture</code>. Now, apply the <code class="literal">BlurHorizontal.glsl</code> shader to it, which will render the scene to <code class="literal">BlurTexture</code>.</li><li class="listitem">Finally, <code class="literal">RenderVerticalBlur()</code> uses <code class="literal">BlurTexture</code> and <code class="literal">SceneTexture</code> as an input and applies the <code class="literal">BlurVertical.glsl</code> shader on it, which will apply the vertical blur pass and mix it in the scene texture.</li><li class="listitem">Now, use the <code class="literal">blurFboId2</code> FBO and reuse the first FBO's texture and pass it on to pass 1 (the horizontal blur) using the <code class="literal">RenderHorizontaBlur()</code> function. This will store the processing result of pass 2 in <code class="literal">textureId2</code>.</li><li class="listitem">Now, use the default framebuffer and apply the pass 2 (the vertical blur) to the second FBO's texture (<code class="literal">textureId2</code>).</li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec243"/>How it works...</h2></div></div></div><p>The working principle of the bloom effect is very similar to the previous recipe. Instead, a new stage for blooming is added. First, the scene is rendered to a nondefault framebuffer called <code class="literal">SceneFBO</code>, where it's written in the <code class="literal">SceneTexture</code>. The next stage called <a id="id705" class="indexterm"/>blooming is also performed on an offline framebuffer (<code class="literal">BloomFBO</code>). In this, the texture from the previous stage is used as an input and applied to the bloom fragment shader. The bloom shader converts a color image to luminance, which stores the image information in the linear gradient form. This provides the brightness information of the image, where the bright portions are detected by comparing the gradient value to the required threshold. The brightest portions are then written in the <code class="literal">BloomTexture</code> and provided to the Gaussian blur stage.</p><p>In this stage, the input stored in the <code class="literal">BloomTexture</code> from the previous stage is processed using the horizontal Gaussian blur pass where it's stored in <code class="literal">BlurTexture</code> and applied to the vertical pass. During the vertical blur pass, the blurred bright portion is mixed with the original scene using <code class="literal">SceneTexture</code>. This way, the image is mixed with the bright scattered glowing light on the scene:</p><div class="mediaobject"><img src="graphics/5527OT_09_08.jpg" alt="How it works..."/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec244"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Detecting the scene edges with the Sobel operator</em></span></li><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Making the scene blur with the Gaussian blur equation</em></span></li></ul></div></div></div>
<div class="section" title="Painting the scene like a cartoon shading"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec76"/>Painting the scene like a cartoon shading</h1></div></div></div><p>Among <a id="id706" class="indexterm"/>various different kinds of shaders, the toon shader is well known for producing cartoon-shaded scenes. The cartoon shading technique is implemented in the fragment shader. The fundamental basis of this shader is the quantization of colors. In this, a range of colors are represented by a single type of color. Mathematically, color values are constrained from a continuous set of values (in floating numbers) to a relatively small discrete color set (represented by integer values). In addition to the quantization of color, the edges of the geometry are also highlighted using the Sobel operator.</p><p>The following image shows a screenshot from the current recipe, where quantization can be easily seen in various shades of green color. In conjunction, the Sobel operator renders thick black edges:</p><div class="mediaobject"><img src="graphics/5527OT_09_09.jpg" alt="Painting the scene like a cartoon shading"/></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec245"/>Getting ready</h2></div></div></div><p>This recipe<a id="id707" class="indexterm"/> is an extension of our edge detection recipe. With very little changes in the fragment shader, we can create a scene that looks like a painted cartoon. For this recipe, you are advised to thoroughly understand our first recipe in this chapter. This recipe will cover the changes we added to the existing edge detection's fragment shader for implementing the cartoon shader.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec246"/>How to do it...</h2></div></div></div><p>We reused the <code class="literal">EdgeDetectionFragment.glsl</code> and renamed it to <code class="literal">ToonShader.glsl</code>:</p><div class="informalexample"><pre class="programlisting">uniform float quantizationFactor;
void main(){
    // Reuse Edge detection recipe fragment shader and
    // Calculate p00, p10, p20,p01, p21, p02, p12, p22 
    px = p00 + 2.0*p10 + p20 - (p02 + 2.0*p12 + p22);
    py = p00 + 2.0*p01 + p02 - (p20 + 2.0*p21 + p22);
    // Check frequency change with given threshold
    if ((distance = px*px+py*py) &gt; GradientThreshold ){
        outColor = vec4(0.0, 0.0, 0.0, 1.0);
    }else{ // Apply the Cartoon shading
    rgb = texture(Tex1,TexCoord).rgb*quantizationFactor;
    rgb += vec3(0.5, 0.5, 0.5);
    ivec3 intrgb = ivec3(rgb);
    rgb = vec3(intrgb)/ quantizationFactor;
    outColor = vec4(rgb,1.0); 
   }
}</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec247"/>How it works...</h2></div></div></div><p>In the<a id="id708" class="indexterm"/> cartoon shading, each incoming fragment is first <a id="id709" class="indexterm"/>passed through the Sobel operation to check whether it belongs to an edge or not. If it does, the current fragment is rendered with a black edge color; otherwise, it's shaded with the cartoon shading effect.</p><p>In the cartoon shading effect, each fragment color is multiplied by a <code class="literal">quantizationFactor</code> (which is 2.0 in the present case). This is used in the process of the image quantization. In computer graphics, image quantization is a process of limiting a large set of colors to fewer ones. In other words, it groups similar colors as one.</p><p>The obtained color components are added with 0.5 to enhance the chances of producing values greater than 1.0. This is helpful for the next step, where the floating point color space is converted to the integer type. During this process, the decimal part of the color component is chopped off.</p><p>Finally, the effect of the <code class="literal">quantizationFactor</code> multiplication is nullified (we applied this at the beginning), by dividing the integer space color components by <code class="literal">quantizationFactor</code>. The resultant value is applied on the fragment.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec248"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Detecting the scene edges with the Sobel operator</em></span></li></ul></div></div></div>
<div class="section" title="Generating an embossed scene"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec77"/>Generating an embossed scene</h1></div></div></div><p>Embossing is <a id="id710" class="indexterm"/>a technique in which the scene appears raised or highlighted with some 3D depth. The working logic of the emboss shader is similar to the edge detection technique. Here, the detected edges are used to highlight the image based on edge angles.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec249"/>Getting ready</h2></div></div></div><p>For this recipe, we will reuse any of the previous post screen processing recipe implemented in this chapter. This recipe will directly jump to the shader part with an assumption that the reader has understood the fundamental logics of the post processing.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec250"/>How to do it...</h2></div></div></div><p>Create a new fragment shader called <code class="literal">EmbossFrag.glsl</code>, as shown in the following code. There are no changes required for the vertex shader:</p><div class="informalexample"><pre class="programlisting">in vec2 TexCoord;
uniform vec2 pixelSize;
uniform sampler2D Tex1;
layout(location = 0) out vec4 outColor;
uniform float EmbossBrightness, ScreenCoordX;

void main(){
   // Apply Emboss shading
   vec3 p00 = texture(Tex1, TexCoord).rgb;
   vec3 p01 = texture(Tex1, TexCoord + vec2(0.0,
 pixelSize.y)).rgb;

// Consecutive texel difference
   vec3 diff = p00 - p01;

// Find the max value among RGB
   float maximum = diff.r;
   if( abs(diff.g) &gt; abs(maximum) ){ 
   maximum = diff.g;
}
   
if( abs(diff.b) &gt; abs(maximum) ){
    maximum = diff.b;
}

// Choose White, Black, or Emboss color
   float gray = clamp(maximum+EmbossBrightness, 0.0, 1.0);
   outColor = vec4(gray,gray,gray, 1.0);
   
}</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec251"/>How it works...</h2></div></div></div><p>In this recipe, edges are detected by taking the difference between two consecutive texels in any arbitrary direction. The difference of these two results in a new color intensity, where each component (RGB) are compared among themselves to find the greater magnitude component (max). This component is then used to clamp between low (0.0) and high (1.0). This operation results in three color intensities: white (derived from low), black (derived from high) 1.0, and emboss (derived from the max component). The result of the emboss shader is shown in the following image.</p><p>First, the scene<a id="id711" class="indexterm"/> is rendered to a FBO where it's stored in the color buffer. This color buffer is then sent to the emboss shader in the <code class="literal">Tex1</code> variable. The <code class="literal">p00</code> and <code class="literal">p01</code> are represented as two consecutive texels, which are sampled from <code class="literal">Tex1</code> for the current fragment position. The difference is stored in the diff variable. The diff variable is checked to find the maximum magnitude among RGB components, which is stored in the max variable. The max value is clamped using the <code class="literal">clamp()</code> function. The result is finally used as an RGB component of the current fragment:</p><div class="mediaobject"><img src="graphics/5527OT_09_10.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec252"/>There's more...</h2></div></div></div><p>The clamping operation we used in this recipe is performed using the <code class="literal">clamp()</code> GLSL function. This function takes three values: original, lower, and higher range value. If the original value lies between the minimum and maximum range, it returns the original value; otherwise, it returns the minimum range value if the value is smaller than the minimum <a id="id712" class="indexterm"/>one and vice versa.</p><p><span class="strong"><strong>Syntax</strong></span>:</p><div class="informalexample"><pre class="programlisting">void clamp(genType x, genType minVal, genType maxVal);</pre></div><div class="informaltable"><table border="1"><colgroup><col style="text-align: left"/><col style="text-align: left"/></colgroup><thead><tr><th style="text-align: left" valign="bottom">
<p>Variable</p>
</th><th style="text-align: left" valign="bottom">
<p>Description</p>
</th></tr></thead><tbody><tr><td style="text-align: left" valign="top">
<p><code class="literal">x</code></p>
</td><td style="text-align: left" valign="top">
<p>This specifies the value to constrain</p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">minVal</code></p>
</td><td style="text-align: left" valign="top">
<p>This specifies the lower end of the range to constrain <code class="literal">x</code></p>
</td></tr><tr><td style="text-align: left" valign="top">
<p><code class="literal">maxVal</code></p>
</td><td style="text-align: left" valign="top">
<p>This <a id="id713" class="indexterm"/>specifies the upper end of the range to constrain <code class="literal">x</code></p>
</td></tr></tbody></table></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec253"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Implementing grayscale and CMYK conversions</em></span></li></ul></div></div></div>
<div class="section" title="Implementing grayscale and CMYK conversions"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec78"/>Implementing grayscale and CMYK conversions</h1></div></div></div><p>The grayscale <a id="id714" class="indexterm"/>or luminance is an important topic that digital<a id="id715" class="indexterm"/> image processing is incomplete without discussing its practical implementation. Luminance is widely used in various applications of image processing. Edge detection, cartoon shading, and emboss effect are examples that we implemented in this chapter, which make use of luminance. In this recipe, you will learn how to covert an RGB color space to luminance and CMYK.</p><p>Numerically, a grayscale is a linear interpolation between black and white, depending on the color depth. A depth of 8 bits represent 256 varying shades from white to black. However, with four, only 16 shades can be represented. The black color is the darkest possible shade, which is the total absence of transmitted or reflected light. The lightest possible shade is<a id="id716" class="indexterm"/> white, which is the total transmission or reflection of light at all visible. Intermediate shades of gray are represented by equal levels of three primary colors (red, green, and blue) to transmit light or equal amounts of three primary pigments (cyan, magenta, and yellow) for reflected light.</p><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note52"/>Note</h3><p>The ITU-R BT.709 standard provides the weight of these components as follows:</p><p><span class="emphasis"><em>RGB luminance value = 0.2125*(Red) + 0.7154*(Green) + 0.0721*(Blue)</em></span></p></div></div><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec254"/>Getting ready</h2></div></div></div><p>This recipe onwards, we will discuss various image processing techniques that was implemented in this chapter. For these recipes, we have reused the <span class="emphasis"><em>Applying texture with UV mapping</em></span> recipe from <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span>. For the current image processing recipe, we only need to make changes in the fragment shader. Proceed to the next section to understand the changes that need to be made to implement grayscale and CMYK conversions.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec255"/>How to do it...</h2></div></div></div><p>Reuse the <a id="id717" class="indexterm"/>simple texture recipe, as mentioned previously, and <a id="id718" class="indexterm"/>make the following changes in the fragment shader to <a id="id719" class="indexterm"/>implement the grayscale and CMYK recipe:</p><p><span class="strong"><strong>Grayscale recipe</strong></span>:</p><div class="informalexample"><pre class="programlisting">in vec2 TexCoord;
uniform sampler2D Tex1;
  layout(location = 0) out vec4 outColor;
  // Luminance weight as per ITU-R BT.709 standard
  const vec3 luminanceWeight = vec3(0.2125, 0.7154, 0.0721);
  void main() {
  vec4 rgb = texture(Tex1, TexCoord); // Take the color sample
  // Multiply RGB with luminance weight
  float luminace = dot(rgb.rgb, luminanceWeight);
  outColor = vec4(luminace, luminace, luminace, rgb.a); 
}</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec256"/>How it works...</h2></div></div></div><p>Declare a <code class="literal">luminanceWeight</code> variable that contains the weight of RGB components as per the ITU-R BT.709 standard. Use the incoming texture coordinate and sample the corresponding texel from the texture in the <code class="literal">rgb</code> variable. Take the dot product between the luminanceWeight and rgb variable to produce the grayscale image (stored in the luminance variable). The grayscale image output of the current recipe is shown in the following right hand-side image:</p><div class="mediaobject"><img src="graphics/5527OT_09_11.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec257"/>There's more...</h2></div></div></div><p>Images are represented in the RGB color space on color computer monitors. However, when these images are published using the standard printing process, these need to be converted to the CMYK color space. The RGB model is created by adding color components to the black color. This is based on emissive colors. In contrast, the CMYK color is transmissive. Here, the colors are created by subtracting color components from white. In <a id="id720" class="indexterm"/>an RGB to CMYK conversion, the red component<a id="id721" class="indexterm"/> changes to cyan, green to magenta, blue to yellow, and black. The publishing print press uses the CMYK color format, where the RGB space image is converted to four separate single color images, which are used to create four separate printing plates to the printing process.</p><p>The CMYK color space can be calculated from RGB using the following formula:</p><div class="mediaobject"><img src="graphics/5527OT_09_12.jpg" alt="There's more..."/></div><p>However, this simple conversion does not truly match the desired results one would expect after conversion. The following approximation from Adobe Photoshop produces very satisfactory results. The under color removal (<span class="strong"><strong>ucr</strong></span>) and black generation (<span class="strong"><strong>bg</strong></span>) function is given as follows, where <span class="emphasis"><em>Sk=0.1</em></span>, <span class="emphasis"><em>K0 = 0.3</em></span>, and <span class="emphasis"><em>Kmax = 0.9</em></span>. These are the constant values used in the formula:</p><div class="mediaobject"><img src="graphics/5527OT_09_13.jpg" alt="There's more..."/></div><div class="note" title="Note" style=""><div class="inner"><h3 class="title"><a id="note53"/>Note</h3><p><span class="strong"><strong>Under color removal</strong></span> (<span class="strong"><strong>ucr</strong></span>) is the process of eliminating the overlapped yellow, magenta, and<a id="id722" class="indexterm"/> cyan color components that would be added to produce a dark neutral black color, replacing them with black ink called full black. This results in less ink and greater depth in shadows.</p><p><span class="strong"><strong>Black generation</strong></span> (<span class="strong"><strong>bg</strong></span>) is the process of producing a black channel or color. This affects <a id="id723" class="indexterm"/>color channels, when color conversation is performed from the RGB to CMYK color space.</p></div></div><p>The following <a id="id724" class="indexterm"/>image shows the color version and four separated <a id="id725" class="indexterm"/>versions of CMYK in grayscale. The grayscale representation of each component shows the amount of ink required for each darker values, indicating high consumption of ink:</p><div class="mediaobject"><img src="graphics/5527OT_09_14.jpg" alt="There's more..."/></div><p>Here is<a id="id726" class="indexterm"/> the fragment shader code for the CMYK separation<a id="id727" class="indexterm"/> from an RGB color space:</p><div class="informalexample"><pre class="programlisting">   in vec2 TexCoord;
   uniform sampler2D Tex1;
   uniform float ScreenCoordX;
   uniform int caseCYMK;
   layout(location = 0) out vec4 outColor;
   void main() { // Main Entrance
   vec4 rgb  = texture(Tex1, TexCoord);
   vec3 cmy  = vec3(1.0)-rgb.rbg;
   float k   = min(cmy.r, min(cmy.g, cmy.b));

   // fucr (K)= SK*K, SK = 0.1 
   vec3 target  = cmy - 0.1 * k;

   // fbg (K) = 0, when K&lt;K0, K0 =0.3, Kmax =0.9
   // fbg (K) = Kmax*(K-K0)/(1-K0), when K&gt;=K0
   k&lt;0.3 ? k=0.0 : k=0.9*(k-0.3)/0.7; 
   vec4 cmyk = vec4(target, k);
// Since we are interested in the separation of each component
// we subtracted gray scale of each color component from white
   if(caseCYMK == 0){              // CYAN conversion
      outColor = vec4(vec3(1.0 - cmyk.x),rgb.a);
   }else if(caseCYMK == 1){     // MAGENTA conversion
       outColor = vec4(vec3(1.0 - cmyk.y),rgb.a);}
   else if(caseCYMK == 2){     // YELLOW conversion
       outColor = vec4(vec3(1.0 - cmyk.z),rgb.a);}
   else if(caseCYMK == 3){     // BLACK conversion
       outColor = vec4(vec3(1.0 - cmyk.w),rgb.a);}
   else{ outColor = rgb;}      // RGB
}</pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec258"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer<a id="id728" class="indexterm"/> to<a id="id729" class="indexterm"/> the <span class="emphasis"><em>Applying texture with UV mapping</em></span> recipe in <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span></li></ul></div></div></div>
<div class="section" title="Implementing fisheye with barrel distortion"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec79"/>Implementing fisheye with barrel distortion</h1></div></div></div><p>Fisheye <a id="id730" class="indexterm"/>is an effect in which a scene<a id="id731" class="indexterm"/> looks sphered. As a result, edges in the scene look curved and bowed around the center of this virtual sphere. This effect makes the scene look like wrapped around a curved surface.</p><p>The barrel distortion technique is used to achieve the present effect, which can be applied to fragments or vertices. This recipe will implement the barrel distortion on the fragment shader first and then apply it to the vertex shader. The difference between the two is this; in the former shader, the geometry does not distort. However, texture coordinates are distorted, resulting in a magnifying lens effect or a fisheye lens effect. In the latter technique, the geometry is displaced and creates different amusing distorted shapes. Note that this is not a post processing technique.</p><div class="section" title="Getting ready"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec259"/>Getting ready</h2></div></div></div><p>For this recipe, we can reuse our first recipe and replace the edge detection logic with the current barrel distortion fragment shader: <code class="literal">BarrelDistFishEyeFragment.glsl</code>.</p></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec260"/>How to do it...</h2></div></div></div><p>Modify the <code class="literal">BarrelDistFishEyeFragment.glsl</code>, as shown in the following code:</p><div class="informalexample"><pre class="programlisting">precision mediump float;
in vec2 TexCoord;
uniform sampler2D Tex1;
layout(location = 0) out vec4 outColor;

uniform float BarrelPower;
uniform float ScreenCoordX;

vec2 BarrelDistortion(vec2 p){
    float theta  = atan(p.y, p.x);
    float radius = sqrt(p.x*p.x + p.y*p.y); 
    radius = pow(radius, BarrelPower);
    p.x = radius * cos(theta);
    p.y = radius * sin(theta);
    return (p + 0.5);
}

vec2 xy, uv;
float distance;
void main(){
      if(gl_FragCoord.x &gt; ScreenCoordX){
          // The range of text coordinate is from (0,0)
          // to (1,1). Assuming center of the Texture
          // coordinate system middle of the screen.
          // Shift all coordinate wrt to the new 
          // center. This will be the new position 
          // vector of the displaced coordinate.
          xy = TexCoord - vec2(0.5);
        
         // Calculate the distance from the center point.
         distance = sqrt(xy.x*xy.x+xy.y*xy.y); 
        
         float radius = 0.35;
         // Apply the Barrel Distortion if the distance
         // is within the radius. Our radius is half of 
         // the ST dimension.
         uv = (distance &lt; radius?BarrelDistortion(xy):TexCoord);
        
         if( distance &gt; radius-0.01 &amp;&amp; distance &lt; radius+0.01 ){
             outColor = vec4(1.0, 0.0, 0.0,1.0);
         }
         else{
             // Fetch the UV from Texture Sample
             outColor = texture(Tex1, uv);
         }
     }
     else{
        outColor = texture(Tex1, TexCoord);
     }
}</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec261"/>How it works...</h2></div></div></div><p>This<a id="id732" class="indexterm"/> recipe first renders the scene to<a id="id733" class="indexterm"/> a FBO's color texture, which is then shared with the <code class="literal">SimpleTexture</code> class and applied to the quad geometry with texture coordinates ranging from (0.0, 0.0) to (1.0, 1.0). The quad vertex and texture information are provided to the vertex and fragment shader to process the geometry and fragment information. The barrel distortion technique is implemented in the fragment shader, where each incoming texture coordinate is temporarily converted to the polar coordinate to produce the fisheye effect.</p><p>Texture <a id="id734" class="indexterm"/>coordinates are first translated in<a id="id735" class="indexterm"/> the center (0.5, 0.5) and the distance of these translated texture coordinates is computed from the center. If the translated texture coordinates (<code class="literal">xy</code>) falls outside the given threshold of 0.35 radius, then unaltered texture coordinates (<code class="literal">TexCoord</code>) are used to fetch the sample from <code class="literal">Tex1</code>; otherwise, this coordinate (<code class="literal">xy</code>) is applied to the barrel distortion with the <code class="literal">BarrelDistortion</code> function. The following image shows the radius of the red circle. The <code class="literal">BarrelDistortion</code> function first calculates the length of the texture coordinate with respect<a id="id736" class="indexterm"/> to the center of the logical circle. This obtained length is altered using the barrel power, which shrinks or expands the length. The following image shows different results obtained from various barrel powers (1.0, 0.5, 0.3, and 2.0).</p><p>This altered length is then multiplied by the slope of texture coordinates along the S (horizontal) and T (vertical) components, which will result in a new set of translated texture coordinates. These texture coordinates are retranslated into their old origin (bottom, left). Finally, this retranslated texture coordinate is used to calculate the sampled texture from the input texture coordinate:</p><div class="mediaobject"><img src="graphics/5527OT_09_15.jpg" alt="How it works..."/></div></div><div class="section" title="There's more..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec262"/>There's more...</h2></div></div></div><p>When the barrel distortion is applied to the geometry, it distorts the physical shape of the geometry. The following images show the application of the barrel distortion on different meshes. You can explore this recipe using the <code class="literal">BarrelDistortion_Vtx_Shdr</code> source code provided in this chapter:</p><div class="mediaobject"><img src="graphics/5527OT_09_16.jpg" alt="There's more..."/></div><p>The working logic of this recipe is similar to the previous one except the fact that it's now implemented <a id="id737" class="indexterm"/>in the vertex shader. Here, we<a id="id738" class="indexterm"/> do not need to translate the texture coordinate from the center because, by default, the origin always is the center of the Cartesian coordinate system.</p><p>Use the following code in the vertex shader to apply barrel distortion on the vertex shader:</p><div class="informalexample"><pre class="programlisting">layout(location = 0) in vec4  VertexPosition;
layout(location = 1) in vec3  Normal;
uniform mat4   ModelViewProjectionMatrix, ModelViewMatrix;
uniform mat3    NormalMatrix;
out vec3         normalCoord, eyeCoord, ObjectCoord;
uniform float   BarrelPower;

vec4 BarrelDistortion(vec4 p){
    vec2 v = p.xy / p.w;
    float radius = length(v);
   // Convert to polar coords
    if (radius &gt; 0.0){ 
        float theta = atan(v.y,v.x);
        radius = pow(radius, BarrelPower);
    // Apply distortion
        // Convert back to Cartesian
        v.x = radius * cos(theta); 
        v.y = radius * sin(theta);
        p.xy = v.xy * p.w;
    }
    return p;
}

void main(){
    normalCoord = NormalMatrix * Normal;
    eyeCoord    = vec3 ( ModelViewMatrix * VertexPosition );
    ObjectCoord = VertexPosition.xyz;
    gl_Position = BarrelDistortion(ModelViewProjectionMatrix*
    VertexPosition);
} </pre></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec263"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer <a id="id739" class="indexterm"/>to the <span class="emphasis"><em>Generating the polka dot pattern</em></span> recipe<a id="id740" class="indexterm"/> in <a class="link" href="ch06.html" title="Chapter 6. Working with Shaders">Chapter 6</a>, <span class="emphasis"><em>Working with Shaders</em></span></li></ul></div></div></div>
<div class="section" title="Implementing the binocular view with procedural texturing"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec80"/>Implementing the binocular view with procedural texturing</h1></div></div></div><p>This <a id="id741" class="indexterm"/>recipe implements<a id="id742" class="indexterm"/> a binocular view effect, where a scene is rendered as if it's visualized from the binocular itself. We will implement this effect by programing a procedural shader. Alternatively, in another technique, the alpha-mapped texture is used instead. In this approach, an alpha-masked texture containing a binocular view image is superimposed on top of the scene. This way, only those parts of the scene are visible that belong to the nonmasked texture region.</p><p>The procedural textured approach is also relatively simpler. Here, the scene is programmed in the fragment shader where the binocular view effect is created using texture coordinates of the vertices. Texture coordinates are used to create a logical circular region on the rendered image. The fragment that belongs outside the circumference of this circular region are rendered with an opaque color (say black). This opacity reduces as the distance shrinks toward the center point of this circular region. The tapped point (the single tap gesture) on the device screen is used as a center point of the circular region; this way, the lens can be moved around the screen using touch gestures.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec264"/>How to do it...</h2></div></div></div><p>Use any of the existing image processing recipes and replace the following code in the fragment shader. This fragment shader accepts a few inputs from the OpenGL ES program. The image texture is stored in the <code class="literal">Tex1</code>; the tapped point must be provided in the center variable, which will be treated as the center of the circle. We also require the <code class="literal">horizontalAspectRatio</code> and <code class="literal">verticalAspectRatio</code> aspect ratios so that with different screen resolutions, a circle remains as a circle and not turned to any elliptical shape. Finally, we need the inner and outer radius (<code class="literal">LensInnerRadius</code>, <code class="literal">LensOuterRadius</code>) to define the width of the circular region. The color (<code class="literal">BorderColor</code>) will be used for the mask painting:</p><div class="informalexample"><pre class="programlisting">#version 300 es
precision mediump float;
in vec2 TexCoord; 
uniform sampler2D Tex1;
uniform vec2 center;
uniform float horizontalAspectRatio, verticalAspectRatio;
uniform float LensInnerRadius,LensOuterRadius;
uniform vec4 BorderColor;

layout(location = 0) out vec4 outColor;
void main() {
outColor = texture(Tex1, TexCoord);
   float dx = TexCoord.x-center.x; 
float dy = TexCoord.y-center.y;
  
dx *= horizontalAspectRatio; 
dy *= verticalAspectRatio;
   float distance = sqrt(dx * dx + dy * dy);
   outColor = mix( outColor, BorderColor,
       smoothstep(LensInnerRadius, LensOuterRadius, distance));
  return;
}</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec265"/>How it works...</h2></div></div></div><p>The <a id="id743" class="indexterm"/>incoming texture coordinates is subtracted by <a id="id744" class="indexterm"/>the center position and are translated into new logical coordinates, where transformed texture coordinates or positional vectors (dx, dy) are stored with reference to the center point (<code class="literal">center</code>). This coordinate must be multiplied by the <code class="literal">aspectRatio</code> in the horizontal and vertical directions to eliminate any shape distortion due to the difference in the horizontal and vertical device screen resolution.</p><p>The distance of each positional vector is calculated with the vector length formula <span class="emphasis"><em>P (x, y) = √(x2 + y2)</em></span> and fed into the smoothstep GLSL API. The smooth step API accepts three arguments (<span class="strong"><strong>edge1</strong></span>, <span class="strong"><strong>edge2</strong></span>, and <span class="strong"><strong>x</strong></span>). The first two arguments are two outbound values and the third is the weight. Refer to the following left-hand side image to understand its functioning. This API returns an interpolated value between two edges, based on the weight provided. The output of the smoothstep is used as a weight to feed into another GLSL API called mix. The mix API mixes the border color with the current texture using a weighted value provided by the smoothstep function:</p><div class="mediaobject"><img src="graphics/5527OT_09_17.jpg" alt="How it works..."/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec266"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to the <span class="emphasis"><em>Applying texture with UV mapping</em></span> recipe in <a class="link" href="ch07.html" title="Chapter 7. Textures and Mapping Techniques">Chapter 7</a>, <span class="emphasis"><em>Textures and Mapping Techniques</em></span></li></ul></div></div></div>
<div class="section" title="Twirling the image"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec81"/>Twirling the image</h1></div></div></div><p>Twirling is<a id="id745" class="indexterm"/> a very common effect used in animations. When applied to a rendered scene or image, it distorts the appearance within the circular region and produces a radial circular motion of the texels where these are moved around the center of the circular region, producing a whirlpool-like effect.</p><p>Programmatically, for a given image, an arbitrary texel is chosen as a center. A fixed distance from the center of the circle defines a locus of the circumference. All the texels falling under this circumference are being applied to the rotation. The rotation of the texels within the circle decreases with the distance from the center and diminishes at the circumference edge. The following image shows how the twirl effect looks:</p><div class="mediaobject"><img src="graphics/5527OT_09_18.jpg" alt="Twirling the image"/></div><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec267"/>How to do it...</h2></div></div></div><p>Use the<a id="id746" class="indexterm"/> following code in the fragment shader to implement the twirl effect:</p><div class="informalexample"><pre class="programlisting">in vec2 TexCoord;
uniform sampler2D Tex1;
uniform float ScreenCoordX,twirlRadius,angle,imageHeight, imageWidth;
uniform vec2 center;
float radiusFactor = 3.0;
layout(location = 0) out vec4 outColor;
// Note: the angle is assumed to be in radians to 
// work with trigonometric functions.  
vec4 Twirl(sampler2D tex, vec2 uv, float angle){
    // Get the current texture size of the image
    vec2 texSize = vec2(imageWidth, imageHeight);
    
    // Change the texCoordinate w.r.t. to the image dimensions
    vec2 tc = (uv * texSize) - center;

    // Calculate the distance of the current transformed
 // texture coordinate from the center.
    float distance = sqrt(tc.x*tc.x + tc.y*tc.y);
    if (distance &lt; twirlRadius+angle*radiusFactor){
        float percent   = (twirlRadius - distance)/twirlRadius;
        float theta     = percent * percent * angle;
        float sinus     = sin(theta);
        float cosine    = cos(theta);
        tc = vec2(dot(tc, vec2(cosine, -sinus)), dot(tc,
 vec2(sinus, cosine)));
    }
 return texture(tex, (tc+center) / texSize);
}

void main() {
if(gl_FragCoord.x &gt; ScreenCoordX)
outColor = Twirl(Tex1, TexCoord, angle); 
else
outColor = texture(Tex1, TexCoord);
}</pre></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec268"/>How it works...</h2></div></div></div><p>The twirling effect requires a center point around which the whirlpool effect is produced, this center point is provided by the OpenGL ES program in the center variable. Additionally, we need the size of the image (<code class="literal">imageHeight</code> and <code class="literal">imageWidth</code>), which is used to control the region of animation within the image boundaries.</p><p>Each incoming texture coordinate is converted to its corresponding texel position by multiplying it with the image size and is then translated with respect to the center. The translated coordinates represent the position vector, which is used to calculating the distance from the center point. If the distance is within a given radius threshold, the texels are rotated <a id="id747" class="indexterm"/>around the center with an arbitrary angle specified in the degree. The angle of rotation increases as the distance between the center and the translated coordinate decreases.</p></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec269"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc"><span class="emphasis"><em>Implementing the binocular view with procedural texturing</em></span></li></ul></div></div></div>
<div class="section" title="Sphere illusion with textured quadrilateral"><div class="titlepage"><div><div><h1 class="title"><a id="ch09lvl1sec82"/>Sphere illusion with textured quadrilateral</h1></div></div></div><p>This recipe will demonstrate a performance efficient technique, which makes use of the procedural texture to produce<a id="id748" class="indexterm"/> the illusion<a id="id749" class="indexterm"/> of a real 3D object. In the Gouraud shading, fragments are painted with light shadings based on the direction of the light source and the geometry shape. For instance, in <a class="link" href="ch05.html" title="Chapter 5. Light and Materials">Chapter 5</a>, <span class="emphasis"><em>Light and Materials</em></span>, we implemented the diffuse light on a spherical model, which contains a very high number of vertices. This recipe technique renders the same diffused sphere, but using only four vertices. It fakes the light shading in such a way that the difference between the two becomes indistinguishable.</p><p>The performance is directly proportional to the number of fragments it renders to the screen. For example, the surface area covered by a single fullscreen rendering sphere is equivalent to several tiny spheres covering up the same surface area on the screen.</p><div class="section" title="How to do it..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec270"/>How to do it...</h2></div></div></div><p>Use the following steps to implement sphere with textured quadrilateral:</p><div class="orderedlist"><ol class="orderedlist arabic"><li class="listitem">Create a new class called <code class="literal">TextureQuadSphere</code> derived from the <code class="literal">Model</code> class.</li><li class="listitem">Declare the necessary vertex information for the quad, which will have the sphere rendered in:<div class="informalexample"><pre class="programlisting">float vertexColors[12] = { 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0 };
float texCoords[8]   = { 0.f, 0.f, 1.f, 0.f, 0.f, 1.f, 1.f, 1.f };
float quad[8]       = { -1.f,-1.f,1.f,-1.f,-1.f, 1.f, 1.f,1.f};</pre></div></li><li class="listitem">Add the following <code class="literal">TexQuadSphereVertex.glsl</code> vertex shader:<div class="informalexample"><pre class="programlisting">#version 300 es
uniform mat4 ModelViewProjectMatrix;
layout(location = 0) in vec3  VertexPosition;
layout(location = 1) in vec2  VertexTexCoord;
layout(location = 2) in vec4  VertexColor;
out vec4 TriangleColor; out vec2 TexCoord;

void main() {
 gl_Position = ModelViewProjectMatrix*vec4(VertexPosition,1.0);
 TriangleColor = VertexColor;
 TexCoord = VertexTexCoord;
}</pre></div></li><li class="listitem">There is<a id="id750" class="indexterm"/> no change <a id="id751" class="indexterm"/>required in the <code class="literal">TexQuadSphereFragment.glsl</code>:<div class="informalexample"><pre class="programlisting">#version 300 es
precision mediump float;
in vec4 TriangleColor; 
in vec2 TexCoord;
uniform float ScreenWidth; 
uniform float ScreenHeight;
uniform float ScreenCoordX; 
uniform float ScreenCoordY;
out vec4 FragColor;
vec3 lightDir = normalize(vec3(0.5, 0.5, 1.0));

void main() {
vec2 resolution = vec2(ScreenWidth, ScreenHeight);
   vec2 center     = vec2(resolution.x/2.0, resolution.y/2.0);
    lightDir = normalize(vec3((ScreenCoordX - center.x)
/(ScreenWidth*0.5), (ScreenCoordY - center.y)
/(ScreenHeight*0.5), 1.0));
    
    float radius   = 0.5; // Calculate the sphere radius
 vec2 position  = TexCoord.xy - vec2(0.5, 0.5);
    float z       = sqrt(radius*radius – 
position.x*position.x - position.y*position.y);
    vec3 normal=normalize(vec3(position.x,position.y,abs(z)));
    if (length(position) &gt; radius) { // Outside
        FragColor = vec4(vec3(0.0,0.0,0.0), 0.0);
    } else { // Inside
        float diffuse = max(0.0, dot(normal, lightDir));
        FragColor = vec4(vec3(diffuse), 1.0);
    }
}</pre></div></li></ol></div></div><div class="section" title="How it works..."><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec271"/>How it works...</h2></div></div></div><p>This <a id="id752" class="indexterm"/>technique uses a <a id="id753" class="indexterm"/>square geometry with four texture coordinates for each vertex. Texture coordinates are shared by the vertex shader in the <code class="literal">TexCoord</code> variable with the fragment shader. Texture coordinates are in the range from 0.0 to 1.0. These are subtracted by half dimensions to calculate the positional vector (<code class="literal">position</code>) with respect to the center of the circle. The radius of the circle and the arbitrary position vector from the center of the circle is used to calculate the elevation at each given position.</p><p>This elevation is used with the positional coordinate to produce a normal vector; this normal vector provides the angle it made with incidence light rays. The cosine of this angle is used on the color intensity to produce the diffuse shading effect of light on the logical hemisphere. The incident light ray is calculated with the tap coordinates on the fly using the screen resolution and tapped coordinates <span class="emphasis"><em>x</em></span> and <span class="emphasis"><em>y</em></span> positions.</p><p>The following figure shows the pictorial representation of the previous described working logic. P (x, y, 0.0) represents the position vector (<code class="literal">position</code>), C is the center, and Q is the point on the hemisphere which will be calculated using <span class="emphasis"><em>CQ = CP + PQ</em></span>, as shown in the following figure:</p><div class="mediaobject"><img src="graphics/5527OT_09_19.jpg" alt="How it works..."/></div></div><div class="section" title="See also"><div class="titlepage"><div><div><h2 class="title"><a id="ch09lvl2sec272"/>See also</h2></div></div></div><div class="itemizedlist"><ul class="itemizedlist"><li class="listitem" style="list-style-type: disc">Refer to <a id="id754" class="indexterm"/>the <span class="emphasis"><em>Implementing the per-vertex diffuse light component</em></span> recipe in <a class="link" href="ch05.html" title="Chapter 5. Light and Materials">Chapter 5</a>, <span class="emphasis"><em>Light and Materials</em></span></li></ul></div></div></div></body></html>