- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance and Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Software systems grow with business and changing environments manifested in
    higher complexity, more diverse user demands, and heavier workloads. The ability
    to maintain high performance and scale under growth becomes critical. Performance
    refers to how quickly a system can process and respond to requests, while scalability
    describes a system’s capacity to handle a higher volume of traffic and usage over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: Poor performance can lead to frustrating user experiences, lost productivity,
    and even complete system failures. And systems that can’t scale to meet rising
    demands will quickly become overwhelmed and unusable. Therefore, ensuring optimal
    performance and scalability is a key challenge for any software engineering project.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore the core concepts and principles of performance
    engineering and scalable system design. We’ll discuss common performance bottlenecks
    and mitigation strategies, review techniques for load testing and benchmarking,
    and cover architectural patterns and design choices that enable horizontal and
    vertical scalability.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you’ll have a solid understanding of how to build
    highly performant, scalable systems that can withstand the pressure of real-world
    demands.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensions of performance and scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize performance now or later?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance test planning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executing a performance test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Micro-benchmarking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strategies for performance improvement
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultra-low latency systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can find all the code files used in this chapter on GitHub: [https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-12](https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-12)'
  prefs: []
  type: TYPE_NORMAL
- en: Dimensions of performance and scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Performance is the efficiency of a system in executing tasks and responding
    to requests. It’s measured by various metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Latency**: The time taken for the system to respond to a request.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Throughput**: The number of requests processed in a given time frame.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource utilization**: The percentage of resources (for example CPU, memory,
    network, files, and so on) used during operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concurrent users**: The number of users effectively served by the system
    simultaneously without degradation in performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Page load time**: The total time taken for a screen to fully load, including
    all assets (images, videos, scripts, and so on)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Queue size**: The number of requests waiting to be processed by the server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time to first byte** (**TTFB**): The time that elapsed from when the client
    initiated a request to the client receiving the first byte from the server.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache hit ratio**: The percentage of requests served from the cache versus
    those from the slower secondary data source. A higher ratio indicates more efficient
    caching.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error rate**: The percentage of requests resulting in errors (for example,
    HTTP error statuses). High error rates indicate problems with the application
    or infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scalability is the capacity of a system to handle increased load without degrading
    performance. It indicates how well a system can grow and adapt to an increase
    in user traffic and data volume. Scalability can be categorized into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical scaling**: Adding more resources (for example, CPU or RAM) to a
    single node to increase its capacity. This is also known as **scale up**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: Adding more nodes to distribute the load and improve
    capacity in a distributed system. This is also known as **scale out**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scalability is also the capacity of a system to downsize when the load is reduced.
    Scaling down is usually concerned with the flexible use of resources and cost
    savings. It’s still an important aspect of scalability, but the focus is usually
    on scaling up and scaling out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Scalability can be measured by the following metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability ratio**: The ratio of the increase in performance to the increase
    in resources such as the number of servers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time to scale**: The time taken between adding resources to extra resources
    becoming operational in the system'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These metrics are useful in measuring how a change in the system may affect
    performance and scalability. Without them, it’s difficult to decide whether performance
    should be optimized or not. We’re going to discuss this decision in depth in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Optimize performance now or later?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Engineers and architects often face the question of whether performance should
    be optimized now or later. This happens from the early stages of system design
    to already established production systems.
  prefs: []
  type: TYPE_NORMAL
- en: We all know optimizing performance is crucial, but whether to prioritize it
    from day one isn’t a simple binary question.
  prefs: []
  type: TYPE_NORMAL
- en: You may have heard of someone who said “*Premature optimization is the root
    of evil.*” The statement itself is theatrical, but there are some merits in it.
  prefs: []
  type: TYPE_NORMAL
- en: You may also have heard a quote saying, “*Make it work, make it right, make
    it fast.*” This was coined by software engineer *Kent Beck*.
  prefs: []
  type: TYPE_NORMAL
- en: So, what would be the consequences if a system is optimized prematurely, or
    if we reverse the order to “make it fast” too early?
  prefs: []
  type: TYPE_NORMAL
- en: Spending too much time on performance improvements before understanding user
    behaviors and requirements can lead to wasted effort. Moreover, it creates an
    unnecessarily complex architecture that hinders the team’s productivity. The team
    may have to simplify the over-engineered system, which also requires effort. In
    that sense, the team is punished twice for improving performance too early.
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for optimizing performance and scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several factors to consider regarding whether the system should be
    optimized for performance and scalability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Core features completeness**: If the core features of a system are still
    being developed, then it’s often more important to focus on delivering core features
    and functionality initially. This is the first step: “*Make* *it work.*”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, we must ensure the system behaves as expected as per functional
    requirements. Correctness should always come before performance. This is the second
    step: “*Make* *it right.*”'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Performance metrics**: Before optimizing performance or improving scalability,
    it’s paramount to have current performance metrics as a baseline. A performance
    baseline provides insights into the current system bottlenecks that help the team
    prioritize which area should be improved first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A performance benchmark enables empirical and objective comparison of whether
    a change has resulted in better or worse performance, or whether an attempt to
    improve performance has achieved its goals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Non-functional requirements**: Non-functional requirements are a useful source
    of guidance on whether the system needs to be optimized now. Non-functional requirements
    for performance can be driven by regulatory constraints, external system integration
    conformance, or principles of user experiences.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Critical use cases, user experiences, and competitors**: If the application
    is expected to handle high traffic from the beginning (for example, product launch
    events, training, or marketing campaigns), then early optimization is essential.
    If the application’s performance directly impacts user satisfaction, it’s important
    to address performance concerns early to avoid negative feedback. The current
    performance metrics of competitors also indicate how much the application’s performance
    should be optimized.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability needs**: If rapid growth or scaling needs are anticipated for
    an application, implementing good performance practices from the beginning will
    save time and effort later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Even if it may not be the right time to optimize performance, there are some
    best practices to at least not make performance worse:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Measure first**: Measure the current performance metrics, ideally all operations,
    but as a bottom line, measure the core features and most frequent operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implement basic optimization**: Basic performance best practices such as
    efficient database queries in the early stage of development.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plan for scalability**: Plan and have scalability in mind when designing
    system architecture to allow for easier optimization later without major refactoring.
    Sometimes, it’s about not putting restrictions that would limit scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While it isn’t always necessary to optimize performance on day one, incorporating
    basic performance considerations into your development process can lead to better
    long-term results. Focus on delivering value first, then iterate on performance
    as the application evolves. Let’s go through an example of performance measurement
    to understand this better.
  prefs: []
  type: TYPE_NORMAL
- en: An example of basic performance measurement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here’s a basic example of an operation to be measured:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal of this example is to find out the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Throughput**: How many operations can be performed in a second'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Latency**: How long it takes to finish an operation on average'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A small function, `measureTotalTimeElapsed`, must be defined to measure the
    total time elapsed for all iterations of the operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This function uses the `measureTimeMillis` Kotlin function from Standard Library
    to capture the time spent in repeating the operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, this is the `main` function to launch the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This function defines the operation to be executed 1,000 times. After invoking
    the `measure` **TotalTimeElapsed** function with the Lambda expression that runs
    the `sampleOperation` function, the total time elapsed in milliseconds is returned.
    Then, the throughput is calculated as the number of iterations divided by the
    total elapsed time in seconds. The average latency is calculated as the inverse
    reciprocal of throughput – the total time elapsed divided by the number of iterations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a sample output from running the test:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Since the sample function, `sampleOperation`, only makes the thread sleep for
    1 millisecond, the average latency is 1 millisecond, as expected. The throughput
    in this run is close to 800, but it varies in every run.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kotlin Standard Library provides a few functions for time measurement:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Return time elapsed in milliseconds (used in this example): `measureTimeMillis`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return time elapsed in nanoseconds: `measureNanoTime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return time elapsed as `Duration`: `measureTime`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Return time elapsed and the value returned from the Lambda expression: `measureTimedValue`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For real-life performance critical systems, this is certainly not enough. Due
    to this, in the next section, we’ll cover the main types of performance tests.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Performance tests are a category of test that evaluates the speed, responsiveness,
    and stability of a system under a given workload. In this section, we’ll look
    at the main types of performance tests.
  prefs: []
  type: TYPE_NORMAL
- en: Load testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Load tests aim to assess the behaviors of a system under expected load conditions,
    such as a configured number of concurrent requests. The goal is to identify bottlenecks
    in application or infrastructure where performance may degrade under load. It
    ensures the system can handle anticipated traffic without performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Stress testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Stress tests aim to evaluate the system’s performance under extreme load conditions
    beyond its normal operational capacity. They also help us determine the breaking
    point of the system and how it fails under stress, so proactive monitoring and
    alerts can be deployed for precautions.
  prefs: []
  type: TYPE_NORMAL
- en: Endurance testing (soak testing)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Endurance tests, known as soak tests, focus on the stability and performance
    of a system over an extended period. This extended period is used to identify
    issues that accumulate or emerge over time, such as memory leaks, resource exhaustion,
    or performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Spike testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Spike tests introduce a sudden increase in load (the “spike”) so that we can
    observe how the system reacts in this situation. The result illustrates how the
    system can handle abrupt changes in traffic without failure.
  prefs: []
  type: TYPE_NORMAL
- en: Volume and latency testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Volume tests evaluate the system’s performance with a large volume of data.
    Latency tests measure the time delay between a request and the corresponding response.
    They usually measure metrics such as throughput and latency to ensure the application
    can meet **service-level agreements** (**SLAs**) or **service-level** **objectives**
    (**SLOs**).
  prefs: []
  type: TYPE_NORMAL
- en: Scalability testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scalability tests aim to determine how well the system can scale up or down
    in response to increasing or decreasing loads. It measures the performance of
    the system as resources are added or removed.
  prefs: []
  type: TYPE_NORMAL
- en: Configuration testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Configuration tests aim to identify the optimal configuration for performance.
    They involve running performance tests under different configurations, including
    hardware, software, and the network.
  prefs: []
  type: TYPE_NORMAL
- en: Planning a performance test
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although there are different types of performance tests, planning and executing
    performance tests are similar. The difference is in the details of each step.
    In this section, we’ll explore the journey of planning and executing a performance
    test.
  prefs: []
  type: TYPE_NORMAL
- en: Planning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the planning phase, first, the objectives of the test should be defined.
    This means we must define the information we want to get out of the tests – for
    example, can a household record be created within 50 milliseconds? Can the system
    handle 5,000 concurrent requests without degradation? These objectives are the
    primary drives to plan and execute a performance test. They also determine which
    type of performance tests can be used.
  prefs: []
  type: TYPE_NORMAL
- en: Then, business scenarios for performance tests should be defined. Usually, the
    objectives would have given a great hint at which scenarios would be used, but
    it’s worth exploring the details of the steps involved in each scenario and formalizing
    them as a blueprint of the test script.
  prefs: []
  type: TYPE_NORMAL
- en: The last part of planning is to specify the load levels to run, including the
    number of users and the duration of the test. Sometimes, it isn’t so clear which
    level to run, especially if we want to find the breaking point of the system.
    This is OK initially since performance tests are meant to run iteratively.
  prefs: []
  type: TYPE_NORMAL
- en: Preparation and development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once there’s an initial plan, the performance test can be prepared and developed.
    These activities can happen in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: 'The test script is the core of the test execution. The test needs to be automated
    to achieve consistent results. This involves a big decision on which tool to use.
    Here’s a list of commonly used tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Apache JMeter** ([https://jmeter.apache.org/](https://jmeter.apache.org/)):
    Open source, free, GUI support, distributed testing, plugin support, and Java-based'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LoadRunner by OpenText** ([https://www.opentext.com/](https://www.opentext.com/)):
    Commercial licenses, GUI, integration with CI/CD tools, analytics and reporting
    support, and support for Java'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gatling** ([https://docs.gatling.io/](https://docs.gatling.io/)): Open source,
    commercial licenses with additional features, and scripts can be written in Kotlin'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K6** ([https://k6.io/](https://k6.io/)): Open source, subscription-based
    for cloud features, and can integrate with CI/CD scripts written in **JavaScript**
    (**JS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Locust** ([https://locust.io/](https://locust.io/)): Open source, GUI support,
    distributed testing, and scripts written in Python'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BlazeMeter** ([https://www.blazemeter.com/](https://www.blazemeter.com/)):
    Free with limited features, commercial licenses, cloud-based, GUI support, real-time
    reporting and analytics, integrated with CI/CD, and supports JMeter scripts'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These tools provide comprehensive features such as organizing test scripts,
    managing multiple test configurations, metrics measurement, analytics, and reporting.
    You also have the option to build your own drivers of performance tests. This
    is applicable if your tests are simple and there are sufficient metric measurements
    without external tools.
  prefs: []
  type: TYPE_NORMAL
- en: Appropriate metric measurement needs to be set up according to what the test
    script requires. The metrics can be measured by the testing tools, or by the monitoring
    tools already embedded in the system, as discussed previously in [*Chapter 11*](B21737_11.xhtml#_idTextAnchor358).
    Any missing metrics need to be set up before executing the tests.
  prefs: []
  type: TYPE_NORMAL
- en: Meanwhile, a test environment needs to be set up for execution. Ideally, the
    environment should be comparable to the actual production environment where the
    system runs. If that’s too expensive, an environment of a smaller scale can be
    used to project expected performance, with a degree of inaccuracy in mind.
  prefs: []
  type: TYPE_NORMAL
- en: The test environment should be an isolated sandbox that does nothing but the
    performance tests. It can be a challenge for some organizations to replicate a
    production-like environment for performance testing. Replicating an environment
    with data alone may already be a challenge for some organizations. In addition,
    the environment needs to have the necessary data to run the test scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, the system has integration with third-party systems. In this case,
    the external integration would need to be stubbed out with simulators.
  prefs: []
  type: TYPE_NORMAL
- en: Execution and iteration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once we have the test scripts, test environments, and corresponding metrics
    set up, we’re ready to execute the performance test. It’s vital to allow an iterative
    feedback loop where tests can run multiple times, and there could be changes between
    each test. Within each iteration, the same operation should be executed numerous
    times so that we have enough data points to perform analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The tests should be run twice at a minimum, where the initial run identifies
    a bottleneck, then a change is made with the intent to eliminate the bottleneck,
    and finally, another run proves the bottleneck no longer exists, as indicated
    by metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Realistically, another bottleneck will emerge after the biggest one is eliminated.
    The performance landscape will change for every change that’s made to improve
    performance. The iteration can end when the objectives are completed, or a new
    problem may be discovered during the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The iterative execution of a performance test can be seen in *Figure 12**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – A sample workflow of performance testing](img/B21737_12_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – A sample workflow of performance testing
  prefs: []
  type: TYPE_NORMAL
- en: In each run, the test is executed by running the test script. The test script
    usually starts with warm-up operations. For example, if we’re going to send 10,000
    requests 100 times, the first 10 times can be treated as a warm-up, so the metrics
    aren’t considered.
  prefs: []
  type: TYPE_NORMAL
- en: Warming up allows the system to reach a stable state before actual performance
    measurements are taken. Processing initial requests triggers the cache so that
    it populates with frequently accessed data. It also allows the system to allocate
    resources such as thread, memory, and database connections effectively. Other
    transient factors such as just-in-time compilation, garbage collection, and resource
    contention can be reduced by warming up the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'After running the test, a report should be generated from the metric data that’s
    been collected. The report should be constructed in a format that allows iterations
    to be compared. The raw data is then analyzed to produce statistical figures such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The mean and median percentiles of the response time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average and peak throughputs; increase in throughput divided by the increase
    in resource which indicates scaling efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The overall error rate and error rates by types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Average and maximum latency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of concurrent users being handled without degradation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The time for which the load was maintained
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From these numbers, some bottlenecks may be identified. Some figures can be
    below the non-functional requirements, SLAs, or SLOs. Some figures can stand out
    as particularly slow compared to others. These bottlenecks drive the changes required
    to improve overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Especially in early iterations, deficiencies in the test scripts may be identified.
    It isn’t uncommon to realize the test script itself isn’t efficient and causes
    slowness in the system. The test script may have unnecessary loops or complex
    logic that slows down the script’s execution time. The test script may have an
    artificial wait time between requests, which as a result limits the throughput.
    Other factors, such as error handling, synchronous operations, resource contention,
    and network performance, can also skew the result of performance tests. These
    findings lead to the test script being reviewed and updated for future runs.
  prefs: []
  type: TYPE_NORMAL
- en: After changes are made, the performance tests should be executed again to examine
    whether the target performance statistical figures have improved, and at the same,
    to ensure the changes don’t deteriorate system performance in other areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'This repetitive exercise carries on until we’re satisfied with the results.
    There are several possibilities where the iteration should stop:'
  prefs: []
  type: TYPE_NORMAL
- en: The test objectives have been completed – for example, we’ve detected the maximum
    number of requests the system can handle without performance degradation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The performance metrics have fulfilled the non-functional requirements, SLAs,
    or SLOs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The time spent on performance testing has exceeded the original time-boxed duration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performance testing is meant to be a recurring exercise. A successful and satisfactory
    performance test only supports the assumption that the system is capable of handling
    requests within the configuration and parameters in the test scripts. The system
    usage pattern is constantly changing due to business growth and new features being
    introduced over time.
  prefs: []
  type: TYPE_NORMAL
- en: Benefits of performance testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance testing provides insights into how the system performs under pre-configured
    loads. It indicates how we can optimize the system to provide a fast and reliable
    user experience, even under heavy load. It helps stakeholders understand system
    limits and make informed decisions about scaling and infrastructure. It also identifies
    potential issues before they impact users, reducing the risk of downtime or service
    degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests are essential for ensuring that applications meet user expectations
    and maintain stability under varying conditions. By conducting different types
    of performance tests, organizations can identify and address potential issues,
    optimize performance, and enhance overall user satisfaction.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll consider a technique that’s used iteratively to measure a function’s
    performance. This technique is called micro-benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-benchmarking
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While performance testing focuses on system-level performance, micro-benchmarking
    is a performance measurement of a small and isolated piece of code at the function
    level. Micro-benchmarking is usually applicable to the following areas:'
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm that sits in the core of the whole system – for example, a search
    algorithm for an internet search engine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function that’s used most frequently by end users
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The function that’s exposed as an API to external systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code path that’s mission-critical and performance-sensitive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When comparing the implementations of a function, an algorithm, or a code change
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kotlin benchmarking** ([https://github.com/Kotlin/kotlinx-benchmark](https://github.com/Kotlin/kotlinx-benchmark))
    is the most popular tool for running benchmarks for Kotlin code. It wraps the
    classic **Java Microbenchmark Harness** (**JMH**) framework, and it supports Kotlin
    through **Java Virtual Machine** (**JVM**), JS, Native, and even **Web** **Assembly**
    (**WASM**).'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up micro-benchmarking with a Gradle Kotlin DSL script
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It’s simple to set up benchmarking with a Gradle Kotlin DSL script. For example,
    for JVM, we need the following plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The first plugin is for Kotlin micro-benchmarking, while the second plugin
    is used to open the final Kotlin classes for instrumentation. Now, we need to
    make sure the plugins and dependencies can be looked up from repositories:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, a code dependency on Kotlin micro-benchmarking needs to be declared:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to configure the `allOpen` plugin so that it only opens Kotlin
    classes with the `State` annotation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The final part of the setup is setting up micro-benchmarking itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The configuration is called `main` and has been chosen to run. It’s possible
    to configure the number of warmup iterations, the number of iterations to be measured,
    and the length of how long each iteration should last. However, an annotation-based
    configuration has been used in this example.
  prefs: []
  type: TYPE_NORMAL
- en: The micro-benchmarking test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The actual benchmark runner code is annotated so that it can be picked by the
    runner for execution with a specific configuration. Please note that this test
    should be placed in the `main` source folder (not the `test` source folder) so
    that it can be picked up by the plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This micro-benchmarking test evaluates the performance of the function that
    combines two `State` annotation triggers the `allOpen` plugin to open this class
    for instrumentation. Then, the `Fork` annotation defines how many threads are
    used for execution. Other annotations specify the number of iterations for warmup,
    execution, and the duration of each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the `setup` annotation function is used to create the data required
    to run the test, while the `combineUUIDBenchmark` function, which has the `Benchmark`
    annotation, is the major function to be measured.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-benchmarking runner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To run micro-benchmarking, we can use the following Gradle command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The summary of the result is printed to the console, while the detailed report
    is generated under the `/``build/reports/benchmarks/main` folder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The format of micro-benchmarking is designed to compare runs. Improvements can
    be made between runs, and the next run should demonstrate whether the changes
    have made a difference.
  prefs: []
  type: TYPE_NORMAL
- en: Micro-benchmarking is a valuable subset of performance testing that focuses
    on code implementation. By understanding the performance characteristics of isolated
    functions, engineers can make targeted optimizations. In contrast, performance
    testing takes a holistic approach to assess how well the entire system performs
    under various conditions. Both practices are essential for delivering high-performance
    systems.
  prefs: []
  type: TYPE_NORMAL
- en: There’s another tool that measures and analyzes the performance of an application,
    but visually with graphical user interfaces. This tool is called the application
    profiler, and we’re going to cover it in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Application profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Profiling works by monitoring and analyzing the performance of an application
    at runtime. Profilers instrument code and intercept calls to collect performance
    measurements, such as elapsed time and the number of invocations. It can generate
    the stack trace of the application to visualize relationships between functions.
  prefs: []
  type: TYPE_NORMAL
- en: The profiler tool also monitors memory allocation and deallocation, analyzes
    the heap dump, and identifies potential memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: At the same time, the profiler tool measures CPU cycles that have been consumed
    by various parts of the code and identifies computing-intensive functions. The
    profiler tool also monitors the usage of other resources, such as file operations,
    network activities, and interactions, among threads to provide a comprehensive
    view of resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: The profiler tool comes with detailed reports that are visualized in the user
    interface to assist engineers in pinpointing the areas that require optimization.
  prefs: []
  type: TYPE_NORMAL
- en: However, running an application with the profiler significantly slows down performance
    due to invasive instrumentation and measurement. The metric data that’s captured
    should be treated as a magnification of the actual runtime and be used to find
    areas that are slow, inefficient, or resource-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several popular profiler tools available for Kotlin engineers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**YourKit Java** **Profiler** ([https://www.yourkit.com/java/profiler/](https://www.yourkit.com/java/profiler/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VisualVM** ([https://visualvm.github.io/startupprofiler.html](https://visualvm.github.io/startupprofiler.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IntelliJ IDEA** **Profiler** ([https://www.jetbrains.com/pages/intellij-idea-profiler/](https://www.jetbrains.com/pages/intellij-idea-profiler/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JProfiler** ([https://www.ej-technologies.com/jprofiler](https://www.ej-technologies.com/jprofiler))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Async** **Profiler** ([https://github.com/async-profiler/async-profiler](https://github.com/async-profiler/async-profiler))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Java Mission** **Control** ([https://www.oracle.com/java/technologies/jdk-mission-control.html](https://www.oracle.com/java/technologies/jdk-mission-control.html))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application profilers should be used to analyze performance-critical operations.
    They don’t usually run in production environments due to instrumentation being
    slowed down significantly. It’s common to run profilers in a lower environment
    with inputs simulating the production environment.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’re going to cover a few performance improvement strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Strategies for performance improvement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving the performance of a system often requires a diverse approach that
    addresses various aspects. No silver bullet magically boosts performance. However,
    some common strategies help engineers navigate the problem to meet the non-functional
    requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Testing, testing, testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Performance tests should be conducted continuously and repetitively. When there’s
    a perceived performance issue, it’s unlikely to know the root cause without running
    performance tests. Instead of blindly applying “performance fixes,” engineers
    should execute performance tests to understand the problem first.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests should be treated as both troubleshooting and discovery tools.
    There are always bottlenecks in the system that surprise engineers.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding expensive operations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: More often than not, performance issues are caused by a mismatch between the
    nature of the operations and the actual implementation. In other words, resources
    are used in unnecessary areas that would use excessive resources and computation
    power. If excessive resources are spent on expensive operations, then there will
    be performance issues.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider an example scenario that demonstrates performance optimization
    by avoiding expensive operations.
  prefs: []
  type: TYPE_NORMAL
- en: Scenario – iteration on expensive operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine that there’s a function that’s expensive to execute. This expense is
    high for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s a remote synchronous call to another application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s computationally expensive and/or resource-hungry
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It involves files, databases, messaging, networks, or other resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It may be blocked until a result comes back
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We know the following function isn’t expensive, but let’s pretend it is for
    the sake of discussion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'On top of this function, we’d like to run some filtering, mapping, and selection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First, this piece of code filters to take only numbers greater than 3\. Then,
    it invokes the `expensive` function and gets a new number. At the end, only the
    first two numbers are selected. The `println` function is called to show which
    value is evaluated in the `filter`, `map`, or `take` function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing this piece of code produces the following console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: All five numbers are evaluated if they’re greater than 3\. The numbers `7`,
    `23`, and `63` are greater than 3, so they’re passed to the `expensive` operation.
    Finally, only the first two numbers from the expensive operation are returned.
  prefs: []
  type: TYPE_NORMAL
- en: The expensive operation for the third number isn’t necessary because only the
    first two numbers are selected at the end. In addition, it could have found the
    first two numbers during filtering and stopped checking the rest of the values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimized with the `asSequence` function from Kotlin Standard Library, the
    code looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'However, executing the preceding code prints the following to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'No filtering, no expensive operation, or selection was run. This is because
    the `asSequence` function doesn’t build the list until there’s a terminal function.
    Let’s update the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the execution prints the following to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The sequence operation understands it only takes the first two numbers, so it
    looks for the first two numbers greater than 3 and stops there. The number `63`
    wasn’t even processed. The first number greater than 3 was `7`, so `7` was passed
    to the `expensive` operation. The second number greater than 3 was `23`, so `23`
    was also passed to the `expensive` operation. This implementation has saved one
    `expensive` operation compared to the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: An example of a performance improvement journey
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The households in the village have decided to run a survey to rate each household’s
    services. A vote consists of a rating from 1 to 3:'
  prefs: []
  type: TYPE_NORMAL
- en: '**1**: Good'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**2**: Average'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**3**: Poor'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A household can vote for all other households, but only one vote can be made
    per household. Households are given 1 day to submit all the votes. Let’s also
    assume one household provides only one service.
  prefs: []
  type: TYPE_NORMAL
- en: Each household has a “score,” which is the sum of the rank numbers of all votes
    to that household. The household that has the highest score becomes the household
    that provides the best service in the village.
  prefs: []
  type: TYPE_NORMAL
- en: So, if there are *n* households in the village, the maximum number of votes
    will be `n x (n- 1`). We need a system that calculates the score of all households
    being voted for, and that records all votes as audit records. The system also
    needs to display non-final scores for each household when voting is in progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simplistic architecture of this voting system may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Simulated survey architecture v0.1](img/B21737_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Simulated survey architecture v0.1
  prefs: []
  type: TYPE_NORMAL
- en: 'All households submit their votes to be validated by **Voting Service**. The
    service validates the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: All households involved are valid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A household can’t vote for itself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A household can only vote for another household once
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The vote has a valid rank
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the voting happens in one day for the village, there’s a need for the
    system to respond quickly (**latency**) so that it can process many votes within
    a certain period (**throughput**) and support a vast number of households (**scalability**).
  prefs: []
  type: TYPE_NORMAL
- en: The system expects many concurrent requests to **Voting Service** at a time,
    and that could cause a spike.
  prefs: []
  type: TYPE_NORMAL
- en: It’s possible to scale up the service vertically by adding more resources (CPU,
    memory, and so on). However, there are physical limitations regarding the number
    of CPU sockets or the maximum RAM it can support. Adding more resources also leads
    to diminishing returns, in which performance doesn’t improve proportionally due
    to other bottlenecks. The only running instance is also the single point of failure
    that if this instance fails, the entire system becomes unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, the system can scale out horizontally if we add more instances
    of the service. A **load balancer** can be deployed to distribute load across
    multiple instances of the service, preventing any single instance from becoming
    a bottleneck. This significantly increases throughput by enabling parallel processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The load balancer has some knowledge of the load of each instance, so it can
    route the next request to the instance with the least load. This allows us to
    add more instances to handle increased loads. With that, the architecture has
    changed, as shown in *Figure 12**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Simulated survey architecture v0.2](img/B21737_12_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Simulated survey architecture v0.2
  prefs: []
  type: TYPE_NORMAL
- en: Now, **Voting Service** has two stateful validation rules. The first is that
    the households involved must be valid. The second is that each household can only
    vote for other households once.
  prefs: []
  type: TYPE_NORMAL
- en: The household records are frequently accessed, and they can be queried in the
    database remotely. **Caching** all household records in each service instance
    is a sensible strategy to speed up validation.
  prefs: []
  type: TYPE_NORMAL
- en: Enforcing the rule that one household can only vote for another household would
    benefit from caching. If we cache a list of households that a given household
    has voted for (the *x-voted-by-y-list*), then we can enforce this business rule.
    However, there’s a complication if any instance can handle any household because
    it implies sharing the list.
  prefs: []
  type: TYPE_NORMAL
- en: There are two options we can consider. The first option is that we can use a
    distributed in-memory database such as Redis so the *x-voted-by-y-lists* can be
    shared, at the cost of having a distributed in-memory database and potential resource
    contention.
  prefs: []
  type: TYPE_NORMAL
- en: The second option is to configure the load balancer so that it supports sticky
    routing. Requests from one household always go to the one responsible instance.
    Each instance knows its assignment and can locally cache the *x-voted-by-y-lists*
    from the database during startup. The local cache is also updated as it processes
    incoming requests.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, the bottleneck has shifted to the database since all the traffic
    is eventually funneled into it and each request can only be responded to after
    database operations have finished. This impacts the latency of the response to
    each voting request.
  prefs: []
  type: TYPE_NORMAL
- en: The score needs to be calculated for each household being voted in. This is
    an accumulated number that leaves little room for parallel processing. Each validated
    vote also needs to be kept as an audit record in the database.
  prefs: []
  type: TYPE_NORMAL
- en: However, votes validated by **Voting Service** can be further processed asynchronously.
    Each vote can be partitioned by the household being voted for, so if **Household
    1** votes for **Household 2**, the vote goes to the “bucket” for **Household 2**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Resolve a household to a bucket can be as simple as a modulo function, that
    is, the remainder of a hash number divided by the number of buckets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Each bucket is an event stream. **Voting Service** can respond to the voting
    request after an event is published to the corresponding event topic representing
    the bucket. The vote counting, score calculation, and vote persistence metrics
    will be processed when downstream components consume the event. This change will
    significantly reduce the latency of each voting request.
  prefs: []
  type: TYPE_NORMAL
- en: 'The updated architecture looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Simulated survey architecture v0.3](img/B21737_12_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Simulated survey architecture v0.3
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach has a limitation: the number of buckets that are fixed at runtime.
    Events are already routed to the bucket, and we need to maintain the bucket assignment
    to calculate the score correctly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The downstream operations require persisting the result in the database eventually,
    and we want to avoid overloading the database. Let’s examine the data to be persisted:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Score for each household**: One accumulative number per household; historical
    numbers don’t matter'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vote audit record**: Each record needs to be kept, and each record is independent
    of each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The score numbers and vote audit records are different in their natures, so
    it makes sense for them to be processed differently. It’s better to keep this
    temporary data in a transient local cache to reduce database load but persist
    the values periodically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we can introduce two components:'
  prefs: []
  type: TYPE_NORMAL
- en: The first one, **Vote Counter**, consumes the event stream for its one assigned
    bucket and calculates scores for its responsible households. It doesn’t update
    the score records in the database immediately. Instead, it flushes the latest
    scores to the database with a fixed schedule – for example, every 10 minutes.
    This mechanism “soaks” the spike of votes and turns it into regular updates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are multiple instances of **Vote Counter**, and there should be at least
    two instances consuming one bucket to provide availability. Each score record
    should consist of a household name, the number of votes, the score, and a timestamp.
    There should be de-duplication rules that only persist newer records and skip
    the old ones.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second component, **Vote Journalist**, consumes a batch of events at a time
    and flushes the update into the database in one transaction. If the transaction
    fails, the events in the batch aren’t acknowledged and will be processed again
    later. **Vote Journalist** instances of the same bucket should be configured so
    that only one instance receives the batch of events. The batching processing significantly
    increases the throughput of vote audit record persistence. However, it would require
    performance testing to discover the optimal batch size that can scale with the
    number of votes and still be processed within the memory limits of the process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With all these performance concerns considered, we have the final 1.0 architecture,
    as shown in *Figure 12**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Simulated survey architecture v1.0](img/B21737_12_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Simulated survey architecture v1.0
  prefs: []
  type: TYPE_NORMAL
- en: In this architecture, we’ve optimized the process of load balancing the incoming
    requests to multiple instances of **Voting Service** for validation. This increases
    the throughput and scalability. Then, we introduced local caching of households
    and the *x-voted-by-y-lists* in each service instance to speed up the validation
    process. It also supports horizontal scaling by adding more instances.
  prefs: []
  type: TYPE_NORMAL
- en: 'After, we created a few buckets of event streams for each one responsible for
    several households exclusively. After **Voting Service** validates a request to
    be valid, it responds to the original request and publishes an event stream to
    the corresponding bucket. This reduces the latency for the response to the voting
    request:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vote Counter** is introduced to calculate the scores of households that have
    been assigned to the given bucket. It sends the latest scores to the database
    periodically and soaks up the spike.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vote Journalist** is introduced to receive a batch of events at a time and
    to persist them to the database in one transaction. Batch processing increases
    the throughput of vote audit record persistence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this example, we learned how to optimize the throughput, latency, and scalability
    of a system. Performance improvements are highly situational. We simply shouldn’t
    copy a pattern to another system and believe it will perform. Performance needs
    to be measured and tested. A change is only considered a performance improvement
    when the metrics prove it. However, some known best practices of performance can
    be improved, something we’ll cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices of performance in Kotlin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kotlin has a few features that aim to reduce overhead and therefore improve
    performance. However, these features come with justifications so that engineers
    can make a conscious decision when to use them. They aren’t expected to be used
    everywhere without reasoning.
  prefs: []
  type: TYPE_NORMAL
- en: In-line functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In-line Kotlin functions are simply copied to the caller to reduce the need
    to invoke the function itself. This is particularly useful if there’s a deep stack
    of functions or if there’s a higher-order function.
  prefs: []
  type: TYPE_NORMAL
- en: 'In-line functions can be declared by adding a modifier at the function level,
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The use of immutable and mutable data structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Immutable data eliminates the need for locking in multi-thread environments.
    In Kotlin, `List`, `Set`, and `Map` collections work with immutable data.
  prefs: []
  type: TYPE_NORMAL
- en: However, if we’re building a bigger object, such as a string, it’s advisable
    to use mutable collections or the `StringBuilder` class to avoid unnecessary object
    creation that could trigger garbage collection in Kotlin/JVM.
  prefs: []
  type: TYPE_NORMAL
- en: The use of coroutines for asynchronous operations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kotlin’s coroutine library enables the program to invoke asynchronous operations
    so that the thread isn’t blocked and can perform other operations while waiting
    for the asynchronous result to come back. It enables better resource management
    and quicker response in applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, imagine that there are two time-consuming functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'These two functions have the `suspend` modifier to indicate they can be paused
    and resumed without blocking the thread. The `main` function that uses these two
    suspend functions is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `runBlocking` function starts a coroutine that blocks the current thread
    until its execution is complete. Within this block, there are two `async` functions
    to invoke the two time-consuming `suspend` functions. The `async` function returns
    a `Deferred` object on which we invoke the `await` function to block until the
    result returns. The two numbers that are returned by the respective time-consuming
    functions are added and the sum is printed to the console.
  prefs: []
  type: TYPE_NORMAL
- en: Note that Kotlin code written with the best practices of performance in mind
    would still need to be measured. Empirical results are the only proof.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’re going to briefly mention ultra-low latency systems and how they
    push performance and scalability to the extreme.
  prefs: []
  type: TYPE_NORMAL
- en: Ultra-low latency systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ultra-low latency systems operate in the microsecond or nanosecond magnitude.
    They run in an environment where low response time is critical and even essential.
    They can be seen in financial trading, telecommunications, gaming, and industrial
    automation.
  prefs: []
  type: TYPE_NORMAL
- en: These systems aim to achieve the lowest possible latency, highest efficiency,
    high predictability, and high concurrency regarding processing. They involve all
    aspects of a system to reduce response time, such as network optimization, hardware
    acceleration, load balancing, and efficient algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: These systems are usually written in system-level programming languages such
    as C++ and Rust. However, there are a few ultra-low latency systems written in
    Kotlin or Java that operate in the microsecond magnitude.
  prefs: []
  type: TYPE_NORMAL
- en: 'The low latency systems in Kotlin or Java employ several technical designs
    that aren’t as common:'
  prefs: []
  type: TYPE_NORMAL
- en: Reuse objects, avoid object creation, and avoid garbage collection.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use specific JVM vendors for better performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoid the use of third-party libraries to reduce overhead and ensure you have
    full control over performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the **Disruptor pattern** as it provides a large ring buffer for inter-thread
    lock-free communication and memory barriers for data visibility in a thread.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a single-thread model for each JVM process to reduce context switching,
    lock contention, and the need for synchronous and concurrency processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Write code or design systems that are aware of and optimized for the underlying
    hardware and network infrastructure they run on. This is also called **mechanical
    sympathy**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultra-low latency systems have the justification that they can break a few design
    principles (for example, immutable objects) in exchange for higher performance.
    They are exceptional cases due to the demanding need for low latency, high throughput,
    and quick response time. When developing these systems, performance tests are
    critical and should be part of the normal development activities.
  prefs: []
  type: TYPE_NORMAL
- en: 'Developing ultra-low latency systems is a specialized topic whose content is
    beyond the scope of this chapter. However, there are a few pieces of reading material
    that you may find useful:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Mechanical empathy*, by Martin Thompson ([https://mechanical-sympathy.blogspot.com/](https://mechanical-sympathy.blogspot.com/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*LMAX* *Disruptor* ([https://lmax-exchange.github.io/disruptor/](https://lmax-exchange.github.io/disruptor/))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Simple Binary* *Encoding* ([https://github.com/real-logic/simple-binary-encoding](https://github.com/real-logic/simple-binary-encoding))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Aeron* *messaging* ([https://github.com/real-logic/aeron](https://github.com/real-logic/aeron))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered different dimensions of performance and scalability
    and mentioned a few essential metrics that measure how well a system performs
    and scales. We emphasized the importance of performance tests, several types of
    performance tests, and how to plan one. We also provided an example of micro-benchmarking
    in Kotlin before discussing the use of a profiler to achieve better performance.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we delved into some strategies for performance improvement. We considered
    a scenario where only necessary expensive operations were executed. We also looked
    at an example of a journey of performance improvement for a system in a real-life
    situation. This allowed us to consider a few best practices regarding performance
    in Kotlin through code examples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we briefly introduced ultra-low latency systems and where they can
    be used.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to discuss the topic of software testing.
  prefs: []
  type: TYPE_NORMAL
