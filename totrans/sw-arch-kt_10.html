<html><head></head><body><div><div><div><h1 id="_idParaDest-240" class="chapter-number"><a id="_idTextAnchor334"/>10</h1>
			<h1 id="_idParaDest-241"><a id="_idTextAnchor335"/>Idempotency, Replication, and Recovery Models</h1>
			<p>Distributed systems are very common in modern software architectures. The challenges of ensuring data consistency, fault tolerance, and availability become critical. This chapter is going to cover three key concepts that help address these challenges:</p>
			<ul>
				<li>Idempotency</li>
				<li>Replication</li>
				<li>Recovery models</li>
			</ul>
			<p><strong class="bold">Idempotency</strong> is a fundamental<a id="_idIndexMarker821"/> non-functional system property that ensures operations can be executed safely and repeatedly without causing unintended side effects. In a distributed system, network failures and system crashes are common. Idempotency is essential for maintaining data integrity and consistency. By designing operations to be idempotent, engineers can build more resilient and fault-tolerant systems that can recover from partial failures without compromising the overall system state.</p>
			<p><strong class="bold">Replication</strong>, on the other <a id="_idIndexMarker822"/>hand, is a technique that’s used to improve the availability and durability of data in distributed systems. By maintaining multiple copies of data across different nodes, replication provides redundancy and helps ensure that the system can continue to operate even if one or more nodes fail. However, replication introduces its own set of challenges, such as ensuring consistency between replicas and efficiently managing the replication process.</p>
			<p>Finally, <strong class="bold">recovery models</strong> define<a id="_idIndexMarker823"/> the strategies and mechanisms that are used to restore the state of a distributed system after a failure or disruption. These models can range from simple backup-and-restore approaches to more sophisticated techniques. Choosing the right recovery model is crucial for building resilient distributed systems that can weather unexpected events and maintain high levels of availability and responsiveness.</p>
			<p>In this chapter, we’ll explore each of these topics in greater depth, discussing their underlying principles, trade-offs, and best practices for applying them in real-world distributed applications. After this chapter, you should be able to implement idempotency, replication, and recovery models at a level suitable for your system.</p>
			<h1 id="_idParaDest-242"><a id="_idTextAnchor336"/>Technical requirements</h1>
			<p>You can find the code files used in this chapter on GitHub: <a href="https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-10">https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-10</a></p>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor337"/>Idempotency</h1>
			<p>Idempotency is a <a id="_idIndexMarker824"/>concept in software engineering that refers to the non-functional property of operations that can be performed multiple times while still having the same effect as performing it only once. In other words, an idempotent operation can be safely repeated without side effects. Let’s cover a short scenario where idempotency is required.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor338"/>A use case where idempotency is required</h2>
			<p>Imagine that we’re<a id="_idIndexMarker825"/> building an online banking application. A key capability is <strong class="bold">Transfer Funds</strong>, in which a user transfers money from one account to another. This capability is a fundamental yet critical part of the system, and it needs to be implemented in a way that ensures the integrity and reliability of the user’s financial transactions.</p>
			<p>If the <strong class="bold">Transfer Funds</strong> operation isn’t idempotent, then the user could accidentally click the <strong class="bold">Transfer</strong> button multiple times, and the system would execute the transfer operation multiple times, resulting in an unintended debit from the source account and a corresponding credit to the destination account.</p>
			<p>Most mature user interfaces can avoid this situation by blocking the button once it’s pushed until a response is received. However, there are also API integrations that require idempotency.</p>
			<p>This result isn’t intended by the user, and it has multiple consequences. First, if the user has insufficient funds from the second and subsequent transfer, the user will have overdraft funds and be subject to interest charges. Second, these incidents trigger user complaints and can result in the potential involvement of financial regulatory bodies. Not only the user experience but also the unintended side effect results in reputational damage to the bank.</p>
			<p>To prevent these issues, the <strong class="bold">Transfer Funds</strong> operation should be designed to be idempotent. This means that no matter how many times the user clicks the <strong class="bold">Transfer Funds</strong> button, the <a id="_idIndexMarker826"/>system will only execute the transfer once, ensuring that the final state of the accounts is correct and matches the user’s intent.</p>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor339"/>Key aspects of idempotency</h2>
			<p>Idempotency is an <a id="_idIndexMarker827"/>important concept in software development, particularly in the context of distributed systems, APIs, and data processing pipelines. Here are some key aspects of idempotency:</p>
			<ul>
				<li><strong class="bold">Constant outcomes</strong>: An idempotent operation always produces the same result, regardless of how many times it’s executed. If an operation isn’t idempotent, each subsequent execution might produce a different outcome.</li>
				<li><strong class="bold">Error handling and retries</strong>: Idempotency helps in handling errors and retries gracefully. If an operation fails, the system can safely retry the operation without causing unintended side effects.</li>
				<li><strong class="bold">Data consistency</strong>: Idempotent operations ensure data consistency by preventing accidental data modifications or duplications, which can occur when retrying non-idempotent operations.</li>
				<li><strong class="bold">Scalability and reliability</strong>: Idempotency is crucial in distributed systems, where multiple instances of an application may be processing the same request concurrently. Idempotent operations allow the system to scale and handle failures without compromising data integrity.</li>
			</ul>
			<p>Let’s cover a few practical scenarios where idempotency can be applied.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor340"/>Scenario 1 – evolutionary database migration script</h2>
			<p>Evolutionary databases <a id="_idIndexMarker828"/>aim to create database systems that can evolve and adapt to changes over time. They aren’t defined by static and rigid models. The database schema is defined by incremental changes that build the target schema.</p>
			<p>Consider Flyway, an open source database migration tool. The incremental changes are specified by SQL scripts:</p>
			<pre class="source-code">
V1_create_new_tables.sql
V2_add_new_columns.sql</pre>			<p>For simplicity’s sake, let’s assume that the <code>V1</code> script only contains the following statement for creating a table:</p>
			<pre class="console">
CREATE TABLE HOUSEHOLD (
id UUID primary key,
name text not null
);</pre>			<p>The <code>CREATE</code> SQL statement will create a new table called <code>HOUSEHOLD</code> if it doesn’t already exist. Otherwise, an error will be reported and the <code>V1</code> script will fail. In other words, it isn’t idempotent, and repeated executions don’t have the same outcome. Here’s an idempotent version of the script:</p>
			<pre class="console">
CREATE TABLE IF NOT EXISTS HOUSEHOLD (
id UUID primary key,
name text not null
);</pre>			<p>The <code>IF NOT EXIST</code> syntax ensures the table is created if it doesn’t exist, or nothing is performed if the table already exists. The outcome is the same in either case, which is that the <code>HOUSEHOLD</code> table exists in the database.</p>
			<p>The execution of the <code>V2</code> script will add a new column to this table as a non-null column. Some database vendors support clever SQL statements that create a non-null column and populate values in the same statement. For the sake of this argument, let’s assume that this isn’t supported. We’ve resorted to the classic approach of adding a nullable column, populating the value, and then setting the column to non-null. Like the modified <code>V1</code> script, we can make it idempotent:</p>
			<pre class="console">
ALTER TABLE IF EXISTS HOUSEHOLD ADD COLUMN deleted boolean;
UPDATE HOUSEHOLD SET deleted = false;
ALTER TABLE IF EXISTS HOUSEHOLD ALTER COLUMN IF EXISTS deleted SET NOT NULL;
COMMIT;</pre>			<p>The <code>IF EXISTS</code> syntax ensures that the table or columns will be altered if they exist, or nothing is performed if they don’t. The outcome is the same and therefore it’s idempotent. The classic guideline would have suggested that the <code>ALTER TABLE</code> is DDL and <code>UPDATE</code> is DML. This<a id="_idIndexMarker830"/> was suggested because DDL is immediately committed while DML requires an explicit commit. However, with idempotency, this is no longer an issue as each statement can be repeated to produce the same outcome.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor341"/>Scenario 2 – create/update operations</h2>
			<p>Using the real-life example <a id="_idIndexMarker831"/>of villagers exchanging services, there’s a business case to ensure households are kept in the system record. However, the household users don’t know if the household has already been persisted.</p>
			<p>A CRUD-based system may define create and update as two independent operations. These operations are well-suited as users want the household records to persist regardless of whether they exist. There could have been a network outage, so users may not know if their previous requests were successful.</p>
			<p>In other words, users want an operation that can be repeated and yet generate the same outcome. They need an idempotent operation to ensure the household records have been stored, despite whether they already exist.</p>
			<p>This operation is often referred <a id="_idIndexMarker832"/>to as <strong class="bold">upsert</strong>, which means <strong class="bold">update or insert</strong>. The key characteristics of an upsert operation are as follows:</p>
			<ol>
				<li><strong class="bold">Idempotent</strong>: It can be executed repeatedly with the same outcome. If the record already exists, the record is updated; if the record doesn’t exist, the record is created.</li>
				<li><strong class="bold">Atomic</strong>: The operation is executed in a transaction of serialized isolation. This means the operation was either completed or it didn’t happen.</li>
				<li><strong class="bold">Option 1 – pessimistic</strong>: The pessimistic approach would check if the record already exists or not to determine whether it’s an update or a create operation.</li>
				<li><strong class="bold">Option 2 – optimistic</strong>: The optimistic approach would assume the record either exists or not and perform an update or create operation, respectively. If the update operation hasn’t found the record, it switches to the create operation. Alternatively, if the create operation fails due to a unique constraint violation, it switches to the update operation.</li>
			</ol>
			<p>Here’s an example of <a id="_idIndexMarker833"/>the upsert operation for a household in an SQL statement. It’s implementing the optimistic approach:</p>
			<pre class="console">
INSERT INTO HOUSEHOLD (id, name, email) VALUES ('d0275532-1a0a-4787-a079-b1292ad4aadf', 'Whittington', 'info@ whittington'.com') ON DUPLICATE KEY UPDATE name = 'Whittington', email = 'info@ whittington'.com';</pre>			<p>This SQL statement attempts to insert a new household record. If the record doesn’t exist, a new row is inserted. If the execution hits a duplicate key violation, it becomes an update operation to <code>name</code> and <code>email</code>.</p>
			<p>If this operation is exposed as an external API – that is, as a REST endpoint – the contract can be expressed in the following ways:</p>
			<ol>
				<li><code>GET</code>: Multiple invocations of the <code>GET</code> endpoint shall return the same result, given the system state remains unchanged.</li>
				<li><code>PUT</code>: The <code>PUT</code> endpoint implies creating a new resource or replacing a representation of the household with the request payload.</li>
				<li><code>DELETE</code>: The <code>DELETE</code> endpoint intends to remove the resource, regardless of whether it already exists. If the resource isn’t found, then it should return a successful <strong class="bold">Hypertext Transfer Protocol</strong> (<strong class="bold">HTTP</strong>) status<a id="_idIndexMarker834"/> code.</li>
			</ol>
			<p>The HTTP method itself doesn’t bring idempotency. For example, if the response payload of the <code>GET</code> endpoint contains the current time or random values, then multiple invocations don’t return the same result, and therefore it’s not idempotent.</p>
			<p>The <code>POST</code> and <code>PATCH</code> endpoints aren’t defined as idempotent. The <code>POST</code> endpoint in REST architecture implies the request to create a resource and assumes the resource was absent. The <code>PATCH</code> endpoint assumes the resource already exists so that the resource can be partially <a id="_idIndexMarker835"/>updated.</p>
			<p class="callout-heading">HTTP methods</p>
			<p class="callout">HTTP defines a few methods to categorize the request to perform an action on a resource. The <code>GET</code> method is a read-only operation that returns data from the server. The <code>POST</code> method creates resources in a server. The <code>PUT</code> method replaces or creates a resource. The <code>PATCH</code> method partially updates an existing resource. The <code>DELETE</code> method removes a resource from the server. The <code>HEAD</code> method returns the header of the resource without the body content. The <code>OPTIONS</code> method describes the options to communicate with the specific resource. Finally, the <code>TRACE</code> method is a diagnosis operation that echoes the final receipt of the request to provide information for troubleshooting.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor342"/>Scenario 3 – processing events in sequence</h2>
			<p>Something that consumes events from a stream or a topic usually takes an event one at a time and processes them sequentially. If the sequence of events that’s processed is important, then there’s a need to gracefully process events in the face of duplication and out of sequence.</p>
			<p>There are two levels where an event sequence could be compromised. The first level is the transport level, where the offset of the last consumed event is reset to older events due to network issues, partition changes, or consumer group changes. The second level is the application level and is where the publisher has sent older events.</p>
			<p>Application-level deduplication at the consumer level could handle event sequences being compromised at the transport or application level. However, that would require publishers to provide sequential information on each event. This could be a sequence number on the event or a timestamp where an event occurred.</p>
			<p>The consumer can maintain the last processed sequence number or timestamp per publisher. If the consumer receives an event where the sequence number is lower than the last to be processed, or where the timestamp is older than the last to be processed, then the consumer skips this event until a newer event is received.</p>
			<p>Here’s an example implementation<a id="_idIndexMarker836"/> of an event listener that prevents older events from being processed:</p>
			<pre class="source-code">
class HouseholdEventListener {
    var lastProcessedTime: Instant? = null
    @KafkaListener(
        topics = ["\${household-v1-topic}"],
        clientIdPrefix = "\${client-id}",
        groupId = "\${group-id}",
        containerFactory = "kafkaListenerContainerFactory",
        properties = ["auto.offset.reset=earliest"]
    )
    fun onMessage(
        @Payload(required = false) event: HouseholdEvent?,
        @Header(name = "kafka_eventTime", required = true) key: String,
    ) {
        if (lastProcessedTime != null &amp;&amp; event?.time?.isBefore(lastProcessedTime) == true) {
            log.warn { "Skipping event with time ${event.time} because it is before the last processed time $lastProcessedTime" }
            return
        }
        // some processing logic here
        lastProcessedTime = event?.time
    }
}</pre>			<p>Here, <code>HouseholdEventListener</code> keeps the timestamp of the last processed event. The incoming events from Kafka have a header field, <code>kafka_eventTime</code>, that’s provided by the publisher. The value is when the event occurred, not when the event was published.</p>
			<p>The first event process wouldn’t perform any timestamp check. Subsequently, the listener would skip processing if the event timestamp from the header is earlier than the last processed timestamp. This indicates that the incoming event is old and can be skipped.</p>
			<p>If the event isn’t skipped and has finished processing, the last processed timestamp is updated, and the event is acknowledged by the Kafka broker. The listener is now ready to consume another event.</p>
			<p>In a production system, the last processed time should be persisted in the database and be in the same transaction where business processing takes place. The last processed time should be restored when the listener starts. This would allow the listener to resume its consumption of events after a restart.</p>
			<p>This implementation<a id="_idIndexMarker837"/> illustrates how a consumer can detect an older event with the help of the publisher. The older event isn’t processed, and the consumer can keep the last processed timestamp as an offset to verify the next event.</p>
			<p>To extend to this example, the timestamp of the last processed event can be persisted in a database so that the value is restored after a restart.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor343"/>Scenario 4 – the multiple bounded context saga</h2>
			<p>A <strong class="bold">saga</strong> is a<a id="_idIndexMarker838"/> pattern in <strong class="bold">domain-driven design</strong> (<strong class="bold">DDD</strong>) that <a id="_idIndexMarker839"/>involves<a id="_idIndexMarker840"/> distributed transactions. The challenge is to maintain data consistency across multiple bounded contexts.</p>
			<p>Let’s use our bank transfer example, where we need idempotent operations to ensure the fund is only transferred once and only once. The banking phone application intends to send a request to the backend service.</p>
			<p>However, there are multiple backend services involved in this operation. First, there’s <strong class="bold">Transfer Service</strong>, which validates the request.</p>
			<p>Once validated, it needs to reserve the amount in the withdrawing account until the transfer has been completed. This is done by another service called <strong class="bold">Account Service</strong>.</p>
			<p><strong class="bold">Account Service</strong> orchestrates reserving funds by moving funds from a customer account to the corporate account. Later, it orchestrates adding funds by moving funds from the corporate account to a customer account. This is done by communicating with the legacy <strong class="bold">Core </strong><strong class="bold">Banking System</strong>.</p>
			<p>Once the funds have been <a id="_idIndexMarker841"/>reserved, <strong class="bold">Transfer Service</strong> can request the <a id="_idIndexMarker842"/>second part of the transfer by moving the funds from the corporate account to the customer account. The request is handled by <strong class="bold">Account Service</strong>, which communicates with the legacy <strong class="bold">Core Banking System</strong> to transfer the funds. Once this has been acknowledged and completed by <strong class="bold">Core Banking System</strong>, <strong class="bold">Account Service</strong> returns the result to <strong class="bold">Transfer Service</strong> and thus completes the transfer.</p>
			<p>This interaction is demonstrated in <em class="italic">Figure 10</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21737_10_1.jpg" alt="Figure 10.1 – Example sequence for a bank transfer" width="1120" height="1296"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Example sequence for a bank transfer</p>
			<p>Making the<a id="_idIndexMarker843"/> whole <a id="_idIndexMarker844"/>transfer operation idempotent is complex because transactions are distributed among services. Moreover, we need a way to identify that the user only wants the transfer funds once, despite multiple attempts from the banking application.</p>
			<p>Often, some parts of the system are legacy systems that may not be enhanced so easily. In this case, let’s assume <strong class="bold">Core Banking System</strong> can’t take the idempotency key in the request.</p>
			<p>Let’s explore how each<a id="_idIndexMarker845"/> component involved in this process can<a id="_idIndexMarker846"/> work toward idempotency.</p>
			<h3>Banking application</h3>
			<p>The first step should be<a id="_idIndexMarker847"/> the banking application <a id="_idIndexMarker848"/>generating an <strong class="bold">idempotency key</strong>, which can identify multiple attempts belonging to the same user intent. Ideally, the idempotency key should be carried to all services involved.</p>
			<h3>Transfer Service</h3>
			<p><strong class="bold">Transfer Service</strong> can cache <a id="_idIndexMarker849"/>these idempotency keys for a certain period. Within that period, the same idempotency keys are treated as duplicated requests.</p>
			<p>To avoid consistency issues under concurrent requests, many systems use explicit locks to ensure the requests of the same idempotency keys are processed only one at a time, across multiple instances.</p>
			<p>The service can decide to skip the remaining interactions with other services and return the response that was sent previously to the banking service. This approach is OK if we are sure the remaining services have acknowledged completion of the request.</p>
			<p>If, for example, there was a timeout when <strong class="bold">Transfer Service</strong> communicated with <strong class="bold">Account Service</strong>, then it may be sensible to repeat the interaction with <strong class="bold">Account Service</strong>. This allows the operation to be repaired and continue until completion. This approach also assumes that <strong class="bold">Account Service</strong> can handle duplicated requests in an idempotent manner.</p>
			<h3>Account Service</h3>
			<p>In this operation, <strong class="bold">Account Service</strong> provides<a id="_idIndexMarker850"/> two functionalities: reserve funds and add funds. To be able to identify duplicate requests, the idempotency keys should be persisted alongside the records related to holding and moving funds.</p>
			<p>When <strong class="bold">Account Service</strong> handles the requests of reserving or adding funds, it must check whether duplicate requests already exist by using the idempotency keys. If they do, <strong class="bold">Account Service</strong> returns the response from the records, as if it had been processed this time.</p>
			<p>If the reserve fund request is rejected by <strong class="bold">Core Banking System</strong> due to insufficient funds, <strong class="bold">Account Service</strong> needs to roll back the operation by reversing the fund back to the withdrawal account.</p>
			<p>Like <strong class="bold">Transfer Service</strong>, there should be some form of explicit locking to ensure only one request for a given<a id="_idIndexMarker851"/> idempotency key is processed at a time across multiple instances.</p>
			<h3>Core Banking System</h3>
			<p><strong class="bold">Core Banking System</strong> is a legacy<a id="_idIndexMarker852"/> system that doesn’t support idempotency. It isn’t able to take idempotency keys or process them. Since <strong class="bold">Account Service</strong> is the service that communicates with <strong class="bold">Core Banking System</strong>, <strong class="bold">Account Service</strong> should persist the response from <strong class="bold">Core Banking System</strong> together with the corresponding idempotency key.</p>
			<p>If the record of the response already exists with the idempotency key, <strong class="bold">Account Service</strong> skips communication with <strong class="bold">Core Banking System</strong> and uses the previously persisted response from <strong class="bold">Core Banking System</strong> to complete the process.</p>
			<p>This is getting complex as there could be a timed-out request for <strong class="bold">Core Banking System</strong>. <strong class="bold">Account Service</strong> doesn’t know whether <strong class="bold">Core Banking System</strong> has processed the transfer or not. <strong class="bold">Account Service</strong> would need to query the recent transaction history to identify the previous request to <strong class="bold">Core Banking System</strong>, either success or failure, to recover and resume the transfer operation. Otherwise, retrying may still result in an inconsistent state.</p>
			<p>Sometimes, this recovery may even involve manual correction, which is error-prone. You can see when a process can’t be idempotent as it becomes substantially more complex, inefficient, and expensive.</p>
			<p>With that, we’ve run through four scenarios where idempotency is required, and we’ve explored multiple approaches for these scenarios. Now, let’s delve into a concept related to idempotency – replication.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor344"/>Replication</h1>
			<p><strong class="bold">Replication</strong> serves as a<a id="_idIndexMarker853"/> safeguard against potential failures, allowing the system to maintain continuity of service even when individual components malfunction or become unavailable.</p>
			<p>This aspect of replication has a close relationship with recovery, which will be covered later in this chapter. In short, some replication techniques can prevent system downtime, which requires recovery. Also, some replication techniques enable and enhance recovery processes.</p>
			<p>Another aspect of replication is that it can improve system performance by distributing load to multiple nodes, as well as by allowing the system to scale based on traffic.</p>
			<p>The copy of the data or running instances is usually called a <em class="italic">replica</em>. There are many areas where replication is applicable. Let’s take a look.</p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor345"/>Data redundancy</h2>
			<p>Multiple replicas are<a id="_idIndexMarker854"/> distributed across different nodes or servers. If <a id="_idIndexMarker855"/>one node fails, the data can still be accessed from the replicated copies on other nodes. It also prevents data loss if some nodes become permanently unavailable.</p>
			<p>This redundancy ensures that the overall system can continue to function, even if some nodes or components are unavailable.</p>
			<p>This can apply to relational databases, NoSQL databases, durable message brokers, distributed object caches, and<a id="_idIndexMarker856"/> nodes in <strong class="bold">peer-to-peer</strong> (<strong class="bold">P2P</strong>) networks.</p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor346"/>Service redundancy</h2>
			<p>Having the running<a id="_idIndexMarker857"/> service instances of the system <a id="_idIndexMarker858"/>distributed and replicated brings a few key benefits.</p>
			<p>First, requests can be routed to the most available and responsive replica, reducing the risk of overloading a single node and improving overall system performance. This load balancing helps maintain availability by preventing bottlenecks and ensuring that the system can handle increased traffic or workloads.</p>
			<p>Second, it enables the system to scale out by adding more replicas or instances as demand increases. This horizontal scalability allows the system to handle higher loads and maintain availability as the number of requests or resources required grows.</p>
			<p>Moreover, if a primary node becomes unavailable, the system can automatically failover to a secondary or backup replica, ensuring a seamless transition.</p>
			<p>The secondary replica can take over the workload, maintaining service continuity and high availability.</p>
			<p>Replication also facilitates faster recovery as the system can restore services by promoting a healthy replica to become the new primary.</p>
			<p>It’s also common for data and services to be replicated across multiple geographical locations and data centers. This practice can improve availability in the event of regional failures or disasters. If one data center or region experiences an outage, the system can continue to operate<a id="_idIndexMarker859"/> using the replicas in other locations, ensuring <a id="_idIndexMarker860"/>that the service remains available to users.</p>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor347"/>CAP theorem</h2>
			<p>Let’s look at a <a id="_idIndexMarker861"/>couple of replication and recovery models that <a id="_idIndexMarker862"/>we should discuss. They cater to various levels of consistency, availability performance, and scalability non-functional requirements.</p>
			<p>According to the <strong class="bold">CAP theorem</strong>, also known<a id="_idIndexMarker863"/> as <strong class="bold">Brewer’s theorem</strong>, it’s impossible for a distributed system to provide all three of the following non-functional properties simultaneously:</p>
			<ul>
				<li><strong class="bold">Consistency (C)</strong>: All nodes in the system have the same data at the same time. Consistency ensures that the data is always in a valid state</li>
				<li><strong class="bold">Availability (A)</strong>: Every request receives a non-error response, but there’s no guarantee that it contains the most recent data</li>
				<li><strong class="bold">Partition tolerance (P)</strong>: The system continues to operate despite arbitrary message loss or failure of part of the system</li>
			</ul>
			<p>The theorem states that when communication between nodes fails, a distributed system can only satisfy two of the three properties (C, A, or P) at the <a id="_idIndexMarker864"/>same time. This is known as the <strong class="bold">CAP trade-off</strong>.</p>
			<p class="callout-heading">The history of the CAP theorem</p>
			<p class="callout">The CAP theorem was proposed by Eric Brewer in 2000 during the Symposium on <strong class="bold">Principles of Distributed Computing</strong> (<strong class="bold">PODC</strong>). The <a id="_idIndexMarker865"/>theorem was later proved by Seth Gilbert and Nancy Lynch of Massachusetts Institute of Technology in 2002, in their paper <em class="italic">Brewer’s Conjecture and the Feasibility of Consistent, Available, Partition-Tolerant </em><em class="italic">Web Services</em>.</p>
			<p>The three possible choices are as follows:</p>
			<ul>
				<li><strong class="bold">Consistency and partition tolerance (CP)</strong>: The <a id="_idIndexMarker866"/>system sacrifices availability to uphold strong consistency in the face of a network partition. This is common in traditional database systems, such as relational databases.</li>
				<li><strong class="bold">Availability and partition tolerance (AP)</strong>: The <a id="_idIndexMarker867"/>system remains available but forgoes maintaining consistency during network failure. This is common in NoSQL databases.</li>
				<li><strong class="bold">Consistency and availability (CA)</strong>: The system offers both consistency and availability, but <a id="_idIndexMarker868"/>this is only possible in a fully connected system with no network partitions. In practice, it rarely happens, and the system must choose between consistency and availability.</li>
			</ul>
			<p>Although there are <a id="_idIndexMarker869"/>three combinations, the choice is more fluid and<a id="_idIndexMarker870"/> situational. For example, a system may be initially AP, but as more nodes fail, it may fall back to a single node running with CA.</p>
			<p>The CAP theorem is a concept that helps developers understand the trade-offs they need to make when designing a distributed system. It’s an important consideration when you’re choosing the appropriate data storage and processing solutions for a particular application.</p>
			<p>When exploring these models, it’s important to understand and discover the non-functional properties your system should aim for. Not all models are suitable for all systems. It’s about<a id="_idIndexMarker871"/> finding the most suitable models based on your needs and anticipated scenarios.</p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor348"/>Model 1 – primary-secondary</h2>
			<p>The <strong class="bold">primary-secondary</strong> (also known<a id="_idIndexMarker872"/> as <strong class="bold">single-leader</strong>) replication <a id="_idIndexMarker873"/>has a <strong class="bold">Primary</strong> node (the “leader”) that handles all write operations and replicates data changes to the <strong class="bold">Secondary</strong> nodes (the “followers”). Single-leader replication is demonstrated in <em class="italic">Figure 10</em><em class="italic">.2</em>:</p>
			<div><div><img src="img/B21737_10_2.jpg" alt="Figure 10.2 – Primary-secondary replication" width="615" height="694"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Primary-secondary replication</p>
			<h3>Read and write operations</h3>
			<p>The <strong class="bold">Primary</strong> node is responsible for all write operations. Whether the <strong class="bold">Primary</strong> or <strong class="bold">Secondary</strong> nodes should serve read operations has a profound impact on system quality attributes such as consistency, throughput, availability, and resilience.</p>
			<p>If the <strong class="bold">Primary</strong> node serves all read operations, then the <strong class="bold">Secondary</strong> nodes can be either cold backup or hot standby. Cold backup implies the <strong class="bold">Secondary</strong> nodes aren’t running but the data files are being replicated. Hot standby implies the <strong class="bold">Secondary</strong> nodes are up but not serving any request.</p>
			<p>This setup provides strong consistency, but serving both read and write operations means the <strong class="bold">Primary</strong> node takes all the load. This increases resource consumption and makes it more challenging to achieve high performance. Moreover, if the <strong class="bold">Primary</strong> node fails, it may take some time for the cold backup to start up and cause an outage. The hot standby would have better availability as the <strong class="bold">Secondary</strong> nodes are already running, but all read requests to the failed primary node are still impacted. This will cause a “blip” until one of the <strong class="bold">Secondary</strong> nodes becomes the <strong class="bold">Primary</strong> node.</p>
			<p>If <strong class="bold">Secondary</strong> nodes serve read requests, the throughput of read operations is increased. More nodes are available to handle read requests. If some of the <strong class="bold">Secondary</strong> nodes fail, others can continue to operate. This approach comes with the trade-off of potential inconsistency issues. Imagine if one of the <strong class="bold">Secondary</strong> nodes failed to connect to the <strong class="bold">Primary</strong> node; this <strong class="bold">Secondary</strong> node would have outdated data but still performs a read operation and provides outdated data, something that’s inconsistent with <a id="_idIndexMarker874"/>other nodes.</p>
			<h3>Replication</h3>
			<p>When you’re replicating data changes from the <strong class="bold">Primary</strong> node to <strong class="bold">Secondary</strong> nodes, you have two options: synchronous or asynchronous replication. An example sequence diagram of the synchronization process is shown in <em class="italic">Figure 10</em><em class="italic">.3</em>:</p>
			<div><div><img src="img/B21737_10_3.jpg" alt="Figure 10.3 – Primary-secondary synchronization – synchronous (left)/asynchronous (right)" width="1650" height="715"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Primary-secondary synchronization – synchronous (left)/asynchronous (right)</p>
			<p>This diagram is split vertically into two approaches. On the left-hand side, we have synchronous replication. Here, a write request is sent to the primary node. The primary node updates the data in its local storage but doesn’t commit the transaction. Then, it sends the data change to all secondary nodes.</p>
			<p>This is a blocking and synchronous process where the primary node waits for responses from all secondary nodes. If all responses are successful, then the primary node commits the transaction and flushes the changes to local storage. Finally, a response is returned to the original requester. The synchronized approach maintains strong data consistency across all nodes at the cost of higher latency due to synchronous communication between the primary and secondary nodes.</p>
			<p>On the right-hand side, after the primary node completes the write request, the data change is committed to local storage, and the response is returned to the requester. The data changes are synchronized in the background without blocking. This is either done as a scheduled background process or as an event that’s published to the secondary nodes. This approach has reduced latency as replication isn’t required to return a response. However, it introduces scenarios where data could be inconsistent.</p>
			<p>If the communication between the primary and some secondary nodes fails, some of the secondary nodes will have the latest data and some won’t. Meanwhile, all secondary nodes serve read operations that return different versions of the same data.</p>
			<p>The risk of inconsistency can be mitigated by stamping the data with a version number or timestamp. Any outdated data can be spotted and then skipped.</p>
			<p>The requester can also have a sticky connection with the secondary nodes serving read requests. The data that’s returned to the requester will change in tandem with the secondary node. This provides some level of reliability that a request won’t get one version of the data, and<a id="_idIndexMarker875"/> then get an older version.</p>
			<h3>Failover</h3>
			<p>If the primary node fails, one of the secondary nodes needs to become the primary node. The new primary node can be determined by the round-robin rule, or a potentially more complex leader election algorithm.</p>
			<p>If data is replicated asynchronously, losing a primary node may result in losing the latest data. This happens if the primary node has updated its local storage and returned the result, but then fails before it can notify secondary nodes.</p>
			<p>It’s even worse if the failed primary node gets backed up but loses the connection to some of the secondary nodes. Here, a new primary node may have been assigned. We now have a split-brain situation where there are two primary nodes, and secondary nodes are fragmented. This usually requires manual intervention to shut down one of the primary nodes and reconnect all secondary nodes to the one primary node.</p>
			<p>Primary-secondary <a id="_idIndexMarker876"/>replications are commonly used in highly available databases and message brokers.</p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor349"/>Model 2 – partitioned and distributed</h2>
			<p><strong class="bold">Partitioned and distributed</strong> (known as <strong class="bold">multi-leader</strong>) replication<a id="_idIndexMarker877"/> distributes<a id="_idIndexMarker878"/> data management into<a id="_idIndexMarker879"/> partitions. It allows multiple nodes to serve requests at the same time. These nodes replicate the changes to the other nodes, enabling higher write throughput and availability.</p>
			<p>It’s typically used when data and services are replicated across multiple geographical locations, often in different data centers or cloud regions. This provides availability and resilience against regional failures or disasters. This is illustrated in <em class="italic">Figure 10</em><em class="italic">.4</em>:</p>
			<div><div><img src="img/B21737_10_4.jpg" alt="Figure 10.4 – Partitioned and distributed replication" width="1464" height="695"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Partitioned and distributed replication</p>
			<p>Requests are geographically partitioned so that users of a given region can access the corresponding services in that region. Within this region, this partitioned and distributed replication can behave exactly like primary-secondary replication, where primary nodes serve write requests and secondary nodes serve read requests.</p>
			<p>Across regions, an additional synchronization process occurs so that the data in one region is copied to another. Some data is fully partitioned and regional, which means that all requests for the data are served within the designated region in normal circumstances. Some data is shared and may need to be fully replicated. This introduces the need to resolve conflicts if it’s updated in both regions.</p>
			<p>This setup is more complex compared to primary-secondary replication. However, it can be justified if there are non-functional requirements such as the following:</p>
			<ul>
				<li>Serve requests coming from multiple geographic regions</li>
				<li>Recover in the face of total data center failure</li>
				<li>Decouple from a particular cloud provider architecturally and operationally</li>
				<li>Support offline operations</li>
				<li>Support collaborative update operations</li>
			</ul>
			<p>On the other hand, it will become difficult to uphold strong consistency if the same data across multiple regions can’t be updated at the same time.</p>
			<p>If a data center has failed, requests for the corresponding partition should be routed to the running data<a id="_idIndexMarker880"/> center. Data that hasn’t been replicated in the running data center would be lost. In this situation, clients may need to roll back to the last replicated state.</p>
			<h3>Resolving write conflicts and avoiding lost updates</h3>
			<p>Partitioned and distributed replication requires some mechanisms to resolve write conflicts in which the same piece of data is updated simultaneously, and perhaps differently. Let’s illustrate the resolution of write conflicts with a real-life example.</p>
			<p>Imagine that each household in a village has a record of its name and a contact email address. The <em class="italic">Whittington</em> household has a record in the repository with an email address of <em class="italic">info@whittington.com</em>.</p>
			<p>This record is exposed to two different clients. Each client has read the email address, <em class="italic">info@whittington.com</em>. One client has updated the email address to <em class="italic">query@whittington.com</em>, while the other one has updated it to <em class="italic">contact@whittington.com</em>. The two clients attempt to update the value in the repository by providing their updated ones. The repository is going to receive the write requests from these two clients.</p>
			<p>Both clients determine the new value based on the current value they receive:</p>
			<ul>
				<li><strong class="bold">Client A</strong>: Update the current email address from <em class="italic">info@whittington.com</em> to <em class="italic">query@whittington.com</em></li>
				<li><strong class="bold">Client B</strong>: Update the current email address from <em class="italic">info@whittington.com</em> to <em class="italic">contact@whittington.com</em></li>
			</ul>
			<p>If Client A requests an update earlier than Client B does, then the process of updating the email address to <em class="italic">query@whittington.com</em> would be lost. This is because Client B almost immediately overwrote the value with <em class="italic">contact@whittington.com</em> without knowing Client A had also requested an update. This problem is <a id="_idIndexMarker881"/>called the <strong class="bold">lost </strong><strong class="bold">update</strong> problem.</p>
			<p>This problem is typically solved by having a version number or timestamp on the data. If an incoming request update is identified as older than the one in the system record, then it’s safe to skip the update. Having a monotonic increasing version number is a preferred method <a id="_idIndexMarker882"/>compared to timestamps due to the risk that the system clock on each machine can be different.</p>
			<p>We can model this situation with the following data class:</p>
			<pre class="source-code">
data class Household(
    val version: Int,
    val name: String,
    val email: String,
)</pre>			<p>Here, the <code>Household</code> class has a <code>version</code> field as an integer. This will be used for comparison during the update operation. There’s also a repository class for <code>Household</code> to handle the update request. Here’s the scenario simulated in code:</p>
			<pre class="source-code">
fun main() {
    val repo = HouseholdRepository()
    val name = "Whittington"
    val email1 = "info@whittington.com"
    val email2a = "query@whittington.com"
    val email2b = "contact@whittington.com"
    val household1 = Household(0, name, email1)</pre>			<p>First, a <code>household</code> record is created as a version, after which there are two updates based on it:</p>
			<pre class="source-code">
    repo.create(name) { household1 }
    repo.update(name) { household1.copy(version = 1, email = email2a)}
    repo.update(name) { household1.copy(version = 1, email = email2b)}
    repo.get(name)?.also {
        println("${it.version}, ${it.email}")
    }
}</pre>			<p>In this situation, we would expect the second update to be skipped because it was based on version zero. The second update would require refreshing the <code>household</code> record to version one and <a id="_idIndexMarker883"/>computing the potential update.</p>
			<p>A version check should be in place in the repository to prevent the lost update problem. Here’s an example implementation:</p>
			<pre class="source-code">
class HouseholdRepository {
    private val values: ConcurrentMap&lt;String, Household&gt; = ConcurrentHashMap()</pre>			<p>The <code>HouseholdRepository</code> class holds a <code>ConcurrentMap</code> interface that uses the household name as the key. The <code>create</code> function makes use of the atomic <code>putIfAbsent</code> function to ensure the value won’t be overwritten by mistake:</p>
			<pre class="source-code">
    fun create(
        key: String,
        callback: () -&gt; Household
    ): Household {
        val household = callback()
        val result = values.putIfAbsent(key, household)
        return result ?: household
    }</pre>			<p>The <code>update</code> function checks that the updated value must be one version higher than the existing value by using the atomic <code>computeIfPresent</code> function:</p>
			<pre class="source-code">
    fun update(
        key: String,
        callback: (Household) -&gt; Household
    ): Household? = values.computeIfPresent(key) { _, existing -&gt;
        callback(existing).let { updated -&gt;
            if (updated.version == existing.version + 1) {
                updated
            } else {
                existing
            }
        }
    }</pre>			<p>For<a id="_idIndexMarker884"/> completeness, there’s also a <code>get</code> function so that we can get what’s kept in the map after the run:</p>
			<pre class="source-code">
    fun get(key: String): Household? = values[key]
}</pre>			<p>The output of the program is as follows:</p>
			<pre class="console">
1, query@whittington.com</pre>			<p>This means the second update is skipped.</p>
			<h2 id="_idParaDest-256"><a id="_idTextAnchor350"/>Model 3 – quorum-based replication</h2>
			<p><strong class="bold">Quorum-based</strong> (also known as <strong class="bold">leaderless</strong>) <strong class="bold">replication</strong> requires nodes to agree on the state of the <a id="_idIndexMarker885"/>data <a id="_idIndexMarker886"/>before <a id="_idIndexMarker887"/>committing a write operation. This ensures consistency and availability, even if some nodes have failed.</p>
			<p>The key difference of quorum-based replication is the lack of a primary node, a leader, or a central coordinator. Instead, the data is decentralized and distributed among the nodes in the cluster:</p>
			<div><div><img src="img/B21737_10_5.jpg" alt="Figure 10.5 – Quorum-based replication" width="723" height="628"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Quorum-based replication</p>
			<p>A write operation is only considered successful if it’s acknowledged by the majority (quorum) of the participating nodes in the system. This quorum requirement ensures that a write is only committed if it’s been replicated to enough nodes, making the system resilient to individual node failures.</p>
			<p>The quorum size is typically set to at least more than half of the total nodes, ensuring that even if some fail, the system can still make progress and maintain a consistent state. The data that’s synchronized among the nodes is versioned for a couple of reasons:</p>
			<ul>
				<li>The data synchronization process needs to identify an older version of the data, as well as increment the version to indicate an update</li>
				<li>Clients can read the version to understand whether the data that’s received is outdated</li>
			</ul>
			<p>For example, in a five-node cluster, a quorum size of three would be required for a write operation to succeed. This way, the system can tolerate the failure of up to two nodes without compromising data consistency.</p>
			<p>Since all the nodes have the same state, there’s no actual failover mechanism. Instead, each request would need to be able to remove duplicated or older responses. This can be done if the data is versioned.</p>
			<p>Quorum-based replication is commonly used in distributed databases, key-value stores, P2P networks, blockchains, and<a id="_idIndexMarker888"/> coordination services, where<a id="_idIndexMarker889"/> maintaining strong consistency and availability in the face of node failures is of utmost importance.</p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor351"/>Comparing the three replication models</h2>
			<p>Choosing the<a id="_idIndexMarker890"/> appropriate replication mode in a database or data system depends on several factors, including non-functional requirements regarding consistency, availability, performance, and fault tolerance. Here’s a summary and use case for each model:</p>
			<table id="table001-3" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Primary-Secondary</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Partitioned </strong><strong class="bold">and Distributed</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Quorum-Based</strong></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Strong consistency</p>
						</td>
						<td class="No-Table-Style">
							<p>Eventual consistency</p>
						</td>
						<td class="No-Table-Style">
							<p>Configurable consistency</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Simple and easy to maintain</p>
						</td>
						<td class="No-Table-Style">
							<p>Increased complexity</p>
						</td>
						<td class="No-Table-Style">
							<p>Complex quorum maintenance</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Low tolerance for data loss</p>
						</td>
						<td class="No-Table-Style">
							<p>Challenges in conflict resolution</p>
						</td>
						<td class="No-Table-Style">
							<p>Fault tolerance</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Performance is limited by the capacity of the leader and replication lag</p>
						</td>
						<td class="No-Table-Style">
							<p>Performance is limited by the capacity of the leader and replication lag</p>
						</td>
						<td class="No-Table-Style">
							<p>Additional latency to achieve consensus for each change</p>
							<p>Higher resource usage</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Less available</p>
						</td>
						<td class="No-Table-Style">
							<p>Highly available; load balancer options are available</p>
						</td>
						<td class="No-Table-Style">
							<p>Depends on the number of nodes available</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Single point of failure</p>
						</td>
						<td class="No-Table-Style">
							<p>No single point of failure</p>
						</td>
						<td class="No-Table-Style">
							<p>No single point of failure</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Suitable for traditional databases and systems that read more often than write (for example, content management systems)</p>
						</td>
						<td class="No-Table-Style">
							<p>Suitable for systems spread across different regions and collaborative applications</p>
						</td>
						<td class="No-Table-Style">
							<p>Suitable for distributed data stores and critical systems that aren’t latency-sensitive</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>The failover mechanism is part of the recovery process, but it focuses on shifting the workload to other running nodes. Recovery also covers bringing up nodes that weren’t running. These <a id="_idIndexMarker891"/>approaches will be covered in the next section.</p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor352"/>Recovery</h1>
			<p>The recovery process of a<a id="_idIndexMarker892"/> system heavily relies on accessible data replicas, except stateless systems. This implies that the recovery approach heavily relies on the replication approach.</p>
			<h2 id="_idParaDest-259"><a id="_idTextAnchor353"/>Snapshots and checkpoints</h2>
			<p>The most common <a id="_idIndexMarker893"/>approach for recovery is to have a snapshot of the last known system state. Periodically saving the state of the distributed system is<a id="_idIndexMarker894"/> known as <strong class="bold">checkpointing</strong>.</p>
			<p>In the event of a failure, the system can be rolled back to the last known good checkpoint to restore the system to a consistent state. Data that didn’t persist in the snapshot will be lost. The amount of data loss would depend on how often the snapshots are taken.</p>
			<h2 id="_idParaDest-260"><a id="_idTextAnchor354"/>Change logs</h2>
			<p>A system state can also <a id="_idIndexMarker895"/>be restored by replaying the change logs of all operations and transactions within the distributed system.</p>
			<p>It’s common to recover distributed systems using a combination of checkpoints and change logs. This is similar to the event sourcing recovery method mentioned in <a href="B21737_09.xhtml#_idTextAnchor307"><em class="italic">Chapter 9</em></a>, where an aggregate is stored by replaying all related events.</p>
			<p>This approach helps recover from failures by replaying the missed or lost operations.</p>
			<h2 id="_idParaDest-261"><a id="_idTextAnchor355"/>Re-route and re-balance</h2>
			<p>After a node is <a id="_idIndexMarker896"/>brought up, it needs to create or join a network of nodes. Requests may need to be re-routed and partitions may need to be re-balanced.</p>
			<p>This may also trigger <a id="_idIndexMarker897"/>the election of a new<a id="_idIndexMarker898"/> primary node. Consensus protocols such as <strong class="bold">Raft</strong> (<a href="https://raft.github.io/">https://raft.github.io/</a>) and <strong class="bold">Paxos</strong> (<a href="https://www.microsoft.com/en-us/research/publication/part-time-parliament/">https://www.microsoft.com/en-us/research/publication/part-time-parliament/</a>) may be used to coordinate the actions of the other nodes, ensuring the system remains operational even when individual nodes fail.</p>
			<h2 id="_idParaDest-262"><a id="_idTextAnchor356"/>Case study – Raft leader election</h2>
			<p>To<a id="_idIndexMarker899"/> demonstrate <a id="_idIndexMarker900"/>the details of recovery, we’re going to walk through a simplified <strong class="bold">Raft</strong> leader election process, as demonstrated in <em class="italic">Figure 10</em><em class="italic">.5</em>:</p>
			<div><div><img src="img/B21737_10_6.jpg" alt="Figure 10.6 – Node state transition in Raft leader election" width="1159" height="917"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Node state transition in Raft leader election</p>
			<p>Raft uses primary-secondary replication in which the primary node replicates data changes to all secondary<a id="_idIndexMarker901"/> nodes. The primary node keeps<a id="_idIndexMarker902"/> an integer <a id="_idIndexMarker903"/>called <strong class="bold">Terms</strong>; this number increments for each election. Each request that’s received by the primary node is stamped by Terms.</p>
			<p>The primary node broadcasts heartbeat messages to all secondary nodes. They’re like pulses to keep announcing that the primary node has been up.</p>
			<p>When a secondary node hasn’t received heartbeat messages over the configured time, it becomes a candidate and calls other secondary nodes to vote for itself.</p>
			<p>Other secondary nodes can either accept or reject their votes. When a candidate has been accepted by receiving the most votes, they become the primary node, and others revert to followers.</p>
			<p>This mechanism happens concurrently and means there will be conflicts to resolve:</p>
			<ul>
				<li><strong class="bold">Conflicting elections</strong>: Conflicting elections can happen if the timed-out configurations for heartbeat messages are the same among all secondary nodes. This can be avoided by randomizing the timed-out configuration in each node. Moreover, if there are ties of conflicting elections, all elections are called off, after which another election can be called.</li>
				<li><strong class="bold">Multiple leaders</strong>: If a part of the network is disconnected from another, we can end up with a split-brain situation, where each inter-connected portion starts its own election. Since the majority should be more than half of the total number of nodes, only one part can reach majority votes and elect a leader.<p class="list-inset">If the original leader is in the smaller part of the split network, there will be multiple leaders when the whole network has recovered. At this point, the value <em class="italic">Terms</em> can be used to make the original leader step down because the new leader will have a higher term value than the original.</p></li>
				<li><strong class="bold">Outdated candidates</strong>: Some secondary nodes could be behind others in replication but would still call for an election and put themselves as candidates. If one of them became the primary node, its outdated data became the source of truth, and some updates could be lost.</li>
			</ul>
			<p>To avoid this situation, the secondary nodes will reject candidates whose Terms are lower than other <a id="_idIndexMarker904"/>candidates, and whose data isn’t up to date. A <a id="_idIndexMarker905"/>candidate who has outdated data can be spotted by the number of items in the change log.</p>
			<h1 id="_idParaDest-263"><a id="_idTextAnchor357"/>Summary</h1>
			<p>In this chapter, we covered three topics: idempotency, replication, and recovery. First, we discussed four scenarios where idempotency is useful and how it can be achieved with reference implementation.</p>
			<p>Then, we briefly mentioned how to replicate data and services. We brought up the CAP theorem, in which trade-offs need to be considered for each system. We also delved into three models of replication, namely primary-secondary, partitioned and distributed, and quorum-based.</p>
			<p>Finally, we covered some common mechanisms of recovery, outlining how a newly launched node can become operational in the context of distributed systems.</p>
			<p>In the next chapter, we’ll cover the audit and monitoring aspects of a distributed system.</p>
		</div>
	</div></div></body></html>