<html><head></head><body>
		<div id="_idContainer162">
			<h1 id="_idParaDest-229"><em class="italic"><a id="_idTextAnchor484"/>Chapter 15</em>: Recognition with Vision Framework</h1>
			<p>The Vision framework has been available to developers for a few years now. Apple has been introducing better and better features for it, from text recognition to image recognition. On iOS 14, Vision comes with more improvements to text recognition and other existing functions, but it also allows developers to perform two different actions: hand and body pose recognition. The possibilities that these new features open up for developers are limitless! Just think about gym apps, yoga apps, health apps, and so on.</p>
			<p>In this chapter, we are going to learn about the basics of the Vision framework and how to use the new advancements in text recognition. We will also learn about the new hand landmark recognition, building a demo app that can detect the tips of the four fingers and the thumb. The chapter code bundle also provides a similar example demonstrating body pose recognition. We will discuss these topics in the following sections:</p>
			<ul>
				<li>Introduction to the Vision framework</li>
				<li>Recognizing text in images</li>
				<li>Recognizing hand landmarks in real time</li>
			</ul>
			<p>By the end of this chapter, you will be able to work with the Vision framework with total confidence, being able to apply the techniques explained in this chapter to implement any type of recognition that Vision provides, from the recognition of text in images to the recognition of hand and body poses in videos.</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor485"/>Technical requirements</h1>
			<p>The code bundle for this chapter includes a starter project called <strong class="source-inline">HandDetection_start</strong> and a couple of playground files named <strong class="source-inline">Vision.playground</strong> and <strong class="source-inline">RecognitionPerformance_start.playground</strong>. It also contains a completed example for body pose detection named <strong class="source-inline">BodyPoseDetection_completed</strong>. You can find them in the code bundle repository:</p>
			<p>https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor486"/>Introduction to the Vision framework</h1>
			<p>Since the beginning of the App Store, there have been many apps that use the camera to build great <a id="_idIndexMarker772"/>functionalities using image and video recognition. Think of the bank apps that can now scan a check or a credit card so that the user doesn't need to input all the numbers. There are networking apps that can take a picture of a business card and extract the relevant information. Even the Photos app from your iPhone can detect faces in your photographs and classify them. </p>
			<p>The Vision framework provides developers with a robust set of features to make it easier than ever to achieve these functionalities: from text and image recognition to barcode detection, face landmarks analysis, and now, with iOS 14, hand and body pose recognition.</p>
			<p>Vision also allows the use of Core ML models to allow developers to enhance object classification and detection in their apps. Vision has been available since iOS 11 and macOS 10.13.</p>
			<p>There are several concepts in Vision that are common to any type of detection (text detection, image detection, barcode detection, and so on), including the <strong class="source-inline">VNRequest</strong>, <strong class="source-inline">VNRequestHandler</strong>, and <strong class="source-inline">VNObservation</strong> entities:</p>
			<ul>
				<li><strong class="source-inline">VNRequest</strong> is the task that we want to perform. For example, <strong class="source-inline">VNDetectAnimalRequest</strong> would be used to detect animals in a picture.</li>
				<li><strong class="source-inline">VNRequestHandler</strong> is how we want to detect. It lets us define a completion handler where we can play around with the results and shape them in the way that we need.</li>
				<li><strong class="source-inline">VNObservation</strong> encapsulates the results.</li>
			</ul>
			<p>Let's look at an example that combines all these concepts and shows how Vision can easily help us to detect text inside an image. Open the playground named <strong class="source-inline">Vision.playground</strong>. This example code is grabbing an image from a specific URL and trying to extract/detect any text on it. The image being used is this one:</p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/Figure_15.01_B14717.jpg" alt="Figure 15.01 – Example image to extract text with Vision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.01 – Example image to extract text with Vision</p>
			<p>If we try to extract text from this image, we should get results such as <em class="italic">Swift Data Structure and Algorithms</em>, or the name of the authors, or the description below the title. Let's review the code in the <a id="_idIndexMarker773"/>playground:</p>
			<p class="source-code">import Vision</p>
			<p class="source-code">let imageUrl = URL(string: "http://marioeguiluz.com/img/portfolio/Swift%20Data%20Structures%20and%20Algorithms%20Mario%20Eguiluz.jpg")!</p>
			<p class="source-code">// 1. Create a new image-request handler.</p>
			<p class="source-code">let requestHandler = <strong class="bold">VNImageRequestHandler</strong>(url: imageUrl, options: [:])</p>
			<p class="source-code">// 2. Create a new request to recognize text.</p>
			<p class="source-code">let request = <strong class="bold">VNRecognizeTextRequest</strong> { (request, error) in</p>
			<p class="source-code">  guard let observations = request.results as? [VNRecognizedTextObservation] else { return }</p>
			<p class="source-code">  let recognizedStrings = <strong class="bold">observations</strong>.compactMap { observation in</p>
			<p class="source-code">    // Return the string of the top VNRecognizedText instance.</p>
			<p class="source-code">    return observation.topCandidates(1).first?.string</p>
			<p class="source-code">  }</p>
			<p class="source-code">  // Process the recognized strings.</p>
			<p class="source-code">  print(recognizedStrings)</p>
			<p class="source-code">}</p>
			<p class="source-code">// 3. Select .accurate or .fast levels</p>
			<p class="source-code">request.recognitionLevel = .accurate</p>
			<p class="source-code">do {</p>
			<p class="source-code">  // 4. Perform the text-recognition request.</p>
			<p class="source-code">  try requestHandler.perform([request])</p>
			<p class="source-code">} catch {</p>
			<p class="source-code">  print("Unable to perform the requests: \(error).")</p>
			<p class="source-code">}</p>
			<p>Let's go through <a id="_idIndexMarker774"/>the numbered comments:</p>
			<ol>
				<li>First, we are creating a <strong class="source-inline">VNImageRequestHandler</strong> instance with a given image URL. We instantiate this handler to perform Vision requests on an image. Remember that we need to call <strong class="source-inline">perform(_:)</strong> later on to launch the analysis.</li>
				<li>Now we create a <strong class="source-inline">request(VNRecognizeTextRequest)</strong> instance that we will perform on the <strong class="source-inline">requestHandler</strong> instance instantiated previously. You can perform multiple requests on a <strong class="source-inline">requestHandler</strong> instance. We define a block of code to be executed when the request finishes. In this block, we are extracting the observations from the request results (<strong class="source-inline">VNRecognizedTextObservation</strong> instances). These observations will contain potential outcomes for the analyzed text from the image (<strong class="source-inline">VNRecognizedText</strong> instances). We print <strong class="source-inline">topCandidate</strong> from each observation, which should be the best match according to the Vision parameters.</li>
				<li>We can specify the recognition level for the request. In this example, we are using <strong class="source-inline">.accurate</strong> (the alternative is <strong class="source-inline">.fast</strong>). We will see later the results with <strong class="source-inline">.fast</strong> and when to use one or the other.</li>
				<li>Finally, we are <a id="_idIndexMarker775"/>performing the request on the <strong class="source-inline">requestHandler</strong> instance to execute everything with the <strong class="source-inline">perform(_:)</strong> method.</li>
			</ol>
			<p>If you execute the code, the console in the playground will display the following:</p>
			<p class="source-code">["Erik Azar, Mario Eguiluz", "Swift Data", "Structure and", "Algorithms", "Master the most common algorithms and data structures,", "and learn how to implement them efficiently using the most", "up-to-date features of Swift", "Packt&gt;"]</p>
			<p>Those seem to be great results, right? If you recheck the image, we are extracting the correct text from it! The author names, the title (per line), the description, and more! Seems to be a great result! But have you noticed that when you execute the playground, it takes a while to finish? This is because we are using the <strong class="source-inline">.accurate</strong> option. Let's see what happens if we use <strong class="source-inline">.fast</strong> instead. Change it in the playground code:</p>
			<p class="source-code">// 3. Select .accurate or .fast levels</p>
			<p class="source-code">request.recognitionLevel = .fast</p>
			<p>The output is as follows:</p>
			<p class="source-code">["Swift Data", "Structure and", "Algorithms", "upto4atefeaturesofSwift3", "Packt&gt;", "ErfkAz•r. M•rb Eguluz", "ml5tertket(w4VIthMsarodats5tr&amp;KtUre", "learnItolpIettmeffK1WttIY5lt1fft", "LIJJ"]</p>
			<p>This time, the analysis can be done faster, but as you can see, the results are far worse for what we wanted (we wanted to detect the text properly!). Why should anyone prefer speed over accuracy? Well, for some apps, speed is critical and it is fine to sacrifice some accuracy for it. Think of real-time camera-based translations or applying real-time filters to take photos. In these scenarios, you need fast processing. We will discuss this further later in the chapter.</p>
			<p>This playground example should help you to have a grasp of the incredible potential that Vision contains. Just with a <a id="_idIndexMarker776"/>few lines of code, we were able to process and extract the text of an image with no issues or complex operations. Vision allows developers to do amazing things. Let's dive deeper into it in the following sections, starting with a more detailed look at text detection for images.</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor487"/>Recognizing text in images</h1>
			<p>The <a id="_idIndexMarker777"/>Vision framework has been improving its detection of text in images since its first iteration. In this section, we are going to learn some state-of-the-art techniques to obtain the best results on iOS 14.</p>
			<p>We saw in the previous section that text detection in Vision can happen in two different ways, as defined by the value of <strong class="source-inline">recognitionLevel</strong> that we specify in the request: <strong class="source-inline">.fast</strong> and <strong class="source-inline">.accurate</strong>. Let's see the differences:</p>
			<ul>
				<li><strong class="bold">Fast recognition</strong>: This uses character recognition to <a id="_idIndexMarker778"/>detect text character by character within a bounding box. It is optimized for real-time recognition and uses a smaller memory footprint than <strong class="source-inline">.accurate</strong>. It doesn't handle rotated text or different fonts as well as the <strong class="source-inline">.accurate</strong> method.</li>
				<li><strong class="bold">Accurate recognition</strong>: This uses a neural network to detect strings and full lines, and then recognizes the <a id="_idIndexMarker779"/>words and sentences. By using a neural network and identifying words, the framework can detect or correct observations for certain characters that are difficult to extract. It takes more time than <strong class="source-inline">.fast</strong> but is more accurate (of course!). It works in the same way that our brain recognizes words. If you read the word "m0untain," your brain can extract "mountain" from it, and it knows that the 0 (zero) stands for an o. If you use <strong class="source-inline">.fast</strong>, which recognizes character by character, the 0 (zero) would still be a 0 (zero) in your results, because no context is taken into account.</li>
			</ul>
			<p>In both cases, after the initial recognition phase is finished, results are passed into a traditional natural language processor for language processing, and the outcome of that is the results (observations). This whole process happens exclusively on the device.</p>
			<p>So, when should anyone use <strong class="source-inline">.fast</strong>, you might wonder. Well, there are scenarios in which it is more convenient than <strong class="source-inline">.accurate</strong>:</p>
			<ul>
				<li>To read codes or barcodes quickly</li>
				<li>When user interactivity is a crucial aspect, so you want a fast response from the text detection</li>
			</ul>
			<p>To demonstrate the differences between the recognition levels, let's analyze the same image <a id="_idIndexMarker780"/>using different techniques. You will also learn some useful tricks that you can apply to your projects. Follow the steps given here:</p>
			<ol>
				<li value="1">Go ahead and open the playground named <strong class="source-inline">RecognitionPerformance_start.playground</strong>. The code is roughly the same as what we tried in the previous section. <p>The only difference is that the image that we are using now contains a 4-digit number that represents the serial number of the book:</p><div id="_idContainer158" class="IMG---Figure"><img src="image/Figure_15.02_B14717.jpg" alt="Figure 15.02 – Book cover with a serial number (1015) below the author names&#13;&#10;"/></div><p class="figure-caption">Figure 15.02 – Book cover with a serial number (1015) below the author names</p><p>If you pay close attention to the number font, you will see that it might be tricky for a computer to tell whether some digits are numbers or letters. This has been done on purpose. In this <a id="_idIndexMarker781"/>example, we are going to test the capabilities of Vision.</p></li>
				<li>Go ahead and execute the playground code. The console output should look like this:<p class="source-code"><strong class="bold">["Erik Azar, Mario Eguiluz", "1015", "Swift Data", "Structure and", "Algorithms", "Master the most common algorithms and data structures,", "and learn how to implement them efficiently using the most", "up-to-date features of Swift", "Packt&gt;"]</strong></p><p class="source-code"><strong class="bold">1.9300079345703125 seconds</strong></p></li>
			</ol>
			<p>We have successfully retrieved the serial number of the book: <strong class="source-inline">1015</strong>. The code is also taking a measure of how long it takes to finish the text-recognition process. In our case, it was <strong class="bold">1.93 seconds</strong> (this can differ from computer to computer and also between executions). Can we do better than that? Let's try out some techniques that will help us to improve this processing time while keeping the same accuracy. We are going to start with the <strong class="bold">region of interest</strong>.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor488"/>Region of interest</h2>
			<p>Sometimes, when we are analyzing an image with Vision, we don't need to process the whole image. For example, if <a id="_idIndexMarker782"/>we are processing a <a id="_idIndexMarker783"/>specific type of form where we know in advance that the first name always goes at the top of the document, we may want to just process that area. Processing the whole form would only waste time and resources if we just need a specific area.</p>
			<p>Let's assume that in the previous example (the book cover), the serial number that we want to extract is always in the top-left area. How can we speed up the 1.93-seconds processing time? We can do so by defining a region of interest. Defining a region of interest will tell Vision to only process that area and avoid the rest of the image. That will result in a faster processing time. </p>
			<p><strong class="source-inline">regionOfInterest</strong> is a <strong class="source-inline">CGRect</strong> property of <strong class="source-inline">VNRequest</strong>:</p>
			<ul>
				<li>It defines a rectangular area in which the request will be performed.</li>
				<li>The rectangle is normalized to the dimensions of the image, meaning that the width and height of the region of interest go from 0 to 1.</li>
				<li>The origin of the rectangle is in the bottom-left corner of the image, which is (0,0). The top-right corner will be (1,1).</li>
				<li>The default value is <strong class="source-inline">{{0,0},{1,1}}</strong>, which covers everything from the bottom-left corner (0,0) to the top-right corner, with width 1 and height 1: the whole image.</li>
			</ul>
			<p>In the following figure, you can see the region of interest that we need to define to capture the serial number (<strong class="bold">1015</strong>):</p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/Figure_15.03_B14717.jpg" alt="Figure 15.03 – Region of interest&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.03 – Region of interest</p>
			<p>Let's add that region to <a id="_idIndexMarker784"/>the code from the <a id="_idIndexMarker785"/>previous section: </p>
			<ol>
				<li value="1">In the <strong class="source-inline">ScanPerformance_start.playground</strong> project, add the following code just after setting <strong class="source-inline">recoginitionLevel</strong> to <strong class="source-inline">.accurate</strong>:<p class="source-code">request.regionOfInterest = CGRect(x: 0, y: 0.8, width: 0.7, height: 0.2)</p></li>
				<li>Now launch the playground and check the result in the console:<p class="source-code"><strong class="bold">["Erik Azar, Mario Eguiluz", "1015"]</strong></p><p class="source-code"><strong class="bold">1.2314139604568481 seconds</strong></p><p>There are a couple of differences when comparing these results to the previous ones:</p><ul><li>We are no longer extracting that much text. Now that we are defining a region of interest, we just extract the words/digits that are contained in that area.</li><li>We reduced the processing time from 1.93 seconds to 1.23 seconds. That is 36% faster.</li></ul></li>
				<li>Let's now try to reduce the region of interest to catch just the serial number. Modify the region to the following:<p class="source-code">request.regionOfInterest = CGRect(x: 0, y: 0.8, width: 0.3, height: 0.1)</p></li>
				<li>Launch the playground. Now the console output is as follows:<p class="source-code"><strong class="bold">["1015"]</strong></p><p class="source-code"><strong class="bold">0.8156689405441284 seconds</strong></p><p>Great! If we have to work with the results now, we don't need to do extra work to <a id="_idIndexMarker786"/>discard values that we <a id="_idIndexMarker787"/>don't need. We reduced the region of interest to pinpoint the exact string that we needed and saved some processing time.</p><p>At this point, why don't we try to use <strong class="source-inline">.fast</strong> for <strong class="source-inline">recognitionLevel</strong> instead of <strong class="source-inline">.accurate</strong>, if what we want is speed? Let's see what happens. </p></li>
				<li>Modify this line to use <strong class="source-inline">.fast</strong>:<p class="source-code">request.recognitionLevel = .fast</p></li>
				<li>Save and execute. Check the console output:<p class="source-code"><strong class="bold">["Iois"]</strong></p><p class="source-code"><strong class="bold">0.5968900661468506 seconds</strong></p></li>
			</ol>
			<p>You can see how this time, the processing time has been made shorter again, but the result is not accurate at all. Instead of detecting <strong class="source-inline">1015</strong>, we have wrongly obtained <strong class="source-inline">Iois</strong>. </p>
			<p>However, there is a common way to fix this situation in scenarios where we have domain knowledge. In our case, we know that the processed characters should be numbers only. Therefore, we can adjust the output from Vision to improve the results and fix misclassifications. For example, see the following adjustments:</p>
			<ul>
				<li>The character "I" can be "1."</li>
				<li>The character "o" can be "0."</li>
				<li>The character "s" can be "5."</li>
			</ul>
			<p>Let's implement this in the code:</p>
			<ol>
				<li value="1">At the very end <a id="_idIndexMarker788"/>of the playground file, add the <a id="_idIndexMarker789"/>following method:<p class="source-code">extension Character {</p><p class="source-code">  func transformToDigit() -&gt; Character {</p><p class="source-code">    let conversionTable = [</p><p class="source-code">      "s": "5",</p><p class="source-code">      "S": "5",</p><p class="source-code">      "o": "0",</p><p class="source-code">      "O": "0",</p><p class="source-code">      "i": "1",</p><p class="source-code">      "I": "1"</p><p class="source-code">    ]</p><p class="source-code">    var current = String(self)</p><p class="source-code">    if let alternativeChar = conversionTable[current] {</p><p class="source-code">      current = alternativeChar</p><p class="source-code">    }</p><p class="source-code">    return current.first!</p><p class="source-code">  }</p><p class="source-code">}</p><p>We are extending the <strong class="source-inline">Character</strong> class by adding a new method named <strong class="source-inline">transformToDigit()</strong>. This new method is going to help us to improve potential misclassifications. Note how in the method itself, we have a table of letter characters that relate to a similarly shaped number. What we are doing is just transforming those letters into the corresponding digits.</p></li>
				<li>Let's use it now. Below the <strong class="source-inline">print(recognizedStrings)</strong> line, add the following code:<p class="source-code">if let serialNumber = recognizedStrings.first {</p><p class="source-code">  let serialNumberDigits = serialNumber.map { $0.transformToDigit() }</p><p class="source-code">  print(serialNumberDigits)</p><p class="source-code">}</p><p>We are getting the result of the <a id="_idIndexMarker790"/>Vision process; in our case, it was <strong class="source-inline">"Iois"</strong>, and for each <a id="_idIndexMarker791"/>character, we are applying to it our new <strong class="source-inline">transformToDigit()</strong> method. </p></li>
				<li>Execute the code, and you will see the following result in the console:<p class="source-code"><strong class="bold">["Iois"]</strong></p><p class="source-code"><strong class="bold">["1", "0", "1", "5"]</strong></p><p class="source-code"><strong class="bold">0.5978780269622803 seconds</strong></p></li>
			</ol>
			<p>That looks great! Note how the <strong class="source-inline">"Iois"</strong> result is now looking much better when transformed to <strong class="source-inline">"1" "0" "1" "5"</strong>.  Also, note how the processing time didn't increase that much; this operation is relatively easy to compute.</p>
			<p>Now let's summarize what we have done in this section and the improvements we made in each step. We started by processing a whole image and using the <strong class="source-inline">.accurate</strong> recognition level, and that took us 1.93 seconds. Then, we applied a region of interest to just process part of the image that we were interested in, reducing the processing time to 1.23 seconds. After that, we changed from <strong class="source-inline">.accurate</strong> to <strong class="source-inline">.fast</strong>. This move reduced the processing time to 0.59 seconds, but the results were incorrect. Finally, we implemented an easy algorithm to improve the results and make them as good as with the <strong class="source-inline">.accurate</strong> level. So, in the end, we got perfect results and a processing time of 0.59 seconds rather than 1.93! </p>
			<p>In the next section, you will learn about one of the new features of iOS14, hand detection.</p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor489"/>Recognizing hand landmarks in real time</h1>
			<p>One of the additions to Vision in iOS 14 is hand detection. This new feature to detect hands in images and <a id="_idIndexMarker792"/>video allows developers to find with great detail the positions of the wrist and the individual fingers in a <a id="_idIndexMarker793"/>video frame or photo. </p>
			<p>In this section, we are going to explain the basics behind hand detection, and we will demonstrate how it works with a sample project. Let's start with the hand landmarks that we will be able to recognize.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor490"/>Understanding hand landmarks</h2>
			<p>There are 21 landmarks <a id="_idIndexMarker794"/>that we will be able to detect in a hand:</p>
			<ul>
				<li>4 in the thumb</li>
				<li>4 in each finger (16 in total)</li>
				<li>1 in the wrist</li>
			</ul>
			<p>As you can see, Vision differentiates between finger and thumb. In both the finger and thumb, there are 4 points of interest. The following figure shows how these landmarks are distributed:</p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/Figure_15.04_B14717.jpg" alt="Figure 15.04 – Finger and thumb landmarks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.04 – Finger and thumb landmarks</p>
			<p>Note how there is also a <a id="_idIndexMarker795"/>landmark in the middle of the wrist.</p>
			<p>For the four fingers, we can access each of them individually using the following keys:</p>
			<ul>
				<li><strong class="source-inline">littleFinger</strong></li>
				<li><strong class="source-inline">middleFinger</strong></li>
				<li><strong class="source-inline">ringFinger</strong></li>
				<li><strong class="source-inline">indexFinger</strong></li>
			</ul>
			<p>Inside each of them, we can <a id="_idIndexMarker796"/>access the four different landmarks:</p>
			<ul>
				<li><strong class="bold">TIP</strong></li>
				<li><strong class="bold">DIP</strong></li>
				<li><strong class="bold">PIP</strong></li>
				<li><strong class="bold">MCP</strong></li>
			</ul>
			<p>Note how for the thumb, these names are slightly different (TIP, IP, PIP, and CMC). In the example code that we will build later in this section, we will demonstrate how to use these points and each of the fingers plus the thumb.</p>
			<p>Vision is capable of detecting more than just one hand at a time. We can specify the maximum amount of hands that we want to detect. This parameter will have an impact on the performance of our detection. Use <strong class="source-inline">maximumHandCount</strong> to set the limit.</p>
			<p>For performance and accuracy, it is also better if the hand is not near the edges of the frame, if the light conditions <a id="_idIndexMarker797"/>are good, and if the hands are perpendicular to the camera angle (so the whole hand is visible, not just the edge of it). Also, take into account that feet can be recognized as hands sometimes, so avoid mixing them. </p>
			<p>That is enough theory; let's jump straight into a code example! We will build a demo app that will be able to detect hand landmarks using the front video camera of a phone and will display an overlay on the detected points.</p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor491"/>Implementing hand detection</h2>
			<p>In this section, we are <a id="_idIndexMarker798"/>going to implement a demo app that will be able to detect hand landmarks using the front video camera of a phone.</p>
			<p>The code bundle of this project contains the initial project and also the final result. Go ahead and open the project named <strong class="source-inline">HandDetection_start</strong>.</p>
			<p>The project contains two main files: A <strong class="source-inline">UIView</strong> instance named <strong class="source-inline">CameraView.swift</strong> and a UIViewController instance called <strong class="source-inline">CameraViewController.swift</strong>. </p>
			<p>The view contains helper methods to draw points on coordinates. It will serve as an overlay to draw on top of the camera feed. Just know that the <strong class="source-inline">showPoints(_ points: [CGPoint], colour: UIColor)</strong> method will allow us to draw an array of <strong class="source-inline">CGPoint</strong> structs into the overlay on top of the video camera feed.</p>
			<p>The view controller will be the centerpiece of the example and is where we are going to implement the relevant code to perform the hand detection. Go ahead and open the <strong class="source-inline">CameraViewController.swift</strong> file. Let's examine the code skeleton that we will fill out step by step.</p>
			<p>At the top of the file, we are <a id="_idIndexMarker799"/>defining four properties:</p>
			<ul>
				<li><strong class="bold">The Vision hand detection request</strong>: <strong class="source-inline">handPoseRequest: VNDetectHumanHandPoseRequest</strong>. We will apply this request at the top of the video stream, to detect hand landmarks in each frame. If we detect any, we will display some points in the overlay to show them.</li>
				<li><strong class="bold">The properties to work with the front video queue, the overlay, and the video stream</strong>: <strong class="source-inline">videoDataOutputQueue</strong>, <strong class="source-inline">cameraView</strong>, and <strong class="source-inline">cameraFeedSession</strong>.</li>
			</ul>
			<p>With the <strong class="source-inline">viewDidAppear</strong> and <strong class="source-inline">viewWillDisappear</strong> methods, we are starting/creating and stopping <strong class="source-inline">AVCaptureSession</strong> for the camera. </p>
			<p>And finally, in the next four methods, we have four TODO comments, which we are going to implement one by one to create this app. Let's summarize the TODO tasks that we are going to <a id="_idIndexMarker800"/>perform:</p>
			<ul>
				<li><strong class="bold">TODO 1</strong>: Detect one hand only.</li>
				<li><strong class="bold">TODO 2</strong>: Create a video session.</li>
				<li><strong class="bold">TODO 3</strong>: Perform hand detection on the video session.</li>
				<li><strong class="bold">TODO 4</strong>: Process and display detected points.</li>
			</ul>
			<p>We are going to implement these four tasks in the following subsections.</p>
			<h3>Detecting hands</h3>
			<p>Vision can do more than <a id="_idIndexMarker801"/>detect one hand at a time. The more hands we ask it to detect, the more it will impact performance. In our example, we only want to detect one hand. By setting <strong class="source-inline">maximumHandCount</strong> to <strong class="source-inline">1</strong> on the request, we will improve performance. </p>
			<p>Let's start by adding this code below <strong class="source-inline">// TODO 1</strong>:</p>
			<p class="source-code">// TODO 1: Detect one hand only.</p>
			<p class="source-code">handPoseRequest.maximumHandCount = 1</p>
			<p>Now, let's create a video session to capture the video stream from the front video camera of the device.</p>
			<h3>Creating a video session</h3>
			<p>For <a id="_idIndexMarker802"/>the second task, we are going to fill out the code inside the <strong class="source-inline">setupAVSession()</strong> method. Go ahead and paste the following code inside the method:</p>
			<p class="source-code">// TODO 2: Create video session</p>
			<p class="source-code">// 1 - Front camera as input</p>
			<p class="source-code">guard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {</p>
			<p class="source-code">  fatalError("No front camera.")</p>
			<p class="source-code">}</p>
			<p class="source-code">// 2- Capture input from the camera</p>
			<p class="source-code">guard let deviceInput = try? AVCaptureDeviceInput(device: videoDevice) else {</p>
			<p class="source-code">  fatalError("No video device input.")</p>
			<p class="source-code">}</p>
			<p>First, we are creating a <strong class="source-inline">videoDevice: AVCaptureDevice</strong> instance by querying for the video front camera (if it exists!) with this:</p>
			<p class="source-code">guard let <strong class="bold">videoDevice</strong> = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {</p>
			<p class="source-code">  fatalError("No front camera.")</p>
			<p class="source-code">}</p>
			<p>Then, we use that <strong class="source-inline">videoDevice</strong> to generate a <strong class="source-inline">deviceInput: AVCaptureDeviceInput</strong> instance, which will be the video device used for the stream, with the following code:</p>
			<p class="source-code">guard let <strong class="bold">deviceInput</strong> = try? AVCaptureDeviceInput(device: videoDevice) else {</p>
			<p class="source-code">  fatalError("No video device input.")</p>
			<p class="source-code">}</p>
			<p>Now add <a id="_idIndexMarker803"/>this code:</p>
			<p class="source-code">let session = AVCaptureSession()</p>
			<p class="source-code">session.beginConfiguration()</p>
			<p class="source-code">session.sessionPreset = AVCaptureSession.Preset.high</p>
			<p class="source-code">// Add video input to session</p>
			<p class="source-code">guard session.canAddInput(deviceInput) else {</p>
			<p class="source-code">  fatalError("Could not add video device input to the session")</p>
			<p class="source-code">}</p>
			<p class="source-code">session.addInput(deviceInput)</p>
			<p class="source-code">let dataOutput = AVCaptureVideoDataOutput()</p>
			<p class="source-code">if session.canAddOutput(dataOutput) {</p>
			<p class="source-code">  session.addOutput(dataOutput)</p>
			<p class="source-code">  // Add a video data output.</p>
			<p class="source-code">  dataOutput.alwaysDiscardsLateVideoFrames = true</p>
			<p class="source-code">  dataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: Int(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)]</p>
			<p class="source-code">  dataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)</p>
			<p class="source-code">} else {</p>
			<p class="source-code">  fatalError("Could not add video data output to the session")</p>
			<p class="source-code">}</p>
			<p class="source-code">session.commitConfiguration()</p>
			<p class="source-code">cameraFeedSession = session</p>
			<p>After creating the <strong class="source-inline">videoDevice</strong> instance, we are creating a new <strong class="source-inline">session: AVCaptureSession</strong> instance. With the session created, we assign <strong class="source-inline">videoDevice</strong> as the input and create and configure an output to handle the video stream. We assign the class itself as <strong class="source-inline">dataOutput AVCaptureVideoDataOutputSampleBufferDelegate</strong> by calling this:</p>
			<p class="source-code">dataOutput.<strong class="bold">setSampleBufferDelegate</strong>(self, queue: videoDataOutputQueue)</p>
			<p>This means that when the <a id="_idIndexMarker804"/>front video camera captures new frames, our session will handle them and send them to our delegate method, which we are going to implement in the next step (TODO 3).</p>
			<h3>Performing hand detection in the video session</h3>
			<p>Now that we have set up <a id="_idIndexMarker805"/>and configured a video session, it is time to handle every frame as it comes and tries to detect any hands and their landmarks! We need to implement the <strong class="source-inline">captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)</strong> method.</p>
			<p>Under the <strong class="source-inline">// TODO 3: Perform hand detection on the video session</strong> line, add this code:</p>
			<p class="source-code">var thumbTip: CGPoint?</p>
			<p class="source-code">var indexTip: CGPoint?</p>
			<p class="source-code">var ringTip: CGPoint?</p>
			<p class="source-code">var middleTip: CGPoint?</p>
			<p class="source-code">var littleTip: CGPoint?</p>
			<p>We want to detect the tip of the four fingers (index, ring, middle, and little) and the thumb. So, we are creating five variables of type <strong class="source-inline">CGPoint</strong> to store their coordinates, if they are found.</p>
			<p>Just after these new lines, add the following code:</p>
			<p class="source-code">let handler = VNImageRequestHandler(cmSampleBuffer: <strong class="bold">sampleBuffer</strong>, orientation: .up, options: [:])</p>
			<p class="source-code">do {</p>
			<p class="source-code">  try handler.perform([<strong class="bold">handPoseRequest</strong>])</p>
			<p class="source-code">  guard let observation = handPoseRequest.results?.first else {</p>
			<p class="source-code">    return</p>
			<p class="source-code">  }</p>
			<p class="source-code">	// Get observation points</p>
			<p class="source-code">} catch {</p>
			<p class="source-code">  cameraFeedSession?.stopRunning()</p>
			<p class="source-code">  fatalError(error.localizedDescription)</p>
			<p class="source-code">}</p>
			<p>With this code, we are <a id="_idIndexMarker806"/>asking Vision to execute <strong class="source-inline">handPoseRequest</strong> over <strong class="source-inline">sampleBuffer</strong> (the video stream). Then, we guard (using <strong class="source-inline">guard</strong>) against the case in which we don't detect any observations (so that if there is no hand in the video frame, we just stop at this point).</p>
			<p>But if the guard doesn't trigger, it means that we have some hand landmarks and we need to process them. Add the following code just after the <strong class="source-inline">// Get observation points</strong> line:</p>
			<p class="source-code">let thumbPoints = try observation.<strong class="bold">recognizedPoints</strong>(.<strong class="bold">thumb</strong>)</p>
			<p class="source-code">let indexFingerPoints = try observation.recognizedPoints(.indexFinger)</p>
			<p class="source-code">let ringFingerPoints = try observation.recognizedPoints(.ringFinger)</p>
			<p class="source-code">let middleFingerPoints = try observation.recognizedPoints(.middleFinger)</p>
			<p class="source-code">let littleFingerPoints = try observation.recognizedPoints(.littleFinger)</p>
			<p class="source-code">guard let littleTipPoint = littleFingerPoints[.littleTip], let middleTipPoint = middleFingerPoints[.middleTip], let ringTipPoint = ringFingerPoints[.ringTip], let indexTipPoint = indexFingerPoints[.indexTip], let thumbTipPoint = thumbPoints[.thumbTip] else {</p>
			<p class="source-code">  return</p>
			<p class="source-code">}</p>
			<p>Now we are extracting from the observation any instances of <strong class="source-inline">recognizedPoints()</strong> that are related to the thumb and the four fingers. Note that we use <strong class="source-inline">try</strong> to do this operation because a result is <a id="_idIndexMarker807"/>not guaranteed. With the extracted recognized points, we later unwrap the TIP point of each finger and thumb with the <strong class="source-inline">guard</strong> statement.</p>
			<p>At this point, we should have five variables with the coordinates of the TIP point of each finger plus the thumb.</p>
			<p>Although we already have the five coordinates that we are looking for, we still need to perform an extra step. Vision coordinates are different from <strong class="source-inline">AVFoundation</strong> ones. Let's transform them; add the following code just after the last <strong class="source-inline">guard</strong> statement:</p>
			<p class="source-code">thumbTip = CGPoint(x: thumbTipPoint.location.x, y: 1 - thumbTipPoint.location.y)</p>
			<p class="source-code">indexTip = CGPoint(x: indexTipPoint.location.x, y: 1 - indexTipPoint.location.y)</p>
			<p class="source-code">ringTip = CGPoint(x: ringTipPoint.location.x, y: 1 - ringTipPoint.location.y)</p>
			<p class="source-code">middleTip = CGPoint(x: middleTipPoint.location.x, y: 1 - middleTipPoint.location.y)</p>
			<p class="source-code">littleTip = CGPoint(x: littleTipPoint.location.x, y: 1 - littleTipPoint.location.y)</p>
			<p>As you can see, the <strong class="source-inline">x</strong> coordinate is the same in both systems, but the <strong class="source-inline">y</strong> coordinate is different. In Vision, the bottom-left corner is the (0,0). So, we just need to subtract the <strong class="source-inline">y</strong> coordinate of the Vision point to 1 to get a result on the <strong class="source-inline">AVFoundation</strong> system.</p>
			<p>Great! At this point, we have the hand landmarks detection system up and running, with a result in the form of <strong class="source-inline">AVFoundation</strong> CGPoint coordinates. The last step is to draw those points!</p>
			<p>Add the following code after the <strong class="source-inline">catch</strong> block (outside of it), just at the end of the <strong class="source-inline">func captureOutput(…)</strong> method:</p>
			<p class="source-code">DispatchQueue.main.sync {</p>
			<p class="source-code">  self.processPoints([thumbTip, indexTip, ringTip, middleTip, littleTip])</p>
			<p class="source-code">}</p>
			<p>We are <a id="_idIndexMarker808"/>calling the <strong class="source-inline">processPoints(…)</strong> method inside the main thread because we want it to work on the UI, so we ensure that everything works perfectly by dispatching this work into the correct thread. Let's implement the <strong class="source-inline">processPoints(…)</strong> method next.</p>
			<h3>Processing and displaying detected points</h3>
			<p>After the hand <a id="_idIndexMarker809"/>landmarks have <a id="_idIndexMarker810"/>been detected inside the <strong class="source-inline">captureOutput(…)</strong> method, we now want to draw them into the camera overlay. Replace the empty implementation of <strong class="source-inline">processPoints(…)</strong> with this one:</p>
			<p class="source-code">func processPoints(_ fingerTips: [CGPoint?]) {</p>
			<p class="source-code">  // Convert points from AVFoundation coordinates to UIKit // coordinates.</p>
			<p class="source-code">  let previewLayer = cameraView.previewLayer</p>
			<p class="source-code">  let convertedPoints = fingerTips</p>
			<p class="source-code">    .compactMap {$0}</p>
			<p class="source-code">    .compactMap {previewLayer.layerPointConverted(fromCaptureDevicePoint: $0)}</p>
			<p class="source-code">  // Display converted points in the overlay</p>
			<p class="source-code">  cameraView.showPoints(convertedPoints, color: .red)</p>
			<p class="source-code">}</p>
			<p>Remember how we are using <strong class="source-inline">CGPoints</strong> converted to <strong class="source-inline">AVFoundation</strong> coordinates? Now we want to convert those points into the <strong class="source-inline">UIKit</strong> preview layer. We are performing <strong class="source-inline">map</strong> over them, and <a id="_idIndexMarker811"/>finally, we are calling <a id="_idIndexMarker812"/>the <strong class="source-inline">cameraView</strong> helper method <strong class="source-inline">showPoints</strong> to display them.</p>
			<p>Everything is now in place! It is time to build and run the application. You will see the selfie camera triggering, and if you point it at your hand, the tips of your fingers and thumb should be overlayed with red dots. Give it a try and you should get something like the following:</p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/Figure_15.05_B14717.jpg" alt="Figure 15.05 – TIP detection&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.05 – TIP detection</p>
			<p>However, this approach still has some issues! Try this: let the app detect your hand, and then remove the hand from the camera's view – the red dots are still on the overlay! They are not cleaned up when no hand is detected. </p>
			<p>This has an easy fix. The reason for it is that inside the <strong class="source-inline">captureOutput(…)</strong> method, we are not always executing the <strong class="source-inline">processPoints(…)</strong> method. There are times (the <strong class="source-inline">guard</strong> statements) where we <a id="_idIndexMarker813"/>return without calling it. The solution is to wrap the <strong class="source-inline">processPoints(…)</strong> block into a <strong class="source-inline">defer</strong>, moving <a id="_idIndexMarker814"/>it to the beginning of the code, just after we define the five properties to store the coordinates of each tip. It should look like this:</p>
			<p class="source-code">public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {</p>
			<p class="source-code">  var thumbTip: CGPoint?</p>
			<p class="source-code">  var indexTip: CGPoint?</p>
			<p class="source-code">  var ringTip: CGPoint?</p>
			<p class="source-code">  var middleTip: CGPoint?</p>
			<p class="source-code">  var littleTip: CGPoint?</p>
			<p class="source-code">  <strong class="bold">defer {</strong></p>
			<p class="source-code"><strong class="bold">    DispatchQueue.main.sync {</strong></p>
			<p class="source-code"><strong class="bold">      self.processPoints([thumbTip, indexTip, ringTip, middleTip, littleTip])</strong></p>
			<p class="source-code"><strong class="bold">    }</strong></p>
			<p class="source-code"><strong class="bold">  }</strong></p>
			<p class="source-code">…</p>
			<p class="source-code">}</p>
			<p>The highlighted code is the part that we have wrapped into a <strong class="source-inline">defer</strong> (so it will always execute <a id="_idIndexMarker815"/>before returning the method). Execute <a id="_idIndexMarker816"/>the app again, and you will notice that when there is no hand on the screen, the red dots will not be there either! We are calling <strong class="source-inline">processPoints</strong> with empty values, so nothing is being drawn. With this last step, we have a working example of hand landmark detection up and running! Congratulations!</p>
			<p class="callout-heading">Body pose detection</p>
			<p class="callout">Vision also provides body pose detection on iOS 14. Body pose detection is quite similar to hand detection, so we are not going to give a step-by-step demo of it. But the code bundle of this book contains an example app similar to the one in this section but for body pose detection. You can check out the project named <strong class="source-inline">BodyPoseDetection_completed</strong> and see the little differences that it has from the hand detection project.</p>
			<p>In this section, we have learned about the new Vision methods to detect hand landmarks and how to detect hand landmarks using the video <a id="_idIndexMarker817"/>stream of a phone <a id="_idIndexMarker818"/>as input (instead of just detecting a hand in a static image). We also provided a similar demo that can be used for body pose detection. Let's jump into <a id="_idTextAnchor492"/>the summary to finish the chapter.</p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor493"/>Summary</h1>
			<p>We started this chapter by learning about the basic building blocks of every Vision feature: how to use a <strong class="source-inline">VNRequest</strong> instance, its corresponding <strong class="source-inline">VNRequestHandler</strong> instances, and the resulting <strong class="source-inline">VNObservation</strong> instances. </p>
			<p>After learning the basics, we applied them to text recognition. We compared different recognition levels by using <strong class="source-inline">.fast</strong> and <strong class="source-inline">.accurate</strong>. We also learned about regions of interest and how they can affect the performance of Vision requests. Finally, we improved our results in text recognition by applying domain knowledge, fixing potential errors and misreads from Vision.</p>
			<p>Finally, we learned about the new hand landmarks recognition capability. But this time, we also learned how to apply Vision requests to real-time video streams. We were able to detect hand landmarks in a video feed from a device's front camera and display an overlay to show the results. This chapter also provided a similar example that could be applied to body pose recognition.</p>
			<p>In the next chapter, we will learn about a brand new feature of iOS 14: widgets!</p>
		</div>
	</body></html>