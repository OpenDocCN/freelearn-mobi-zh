<html><head></head><body>
		<div><h1 id="_idParaDest-229"><em class="italic"><a id="_idTextAnchor484"/>Chapter 15</em>: Recognition with Vision Framework</h1>
			<p>The Vision framework has been available to developers for a few years now. Apple has been introducing better and better features for it, from text recognition to image recognition. On iOS 14, Vision comes with more improvements to text recognition and other existing functions, but it also allows developers to perform two different actions: hand and body pose recognition. The possibilities that these new features open up for developers are limitless! Just think about gym apps, yoga apps, health apps, and so on.</p>
			<p>In this chapter, we are going to learn about the basics of the Vision framework and how to use the new advancements in text recognition. We will also learn about the new hand landmark recognition, building a demo app that can detect the tips of the four fingers and the thumb. The chapter code bundle also provides a similar example demonstrating body pose recognition. We will discuss these topics in the following sections:</p>
			<ul>
				<li>Introduction to the Vision framework</li>
				<li>Recognizing text in images</li>
				<li>Recognizing hand landmarks in real time</li>
			</ul>
			<p>By the end of this chapter, you will be able to work with the Vision framework with total confidence, being able to apply the techniques explained in this chapter to implement any type of recognition that Vision provides, from the recognition of text in images to the recognition of hand and body poses in videos.</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor485"/>Technical requirements</h1>
			<p>The code bundle for this chapter includes a starter project called <code>HandDetection_start</code> and a couple of playground files named <code>Vision.playground</code> and <code>RecognitionPerformance_start.playground</code>. It also contains a completed example for body pose detection named <code>BodyPoseDetection_completed</code>. You can find them in the code bundle repository:</p>
			<p>https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition</p>
			<h1 id="_idParaDest-231"><a id="_idTextAnchor486"/>Introduction to the Vision framework</h1>
			<p>Since the beginning of the App Store, there have been many apps that use the camera to build great <a id="_idIndexMarker772"/>functionalities using image and video recognition. Think of the bank apps that can now scan a check or a credit card so that the user doesn't need to input all the numbers. There are networking apps that can take a picture of a business card and extract the relevant information. Even the Photos app from your iPhone can detect faces in your photographs and classify them. </p>
			<p>The Vision framework provides developers with a robust set of features to make it easier than ever to achieve these functionalities: from text and image recognition to barcode detection, face landmarks analysis, and now, with iOS 14, hand and body pose recognition.</p>
			<p>Vision also allows the use of Core ML models to allow developers to enhance object classification and detection in their apps. Vision has been available since iOS 11 and macOS 10.13.</p>
			<p>There are several concepts in Vision that are common to any type of detection (text detection, image detection, barcode detection, and so on), including the <code>VNRequest</code>, <code>VNRequestHandler</code>, and <code>VNObservation</code> entities:</p>
			<ul>
				<li><code>VNRequest</code> is the task that we want to perform. For example, <code>VNDetectAnimalRequest</code> would be used to detect animals in a picture.</li>
				<li><code>VNRequestHandler</code> is how we want to detect. It lets us define a completion handler where we can play around with the results and shape them in the way that we need.</li>
				<li><code>VNObservation</code> encapsulates the results.</li>
			</ul>
			<p>Let's look at an example that combines all these concepts and shows how Vision can easily help us to detect text inside an image. Open the playground named <code>Vision.playground</code>. This example code is grabbing an image from a specific URL and trying to extract/detect any text on it. The image being used is this one:</p>
			<div><div><img src="img/Figure_15.01_B14717.jpg" alt="Figure 15.01 – Example image to extract text with Vision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.01 – Example image to extract text with Vision</p>
			<p>If we try to extract text from this image, we should get results such as <em class="italic">Swift Data Structure and Algorithms</em>, or the name of the authors, or the description below the title. Let's review the code in the <a id="_idIndexMarker773"/>playground:</p>
			<pre>import Vision
let imageUrl = URL(string: "http://marioeguiluz.com/img/portfolio/Swift%20Data%20Structures%20and%20Algorithms%20Mario%20Eguiluz.jpg")!
// 1. Create a new image-request handler.
let requestHandler = <strong class="bold">VNImageRequestHandler</strong>(url: imageUrl, options: [:])
// 2. Create a new request to recognize text.
let request = <strong class="bold">VNRecognizeTextRequest</strong> { (request, error) in
  guard let observations = request.results as? [VNRecognizedTextObservation] else { return }
  let recognizedStrings = <strong class="bold">observations</strong>.compactMap { observation in
    // Return the string of the top VNRecognizedText instance.
    return observation.topCandidates(1).first?.string
  }
  // Process the recognized strings.
  print(recognizedStrings)
}
// 3. Select .accurate or .fast levels
request.recognitionLevel = .accurate
do {
  // 4. Perform the text-recognition request.
  try requestHandler.perform([request])
} catch {
  print("Unable to perform the requests: \(error).")
}</pre>
			<p>Let's go through <a id="_idIndexMarker774"/>the numbered comments:</p>
			<ol>
				<li>First, we are creating a <code>VNImageRequestHandler</code> instance with a given image URL. We instantiate this handler to perform Vision requests on an image. Remember that we need to call <code>perform(_:)</code> later on to launch the analysis.</li>
				<li>Now we create a <code>request(VNRecognizeTextRequest)</code> instance that we will perform on the <code>requestHandler</code> instance instantiated previously. You can perform multiple requests on a <code>requestHandler</code> instance. We define a block of code to be executed when the request finishes. In this block, we are extracting the observations from the request results (<code>VNRecognizedTextObservation</code> instances). These observations will contain potential outcomes for the analyzed text from the image (<code>VNRecognizedText</code> instances). We print <code>topCandidate</code> from each observation, which should be the best match according to the Vision parameters.</li>
				<li>We can specify the recognition level for the request. In this example, we are using <code>.accurate</code> (the alternative is <code>.fast</code>). We will see later the results with <code>.fast</code> and when to use one or the other.</li>
				<li>Finally, we are <a id="_idIndexMarker775"/>performing the request on the <code>requestHandler</code> instance to execute everything with the <code>perform(_:)</code> method.</li>
			</ol>
			<p>If you execute the code, the console in the playground will display the following:</p>
			<pre>["Erik Azar, Mario Eguiluz", "Swift Data", "Structure and", "Algorithms", "Master the most common algorithms and data structures,", "and learn how to implement them efficiently using the most", "up-to-date features of Swift", "Packt&gt;"]</pre>
			<p>Those seem to be great results, right? If you recheck the image, we are extracting the correct text from it! The author names, the title (per line), the description, and more! Seems to be a great result! But have you noticed that when you execute the playground, it takes a while to finish? This is because we are using the <code>.accurate</code> option. Let's see what happens if we use <code>.fast</code> instead. Change it in the playground code:</p>
			<pre>// 3. Select .accurate or .fast levels
request.recognitionLevel = .fast</pre>
			<p>The output is as follows:</p>
			<pre>["Swift Data", "Structure and", "Algorithms", "upto4atefeaturesofSwift3", "Packt&gt;", "ErfkAz•r. M•rb Eguluz", "ml5tertket(w4VIthMsarodats5tr&amp;KtUre", "learnItolpIettmeffK1WttIY5lt1fft", "LIJJ"]</pre>
			<p>This time, the analysis can be done faster, but as you can see, the results are far worse for what we wanted (we wanted to detect the text properly!). Why should anyone prefer speed over accuracy? Well, for some apps, speed is critical and it is fine to sacrifice some accuracy for it. Think of real-time camera-based translations or applying real-time filters to take photos. In these scenarios, you need fast processing. We will discuss this further later in the chapter.</p>
			<p>This playground example should help you to have a grasp of the incredible potential that Vision contains. Just with a <a id="_idIndexMarker776"/>few lines of code, we were able to process and extract the text of an image with no issues or complex operations. Vision allows developers to do amazing things. Let's dive deeper into it in the following sections, starting with a more detailed look at text detection for images.</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor487"/>Recognizing text in images</h1>
			<p>The <a id="_idIndexMarker777"/>Vision framework has been improving its detection of text in images since its first iteration. In this section, we are going to learn some state-of-the-art techniques to obtain the best results on iOS 14.</p>
			<p>We saw in the previous section that text detection in Vision can happen in two different ways, as defined by the value of <code>recognitionLevel</code> that we specify in the request: <code>.fast</code> and <code>.accurate</code>. Let's see the differences:</p>
			<ul>
				<li><code>.accurate</code>. It doesn't handle rotated text or different fonts as well as the <code>.accurate</code> method.</li>
				<li><code>.fast</code> but is more accurate (of course!). It works in the same way that our brain recognizes words. If you read the word "m0untain," your brain can extract "mountain" from it, and it knows that the 0 (zero) stands for an o. If you use <code>.fast</code>, which recognizes character by character, the 0 (zero) would still be a 0 (zero) in your results, because no context is taken into account.</li>
			</ul>
			<p>In both cases, after the initial recognition phase is finished, results are passed into a traditional natural language processor for language processing, and the outcome of that is the results (observations). This whole process happens exclusively on the device.</p>
			<p>So, when should anyone use <code>.fast</code>, you might wonder. Well, there are scenarios in which it is more convenient than <code>.accurate</code>:</p>
			<ul>
				<li>To read codes or barcodes quickly</li>
				<li>When user interactivity is a crucial aspect, so you want a fast response from the text detection</li>
			</ul>
			<p>To demonstrate the differences between the recognition levels, let's analyze the same image <a id="_idIndexMarker780"/>using different techniques. You will also learn some useful tricks that you can apply to your projects. Follow the steps given here:</p>
			<ol>
				<li value="1">Go ahead and open the playground named <code>RecognitionPerformance_start.playground</code>. The code is roughly the same as what we tried in the previous section. <p>The only difference is that the image that we are using now contains a 4-digit number that represents the serial number of the book:</p><div><img src="img/Figure_15.02_B14717.jpg" alt="Figure 15.02 – Book cover with a serial number (1015) below the author names&#13;&#10;"/></div><p class="figure-caption">Figure 15.02 – Book cover with a serial number (1015) below the author names</p><p>If you pay close attention to the number font, you will see that it might be tricky for a computer to tell whether some digits are numbers or letters. This has been done on purpose. In this <a id="_idIndexMarker781"/>example, we are going to test the capabilities of Vision.</p></li>
				<li>Go ahead and execute the playground code. The console output should look like this:<pre><strong class="bold">["Erik Azar, Mario Eguiluz", "1015", "Swift Data", "Structure and", "Algorithms", "Master the most common algorithms and data structures,", "and learn how to implement them efficiently using the most", "up-to-date features of Swift", "Packt&gt;"]</strong>
<strong class="bold">1.9300079345703125 seconds</strong></pre></li>
			</ol>
			<p>We have successfully retrieved the serial number of the book: <code>1015</code>. The code is also taking a measure of how long it takes to finish the text-recognition process. In our case, it was <strong class="bold">1.93 seconds</strong> (this can differ from computer to computer and also between executions). Can we do better than that? Let's try out some techniques that will help us to improve this processing time while keeping the same accuracy. We are going to start with the <strong class="bold">region of interest</strong>.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor488"/>Region of interest</h2>
			<p>Sometimes, when we are analyzing an image with Vision, we don't need to process the whole image. For example, if <a id="_idIndexMarker782"/>we are processing a <a id="_idIndexMarker783"/>specific type of form where we know in advance that the first name always goes at the top of the document, we may want to just process that area. Processing the whole form would only waste time and resources if we just need a specific area.</p>
			<p>Let's assume that in the previous example (the book cover), the serial number that we want to extract is always in the top-left area. How can we speed up the 1.93-seconds processing time? We can do so by defining a region of interest. Defining a region of interest will tell Vision to only process that area and avoid the rest of the image. That will result in a faster processing time. </p>
			<p><code>regionOfInterest</code> is a <code>CGRect</code> property of <code>VNRequest</code>:</p>
			<ul>
				<li>It defines a rectangular area in which the request will be performed.</li>
				<li>The rectangle is normalized to the dimensions of the image, meaning that the width and height of the region of interest go from 0 to 1.</li>
				<li>The origin of the rectangle is in the bottom-left corner of the image, which is (0,0). The top-right corner will be (1,1).</li>
				<li>The default value is <code>{{0,0},{1,1}}</code>, which covers everything from the bottom-left corner (0,0) to the top-right corner, with width 1 and height 1: the whole image.</li>
			</ul>
			<p>In the following figure, you can see the region of interest that we need to define to capture the serial number (<strong class="bold">1015</strong>):</p>
			<div><div><img src="img/Figure_15.03_B14717.jpg" alt="Figure 15.03 – Region of interest&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.03 – Region of interest</p>
			<p>Let's add that region to <a id="_idIndexMarker784"/>the code from the <a id="_idIndexMarker785"/>previous section: </p>
			<ol>
				<li value="1">In the <code>ScanPerformance_start.playground</code> project, add the following code just after setting <code>recoginitionLevel</code> to <code>.accurate</code>:<pre>request.regionOfInterest = CGRect(x: 0, y: 0.8, width: 0.7, height: 0.2)</pre></li>
				<li>Now launch the playground and check the result in the console:<pre><strong class="bold">["Erik Azar, Mario Eguiluz", "1015"]</strong>
<strong class="bold">1.2314139604568481 seconds</strong></pre><p>There are a couple of differences when comparing these results to the previous ones:</p><ul><li>We are no longer extracting that much text. Now that we are defining a region of interest, we just extract the words/digits that are contained in that area.</li><li>We reduced the processing time from 1.93 seconds to 1.23 seconds. That is 36% faster.</li></ul></li>
				<li>Let's now try to reduce the region of interest to catch just the serial number. Modify the region to the following:<pre>request.regionOfInterest = CGRect(x: 0, y: 0.8, width: 0.3, height: 0.1)</pre></li>
				<li>Launch the playground. Now the console output is as follows:<pre><code>.fast</code> for <code>recognitionLevel</code> instead of <code>.accurate</code>, if what we want is speed? Let's see what happens. </p></li>
				<li>Modify this line to use <code>.fast</code>:<pre>request.recognitionLevel = .fast</pre></li>
				<li>Save and execute. Check the console output:<pre><strong class="bold">["Iois"]</strong>
<strong class="bold">0.5968900661468506 seconds</strong></pre></li>
			</ol>
			<p>You can see how this time, the processing time has been made shorter again, but the result is not accurate at all. Instead of detecting <code>1015</code>, we have wrongly obtained <code>Iois</code>. </p>
			<p>However, there is a common way to fix this situation in scenarios where we have domain knowledge. In our case, we know that the processed characters should be numbers only. Therefore, we can adjust the output from Vision to improve the results and fix misclassifications. For example, see the following adjustments:</p>
			<ul>
				<li>The character "I" can be "1."</li>
				<li>The character "o" can be "0."</li>
				<li>The character "s" can be "5."</li>
			</ul>
			<p>Let's implement this in the code:</p>
			<ol>
				<li value="1">At the very end <a id="_idIndexMarker788"/>of the playground file, add the <a id="_idIndexMarker789"/>following method:<pre>extension Character {
  func transformToDigit() -&gt; Character {
    let conversionTable = [
      "s": "5",
      "S": "5",
      "o": "0",
      "O": "0",
      "i": "1",
      "I": "1"
    ]
    var current = String(self)
    if let alternativeChar = conversionTable[current] {
      current = alternativeChar
    }
    return current.first!
  }
}</pre><p>We are extending the <code>Character</code> class by adding a new method named <code>transformToDigit()</code>. This new method is going to help us to improve potential misclassifications. Note how in the method itself, we have a table of letter characters that relate to a similarly shaped number. What we are doing is just transforming those letters into the corresponding digits.</p></li>
				<li>Let's use it now. Below the <code>print(recognizedStrings)</code> line, add the following code:<pre>if let serialNumber = recognizedStrings.first {
  let serialNumberDigits = serialNumber.map { $0.transformToDigit() }
  print(serialNumberDigits)
}</pre><p>We are getting the result of the <a id="_idIndexMarker790"/>Vision process; in our case, it was <code>"Iois"</code>, and for each <a id="_idIndexMarker791"/>character, we are applying to it our new <code>transformToDigit()</code> method. </p></li>
				<li>Execute the code, and you will see the following result in the console:<pre><strong class="bold">["Iois"]</strong>
<strong class="bold">["1", "0", "1", "5"]</strong>
<strong class="bold">0.5978780269622803 seconds</strong></pre></li>
			</ol>
			<p>That looks great! Note how the <code>"Iois"</code> result is now looking much better when transformed to <code>"1" "0" "1" "5"</code>.  Also, note how the processing time didn't increase that much; this operation is relatively easy to compute.</p>
			<p>Now let's summarize what we have done in this section and the improvements we made in each step. We started by processing a whole image and using the <code>.accurate</code> recognition level, and that took us 1.93 seconds. Then, we applied a region of interest to just process part of the image that we were interested in, reducing the processing time to 1.23 seconds. After that, we changed from <code>.accurate</code> to <code>.fast</code>. This move reduced the processing time to 0.59 seconds, but the results were incorrect. Finally, we implemented an easy algorithm to improve the results and make them as good as with the <code>.accurate</code> level. So, in the end, we got perfect results and a processing time of 0.59 seconds rather than 1.93! </p>
			<p>In the next section, you will learn about one of the new features of iOS14, hand detection.</p>
			<h1 id="_idParaDest-234"><a id="_idTextAnchor489"/>Recognizing hand landmarks in real time</h1>
			<p>One of the additions to Vision in iOS 14 is hand detection. This new feature to detect hands in images and <a id="_idIndexMarker792"/>video allows developers to find with great detail the positions of the wrist and the individual fingers in a <a id="_idIndexMarker793"/>video frame or photo. </p>
			<p>In this section, we are going to explain the basics behind hand detection, and we will demonstrate how it works with a sample project. Let's start with the hand landmarks that we will be able to recognize.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor490"/>Understanding hand landmarks</h2>
			<p>There are 21 landmarks <a id="_idIndexMarker794"/>that we will be able to detect in a hand:</p>
			<ul>
				<li>4 in the thumb</li>
				<li>4 in each finger (16 in total)</li>
				<li>1 in the wrist</li>
			</ul>
			<p>As you can see, Vision differentiates between finger and thumb. In both the finger and thumb, there are 4 points of interest. The following figure shows how these landmarks are distributed:</p>
			<div><div><img src="img/Figure_15.04_B14717.jpg" alt="Figure 15.04 – Finger and thumb landmarks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.04 – Finger and thumb landmarks</p>
			<p>Note how there is also a <a id="_idIndexMarker795"/>landmark in the middle of the wrist.</p>
			<p>For the four fingers, we can access each of them individually using the following keys:</p>
			<ul>
				<li><code>littleFinger</code></li>
				<li><code>middleFinger</code></li>
				<li><code>ringFinger</code></li>
				<li><code>indexFinger</code></li>
			</ul>
			<p>Inside each of them, we can <a id="_idIndexMarker796"/>access the four different landmarks:</p>
			<ul>
				<li><strong class="bold">TIP</strong></li>
				<li><strong class="bold">DIP</strong></li>
				<li><strong class="bold">PIP</strong></li>
				<li><strong class="bold">MCP</strong></li>
			</ul>
			<p>Note how for the thumb, these names are slightly different (TIP, IP, PIP, and CMC). In the example code that we will build later in this section, we will demonstrate how to use these points and each of the fingers plus the thumb.</p>
			<p>Vision is capable of detecting more than just one hand at a time. We can specify the maximum amount of hands that we want to detect. This parameter will have an impact on the performance of our detection. Use <code>maximumHandCount</code> to set the limit.</p>
			<p>For performance and accuracy, it is also better if the hand is not near the edges of the frame, if the light conditions <a id="_idIndexMarker797"/>are good, and if the hands are perpendicular to the camera angle (so the whole hand is visible, not just the edge of it). Also, take into account that feet can be recognized as hands sometimes, so avoid mixing them. </p>
			<p>That is enough theory; let's jump straight into a code example! We will build a demo app that will be able to detect hand landmarks using the front video camera of a phone and will display an overlay on the detected points.</p>
			<h2 id="_idParaDest-236"><a id="_idTextAnchor491"/>Implementing hand detection</h2>
			<p>In this section, we are <a id="_idIndexMarker798"/>going to implement a demo app that will be able to detect hand landmarks using the front video camera of a phone.</p>
			<p>The code bundle of this project contains the initial project and also the final result. Go ahead and open the project named <code>HandDetection_start</code>.</p>
			<p>The project contains two main files: A <code>UIView</code> instance named <code>CameraView.swift</code> and a UIViewController instance called <code>CameraViewController.swift</code>. </p>
			<p>The view contains helper methods to draw points on coordinates. It will serve as an overlay to draw on top of the camera feed. Just know that the <code>showPoints(_ points: [CGPoint], colour: UIColor)</code> method will allow us to draw an array of <code>CGPoint</code> structs into the overlay on top of the video camera feed.</p>
			<p>The view controller will be the centerpiece of the example and is where we are going to implement the relevant code to perform the hand detection. Go ahead and open the <code>CameraViewController.swift</code> file. Let's examine the code skeleton that we will fill out step by step.</p>
			<p>At the top of the file, we are <a id="_idIndexMarker799"/>defining four properties:</p>
			<ul>
				<li><code>handPoseRequest: VNDetectHumanHandPoseRequest</code>. We will apply this request at the top of the video stream, to detect hand landmarks in each frame. If we detect any, we will display some points in the overlay to show them.</li>
				<li><code>videoDataOutputQueue</code>, <code>cameraView</code>, and <code>cameraFeedSession</code>.</li>
			</ul>
			<p>With the <code>viewDidAppear</code> and <code>viewWillDisappear</code> methods, we are starting/creating and stopping <code>AVCaptureSession</code> for the camera. </p>
			<p>And finally, in the next four methods, we have four TODO comments, which we are going to implement one by one to create this app. Let's summarize the TODO tasks that we are going to <a id="_idIndexMarker800"/>perform:</p>
			<ul>
				<li><strong class="bold">TODO 1</strong>: Detect one hand only.</li>
				<li><strong class="bold">TODO 2</strong>: Create a video session.</li>
				<li><strong class="bold">TODO 3</strong>: Perform hand detection on the video session.</li>
				<li><strong class="bold">TODO 4</strong>: Process and display detected points.</li>
			</ul>
			<p>We are going to implement these four tasks in the following subsections.</p>
			<h3>Detecting hands</h3>
			<p>Vision can do more than <a id="_idIndexMarker801"/>detect one hand at a time. The more hands we ask it to detect, the more it will impact performance. In our example, we only want to detect one hand. By setting <code>maximumHandCount</code> to <code>1</code> on the request, we will improve performance. </p>
			<p>Let's start by adding this code below <code>// TODO 1</code>:</p>
			<pre>// TODO 1: Detect one hand only.
handPoseRequest.maximumHandCount = 1</pre>
			<p>Now, let's create a video session to capture the video stream from the front video camera of the device.</p>
			<h3>Creating a video session</h3>
			<p>For <a id="_idIndexMarker802"/>the second task, we are going to fill out the code inside the <code>setupAVSession()</code> method. Go ahead and paste the following code inside the method:</p>
			<pre>// TODO 2: Create video session
// 1 - Front camera as input
guard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {
  fatalError("No front camera.")
}
// 2- Capture input from the camera
guard let deviceInput = try? AVCaptureDeviceInput(device: videoDevice) else {
  fatalError("No video device input.")
}</pre>
			<p>First, we are creating a <code>videoDevice: AVCaptureDevice</code> instance by querying for the video front camera (if it exists!) with this:</p>
			<pre>guard let <strong class="bold">videoDevice</strong> = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {
  fatalError("No front camera.")
}</pre>
			<p>Then, we use that <code>videoDevice</code> to generate a <code>deviceInput: AVCaptureDeviceInput</code> instance, which will be the video device used for the stream, with the following code:</p>
			<pre>guard let <strong class="bold">deviceInput</strong> = try? AVCaptureDeviceInput(device: videoDevice) else {
  fatalError("No video device input.")
}</pre>
			<p>Now add <a id="_idIndexMarker803"/>this code:</p>
			<pre>let session = AVCaptureSession()
session.beginConfiguration()
session.sessionPreset = AVCaptureSession.Preset.high
// Add video input to session
guard session.canAddInput(deviceInput) else {
  fatalError("Could not add video device input to the session")
}
session.addInput(deviceInput)
let dataOutput = AVCaptureVideoDataOutput()
if session.canAddOutput(dataOutput) {
  session.addOutput(dataOutput)
  // Add a video data output.
  dataOutput.alwaysDiscardsLateVideoFrames = true
  dataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: Int(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)]
  dataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)
} else {
  fatalError("Could not add video data output to the session")
}
session.commitConfiguration()
cameraFeedSession = session</pre>
			<p>After creating the <code>videoDevice</code> instance, we are creating a new <code>session: AVCaptureSession</code> instance. With the session created, we assign <code>videoDevice</code> as the input and create and configure an output to handle the video stream. We assign the class itself as <code>dataOutput AVCaptureVideoDataOutputSampleBufferDelegate</code> by calling this:</p>
			<pre>dataOutput.<strong class="bold">setSampleBufferDelegate</strong>(self, queue: videoDataOutputQueue)</pre>
			<p>This means that when the <a id="_idIndexMarker804"/>front video camera captures new frames, our session will handle them and send them to our delegate method, which we are going to implement in the next step (TODO 3).</p>
			<h3>Performing hand detection in the video session</h3>
			<p>Now that we have set up <a id="_idIndexMarker805"/>and configured a video session, it is time to handle every frame as it comes and tries to detect any hands and their landmarks! We need to implement the <code>captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)</code> method.</p>
			<p>Under the <code>// TODO 3: Perform hand detection on the video session</code> line, add this code:</p>
			<pre>var thumbTip: CGPoint?
var indexTip: CGPoint?
var ringTip: CGPoint?
var middleTip: CGPoint?
var littleTip: CGPoint?</pre>
			<p>We want to detect the tip of the four fingers (index, ring, middle, and little) and the thumb. So, we are creating five variables of type <code>CGPoint</code> to store their coordinates, if they are found.</p>
			<p>Just after these new lines, add the following code:</p>
			<pre>let handler = VNImageRequestHandler(cmSampleBuffer: <strong class="bold">sampleBuffer</strong>, orientation: .up, options: [:])
do {
  try handler.perform([<strong class="bold">handPoseRequest</strong>])
  guard let observation = handPoseRequest.results?.first else {
    return
  }
	// Get observation points
} catch {
  cameraFeedSession?.stopRunning()
  fatalError(error.localizedDescription)
}</pre>
			<p>With this code, we are <a id="_idIndexMarker806"/>asking Vision to execute <code>handPoseRequest</code> over <code>sampleBuffer</code> (the video stream). Then, we guard (using <code>guard</code>) against the case in which we don't detect any observations (so that if there is no hand in the video frame, we just stop at this point).</p>
			<p>But if the guard doesn't trigger, it means that we have some hand landmarks and we need to process them. Add the following code just after the <code>// Get observation points</code> line:</p>
			<pre>let thumbPoints = try observation.<strong class="bold">recognizedPoints</strong>(.<strong class="bold">thumb</strong>)
let indexFingerPoints = try observation.recognizedPoints(.indexFinger)
let ringFingerPoints = try observation.recognizedPoints(.ringFinger)
let middleFingerPoints = try observation.recognizedPoints(.middleFinger)
let littleFingerPoints = try observation.recognizedPoints(.littleFinger)
guard let littleTipPoint = littleFingerPoints[.littleTip], let middleTipPoint = middleFingerPoints[.middleTip], let ringTipPoint = ringFingerPoints[.ringTip], let indexTipPoint = indexFingerPoints[.indexTip], let thumbTipPoint = thumbPoints[.thumbTip] else {
  return
}</pre>
			<p>Now we are extracting from the observation any instances of <code>recognizedPoints()</code> that are related to the thumb and the four fingers. Note that we use <code>try</code> to do this operation because a result is <a id="_idIndexMarker807"/>not guaranteed. With the extracted recognized points, we later unwrap the TIP point of each finger and thumb with the <code>guard</code> statement.</p>
			<p>At this point, we should have five variables with the coordinates of the TIP point of each finger plus the thumb.</p>
			<p>Although we already have the five coordinates that we are looking for, we still need to perform an extra step. Vision coordinates are different from <code>AVFoundation</code> ones. Let's transform them; add the following code just after the last <code>guard</code> statement:</p>
			<pre>thumbTip = CGPoint(x: thumbTipPoint.location.x, y: 1 - thumbTipPoint.location.y)
indexTip = CGPoint(x: indexTipPoint.location.x, y: 1 - indexTipPoint.location.y)
ringTip = CGPoint(x: ringTipPoint.location.x, y: 1 - ringTipPoint.location.y)
middleTip = CGPoint(x: middleTipPoint.location.x, y: 1 - middleTipPoint.location.y)
littleTip = CGPoint(x: littleTipPoint.location.x, y: 1 - littleTipPoint.location.y)</pre>
			<p>As you can see, the <code>x</code> coordinate is the same in both systems, but the <code>y</code> coordinate is different. In Vision, the bottom-left corner is the (0,0). So, we just need to subtract the <code>y</code> coordinate of the Vision point to 1 to get a result on the <code>AVFoundation</code> system.</p>
			<p>Great! At this point, we have the hand landmarks detection system up and running, with a result in the form of <code>AVFoundation</code> CGPoint coordinates. The last step is to draw those points!</p>
			<p>Add the following code after the <code>catch</code> block (outside of it), just at the end of the <code>func captureOutput(…)</code> method:</p>
			<pre>DispatchQueue.main.sync {
  self.processPoints([thumbTip, indexTip, ringTip, middleTip, littleTip])
}</pre>
			<p>We are <a id="_idIndexMarker808"/>calling the <code>processPoints(…)</code> method inside the main thread because we want it to work on the UI, so we ensure that everything works perfectly by dispatching this work into the correct thread. Let's implement the <code>processPoints(…)</code> method next.</p>
			<h3>Processing and displaying detected points</h3>
			<p>After the hand <a id="_idIndexMarker809"/>landmarks have <a id="_idIndexMarker810"/>been detected inside the <code>captureOutput(…)</code> method, we now want to draw them into the camera overlay. Replace the empty implementation of <code>processPoints(…)</code> with this one:</p>
			<pre>func processPoints(_ fingerTips: [CGPoint?]) {
  // Convert points from AVFoundation coordinates to UIKit // coordinates.
  let previewLayer = cameraView.previewLayer
  let convertedPoints = fingerTips
    .compactMap {$0}
    .compactMap {previewLayer.layerPointConverted(fromCaptureDevicePoint: $0)}
  // Display converted points in the overlay
  cameraView.showPoints(convertedPoints, color: .red)
}</pre>
			<p>Remember how we are using <code>CGPoints</code> converted to <code>AVFoundation</code> coordinates? Now we want to convert those points into the <code>UIKit</code> preview layer. We are performing <code>map</code> over them, and <a id="_idIndexMarker811"/>finally, we are calling <a id="_idIndexMarker812"/>the <code>cameraView</code> helper method <code>showPoints</code> to display them.</p>
			<p>Everything is now in place! It is time to build and run the application. You will see the selfie camera triggering, and if you point it at your hand, the tips of your fingers and thumb should be overlayed with red dots. Give it a try and you should get something like the following:</p>
			<div><div><img src="img/Figure_15.05_B14717.jpg" alt="Figure 15.05 – TIP detection&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.05 – TIP detection</p>
			<p>However, this approach still has some issues! Try this: let the app detect your hand, and then remove the hand from the camera's view – the red dots are still on the overlay! They are not cleaned up when no hand is detected. </p>
			<p>This has an easy fix. The reason for it is that inside the <code>captureOutput(…)</code> method, we are not always executing the <code>processPoints(…)</code> method. There are times (the <code>guard</code> statements) where we <a id="_idIndexMarker813"/>return without calling it. The solution is to wrap the <code>processPoints(…)</code> block into a <code>defer</code>, moving <a id="_idIndexMarker814"/>it to the beginning of the code, just after we define the five properties to store the coordinates of each tip. It should look like this:</p>
			<pre>public func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
  var thumbTip: CGPoint?
  var indexTip: CGPoint?
  var ringTip: CGPoint?
  var middleTip: CGPoint?
  var littleTip: CGPoint?
  <strong class="bold">defer {</strong>
<strong class="bold">    DispatchQueue.main.sync {</strong>
<strong class="bold">      self.processPoints([thumbTip, indexTip, ringTip, middleTip, littleTip])</strong>
<strong class="bold">    }</strong>
<strong class="bold">  }</strong>
…
}</pre>
			<p>The highlighted code is the part that we have wrapped into a <code>defer</code> (so it will always execute <a id="_idIndexMarker815"/>before returning the method). Execute <a id="_idIndexMarker816"/>the app again, and you will notice that when there is no hand on the screen, the red dots will not be there either! We are calling <code>processPoints</code> with empty values, so nothing is being drawn. With this last step, we have a working example of hand landmark detection up and running! Congratulations!</p>
			<p class="callout-heading">Body pose detection</p>
			<p class="callout">Vision also provides body pose detection on iOS 14. Body pose detection is quite similar to hand detection, so we are not going to give a step-by-step demo of it. But the code bundle of this book contains an example app similar to the one in this section but for body pose detection. You can check out the project named <code>BodyPoseDetection_completed</code> and see the little differences that it has from the hand detection project.</p>
			<p>In this section, we have learned about the new Vision methods to detect hand landmarks and how to detect hand landmarks using the video <a id="_idIndexMarker817"/>stream of a phone <a id="_idIndexMarker818"/>as input (instead of just detecting a hand in a static image). We also provided a similar demo that can be used for body pose detection. Let's jump into <a id="_idTextAnchor492"/>the summary to finish the chapter.</p>
			<h1 id="_idParaDest-237"><a id="_idTextAnchor493"/>Summary</h1>
			<p>We started this chapter by learning about the basic building blocks of every Vision feature: how to use a <code>VNRequest</code> instance, its corresponding <code>VNRequestHandler</code> instances, and the resulting <code>VNObservation</code> instances. </p>
			<p>After learning the basics, we applied them to text recognition. We compared different recognition levels by using <code>.fast</code> and <code>.accurate</code>. We also learned about regions of interest and how they can affect the performance of Vision requests. Finally, we improved our results in text recognition by applying domain knowledge, fixing potential errors and misreads from Vision.</p>
			<p>Finally, we learned about the new hand landmarks recognition capability. But this time, we also learned how to apply Vision requests to real-time video streams. We were able to detect hand landmarks in a video feed from a device's front camera and display an overlay to show the results. This chapter also provided a similar example that could be applied to body pose recognition.</p>
			<p>In the next chapter, we will learn about a brand new feature of iOS 14: widgets!</p>
		</div>
	</body></html>