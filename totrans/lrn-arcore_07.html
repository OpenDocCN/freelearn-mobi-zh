<html><head></head><body>
        

                            
                    <h1 class="header-title">Light Estimation</h1>
                
            
            
                
<p class="mce-root">Magicians spend hours in front of a mirror, watching and studying every angle of their performance in order to get it just right. They realize that every detail needs to be perfect in order for the audience to believe in the illusion. Even a single mistake can ruin not only the illusion, but the entire performance and credibility of the magician. As harsh as it is, this is no different to what it's like building an AR app. If your app will immerse a user in your world, you need to make it as believable as possible. This includes ensuring that all the virtual objects in a scene look like they belong. Magicians use lighting and perspective tricks to fool the user into believing that something is real. We have already seen how we use perspective, so now we need to cover and enhance our use of lighting.</p>
<p>In this chapter, we will cover how ARCore uses light estimation techniques to make the AR experience more believable to the user. We will then go on to extend some of those basic techniques in order to improve our future AR apps. Here are the main topics we will cover in this chapter:</p>
<ul>
<li>3D rendering</li>
<li>3D lighting</li>
<li>Light estimation</li>
<li>Cg/HLSL shaders</li>
<li>Estimating light direction</li>
</ul>
<p>We will use Unity in this chapter because it provides an easier platform for learning about the rendering process, lighting, and more about shader programs. The shader programs in Unity are a different variety and are definitely worth taking a look at.</p>
<p>While this chapter is less than halfway through the book, a reader should consider this as an advanced chapter. We will again be covering more about shader programs and 3D math concepts. Here's a good site for those of you who want to review or just get a basic understanding of 3D math, through this tutorial, <em>3D Math: Vector Math for 3D Computer Graphics</em> at <a href="http://chortle.ccsu.edu/vectorlessons/vectorindex.html">http://chortle.ccsu.edu/vectorlessons/vectorindex.html</a>. This is an excellent site licensed by <em>Bradley Kjell</em>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">3D rendering</h1>
                
            
            
                
<p>Before we get into talking about light estimation for AR, let's step back and review the rendering process of a 3D model. Take a look at the following diagram that explains the rendering process at a high level:</p>
<div><img src="img/06d37bf8-5c90-4970-8064-be27a97eceaf.jpg" style="width:44.58em;height:14.33em;"/><br/>
<br/>
Typical rendering process for a 3D model</div>
<p>Now, the diagram only visually demonstrates the rendering process. Geometry and vertex shaders never actually render a wireframe model. Rather, they only position and color vertices and surfaces, which are then fed into the pixel/fragment and lighting shaders. This last step is called <strong>rasterization</strong> and represents the final step when the 2D image is generated or rasterized.</p>
<p>The rendering process we are talking about here is for standard real-time rendering on a device's GPU using DirectX or OpenGL. Keep in mind that there are other rendering processes used for real-time (voxel) and non real-time (ray tracing) rendering.<br/></p>
<p>Euclideon have developed a voxel-like rendering technology, which they are claiming to be, in their words, as follows:</p>
<p>"The First Truly Applicable Hologram Tech is Here."</p>
<p>- Euclideon</p>
<p>This sounds very promising and a game changer for AR and VR. However, this technology has come under incredible scrutiny for making, what some feel are outlandish claims of rendering trillions of points without frame rate loss.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Building a test scene</h1>
                
            
            
                
<p>As always, let's take a look at how this looks in our tools. Open up Unity with the sample ARCore project we have already installed, and perform the following steps:</p>
<ol>
<li>From the menu, select File | New Scene. This will create a new empty scene for us in Unity.</li>
<li>From the Project window, drag the Andy prefab from the <kbd>Assets/GoogleARCore/HelloARExample/Prefabs</kbd> folder into the Hierarchy window, as shown in the following screen excerpt:</li>
</ol>
<div><img src="img/64458ed3-7272-431f-b1b5-866bfd43649f.png" style="width:42.83em;height:23.42em;"/></div>
<p>Unity interface showing Andy prefab dragged onto the scene</p>
<ol start="3">
<li>Andy is quite small, so we will adjust his size and the camera so that he fits in the Scene and Game windows better. Select Andy and modify Transform Scale to X as <kbd>25</kbd>, Y as <kbd>25</kbd>, and Z as <kbd>25</kbd>. Then, select Main Camera and modify its Transform Position to Y as <kbd>4</kbd>. This is shown in the following screen excerpt:</li>
</ol>
<div><img src="img/7a24a2fb-c4f9-4cab-bd23-fc151ae513ea.jpg"/></div>
<p>Setting the Transform of Andy and the Main Camera</p>
<ol start="4">
<li>Click on the Game and Scene tabs to switch views and see how the Andy model looks in each view.</li>
</ol>
<p>The Scene window in Unity is for composing your scene objects. This is where you will generally do most of your work in Unity. The Game window represents the view, as close as possible, as it is rendered in game. Unfortunately, for ARCore apps, we are limited to testing on a device and thus unable to generate an accurate game view. This is why, for now anyway, we will work in a separate scene for discovery purposes.</p>
<ol start="5">
<li>From the menu, select GameObject | 3D Object | Plane. This will add a new plane to the scene. Ensure that the plane is positioned at <kbd>0</kbd>,<kbd>0</kbd>,<kbd>0</kbd> by clicking on the Gear icon beside the Transform component in the Inspector window and selecting Reset Position from the menu. After you do that, Andy will be casting a shadow on the plane.</li>
<li>Switch between views again. Expand the Shaded dropdown just under the Scene tab, as shown in the following excerpt:</li>
</ol>
<div><img src="img/6fe07cca-f11f-4345-bef0-b1a41714e3f0.png" style="width:15.17em;height:49.17em;"/></div>
<p>The Draw Mode menu</p>
<ol start="7">
<li>This menu represents the various Draw Modes Unity can support. Some of these may make sense, such as Wireframe, while others less so. In any case, run through the list of each option to see what they do.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Materials, shaders, and textures</h1>
                
            
            
                
<p>Okay, now we have seen how Unity renders a scene and the various draw modes available. However, we still need to go over how an object is colored or textured. In Unity, we typically use materials, shaders, and textures to render 3D objects. A material is essentially an encapsulation of a shader, its dependent textures, and other settings. Let's see what AndyMaterial looks like in Unity by following the given steps:</p>
<ol>
<li>Open the <kbd>Assets/GoogleARCore/HelloARExample/Materials/Andy</kbd> folder in the Project window and select AndyMaterial. Look at the Inspector window and note the name of the Shader (<kbd>ARCoreDiffuseWithLightEstimation</kbd>) at the top. The current Shader uses a simple lighting model and has been optimized for mobile AR, which we don't currently need, so we will change it.</li>
<li>Expand the Shader dropdown in AndyMaterial and select Standard. This will switch the material to using the Standard Shader, as shown in the following screenshot:</li>
</ol>
<div><img src="img/2a606dce-ceea-4292-8a90-48daba9241f9.png" style="width:23.58em;height:31.33em;"/></div>
<p>Switching Andy to use the Standard Unity shader</p>
<ol start="3">
<li>The first thing you will immediately note is that Andy gets very dark. This is because the Metallic and Smoothness are turned way up. Use your mouse to adjust the various values to something more pleasant, as shown by the red arrows in the preceding screenshot. Perhaps a metallic shiny Andy?</li>
</ol>
<p>One thing to note when adjusting materials is that any changes you make to a material will be automatically saved and persisted even when running in the play or demo mode. Sometimes, it is useful to have backups of settings, especially if you found them difficult to achieve.</p>
<ol start="4">
<li>Make a copy of AndyMaterial by selecting it in the Project window and typing <em>Ctrl</em> + <em>D</em> or <em>command</em> + <em>D</em> on Mac. Rename the new material StandardAndyMaterial.</li>
<li>Select AndyMaterial again. Change Shader back to <kbd>ARCore/DiffuseWithLightEstimation</kbd>. Note how the look of Andy quickly changes.</li>
<li>From the menu, select File | Save Scenes. Save the scene to the <kbd>Assets/GoogleARCore/HelloARExample/Scenes</kbd> folder as <kbd>RenderingTest.scene</kbd>.</li>
</ol>
<p>As you can see, there are plenty of options and settings that can go into rendering a 3D object. Feel free to explore on your own what each of the material settings are on the Standard Shader. In the next section, we will expand our understanding of rendering by discussing lighting.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">3D lighting</h1>
                
            
            
                
<p>So far, we have looked at the basics of the rendering process and how a 3D model is rendered. What we omitted in the first section, however, is how lighting plays into this. In order to get a sense of the importance of lights in a 3D scene, how about we go ahead and turn out the lights. Open up Unity to where we left off in the first section and follow along:</p>
<ol>
<li>Select the Directional Light object in the Hierarchy window.</li>
<li>Disable the Light in the Inspector window by unchecking the box beside the object's name. This will turn off or disable the light. You will note that not all the lights go off, however. This is because we have an ambient or global light that is used to account for general light scattering.</li>
</ol>
<ol start="3">
<li>You are now left with a dark object with no lights and shadows. Turn back on the Directional Light by clicking on the checkbox. Take a look at the properties of the Light in the <strong>Inspector</strong> window, as shown:</li>
</ol>
<div><img src="img/8ba30c1d-96a4-4608-a149-0560247d2fa9.png" style="width:30.00em;height:35.00em;"/></div>
<p>Directional Light properties in the Inspector window</p>
<ol start="4">
<li>Play with the Type, Color, <strong>Mode</strong>, and Shadow Type properties in the Inspector window. There are four different types of lights you can work with. The Directional type represents a light source such as the sun, and as such, we only need to identify the direction the light is pointing. For the other light types, such as <strong>point</strong> and <strong>spot</strong>, you will need to position the light in the scene correctly in order to see any effects.</li>
</ol>
<p>We can calculate simple 3D diffuse lighting with the following equation:<br/>
<br/>
<img class="fm-editor-equation" src="img/9e407203-35eb-49da-8758-82d70b640467.png" style="width:8.50em;height:1.42em;"/><br/>
Here:<br/>
<img class="fm-editor-equation" src="img/dee632b5-0408-475d-ae58-5ff5d8339424.png" style="width:1.92em;height:1.33em;"/> is the direction of the light<br/>
<img class="fm-editor-equation" src="img/ec79985f-08b2-431b-b4e5-ce76d43d63c7.png" style="width:1.42em;height:1.33em;"/> is the normal to the surface<br/>
<img class="fm-editor-equation" src="img/6a2d28c1-8564-4d7b-9564-2c7ace5febf8.png" style="width:0.75em;height:1.33em;"/> is the intensity of light [0 to 1]<br/>
<br/>
<img class="fm-editor-equation" src="img/0edecbb9-9645-4bc9-bd5e-7613ef9651a7.png" style="width:0.75em;height:1.33em;"/> is then multiplied by the color in order to determine the resulting lit color.</p>
<ol start="5">
<li>The <strong>Standard</strong> Shader we looked at earlier uses <strong>Physically-Based Rendering </strong>(<strong>PBR</strong>) or a lighting model, which is quite sophisticated. Unfortunately, PBR shaders are currently limited for mobile platforms and often don't work or have poor performance. Often, the devices' GPU cannot support the additional instructions required for a PBR shader. Therefore, we will be limited to writing our own custom lighting shaders.</li>
<li>Let's explore switching shaders on our AndyMaterial so that we can see what effect different lighting models have. Locate AndyMaterial in the <kbd>Assets/GoogleARCore/HelloARExample/Materials/Andy</kbd> folder and select it.</li>
<li>Switch between <kbd>ARCore/DiffuseWithLightEstimation</kbd>, <strong>Mobile Diffuse</strong>, and the <strong>Standard</strong> shaders to see the effects or the different lighting models, as illustrated:</li>
</ol>
<div><img src="img/d41cd325-4480-4822-9f63-a8878a28646f.jpg" style="width:32.50em;height:10.92em;"/></div>
<p>Comparison of lighting models from three different shaders</p>
<ol start="8">
<li>Obviously, the Standard shader looks the most natural, but as we learned, PBR shaders are currently not supported on mobile platforms. Another option would be the Mobile Diffuse shader; let's see how that shader looks in our AR sample app.</li>
<li>Switch the shader to the Mobile Diffuse one and then save the project (File | Save Project).</li>
<li>Connect your device and type <em>Ctrl </em>+ <em>B</em>, <em>command </em>+ <em>B</em> on Mac. This will build and run the app on your device. Play with the app and wait for a surface to be visible and then tap and place Andy.</li>
</ol>
<p>Note anything different about our friend? That's right, he appears to stick out like a hot day in Canada. The reason for this is that the Mobile Diffuse shader is assuming a consistent light source, which means our model is always getting the same light (direction and intensity), except that in the real world, as the user moves, light direction and intensity can change dramatically. Your device's camera will try and compensate for this, but you can still see perceptible changes in lighting, especially if the lighting around the user changes dramatically. You can see this by running the app again, and this time, take a closer look at how the lighting looks different on and around our model. ARCore solves this issue of inconsistent lighting by performing a process called light estimation. We will cover light estimation in detail in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Light estimation</h1>
                
            
            
                
<p>Light estimation is a technique for replicating the lighting conditions of the real world and applying it to our 3D virtual objects. Ideally, we would like to be able to replicate the exact lighting conditions, but of course, we're not there yet. ARCore currently uses an image analysis algorithm to determine light intensity based on the current image from the device. This is then applied as global light to the 3D objects in the scene. Open up Unity again and let's see how this is done by following along the given steps:</p>
<ol>
<li>Locate the AndyMaterial again and revert its shader to <kbd>ARCore/DiffuseWithLightEstimation</kbd>.</li>
<li>Save the project (File | Save Project).</li>
<li>Connect your device and type <em>Ctrl</em> + <em>B</em> (<em>command</em> + <em>B</em> on Mac) to build and run the app on your device. Place a couple of Andy models and alter the lighting conditions. Note how our objects respond to the changes in lighting.</li>
<li>
<p>Go back to Unity and double-click on the <kbd>HelloAR</kbd> scene in the <kbd>Assets/GoogleARCore/HelloARExample/Scenes</kbd> folder to open the scene. Feel free to save your <kbd>RenderingTest</kbd> scene.</p>
</li>
</ol>
<ol start="5">
<li>Direct your attention to the Hierarchy window and double-click on Directional Light<strong> </strong>to focus and highlight it in the Scene window. Note how the light is pointing straight down in the Scene window. In the <strong>Inspector</strong> window, you will see that the Shadow Type is set to No Shadows, and the Intensity is turned down to <kbd>0.7</kbd>, which essentially turns the light into a directional ambient or global light.</li>
<li>Direct your attention back to the Hierarchy window and select <strong>Environmental Light</strong>. Go to the <strong>Inspector</strong> window and click on the Gear icon beside the <strong>Environmental Light (Script)</strong> component. Then, select the <strong>Edit Script</strong> option from the context menu, as shown:</li>
</ol>
<div><img src="img/6c61f973-382a-41ad-a138-9848114b4bd5.png" style="width:29.33em;height:27.42em;"/></div>
<p>Editing the Environmental Light script</p>
<ol start="7">
<li>This will open up the script in your script editor. By default, Unity installs <strong>MonoDevelop</strong>, which will open the script if you have not installed and set a different editor. Scroll down to the <kbd>Update</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px">public void Update()<br/>{<br/><strong>#if UNITY_EDITOR</strong><br/>        // Set _GlobalLightEstimation to 1 in editor, if the value is not set, all materials<br/>        // using light estimation shaders will be black.<br/>        Shader.SetGlobalFloat("_GlobalLightEstimation", 1.0f);<br/><strong>#</strong><strong>else</strong><br/><strong>        if (Frame.TrackingState != FrameTrackingState.Tracking)</strong><br/>        {<br/>            return;<br/>        }<br/><br/>        // Use the following function to compute color scale:<br/>        // * linear growth from (0.0, 0.0) to (1.0, LinearRampThreshold)<br/>        // * slow growth from (1.0, LinearRampThreshold)<br/>            const float LinearRampThreshold = 0.8f;<br/>            const float MiddleGray = 0.18f;<br/>            const float Inclination = 0.4f;<br/><br/>        <strong>float normalizedIntensity = Frame.LightEstimate.PixelIntensity / MiddleGray;</strong><br/>        float colorScale = 1.0f;<br/><br/>        if (normalizedIntensity &lt; 1.0f)<br/>        {<br/>            colorScale = normalizedIntensity * LinearRampThreshold;<br/>        }<br/>        else<br/>        {<br/>            float b = LinearRampThreshold / Inclination - 1.0f;<br/>            float a = (b + 1.0f) / b * LinearRampThreshold;<br/>            colorScale = a * (1.0f - (1.0f / (b * normalizedIntensity + 1.0f)));<br/>        }<br/><br/>        Shader.SetGlobalFloat("_GlobalLightEstimation", colorScale);<br/>#endif<br/>    }<br/>}</pre>
<ol start="8">
<li>The <kbd>#if UNITY_EDITOR</kbd> is a compiler directive that checks whether the code is running in the editor. The reason we do this is so that when the code runs in the Unity editor, we want it to ignore any light estimation calculations. When the code is running in the editor, it will execute the next line; the <kbd>_GlobalLightEstimation</kbd> shader variable is set to <kbd>1</kbd>. This means that when the code is running in the editor, all it does is set our light to <kbd>1.0</kbd>.</li>
</ol>
<p>You will come across the <kbd>#if UNITY_EDITOR</kbd> directive quite frequently when doing mobile development. This directive allows you to write test code that only executes when the code is running in the editor. This allows us to simulate the object running in the editor without the need to worry about ARCore services or device restrictions.</p>
<ol start="9">
<li>Direct your attention to the <kbd>#else</kbd> block of code. This is code that is executed on the device and first checks whether the <kbd>Frame</kbd> is tracking. We have already seen this check in Android. The rest of the code is essentially just math, but if you look at the last highlighted line, you will see a call to <kbd>Frame.LightEstimate.PixelIntensity</kbd>. This is the call where ARCore reads the image from the camera and determines the current pixel intensity; a float value from 0 for a totally black image to <kbd>1</kbd> that is fully white. The intensity is normalized based on a constant called <kbd>MiddleGray</kbd>. The <kbd>MiddleGray</kbd> color or light intensity of <kbd>0.18f</kbd> corresponds roughly to the point where we humans stop recognizing colors.</li>
<li>We then use the <kbd>normalizedIntensity</kbd> to determine whether we want a linear change in lighting, when <kbd>normalizedIntensity</kbd> is less than <kbd>1.0</kbd>, or more gradually, when the intensity is greater than <kbd>1.0</kbd>. That's all that the rest of the math is doing, just making the lighting change more gradually after a certain threshold.</li>
<li>Change the <kbd>MiddleGray</kbd> constant to match the following line:</li>
</ol>
<pre style="padding-left: 60px">const float MiddleGray = 1.0f;</pre>
<ol start="12">
<li>This will convert our light estimation to now use a linear model. Save the code change and return to Unity. Unity will automatically recompile the code and inform you of any errors in the status bar at the bottom of the editor.</li>
<li>Connect your device and build and run. Place an Andy on a surface. Note how dark the figure is; this is because the lighting model is too abrupt.</li>
</ol>
<p>We are using a single channel of color or what you may also call gray scale. This is why we refer to values as a color but it is in fact just a single float. A gray scale color of <kbd>0.18f</kbd> is equivalent to the RGB color (<kbd>0.18f</kbd>, <kbd>0.18f</kbd>, <kbd>0.18f</kbd>) or what ARCore calls <kbd>MiddleGray</kbd>.</p>
<ol start="14">
<li>Change the <kbd>MiddleGray</kbd> constant back to <kbd>0.18f</kbd>, save the project, and run the app. Note the difference in lighting.</li>
</ol>
<p>This covers how ARCore uses image analysis techniques to read the light intensity from the camera's image and converts that value into a global light intensity or color. The lighting value is set on a shader, and we will follow how that value is used in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Cg/HLSL shaders</h1>
                
            
            
                
<p>The shading language used in Unity is a variety of HLSL, or sometimes referred to as Cg. This shading variant provides two different forms of shaders: <strong>surface</strong> and <strong>vertex</strong>/<strong>fragment</strong> shaders. Now, coming from Android, this may sound confusing, since GLSL treats vertex and fragment shaders differently. However, variety of HLSL in Unity treats vertex and fragment shaders as the same, since they reside in the same file and are in the same workflow. A surface shader, which handles the lighting of our model, can be simple or quite complex. The Standard Unity surface shader uses a PBR lighting model, which is quite advanced and not supported on most mobile devices. This issue, combined with our limited ability to track scene lights, limits us to writing our own shaders in order to get our object lighting correct. ARCore provides us with a very simple surface shader that is used in the sample to light the Andy model. Let's open up Unity and take a look at what that shader looks like by following the given steps:</p>
<ol>
<li>Load up the <kbd>HelloAR</kbd> sample project and scene.</li>
<li>Select the AndyMaterial in the <kbd>Assets/GoogleARCore/HelloARExample/Materials/Andy</kbd> folder. Ensure that the Shader is set to <kbd>ARCore/DiffuseWithLightEstimation</kbd>. Switch it back if you changed it.</li>
<li>Click on the Gear icon and from the context menu, select Edit Shader. This will open the shader in your code editor, and it is also shown here for reference:</li>
</ol>
<pre style="padding-left: 60px">Shader "ARCore/DiffuseWithLightEstimation"<br/>{<br/>    Properties<br/>    {<br/>        _MainTex ("Base (RGB)", 2D) = "white" {}<br/>    }<br/><br/>    SubShader<br/>    {<br/>        Tags { "RenderType"="Opaque" }<br/>        LOD 150<br/><br/>        CGPROGRAM<br/>    #pragma surface surf Lambert noforwardadd finalcolor:lightEstimation<br/><br/>        sampler2D _MainTex;<br/>        fixed _GlobalLightEstimation;<br/><br/>        struct Input<br/>        {<br/>            float2 uv_MainTex;<br/>        };<br/><br/>    void lightEstimation(Input IN, SurfaceOutput o, inout fixed4   <br/>                         color)<br/>    {<br/>        color *= _GlobalLightEstimation;<br/>    }<br/><br/>    void surf (Input IN, inout SurfaceOutput o)<br/>    {<br/>        fixed4 c = tex2D(_MainTex, IN.uv_MainTex);<br/>        o.Albedo = c.rgb;<br/>        o.Alpha = c.a;<br/>    }<br/>    ENDCG<br/>  }<br/><br/>    Fallback "Mobile/VertexLit"<br/>}</pre>
<ol start="4">
<li>This is a fairly simple diffuse lighting shader that uses the global light estimate we calculated earlier. It starts by defining itself with this line:</li>
</ol>
<pre style="padding-left: 60px">Shader "ARCore/DiffuseWithLightEstimation"</pre>
<ol start="5">
<li>Next, it defines <kbd>Properties</kbd> in the next code block, where <kbd>_MainTex</kbd> represents the base texture, is called <kbd>"Base (RGB)"</kbd>, and is set to <kbd>2D</kbd>. If you quickly look back at Unity, you can see this property in the <strong>Inspector</strong> window.</li>
<li>The block of code that starts with <kbd>SubShader</kbd> is where the action happens. We first define <kbd>Tags</kbd>, which are sets of key/value pairs that set the rendering order and type parameters. In our example, we set this to <kbd>Opaque</kbd>. Then, we have the following line:</li>
</ol>
<pre style="padding-left: 60px">LOD 150</pre>
<ol start="7">
<li>This determines the <strong>level of detail</strong> of the shader. The <kbd>LOD</kbd> directive is used to determine the complexity or performance requirements of the shader. You can set the value to anything, but typical values are shown in the following list:
<ul>
<li>VertexLit kind of shaders = <kbd>100</kbd></li>
<li>Decal, Reflective VertexLit = <kbd>150</kbd></li>
<li>Diffuse = <kbd>200</kbd></li>
<li>Diffuse Detail, Reflective Bumped Unlit, Reflective Bumped VertexLit = <kbd>250</kbd></li>
<li>Bumped, Specular = <kbd>300</kbd></li>
<li>Bumped Specular = <kbd>400</kbd></li>
<li>Parallax = <kbd>500</kbd></li>
<li>Parallax Specular = <kbd>600</kbd></li>
</ul>
</li>
<li>As you can see from the list, the simple shader represents a low level of detail. This means that lower-level hardware should be able to run this shader without any issue. You can set the maximum shader LOD per shader or globally; check the Unity documentation for further details.</li>
<li>We start our actual shader code with <kbd>CGPROGRAM</kbd> and then define the form of surface shader with the <kbd>#pragma</kbd> directive, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">#pragma surface surf Lambert noforwardadd finalcolor:lightEstimation<br/><br/><strong>#pragma surface surfaceFunction lightModel [optionalparams]</strong><br/></pre>
<ol start="10">
<li>The first part of the directive, <kbd>surface</kbd>, defines this as a surface shader. Then, we see that the <kbd>surf</kbd> function name refers to the main surface function. Then comes the lighting model, <kbd>Lambert</kbd> in this case. After that, the options are set to <kbd>noforwardadd</kbd>, which is just a simple way to limit the number of lights to one. Finally, we use a custom modification function called <kbd>lightEstimation</kbd> that is set with <kbd>finalcolor:lightEstimation</kbd>.</li>
</ol>
<p>This shader uses the Lambert lighting model. You can find plenty of examples of what lighting models Unity supports or how to write your own model at <a href="https://docs.unity3d.com/Manual/SL-SurfaceShaderLightingExamples.html">https://docs.unity3d.com/Manual/SL-SurfaceShaderLightingExamples.html</a>.</p>
<ol start="11">
<li>Just inside the <kbd>#pragma</kbd> directive, we see the definition of the shader inputs: <kbd>_MainTex</kbd>, <kbd>_GlobalLightEstimation</kbd>, and <kbd>struct Input</kbd>. If you recall, <kbd>_GlobalLightEstimation</kbd> is the variable we set inside the <kbd>EnvironmentalLight</kbd> script to represent our global light.</li>
<li>Next, we will jump down a few lines to the <kbd>surf</kbd> function, as follows:</li>
</ol>
<pre style="padding-left: 60px">void surf (Input IN, inout SurfaceOutput o)<br/>{<br/> fixed4 c = tex2D(_MainTex, IN.uv_MainTex);<br/> o.Albedo = c.rgb;<br/> o.Alpha = c.a;<br/> }</pre>
<ol start="13">
<li>This function simply samples the color from our <kbd>_MainTex</kbd> using <kbd>tex2D</kbd> and the input <kbd>uv</kbd> coordinates. Then, it sets the color (<kbd>Albedo</kbd>) and <kbd>Alpha</kbd> from the lookup. This function is called first to determine the color of the surface, and then, its output is passed to the Lambert lighting model, after which the final color is set by the <kbd>lightEstimation</kbd> function.<br/>
An input marked as <kbd>inout</kbd> represents a value that can be modified and will automatically be returned.</li>
<li>Scroll up a bit to the <kbd>lightEstimation</kbd> function. Inside this function, the code, shown as follows, modifies the color based on the value that was set for <kbd>_GlobalLightEstimation</kbd>:</li>
</ol>
<pre style="padding-left: 60px">color *= _GlobalLightEstimation;</pre>
<ol start="15">
<li>Multiplying the color by the global light estimation is the same as adjusting the brightness with a dimmer switch.</li>
<li>Finally, we complete the shader with <kbd>Fallback</kbd> and the name of another shader. This sets the fall back or backup shader if the current shader is unable to run. A shader can fail due to compilation errors or hardware limitations.</li>
</ol>
<p>Now that we have a clear understanding of how the light estimation value we saw generated earlier is used in the shader, we can move to perhaps enhancing our lighting. If you recall, our current light just points straight down, but ideally, we would like to position the light to match the strongest light source. We will look at a simple but effective technique to track and position a light in AR in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Estimating light direction</h1>
                
            
            
                
<p>Google provides us with a robust solution for estimating the amount of light in an AR scene with ARCore. As we learned, light direction is an equally important part of scene lighting. Google didn't intentionally ignore estimating light direction with ARCore; it's just that that problem is really difficult to do right. However, Google did provide us with just enough tools in ARCore to be able to estimate light direction, providing some simple assumptions. First, we need to assume that our user, for now anyway, will remain in the same room or area. Second, our user will need to look in at least an 180 degree arc across their vision, or more simply put, the user just needs to look around. Third, it works best if the real-world environment is lit from a distant single bright source, such as the sun. Based on those assumptions, we can simply store the direction the user saw the brightest image in and use that to reverse calculate our light direction. This may sound more complex than it is, so hopefully, the following diagram can explain this further:</p>
<div><img src="img/6845f85e-4266-4ec6-8a32-f96f0d76bdc2.jpg" style="width:37.92em;height:22.08em;"/></div>
<p>Calculating light direction from camera pixel intensity</p>
<p>Now, this technique may sound quite complicated, but it isn't. We can actually accomplish this with just a few lines of code. Open up Unity and follow along to write our directional light detector:</p>
<ol>
<li>Ensure that the <kbd>HelloAR</kbd> scene of the sample app is loaded.</li>
<li>Select the Environmental Light object in the Hierarchy window.</li>
</ol>
<ol start="3">
<li>Click on the Gear icon beside the Environmental Light (Script) component in the <strong>Inspector</strong> window and select Edit Script from the context menu.</li>
<li>Just beneath the class declaration, add the following lines to declare new variables:</li>
</ol>
<pre style="padding-left: 60px"><strong>public class EnvironmentalLight : MonoBehaviour</strong><br/><strong>{  </strong>//after me<br/>  public GameObject SceneCamera;<br/>  public GameObject SceneLight;<br/>  private float maxGlobal = float.MinValue;<br/>  private Vector3 maxLightDirection;</pre>
<ol start="5">
<li>These variables will hold a reference to the scene camera, light, the max global intensity we find, and the direction we find it.</li>
<li>Scroll down in the code until you see the identified line in the <kbd>Update</kbd> method, and add the following lines:</li>
</ol>
<pre style="padding-left: 60px"><strong>const float Inclination = 0.4f; </strong>//after me<em><strong><br/></strong></em><br/>var pi = Frame.LightEstimate.PixelIntensity;<br/>if(pi &gt; maxGlobal)<br/>{<br/>  maxGlobal = pi;<br/>  SceneLight.transform.rotation = Quaternion.LookRotation(-SceneCamera.transform.forward);<br/>}<br/><br/></pre>
<ol start="7">
<li>All this code does is use <kbd>Frame.LightEstimate.PixelIntensity</kbd> to read the light intensity for the current camera direction. Then, we check whether this value is higher than any previous seen value (<kbd>maxGlobal</kbd>). If it is, we set a new maximum value and rotate the light (<kbd>SceneLight</kbd>) in the opposite direction of the camera, which means that the light will face toward the camera.</li>
</ol>
<p>Be careful when you edit code outside of the <kbd>#if UNITY_EDITOR</kbd> directive. This code won't be compiled until a build is run for the platform, which means that any errors in the code will be identified as build errors. This can be confusing, so be careful to avoid syntax errors when coding these sections.</p>
<ol start="8">
<li>Save the file; that's all the code we need to write in order to adjust the light direction. If you recall from the last section, the diffuse shader we are using doesn't account for light direction. However, ARCore has provided us with another shader that does.</li>
</ol>
<ol start="9">
<li>Return to the editor to find and select the AndyMaterial in the Assets/GoogleARCore/HelloARExample/Materials/Andy folder.</li>
<li>Change the material to use the <kbd>ARCore/SpecularWithLightEstimation</kbd> shader. This material shows the direction of light better.</li>
<li>Select the Environmental Light object in the Hierarchy window. Note how we have two new properties added to the Environmental Light (Script) component. These new properties (Scene Camera and Scene Light) were added because we declared them as <strong>public</strong> fields in the class.</li>
<li>Click on the icon that resembles a bullseye next to the Scene Camera property. Then, as shown in the following excerpt, select the First Person Camera object from the Select GameObject dialog:</li>
</ol>
<div><img src="img/5bf994ec-eaf5-4f99-9495-9f55657f92d8.png" style="width:22.00em;height:33.08em;"/></div>
<p>Setting the Scene Camera and Scene Light properties of the component</p>
<ol start="13">
<li>Close the <strong>Select GameObject</strong> dialog.</li>
<li>Repeat the same process for setting the <strong>Directional Light</strong> as the <strong>Scene Light</strong>.</li>
<li>Connect your device and build and run. Run the app in an area with a single bright light source and see how Andy looks after you place it.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Updating the environmental lighting</h1>
                
            
            
                
<p>Now Andy should be lit from what looks like the brightest light source in the area. However, because we don't currently track changes in light direction, if you change rooms or the lighting changes, then the illusion is broken. Light tracking is difficult, and it's more difficult than tracking a user, except that we can put a simple hack in place to not track the lighting for as long as we do, which is currently forever, if you weren't paying attention. Follow along to put this simple hack in the code we just wrote:</p>
<ol>
<li>Open up the <kbd>EnvrionmentalLight.cs</kbd> script in your text editor of choice. If you forgot how to do this, just look back a few pages.</li>
<li>Add the following line right after and before the lines identified:</li>
</ol>
<pre style="padding-left: 60px"><strong>var pi = Frame.LightEstimate.PixelIntensity; </strong>//after me<br/>maxGlobal *= .98f;<br/><strong>if(pi &gt; maxGlobal){  </strong>//before me</pre>
<ol start="3">
<li>That single line is a degrade function on the <kbd>maxGlobal</kbd> variable. Remember that <kbd>maxGlobal</kbd> is the value we identify as the strongest light source. This simple function, yep function, degrades this value over time. The value of <kbd>.98f</kbd> sets the speed of decay. A value of <kbd>.98f</kbd> represents a fairly quick decay rate, whereas a value of <kbd>.9999f</kbd> would represent a slow decay.</li>
<li>Save the file, and yep, that's it.</li>
<li>Go back to Unity. Connect and build and run the app. Now when you place an Andy, you should quickly see changes in what the app identifies as the strongest light source. Feel free to go back and change the decay rate or alter the function and use your own method; experiment.</li>
</ol>
<p>What we put together is a simple way to track and estimate light direction. As we mentioned, this method works, but it's certainly not without its limitations. In any case, this should give the curious reader enough to continue and extend this further. We also completely avoided a proper discussion of shadows. Fortunately, we will have plenty of time to do that in <a href="843257a4-843b-4278-99df-fec1e2bd996b.xhtml" target="_blank">Chapter 9</a>, <em>Blending Light for Architectural Design</em>, where we will allow the user to transform their own living space.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Exercises</h1>
                
            
            
                
<p><strong>Complete the following exercises on your own</strong>: </p>
<ol>
<li>Change the <kbd>maxGlobal</kbd> rate of decay. You decide whether to make it faster or slower.</li>
<li> Increase or decrease the <kbd>maxGlobal</kbd> rate of decay based on the user's amount of movement. Hint—recall how we tracked the user, and use that to determine how far they have gone or how fast. Use that information to set the rate of decay.</li>
<li> Write your own custom lighting surface shader. This one's difficult, but it's worth the effort.</li>
</ol>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>Certainly, as you become more of an expert in AR, you realize how important lighting is to augmented reality. It's so important that Google developed ARCore with light estimation built in, which is why we spent this entire chapter on the subject. First, we learned about the rendering process in a bit more depth; then, we covered 3D lighting, an essential bit of knowledge that we will need in order to understand the added complexity of lighting in AR. This led us to look at the way ARCore estimates the light levels or global light in an area by taking a closer look at Unity Cg/HLSL shaders and, more specifically, surface shaders. Finally, we implemented a simple but effective hack to track and estimate light direct in a scene, which we left the reader with to improve on in their own time.</p>
<p>Estimating the actual lighting conditions of the environment will be a major hurdle for AR to overcome. However, with the incredible advances in AI and Machine Learning, we will likely see some better solutions come out soon. We will take a closer look at how Machine Learning can assist AR in the next chapter.</p>


            

            
        
    </body></html>