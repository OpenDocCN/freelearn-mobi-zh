# *第 15 章*：使用 Vision 框架进行识别

Vision 框架已经对开发者开放了几年。苹果公司一直在为其引入更好的功能，从文本识别到图像识别。在 iOS 14 中，Vision 框架带来了对文本识别和其他现有功能的更多改进，但它还允许开发者执行两种不同的操作：手部和身体姿态识别。这些新功能为开发者带来的可能性是无限的！只需想想健身房应用、瑜伽应用、健康应用等等。

在本章中，我们将学习关于 Vision 框架的基础知识以及如何使用文本识别的新进展。我们还将了解新的手部关键点识别，构建一个能够检测四个手指和拇指尖端的演示应用。本章代码包还提供了一个类似的示例，展示了人体姿态识别。以下几节将讨论这些主题：

+   Vision 框架简介

+   在图像中识别文本

+   实时识别手部关键点

到本章结束时，你将能够充满信心地使用 Vision 框架，能够将本章中解释的技术应用于实现 Vision 提供的任何类型的识别，从图像中的文本识别到视频中手部和身体姿态的识别。

# 技术要求

本章的代码包包括一个名为 `HandDetection_start` 的入门项目，以及几个名为 `Vision.playground` 和 `RecognitionPerformance_start.playground` 的游乐场文件。它还包含一个名为 `BodyPoseDetection_completed` 的完成示例。你可以在代码包仓库中找到它们：

https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition

# Vision 框架简介

自 App Store 开始以来，许多应用都利用摄像头通过图像和视频识别构建了出色的功能。想想现在可以扫描支票或信用卡的银行应用，这样用户就不需要输入所有数字。还有可以拍照名片并提取相关信息的网络应用。甚至你 iPhone 上的照片应用也能检测照片中的面孔并将它们分类。

Vision 框架为开发者提供了一套强大的功能，使其比以往任何时候都更容易实现这些功能：从文本和图像识别到条形码检测、面部关键点分析，现在，随着 iOS 14 的推出，还有手部和身体姿态识别。

Vision 还允许使用 Core ML 模型，以便开发者能够增强他们应用中的对象分类和检测。Vision 自 iOS 11 和 macOS 10.13 以来一直可用。

Vision中有几个概念在所有类型的检测中都是通用的（文本检测、图像检测、条形码检测等），包括`VNRequest`、`VNRequestHandler`和`VNObservation`实体：

+   `VNRequest`是我们想要执行的任务。例如，`VNDetectAnimalRequest`将用于在图片中检测动物。

+   `VNRequestHandler`是我们想要检测的方式。它允许我们定义一个完成处理程序，在那里我们可以处理结果并按需塑造它们。

+   `VNObservation`封装了结果。

让我们看看一个结合所有这些概念并展示Vision如何轻松帮助我们检测图像中文字的示例。打开名为`Vision.playground`的沙盒。这个示例代码从一个特定的URL获取图像并尝试从中提取/检测任何文本。所使用的图像是这张：

![图15.01 – 使用Vision提取文本的示例图像](img/Figure_15.01_B14717.jpg)

![Figure 15.01 – Example image to extract text with Vision](img/Figure_15.01_B14717.jpg)

图15.01 – 使用Vision提取文本的示例图像

如果我们尝试从这张图像中提取文本，我们应该得到像*Swift 数据结构和算法*或作者姓名，或标题下面的描述这样的结果。让我们回顾沙盒中的代码：

[PRE0]

让我们逐条查看编号的注释：

1.  首先，我们使用给定的图像URL创建一个`VNImageRequestHandler`实例。我们实例化这个处理程序以在图像上执行Vision请求。记住，我们稍后需要调用`perform(_:)`来启动分析。

1.  现在我们创建一个`request(VNRecognizeTextRequest)`实例，我们将在之前实例化的`requestHandler`实例上执行它。你可以在一个`requestHandler`实例上执行多个请求。我们定义了一块代码，当请求完成时将执行该代码。在这个块中，我们从请求结果中提取观察结果（`VNRecognizedTextObservation`实例）。这些观察结果将包含从图像中分析出的文本的潜在结果（`VNRecognizedText`实例）。我们打印出每个观察结果中的`topCandidate`，根据Vision参数，这应该是最佳匹配。

1.  我们可以指定请求的识别级别。在这个例子中，我们使用`.accurate`（另一种选择是`.fast`）。我们将在稍后看到`.fast`的结果以及何时使用其中一个。

1.  最后，我们在`requestHandler`实例上执行请求，使用`perform(_:)`方法执行所有操作。

如果你执行代码，沙盒中的控制台将显示以下内容：

[PRE1]

这些结果看起来很棒，对吧？如果你重新检查图像，我们会从其中提取正确的文本！作者姓名、标题（每行）、描述等等！看起来是个很棒的结果！但你有没有注意到，当你执行沙盒时，它需要一段时间才能完成？这是因为我们使用了`.accurate`选项。让我们看看如果我们使用`.fast`会发生什么。在沙盒代码中更改它：

[PRE2]

输出如下：

[PRE3]

这次，分析可以做得更快，但正如你所见，对于我们所期望的结果（我们希望正确地检测文本！）来说，结果要差得多。为什么有人会优先考虑速度而不是准确性呢？嗯，对于某些应用来说，速度是关键，为了它牺牲一些准确性是可以接受的。想想基于实时摄像头的翻译或者应用实时滤镜拍照。在这些场景中，你需要快速处理。我们将在本章后面进一步讨论这个问题。

这个游乐场示例应该能帮助你了解Vision所包含的惊人潜力。仅仅通过几行代码，我们就能够处理并提取图像中的文本，没有任何问题或复杂的操作。Vision允许开发者做令人惊叹的事情。让我们在接下来的章节中更深入地探讨它，从对图像文本检测的更详细分析开始。

# 在图像中识别文本

自从Vision框架的第一个迭代以来，它一直在改进图像中检测文本的能力。在本节中，我们将学习一些最先进的技术，以在iOS 14上获得最佳结果。

在上一节中，我们看到了在Vision中，文本检测可以通过两种不同的方式发生，这取决于我们在请求中指定的`recognitionLevel`的值：`.fast`和`.accurate`。让我们看看它们的区别：

+   `.accurate`。它处理旋转文本或不同字体的效果不如`.accurate`方法。

+   `.fast`但更准确（当然！）它的工作方式与我们的大脑识别单词的方式相同。如果你读“m0untain”这个词，你的大脑可以从它中提取“mountain”，并且知道0（零）代表一个o。如果你使用`.fast`，它按字符识别，0（零）在你的结果中仍然是0（零），因为没有任何上下文被考虑。

在两种情况下，在初始识别阶段完成后，结果都会传递到一个传统的自然语言处理器进行语言处理，其结果是观察结果。整个过程仅在设备上发生。

那么，什么时候应该使用`.fast`呢？你可能想知道。嗯，有一些场景中它比`.accurate`更方便：

+   为了快速读取代码或条形码

+   当用户交互是一个关键方面，你希望从文本检测中得到快速响应时

为了展示识别级别的差异，让我们使用不同的技术分析相同的图像。你还将学习一些可以应用到你的项目中的有用技巧。按照这里给出的步骤进行：

1.  请打开名为`RecognitionPerformance_start.playground`的游乐场。代码与我们之前章节尝试的代码大致相同。

    现在我们使用的图像中包含一个4位数，代表书籍的序列号：

    ![图15.02 – 作者名字下方带有序列号（1015）的书籍封面](img/Figure_15.02_B14717.jpg)

    ![img/Figure_15.02_B14717.jpg](img/Figure_15.02_B14717.jpg)

    图15.02 – 作者名字下方的带有序列号（1015）的书籍封面

    如果你仔细观察数字字体，你会发现对于计算机来说，判断某些数字是数字还是字母可能有些棘手。这是故意为之的。在这个例子中，我们将测试Vision的能力。

1.  继续执行playground代码。控制台输出应该如下所示：

    [PRE4]

我们已经成功检索到书的序列号：`1015`。代码也在测量完成文本识别过程所需的时间。在我们的案例中，它花费了**1.93秒**（这可能会因计算机而异，也可能因执行而异）。我们能做得更好吗？让我们尝试一些可以帮助我们提高处理时间同时保持相同准确性的技术。我们将从**感兴趣区域**开始。

## 感兴趣区域

有时候，当我们使用Vision分析图像时，我们不需要处理整个图像。例如，如果我们处理的是一种特定的表格，我们事先知道名字总是位于文档的顶部，我们可能只想处理那个区域。如果我们只需要特定区域，处理整个表格只会浪费时间和资源。

让我们假设在之前的例子（书籍封面）中，我们想要提取的序列号总是位于左上角。我们如何加快1.93秒的处理时间？我们可以通过定义感兴趣区域来实现。定义感兴趣区域将告诉Vision只处理该区域，避免处理图像的其余部分。这将导致更快的处理时间。

`regionOfInterest`是`VNRequest`的`CGRect`属性：

+   它定义了一个矩形区域，请求将在该区域内执行。

+   矩形被归一化到图像的尺寸，这意味着感兴趣区域的宽度和高度从0到1。

+   矩形的起点在图像的左下角，即(0,0)。右上角将是(1,1)。

+   默认值是`{{0,0},{1,1}}`，它覆盖了从左下角（0,0）到右上角（1,1），宽度为1，高度为1：整个图像。

在以下图中，你可以看到我们需要定义的感兴趣区域来捕获序列号（**1015**）：

![图15.03 – 感兴趣区域

![图片](img/Figure_15.03_B14717.jpg)

图15.03 – 感兴趣区域

让我们把那个区域添加到上一节中的代码：

1.  在`ScanPerformance_start.playground`项目中，在将`recognitionLevel`设置为`.accurate`之后添加以下代码：

    [PRE5]

1.  现在启动playground并在控制台中检查结果：

    [PRE6]

    与之前的结果相比，有几个不同之处：

    +   我们不再提取那么多文本。现在我们定义了感兴趣区域，我们只提取该区域包含的单词/数字。

    +   我们将处理时间从1.93秒减少到1.23秒。这提高了36%。

1.  现在我们尝试将感兴趣区域缩小，仅捕获序列号。将区域修改为以下：

    [PRE7]

1.  启动游乐场。现在控制台输出如下：

    [PRE8]

1.  将此行修改为使用`.fast`：

    [PRE9]

1.  保存并执行。检查控制台输出：

    [PRE10]

你可以看到这次，处理时间再次缩短，但结果完全不精确。我们检测到的不是`1015`，而是错误地得到了`Iois`。

然而，在具有领域知识的情况下，有一种常见的解决这种情况的方法。在我们的例子中，我们知道处理后的字符应该是数字。因此，我们可以调整从视觉输出的结果来改进结果并修复误分类。例如，查看以下调整：

+   字符“I”可以是“1。”

+   字符“o”可以是“0。”

+   字符“s”可以是“5。”

让我们在代码中实现这个功能：

1.  在游乐场文件的最后，添加以下方法：

    [PRE11]

    我们通过添加一个名为`transformToDigit()`的新方法来扩展`Character`类。这个新方法将帮助我们改进潜在的误分类。注意在方法本身中，我们有一个与形状相似的字母字符表，这些字母与数字相关。我们所做的是将这些字母转换成相应的数字。

1.  现在让我们使用它。在`print(recognizedStrings)`行下方，添加以下代码：

    [PRE12]

    我们正在获取视觉处理的结果；在我们的例子中，它是`"Iois"`，并且对于每个字符，我们对其应用我们新的`transformToDigit()`方法。

1.  执行代码，你将在控制台看到以下结果：

    [PRE13]

看起来很棒！注意现在将`"Iois"`转换成`"1" "0" "1" "5"`后看起来好多了。同时，注意处理时间并没有增加太多；这个操作相对容易计算。

现在我们总结一下本节我们做了什么，以及每一步的改进。我们首先处理了一张整个图像，并使用`.accurate`识别级别，这花费了我们1.93秒。然后，我们应用了感兴趣区域，只处理我们感兴趣的图像部分，将处理时间减少到1.23秒。之后，我们将`.accurate`改为`.fast`。这一改变将处理时间减少到0.59秒，但结果是不正确的。最后，我们实现了一个简单的算法来改进结果，使它们与`.accurate`级别一样好。所以，最终我们得到了完美的结果，处理时间仅为0.59秒，而不是1.93秒！

在下一节中，你将了解iOS14的一个新功能，即手势检测。

# 实时识别手势地标

iOS 14中Vision的一个新增功能是手部检测。这个新功能可以检测图像和视频中的手部，允许开发者以很高的精度找到视频帧或照片中手腕和各个手指的位置。

在本节中，我们将解释手部检测背后的基础知识，并通过一个示例项目演示其工作原理。让我们从我们将能够识别的手部特征点开始。

## 理解手部特征点

我们将能够在手中检测到21个特征点：

+   拇指4个点

+   每个手指4个点（总共16个点）

+   腕部1个点

如您所见，Vision可以区分手指和拇指。在手指和拇指上，都有4个感兴趣点。以下图示显示了这些特征点的分布情况：

![图15.04 – 手指和拇指特征点

](img/Figure_15.04_B14717.jpg)

图15.04 – 手指和拇指特征点

注意，在手腕中间也有一个特征点。

对于四个手指，我们可以使用以下键单独访问它们：

+   `小指`

+   `中指`

+   `无名指`

+   `indexFinger`

在每个手指内部，我们可以访问四个不同的特征点：

+   **指尖**

+   **DIP**

+   **PIP**

+   **MCP**

注意，对于拇指，这些名称略有不同（TIP、IP、PIP和CMC）。在本节稍后我们将构建的示例代码中，我们将演示如何使用这些点和每个手指以及拇指。

Vision能够同时检测不止一个手部。我们可以指定我们想要检测的最大手部数量。此参数将影响检测的性能。使用`maximumHandCount`设置限制。

为了性能和准确性，如果手部不在帧的边缘附近，如果光线条件良好，以及如果手部与摄像头角度垂直（因此整个手部都可见，而不仅仅是边缘），则更好。此外，考虑到有时脚部可能被识别为手部，因此请避免混淆。

理论就到这里；让我们直接进入代码示例！我们将构建一个演示应用程序，该程序将能够使用手机的正面视频摄像头检测手部特征点，并在检测到的点上显示叠加层。

## 实现手部检测

在本节中，我们将实现一个演示应用程序，该程序将能够使用手机的正面视频摄像头检测手部特征点。

该项目的代码包包含初始项目和最终结果。请打开名为`HandDetection_start`的项目。

该项目包含两个主要文件：一个名为`CameraView.swift`的`UIView`实例和一个名为`CameraViewController.swift`的`UIViewController`实例。

视图包含辅助方法来在坐标上绘制点。它将作为覆盖层绘制在摄像头视频流之上。只需知道，`showPoints(_ points: [CGPoint], colour: UIColor)` 方法将允许我们在视频摄像头流的覆盖层上绘制一个 `CGPoint` 结构体的数组。

视图控制器将是示例的核心，我们将在这里实现执行手部检测的相关代码。请打开 `CameraViewController.swift` 文件。让我们检查我们将逐步填充的代码框架。

在文件顶部，我们定义了四个属性：

+   `handPoseRequest: VNDetectHumanHandPoseRequest`。我们将在此视频流的顶部应用此请求，以检测每一帧中的手部关键点。如果我们检测到任何，我们将在覆盖层上显示一些点来显示它们。

+   `videoDataOutputQueue`, `cameraView`, 和 `cameraFeedSession`。

使用 `viewDidAppear` 和 `viewWillDisappear` 方法，我们正在启动/创建和停止摄像头的 `AVCaptureSession`。

最后，在接下来的四个方法中，我们有四个待办事项注释，我们将逐一实现以创建此应用程序。让我们总结一下我们将要执行的待办事项：

+   **待办事项 1**: 只检测一只手。

+   **待办事项 2**: 创建视频会话。

+   **待办事项 3**: 在视频会话中执行手部检测。

+   **待办事项 4**: 处理并显示检测到的点。

我们将在以下小节中实现这四个任务。

### 检测手部

视觉不仅可以一次检测一只手。我们要求它检测的手越多，性能影响就越大。在我们的示例中，我们只想检测一只手。通过在请求中将 `maximumHandCount` 设置为 `1`，我们将提高性能。

让我们从在 `// 待办事项 1` 下方添加以下代码开始：

[PRE14]

现在，让我们创建一个视频会话来捕获设备前置视频摄像头的视频流。

### 创建视频会话

对于第二个任务，我们将填充 `setupAVSession()` 方法内的代码。请将以下代码粘贴到方法中：

[PRE15]

首先，我们通过以下方式创建 `videoDevice: AVCaptureDevice` 实例，查询视频前置摄像头（如果存在！）：

[PRE16]

然后，我们使用那个 `videoDevice` 生成一个 `deviceInput: AVCaptureDeviceInput` 实例，它将是用于流的视频设备，以下代码所示：

[PRE17]

现在添加以下代码：

[PRE18]

在创建 `videoDevice` 实例后，我们创建一个新的 `session: AVCaptureSession` 实例。会话创建后，我们将 `videoDevice` 作为输入，创建并配置一个输出以处理视频流。我们通过调用以下代码将类本身分配为 `dataOutput AVCaptureVideoDataOutputSampleBufferDelegate`：

[PRE19]

这意味着当前置视频摄像头捕获新的帧时，我们的会话将处理它们并将它们发送到我们的代理方法，我们将在下一步（待办事项 3）中实现。

### 在视频会话中执行手部检测

现在我们已经设置并配置了视频会话，是时候处理每一帧了，并尝试检测任何手部和它们的地标！我们需要实现`captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection)`方法。

在`// TODO 3: Perform hand detection on the video session`行下，添加以下代码：

[PRE20]

我们想要检测四个手指（食指、中指、无名指和小指）以及大拇指的指尖。因此，我们创建了五个类型为`CGPoint`的变量来存储它们的坐标，如果找到了的话。

在这些新行之后，添加以下代码：

[PRE21]

使用这段代码，我们要求Vision在`sampleBuffer`（视频流）上执行`handPoseRequest`。然后，我们使用`guard`（使用`guard`）来防止没有检测到观察结果的情况（这样如果视频帧中没有手，我们就会在这里停止）。

但是如果`guard`没有触发，这意味着我们有一些手部地标需要处理。在`// Get observation points`行之后添加以下代码：

[PRE22]

现在我们正在从观察结果中提取与拇指和四个手指相关的任何`recognizedPoints()`实例。请注意，我们使用`try`来执行此操作，因为结果并不保证。使用提取出的识别点，我们稍后使用`guard`语句解开每个手指和大拇指的指尖。

在这个阶段，我们应该有五个变量，分别存储每个手指的指尖坐标以及大拇指的坐标。

尽管我们已经有了我们正在寻找的五个坐标，但我们仍然需要执行一个额外的步骤。Vision坐标与`AVFoundation`坐标不同。让我们转换它们；在最后一个`guard`语句之后添加以下代码：

[PRE23]

如您所见，两个系统中的`x`坐标是相同的，但`y`坐标不同。在Vision中，左下角是(0,0)。因此，我们只需要将Vision点的`y`坐标减去1，就可以得到`AVFoundation`系统上的结果。

太棒了！在这个阶段，我们已经有了手部地标检测系统，并以`AVFoundation`的`CGPoint`坐标形式得到结果。最后一步是绘制这些点！

在`catch`块（它外面）之后添加以下代码，正好在`func captureOutput(…)`方法的末尾：

[PRE24]

我们在主线程中调用`processPoints(…)`方法，因为我们希望它在UI上工作，所以我们通过将这项工作调度到正确的线程来确保一切工作完美。接下来，让我们实现`processPoints(…)`方法。

### 处理和显示检测到的点

在`captureOutput(…)`方法内部检测到手部地标后，我们现在想要将它们绘制到相机叠加层中。用以下代码替换`processPoints(…)`的空实现：

[PRE25]

记得我们是如何使用`CGPoints`转换为`AVFoundation`坐标的吗？现在我们想要将这些点转换为`UIKit`预览层。我们正在对它们执行`map`操作，最后，我们调用`cameraView`辅助方法`showPoints`来显示它们。

一切现在都已就绪！是时候构建并运行应用程序了。你会看到自拍相机被触发，如果你将其对准你的手，你的手指和拇指的尖端应该会被红色圆点覆盖。试一试，你应该会得到以下类似的结果：

![图15.05 – TIP检测

](img/Figure_15.05_B14717.jpg)

图15.05 – TIP检测

然而，这种方法仍然存在一些问题！试试这个：让应用程序检测你的手，然后从摄像机的视图中移除手部 – 红色圆点仍然在叠加层上！当没有检测到手时，它们没有被清理。

这有一个简单的解决方案。原因是，在`captureOutput(…)`方法内部，我们并不总是执行`processPoints(…)`方法。有时（`guard`语句）我们返回而不调用它。解决方案是将`processPoints(…)`块封装到`defer`中，将其移动到代码的开头，就在我们定义存储每个尖端坐标的五个属性之后。它应该看起来像这样：

[PRE26]

突出的代码是我们将其封装到`defer`中的部分（因此它将在返回方法之前始终执行）。再次执行应用程序，你会注意到当屏幕上没有手时，红色圆点也不会出现！我们正在使用空值调用`processPoints`，因此没有东西被绘制。通过这一步，我们就有了一个正在运行的手部关键点检测示例！恭喜！

身体姿态检测

Vision还为iOS 14提供了身体姿态检测。身体姿态检测与手部检测非常相似，所以我们不会对其进行逐步演示。但本书的代码包中包含了一个类似本节的应用程序示例，但用于身体姿态检测。你可以查看名为`BodyPoseDetection_completed`的项目，并查看它与手部检测项目之间的细微差别。

在本节中，我们学习了新的Vision方法来检测手部关键点，以及如何使用手机的视频流作为输入来检测手部关键点（而不仅仅是检测静态图像中的手部）。我们还提供了一个类似的演示，可用于身体姿态检测。让我们跳到总结，完成本章。

# 总结

我们从学习每个Vision功能的基石开始本章：如何使用`VNRequest`实例、其对应的`VNRequestHandler`实例以及产生的`VNObservation`实例。

在学习基础知识之后，我们将它们应用于文本识别。我们通过使用`.fast`和`.accurate`比较了不同的识别级别。我们还了解了感兴趣区域及其如何影响视觉请求的性能。最后，通过应用领域知识、修复潜在的视觉错误和误读，我们在文本识别方面提高了我们的结果。

最后，我们学习了新的手部地标识别功能。但这次，我们还学习了如何将视觉请求应用于实时视频流。我们能够在来自设备前摄像头的视频流中检测到手部地标，并显示叠加层以显示结果。本章还提供了一个类似的示例，该示例可以应用于身体姿态识别。

在下一章中，我们将学习iOS 14的一个全新功能：小部件！
