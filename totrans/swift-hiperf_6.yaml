- en: Chapter 6. Architecting Applications for High Performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we talked about different ways of improving code to achieve
    high-performance. Mostly we concentrated on a small part of the code and how to
    improve a function, an algorithm, and a data structure. In this chapter, we will
    concentrate on higher levels. We will talk about how to design an application
    architecture that can be scalable, maintainable, and high-performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter we are going to cover these topics:'
  prefs: []
  type: TYPE_NORMAL
- en: High-performance and concurrency overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Avoiding state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing asynchronous architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving high performance
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the ways to improve application performance is to run code concurrently.
    This not only allows us to run code faster and get the results more quickly, but
    it also frees the main-thread from doing a lot of work and being blocked. You
    should know that the main thread is responsible for events and user input handling.
    All the UI work is performed on the main thread and to achieve a really smooth
    user interaction we should do as little work as possible on the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: Running code concurrently can be a tricky task and sometimes it can lead to
    increased running time for an operation. Making solid concurrent application architecture
    is also not a trivial task and you should plan it carefully.
  prefs: []
  type: TYPE_NORMAL
- en: To take full advantage of concurrency, it is very useful to understand the hardware
    we have at our disposal that allows us to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Device architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to be able to achieve really high-performance, first we need to learn
    and understand what kinds of tools we have at our disposal. If you are making
    an iOS application, your application will run on the iPhone and iPad; for OS X
    it would run on the Mac. Although it might seem that the iPhone and the Mac differ
    a lot, they share the same basic concept and we can think about a Mac as a more
    powerful iPad device.
  prefs: []
  type: TYPE_NORMAL
- en: Nowadays, all computers and even phones use multi-core processors that allow
    us to execute many instructions at the same time in parallel. Starting with the
    iPhone 4s, all iPhones have a dual-core processor and the iPad Air 2 even has
    a 3-core processor. We should fully use that power and take advantage of it.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at how we could design code that could be run in parallel
    on multi-core processors.
  prefs: []
  type: TYPE_NORMAL
- en: Concurrency overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, when you make an application it runs the code in a single-thread
    environment, a main thread. For example, an iOS application would call the `application:`
    `didFinishLaunchingWithOptions` method on the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'A simpler example is an OS X Command Line Tool application. It has only one
    file: `main.swift`. When you start it, the system creates a single main thread
    and runs all the code in the `main.swift` file on that thread.'
  prefs: []
  type: TYPE_NORMAL
- en: 'For testing code, playgrounds are the best. By default, playgrounds stop after
    executing the last line of code and don''t wait for the concurrent code to finish
    executing. We can change this behavior by telling the playgrounds to keep running
    indefinitely. To do that, include these two lines in the playground file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can start playing with concurrency. The first task we need to do to
    run code concurrently is to schedule a task to be run on a different thread. We
    can schedule a task for concurrent execution by using:'
  prefs: []
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GCD** (**Grand Central Dispatch**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operation Queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Threads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the first option, we could use threads. A thread is the most low-level API.
    All the concurrency is built on top of threads and runs multiple threads. We can
    use `NSThread` from the Foundation framework. The simplest way to do this is to
    create a new class with a method that will be the starting point for our new thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see how we could schedule the new thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: You can create a new thread in two ways, by using the `detachNewThreadSelector`
    function or create an instance of `NSThread` and use the `start` function. We
    have to mark our `run` function with the `@objc` attribute because we use it as
    a selector when creating a thread, and `NSThread` is an Objective-C class that
    uses dynamic dispatch for method calling.
  prefs: []
  type: TYPE_NORMAL
- en: The `NSObject` has a simple API for performing a method on a different thread.
    Because our handler inherits for `NSObject` we can use it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Another way is to create a subclass of `NSThread` and override the starting
    point of a thread, the `main` function. In that way we don't need a handler class.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Thread complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even though the code is pretty simple here, working with threads is quite a
    complex operation. We need to take care of managing the state of the thread, correctly
    terminating it, and releasing the resources used by the thread.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a new thread is a very expensive and time-consuming operation, and
    we should avoid it when possible. The way to solve this is to reuse created threads.
    Creating and managing a thread-pool adds extraordinary complexity to the application
    that we don't need.
  prefs: []
  type: TYPE_NORMAL
- en: The process gets even harder when you need to communicate between threads and
    synchronize data between them.
  prefs: []
  type: TYPE_NORMAL
- en: Solution for threads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of solving our initial task that we wanted to run concurrently, now
    we are spending time managing the complexity of that concurrent execution system.
    Fortunately we don''t need to do that as there is a solution: *Don''t use threads*.'
  prefs: []
  type: TYPE_NORMAL
- en: The *iOS and Mac Concurrency Programming Guide* recommends not using threads
    but choosing a high-level API, a GCD, or Operation Queues.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thread APIs are shown in this chapter only for general knowledge. You should
    almost never use threads; use GCD instead.
  prefs: []
  type: TYPE_NORMAL
- en: GCD
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**GCD** (**Grand Central Dispatch**) is a high-level API that is built on top
    of threads and performs all aspects of thread management for you. Instead of working
    with threads, GCD provides a queue and task abstraction. You schedule a task to
    a queue for execution and the queue takes care of everything else. Let''s see
    how we could rewrite our code with GCD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the code looks simpler from the start. Before we dive into
    the details, let''s have a look at GCD and its concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding tasks to the queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Queues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **queue** is a structure that takes care of managing and executing its tasks.
    The queue is a first-in first-out data structure. That means that tasks in the
    queue are started in the order they were added to the queue.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First in first out means that tasks are started in the same order but it doesn't
    mean that they can't be executed simultaneously. Concurrent queues can start many
    tasks at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: The queue itself doesn't have much functionality. The main operation you would
    need to do is to create a queue or get one of the global queues.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three queue types:'
  prefs: []
  type: TYPE_NORMAL
- en: Main queue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concurrent: global and own queues'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serial
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Main queues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A **main queue** represents a queue associated with a main thread. It runs tasks
    serially, one after the other. You would usually use this queue to pass the result
    of an execution from other background queues to the main queue to update the UI
    state. You can get a main queue by calling the `dispatch_get_main_queue` function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Concurrent queues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A **concurrent queue** runs its tasks concurrently. The easiest way to get a
    concurrent queue is to use a global concurrent queue.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: To get a global queue, we need to specify what kind of priority we need. There
    are five types of queue with descending task priority. `USER_INTERACTIVE` is the
    most prioritized queue and `BACKGROUND` is the least.
  prefs: []
  type: TYPE_NORMAL
- en: '`QOS_CLASS_USER_INTERACTIVE`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QOS_CLASS_USER_INITIATED`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QOS_CLASS_DEFAULT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QOS_CLASS_UTILITY`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`QOS_CLASS_BACKGROUND`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_IND
  - PREF_H3
  type: TYPE_NORMAL
- en: Also available are old `DISPATCH_QUEUE_PRIORITY` constants that can be used
    when specifying a queue priority type instead of `QOS_CLASS` but `QOS_CLASS` is
    preferred.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The second flag parameter is reserved and never used, so we just use 0\. The
    global queues are available for use by the whole system and everyone can add tasks
    to them. When all you need is to run some tasks, this is a perfect fit.
  prefs: []
  type: TYPE_NORMAL
- en: Own queues
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When you need to do more complex handling and have full control of what tasks
    are added to the queue, you can create your own queue. Own queues fit well when
    you need to be notified of when all tasks are done, or to do more complex synchronization
    between tasks.
  prefs: []
  type: TYPE_NORMAL
- en: You can create both concurrent and serial queues. Serial queues execute one
    task at a time, one after another, not concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A **task** is a block of code that needs to be run. A task is defined as a `dispatch_block_t`
    type and it is defined as `() -> Void`. We could use a closure or a function as
    a task.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Adding tasks to the queue
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have a queue and we have a task that we want to run. To run a task on a
    particular queue, we need to dispatch it to that queue. We could do this in two
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Synchronous**: `dispatch_sync`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Asynchronous**: `dispatch_async`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both functions are very simple and have the same type. The only differences
    are in their names and the way they work.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Synchronous dispatch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Synchronous dispatch submits a task for execution and waits until the task is
    done.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: When you use a concurrent queue and dispatch a task to it synchronously, the
    queue can run many tasks at the same time, but the `dispatch_sync` method waits
    until the task you submitted is finished.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Never call the `dispatch_sync` function from a task that is executing in the
    same queue. This would cause a deadlock for the serial queue and should be avoided
    for concurrent queues as well.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this example, the `print("1 Done")` instruction and the rest of the code
    will wait until `Task 1` is finished.
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous dispatch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Asynchronous dispatch, on the other hand, submits a task for execution and returns
    it immediately.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If we use the previous example and change it to use `dispatch_async`, `1 Done`
    willnot wait for tasks to be finished. We can also simulate extra work by freezing
    the current thread with a sleep function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As a result, `Task 2` is submitted for execution immediately after `Task 1`
    and it finishes execution before `Task 1`. Here is the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: GCD also has some powerful tools for synchronizing submitted tasks, but we are
    not going to cover them here. If you want to learn more, read the *Concurrency
    Programming Guide* article in the Apple library documentation at [https://developer.apple.com/library/ios/documentation/General/Conceptual/ConcurrencyProgrammingGuide](https://developer.apple.com/library/ios/documentation/General/Conceptual/ConcurrencyProgrammingGuide).
  prefs: []
  type: TYPE_NORMAL
- en: Operation queues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`NSOperationQueue` is built on top of GCD and provides more high-level abstraction
    and an API that allows us to control an application controlflow.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept is very similar to GCD; it has a queue and tasks that you add to
    the particular queue.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The NSOperationQueue provides a more high-level API but it is also a bit slower
    than GCD. NSOperationQueue fits very well with controlling the application flow,
    when tasks need to be chained, depend on each other, or need to be canceled. You
    can achieve the same functionality by using GCD but it would require implementing
    some extra logic that is already implemented by the NSOperationQueue.
  prefs: []
  type: TYPE_NORMAL
- en: GCD works very well when you need to perform a task and get the result and do
    not need to control the application flow.
  prefs: []
  type: TYPE_NORMAL
- en: Further in this chapter we will use GCD for concurrency. Now, let's move on
    and learn some tricks that will help us to make our code architecture more solid
    for concurrent programming.
  prefs: []
  type: TYPE_NORMAL
- en: Designing asynchronous code
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first characteristic of asynchronous code is that it returns immediately
    and notifies the caller when it has completed the operation. The best solution
    is to return the result of the computation as well. This way we get more function
    style *Input -> Output* functions style.
  prefs: []
  type: TYPE_NORMAL
- en: Let's have a look at this simple example. This code has many issues and will
    refactor them all.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have made a GCD structure that provides a nice API for working with GCD
    code. In the preceding example we have used a GCD `backgroundQueue` function;
    here is its implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: On the whole, the calculation code in that example is really bad and we could
    improve it by using a `reduce` method that would actually solve many problems
    and make the code safer and more readable.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: But the main point of that example was to show how dangerous it could be and
    what kinds of issues you could face with this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use this code to see the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The problem is that `calculateAverage` returns immediately as it is supposed
    to and the average is not calculated at this moment. To solve that problem all
    the asynchronous code should have some way to notify a caller when the task is
    completed. The easiest way to do this is to add a callback completion function
    as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now, when using this code, we can use a nice and clear trailing closure syntax
    for the result callback parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: There is one very important issue with this code. It is calling the `result`
    callback function on the background thread. It means that the closure we pass
    to `data.calculateAverage` will be called on the background but for us it's not
    documented and this behavior is not clear. We suppose that we will get that closure
    called on the main thread, because we are calling the `calculateAverage` function
    from the main thread. Let's do that. What we need to do is to switch to the main
    queue and call `result` on the main thread.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The best practice is to always call a callback method on the main queue by default
    if another behavior is not required. If you need to call a callback on the special
    queue, then it should be passed to a function as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: This code works but there is still one improvement that could be done. When
    the result callback gets called, the first thing we do is get the `average` instance.
    It would be way better if the result callback returned the result of its computation.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general terms, it is a good functional programming style for functions to
    take input and return the result, `X -> Y`. These functions are easier to use
    and test.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s refactor this code to pass an average result number to the callback
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The change is not big but the benefits are quite extensive. When we use the
    `calculateAverage` function we get the result directly in the closure as a parameter.
    Now we don't need to access the instance variable of `SalesData`. `SalesData`
    becomes more of a closed-box type with hidden implementation details and because
    of that we will be able to do more refactoring in the future.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The first rule is to avoid a state. When you are doing two things at the same
    time, those two processes should be as independent and isolated as possible. They
    shouldn''t know anything about each other or share any mutable resources. If they
    do, then we would need to take care of synchronizing access to that shared resource,
    which would bring a complexity to our system that we don''t want. Right now in
    our code we have two states: a `revenue` numbers array and the `average` result.
    Both of the processes have access to that state.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first problem in that the code is referencing itself. When you try to access
    an instance variable or a method that is out of the closure scope, you see an
    error message: **Reference to property ''revenue'' in closure requires explicit
    ''self.'' to make capture semantics explicit**.'
  prefs: []
  type: TYPE_NORMAL
- en: Xcode would also propose a fix to this issue, adding explicit self-capturing.
    This would solve the Xcode error but it wouldn't solve the root problem. When
    you see this error, stop and rethink your code design; in some cases it would
    be better to change the code, like in our case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Avoiding state](img/00025.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The second problem is having a mutable state and mutating an instance variable.
    Let''s use our last example once more and see why it''s a bad idea to have a state
    and access instance variables in the concurrent code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this code, it will crash with a **fatal error: Array index out of
    range** error due to getting the number from the array by an index operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: What is happening here is that, when we call `calculateAverage`, the revenue
    array has data, but later we remove all the revenue numbers and the arrays become
    empty; however, the indexes we are iterating over point to an old array size,
    and we are trying to access the index out of the bound arrays.
  prefs: []
  type: TYPE_NORMAL
- en: To solve that problem we should always try removing a state. One way to do that
    is to pass the needed data to a function as arguments or, if you want to have
    some state as in our case, capture the immutable values for a closure.
  prefs: []
  type: TYPE_NORMAL
- en: Capture list
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step to make this code better is to remove accessing mutable array
    variables. The easiest way to solve this is to make a local constant and use it
    in the closure.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This solution would work, because modifying the `revenue` instance variable
    doesn't have an impact on the local constant we have created. This code has one
    small issue. The constant is visible outside the closure, but it's intended to
    be used only inside the closure. It would be better if we could move it to the
    closure. We can do this by using a capture list of a closure. The only one change
    we need to do is to remove the local constant declaration and add it to the closure
    capture list. The rest of the code stays the same.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we used a very short capture list notation, but we could also
    provide an alternative name for the constant we are capturing and add additional
    ARC `weak` or `unowned` attributes.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Immutable state
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having a state in the concurrent code is a bad design but there are two types
    of state: mutable and immutable. In any case, you will need to have some sort
    of a state in the application. If you are going to have a state, make it immutable;
    in that way you will ensure that it won''t be changed and you can safely work
    with it.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a look at our previous code example we could make the `revenue`
    numbers immutable, which would solve the problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The first small change we would do is to change the revenue number array to
    be immutable. Because the `revenue` array is immutable it's not possible to modify
    it after we created an instance, so we need to remove this code.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Because `revenue` is immutable now, it's safe to use it in a concurrent code,
    so we can remove the capture list and use the immutable revenue directly by using
    `self` explicitly as we did before.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '`SalesData` contains immutable sales numbers that cannot be changed. This means
    that, after we have calculated the average value once, it will be the same all
    the time for that instance. The next time we call `calculateAverage`, we don''t
    need to calculate it again if we can reuse it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can even carry out one more step to make it immutable and use `struct` instead
    of a `class` for the `SalesData` type. When we do this, we will get an error saying:'
  prefs: []
  type: TYPE_NORMAL
- en: '**cannot assign to property: ''self'' is immutable**'
  prefs: []
  type: TYPE_NORMAL
- en: '**self.average = sum / self.revenue.count**'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you assign new values to `self.average`, you are modifying a self instance,
    and because structs are immutable by default we need to mark that method as mutating:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Those are only two changes we need to do. Also, when we are using it, we need
    to make an instance of `SalesData` as a variable, because `calculateAverage` is
    mutating it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: So now we can't have a constant `let` immutable `SalesData` instance. This is
    not a sign of good architecture and we should refactor it. Using a struct for
    data entities is a very good solution so we should keep refactoring code with
    this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Divide and conquer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The best way to achieve good, solid application architecture is to structure
    code well, create appropriate abstractions, and separate it into components with
    a single responsibility.
  prefs: []
  type: TYPE_NORMAL
- en: In functional programming it goes even further. The data—and the functions to
    work on that data—are also separated. The OOP concept of data and methods to work
    with it are split into two parts. This makes code even more flexible and reusable.
  prefs: []
  type: TYPE_NORMAL
- en: For concurrent code execution it's particularly important to split your code
    into standalone separate pieces because they can be sent for execution concurrently
    without blocking each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we start refactoring the code let''s analyze it first. The goal is to
    identify a component with a single responsibility. I did it and here are those
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**In Data**: The first part is our input data. In our case it is a `SalesData`
    structure that holds immutable data in our application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Calculation function**: The next part is our function that knows how to calculate
    the average for `SalesData`. It''s a simple first-class function that takes `SalesData`
    and returns the average. Its mathematical notation would be `f(x) = y` and the
    code notation would be `func calculateAverage(data: SalesData) -> Int`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Result data:** This is a result returned by the calculation function. In
    our example, it is a simple `Int` number that represents an average.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Async execution operation**: The next part is an operation that switches
    to the background thread and back to the main thread and that actually allows
    us to perform asynchronous code execution. In our example it''s a `dispatch_async`
    function call with an appropriate queue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cache**: Once we have calculated an average, we store it and don''t perform
    the calculation again. This is exactly what a cache is for.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we have identified separate components in our application, let's build relations
    and communication between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Divide and conquer](img/00026.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: To keep interaction simple, our application will ask a cache for an average
    value of `SalesData`. If a cache contains an average value, it will return it.
    Otherwise, it will start an async operation and pass `SalesData` to it. The async
    operation will call a `calculateAverage` function, get an average result, and
    pass it back to the cache. The cache will save it and forward it to the application.
  prefs: []
  type: TYPE_NORMAL
- en: 'This might sound a bit complicated when it''s explained in words, but in code
    it''s pretty simple, straightforward, and clear. Before we begin refactoring,
    let''s have a look at the code that we made this structure for:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: The first idea that came to my mind was to follow the FP principle *keep data
    and functions separate* and move the `calculateAverage` function outside a `SalesData`
    type.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: This would work, but there is one issue with this code. The `calculateAverage`
    function can only work with the `SalesData` type, so it should be hidden inside
    the `SalesData` type and not be visible to other types. Also, in the Swift method
    notation is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Swift 2.0 moves to methods over free functions, so it prefers to use an immutable
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Swift 2.0 Methods**: `[1,2,3].map { $0 + 1 }.filter { $0 > 1 }`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Swift 1.2 Function**: `filter(map([1,2,3]) { $0 + 1 }) { $0 > 2 }`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Instead of moving the `calculateAverage` function out of the `SalesData` type,
    let's make it immutable and make it only perform an average calculation instead,
    as we have shown in our schema.
  prefs: []
  type: TYPE_NORMAL
- en: '`SalesData` should:'
  prefs: []
  type: TYPE_NORMAL
- en: Store revenue numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Be an immutable function for calculating its average
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's refactor the `SalesData` structure and remove all other methods to follow
    our new structure
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the solution and it''s very clean and simple. Instead of a function
    we have used a computed property. Swift tends to use more read-only properties
    for immutable data and in our example it will make for better readability in the
    future. Also, we have finally used the `reduce` method for calculating the average.
    We can use it in this way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: The next step is to make it work asynchronously. Let's make a new type for it.
    It should take a `SalesData` and a callback closure, which will return `Int`,
    a calculated average result.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We have added two more helper methods to our GCD type:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This code looks okay, but there is one more issue with it. Calling an average
    is embedded together with switching to the background and main threads. It would
    be better if we kept these functions separate, so they could be reused if we wanted
    to add growth numbers and do a similar calculation for them.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Here we created a `runAsync` generic function that performs some work on the
    background, and returns its result on the main thread. We have used an `@autoclosure(escaping)`
    attribute here in order to be able to pass an expression `data.average, ...)`
    instead of wrapping it into a closure manually. This makes the code syntax cleaner.
  prefs: []
  type: TYPE_NORMAL
- en: Now we can calculate the average in an asynchronous way.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Now it's time to build our last component, a cache. For the caching functionality
    a dictionary would be the best choice. Let's add a dictionary to store average
    results for `SalesData`.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'We have created a `SalesDataCache` struct with one property, a cache, and a
    function to get the average value that either gives a cached value or calculates
    it and then saves it to the cache and returns. A very simple solution, but it
    won''t work. It shows an error: **Type ''SalesData'' does not conform to protocol
    ''Hashable''**.'
  prefs: []
  type: TYPE_NORMAL
- en: The keys in the dictionary have to be `Hashable`, so we need to implement this
    in our `SalesData` type. The `Hashable` protocol requires that we implement the
    `hashValue` property and the equality function.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Implementing a good hash function for an array of numbers is quite complex.
    The easiest way to do it is to add an `id` property to `SalesData` that will uniquely
    identify it.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Now our cache will work and we can finally use it in our application. Let''s
    do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the API we have created is really easy to use. Even though
    there is a lot of logic going on behind the scenes, for you it''s as simple as
    calling one method: `getAverage`.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, we have structured the underlying components in such a way that they can
    be used separately—for example, if we don't need a cache or asynchronous execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'To wrap up the refactoring work on this example, let''s see the full code we
    have ended up with:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Controlling the lifetime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In our code, we have used an `@autoclosure(escaping)` attribute. It is a very
    powerful attribute and it deserves to be covered in detail. There is also an `@noescape`
    attribute. Let's explore them in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Applying the @autoclosure and @noescape attributes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'First, let''s have a look at when and how we could use these attributes. We
    can apply them to a function parameter with a function type. A function type can
    be represented as a method, function, or closure and it has `(parameters) -> (return)`
    notation. Here are a few examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '@autoclosure'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `@autoclosure` attribute can be applied to a parameter with a function
    type that has no arguments and returns any type, `() -> T`. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: When we use an `increase` function without the `@autoclosure` attribute, we
    need to pass a function, a method, or a closure as a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'But in this use case it would be better if we could simply use an expression
    without the need to wrap it in a closure, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: And that's exactly what `@autoclosure` allows us to do. When you make a parameter
    with the `@autoclosure` attribute, the expression you pass as an argument is automatically
    wrapped into a closure for you. It makes your code cleaner. That's all it does.
    No magic; it simply removes boilerplate code for you.
  prefs: []
  type: TYPE_NORMAL
- en: '@noescape'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `@noescape` keyword is more complex and interesting. It can be applied to
    a function parameter with any function type.
  prefs: []
  type: TYPE_NORMAL
- en: The `@noescape` attribute indicates that a closure will be used inside a function
    body, before the function return is called. It means it won't escape the function
    body.
  prefs: []
  type: TYPE_NORMAL
- en: When you apply this attribute, it indicates that a closure will be used synchronously
    inside the function body. Also, it means that it will be released when you leave
    the function. The lifetime of that closure parameter can't outlive the function
    call.
  prefs: []
  type: TYPE_NORMAL
- en: Applying this attribute enables some performance optimization but, more importantly,
    it disables the requirement to explicitly specify "`self.`" when accessing instance
    members.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at some examples to better understand this. For a simple
    example, we will use the same `increase` function, but now we will make it a method
    of a `struct`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The `increase` function call contains an error: **Reference to property ''number''
    in closure requires explicit ''self.'' to make capture semantics explicit**; we
    need to explicitly reference `self.number`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'But let''s have a look at the `increase` function. The `f: ()-> Int` parameter
    is used inside the function body and it''s not leaving its scope. This is a great
    candidate for applying the `@noescape` attribute to it.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Now we don't need to do any further changes and explicitly reference `self.numbers`,
    because `@noescape` guarantees that a closure will be called before we leave that
    function and we can safely reference `self`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apply `@noescape` wherever possible. It adds an extra security level to the
    code. Also, it enables better optimization and increases performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we have a look at methods and function such as `map`, `reduce`, `contains`,
    and others in the Swift standard library, you will see that they are marked with
    the `@noescape` attribute. The golden rule is: *If you call the closure parameter
    before you leave the function, mark it with @noescape*.'
  prefs: []
  type: TYPE_NORMAL
- en: Maybe in the future Swift will automatically do this for you, but for now we
    need to do it ourselves.
  prefs: []
  type: TYPE_NORMAL
- en: '@autoclosure (escaping)'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `@autoclosure` attribute also applies an `@noescape` implicitly. If you
    want to make a parameter an autoclosure, while indicating that it will have a
    bigger lifetime than a function, use an `@autoclosure(escaping)` attribute. It
    could be useful for asynchronous code execution, like in our example with `AsyncOperation`.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the first part of this chapter, we covered multithreading concurrency and
    multi-core device architecture. This general information allows us to understand
    the core principles of concurrent code execution.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we covered three ways to run code asynchronously in Swift,
    by using threads, GCD, and NSOperation. We have explored the differences between
    them and the situations for which each is most suitable.
  prefs: []
  type: TYPE_NORMAL
- en: In the third part of the chapter, we concentrated on architecting asynchronous
    Swift code by using GCD. We have covered important tips such as passing a callback
    function parameter, avoiding a state, using immutable values, and others. Also,
    we have covered two Swift attributes—`@noescape` and `@autoclosure`—that are very
    useful.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will cover one more important performance optimization
    technique: Lazy Loading.'
  prefs: []
  type: TYPE_NORMAL
