<html><head></head><body>
<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06" class="calibre1"/>Chapter 6. Exhibit the Metal in Your Game</h1></div></div></div><p class="calibre8">Up to this point, we have learned quite a bit. We looked into Apple's Swift programming language, got an idea of the general flow of an iOS app, and how to control that through code and/or storyboards. We got an understanding of how 2D games and 2D overlays can be made with<a id="id341" class="calibre1"/> <strong class="calibre9">SpriteKit</strong> and how 3D games can be designed even in the <a id="id342" class="calibre1"/><strong class="calibre9">Xcode</strong> editor with SceneKit. Finally, we reviewed how to create reusable <a id="id343" class="calibre1"/>game logic, components, and AI with the various aspects of <strong class="calibre9">GameplayKit</strong>.</p><p class="calibre8">Essentially, this is all that is needed to get right to planning, coding, and building your own games. If there's a game idea that has come to your mind at this time, go right ahead and start planning it out. The frameworks and Xcode features from the past chapters can help take your abstract ideas and start turning them into what could soon be a playable application.</p><p class="calibre8">However, before moving forward, we'd like to take this time to go over a few more tips, tricks, and topics that we either briefly mentioned or have yet to go over. These topics mainly cover the ways we can optimize our games and get more out of the Apple hardware. In this chapter, we shall review a bit on the rather advanced topic of the Apple Metal low-level graphics API.</p><div><h3 class="title2"><a id="note37" class="calibre1"/>Note</h3><p class="calibre8">Just a warning that the topic of low-level graphics APIs can get rather advanced. This won't be an all-encompassing tutorial on the subject; more of an upper-level summary and a way to appreciate all that SpriteKit and SceneKit do in the background for us. We hope that, at the very least, it makes you wish to pursue how to build your own custom rendering objects that might potentially allow the development of extremely performant and detailed games.</p></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_1"><a id="ch06lvl1sec44" class="calibre1"/>The Apple Metal API and the graphics pipeline</h1></div></div></div><p class="calibre8">One of <a id="id344" class="calibre1"/>the rules, if not <em class="calibre10">the golden rule</em> of modern video game development, is to keep our games running constantly at 60 frames per second or greater. If developing for VR devices and applications, this is of even more importance as dropped frame rates could lead to a sickening and game ending experience for the player.</p><p class="calibre8">In the past, being<a id="id345" class="calibre1"/> lean was the name of the game; hardware limitations prevented much from not only being written to the screen but how much memory storage a game could hold. This limited the number of scenes, characters, effects, and levels. In the past, game development was built more with an engineering mindset, so the developers made the things work with what little they had. Many of the games on 8-bit systems and earlier had levels and characters that were only different because of elaborate sprite slicing and recoloring.</p><p class="calibre8">Over time, advances in hardware, particularly that of GPUs allowed for richer graphical experiences. This leads to the advent of computation-heavy 3D models, real-time lighting, robust shaders, and other effects that we can use to make our games present an even greater player experience; this while trying to stuff it all in that precious .016666 second/60 Hz window.</p><p class="calibre8">To get everything out of the hardware and combat the clash between a designer's need to make the best looking experience and the engineering reality of hardware limitations in even today's CPU/GPUs, Apple developed the Metal API.</p></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_2"><a id="ch06lvl2sec58" class="calibre1"/>CPU/GPU framework levels</h2></div></div></div><p class="calibre8">Metal is <a id="id346" class="calibre1"/>what's known as a low-level GPU API. When we build our games on the iOS platform, there are different levels between the machine code in our GPU/CPU hardware and what we use to design our games. This goes for any piece of computer hardware we work with, be it Apple or others. For example, on the CPU side of things, at the very base of it all is the <a id="id347" class="calibre1"/><strong class="calibre9">machine code</strong>. The next level up is the <a id="id348" class="calibre1"/><strong class="calibre9">assembly language</strong> of the chipset. Assembly language differs based on the CPU chipset and allows the programmer to be as detailed as determining the individual registers to swap data in and out of in the processor. Just a few lines of a for-loop in C/C++ would take up a decent number of lines to code in assembly. The benefit of working in the lower levels of code is that we could make our games run much faster. However, most of the mid-upper level languages/APIs are made to work well enough so that this isn't a necessity anymore.</p><div><h3 class="title2"><a id="note38" class="calibre1"/>Note</h3><p class="calibre8">Game developers have coded in assembly even after the very early days of game development. In the late 1990's, the game developer Chris Sawyer created his game, <strong class="calibre9">Rollercoster Tycoon™</strong>, almost <a id="id349" class="calibre1"/>entirely in the x86 assembly language! Assembly can be a great challenge for any enthusiastic developer who loves to tinker with the inner workings of computer hardware.</p></div><p class="calibre8">Moving up the chain we have where C/C++ code would be and just above that is where we'd find Swift and Objective-C code. Languages such as Ruby and JavaScript, which some developers can use in Xcode, are yet another level up.</p><p class="calibre8">That was about the CPU, now on to the GPU. The <a id="id350" class="calibre1"/><strong class="calibre9">Graphics Processing Unit</strong> (<strong class="calibre9">GPU</strong>) is <a id="id351" class="calibre1"/>the coprocessor that works with the CPU to make the calculations for the visuals we see on the screen. The following diagram shows the GPU, the APIs that work with the GPU, and possible iOS games that can be made based on which framework/API is chosen.</p><div><img src="img/00074.jpeg" alt="CPU/GPU framework levels" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Like the CPU, the lowest level is the processor's machine code. To work as close to the GPU's machine code as possible, many developers would use Silicon Graphics' <strong class="calibre9">OpenGL API</strong>. For<a id="id352" class="calibre1"/> mobile devices, such as the iPhone and iPad, it would be the OpenGL subset, <strong class="calibre9">OpenGL ES</strong>. Apple <a id="id353" class="calibre1"/>provides a helper framework/library to OpenGL ES named <a id="id354" class="calibre1"/><strong class="calibre9">GLKit</strong>. GLKit helps simplify some of the shader logic and lessen the manual work that goes into working with the GPU at this level. For many game developers, this was practically the only option to make 3D games on the iOS device family originally; though some use of iOS's Core Graphics, Core Animation and UIKit frameworks were perfectly fine for simpler games.</p><p class="calibre8">Not too long into the lifespan of the iOS device family, third-party frameworks came into play, which were aimed at game development. Using OpenGL ES as its base, thus sitting directly one level above it, is the <a id="id355" class="calibre1"/><strong class="calibre9">Cocos2D framework</strong>. This was actually the framework used in the original release of Rovio's Angry Birds™ series of games back in 2009. Eventually, Apple realized how important gaming was for the success of the platform and made their own game-centric frameworks, that is, the SpriteKit and SceneKit frameworks. They too, like Cocos2D/3D, sat directly above OpenGL ES. When we made SKSprite nodes or SCNNodes in our Xcode projects, up until the introduction of Metal, OpenGL operations were being used to draw these objects in the update/render cycle behind the scenes. As of iOS 9, SpriteKit and SceneKit use Metal's rendering pipeline to process graphics to the screen. If the device is older, they revert to OpenGL ES as the underlying graphics API.</p></div></div></div>

<div><div><div><div><div><div><h2 class="title1" id="calibre_pb_3"><a id="ch06lvl2sec59" class="calibre1"/>Graphics pipeline overview</h2></div></div></div><p class="calibre8">This <a id="id356" class="calibre1"/>topic can be a book all on its own, but let's take a look at the graphics pipeline to get an idea, at least on an upper level, of what the GPU is <a id="id357" class="calibre1"/>doing during a single rendered frame. We can imagine the graphical data of our games being divided in two main categories:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Vertex data</strong>: This is <a id="id358" class="calibre1"/>the position information of where on the screen this data can be rendered. Vector/vertex data can be expressed as points, lines, or triangles. Remember the old saying about video game graphics, "everything is a triangle." All of those polygons in a game are just a collection of triangles via their point/vector positions. The GPU's <a id="id359" class="calibre1"/><strong class="calibre9">Vertex Processing Unit</strong> (<strong class="calibre9">VPU</strong>) handles this data.</li><li class="listitem"><strong class="calibre9">Rendering/pixel data</strong>: Controlled <a id="id360" class="calibre1"/>by the GPU's Rasterizer, this is the data that tells the GPU how the objects, positioned by the vertex data, will be colored/shaded on the screen. For example, this is where color channels, such as RGB and alpha, are handled. In short, it's the pixel data and what we actually see on the screen.</li></ul></div><p class="calibre8">Here's a diagram showing the graphics pipeline overview:</p><div><img src="img/00075.jpeg" alt="Graphics pipeline overview" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">The graphics pipeline is the sequence of steps it takes to have our data rendered to the screen. The previous diagram is a simplified <a id="id361" class="calibre1"/>example of this process. Here are the main sections that can make up the pipeline:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">Buffer objects</strong>: These<a id="id362" class="calibre1"/> are known as <a id="id363" class="calibre1"/><strong class="calibre9">Vertex Buffer Objects</strong> in OpenGL and are of the class <code class="email">MTLBuffer</code> in the Metal API. These <a id="id364" class="calibre1"/>are the objects we create in our code that are sent from the CPU to the GPU for <strong class="calibre9">primitive processing</strong>. These objects contain data, such as the positions, normal vectors, alphas, colors, and more.</li><li class="listitem"><strong class="calibre9">Primitive processing</strong>: These <a id="id365" class="calibre1"/>are the steps in the GPU that take our <a id="id366" class="calibre1"/>Buffer Objects, break down the various vertex and rendering data in those objects, and then draw this information to the frame buffer, which is the screen output we see on the device.</li></ul></div><p class="calibre8">Before we go over the steps of primitive processing done in Metal, we should first understand the history and basics of shaders.</p></div></div></div>

<div><div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec45" class="calibre1"/>What are shaders?</h1></div></div></div><p class="calibre8">GPUs first <a id="id367" class="calibre1"/>came into use because of none other than the video game industry. Arcade cabinets in the 1970's had GPU chips separate from the main CPU to handle the specialized visual needs of the games compared with other computing applications at the time. Eventually, the need to draw 3D graphics in games in the mid-1990's led to the modern GPU architecture we have now. Shaders were actually first introduced in 1988 by Pixar back when the company was run by Apple's cofounder Steve Jobs. Shaders are little programs we can write directly to the GPU to process the vertex and pixel data. Originally, APIs such as OpenGL ES 1.0 didn't make use of shader processing but instead were what's known as fixed-function APIs. In fixed-function APIs, programmers just referenced simple set rendering commands to the GPU. As GPUs evolved and took more work away from the CPU, the use of shaders increased. Although a rather more advanced way to traverse the graphics pipeline than the fixed-function methodology, shaders allow for even deeper customization of what the GPU displays to the screen. Game developers and 3D artists continue to push visual effects in games with them.</p><p class="calibre8">From OpenGL 2.0 and onwards, shaders were built in the API's C-like language named GLSL. In the Apple Metal API, we build shaders with the Metal Shading Language, which is a subset of C++11 of the file type <code class="email">.metal</code> and can run the pipeline in either Objective-C or Swift with our view controllers.</p></div>

<div><div><div><div><div><h2 class="title1" id="calibre_pb_1"><a id="ch06lvl2sec60" class="calibre1"/>Types of shaders</h2></div></div></div><p class="calibre8">Shaders come in <a id="id368" class="calibre1"/>a number of types that continue to grow as 3D games and art animation continues to progress. The most commonly used are Vertex shaders <a id="id369" class="calibre1"/>and <a id="id370" class="calibre1"/>Fragment shaders. Vertex shaders are used to transform 3D coordinates into 2D coordinates for the screen to display, in short, the positioning data of our graphics. Fragment shaders, also known as <a id="id371" class="calibre1"/>Pixel shaders, are what are used to convert colors and other visual attributes of pixels on the screen. These other attributes of Fragment Shaders can include bump mapping, shadows, and specific highlights as well. We emphasized the word <em class="calibre10">attributes</em> because that's usually the name given for the properties or input of our shader programs.</p><p class="calibre8">Here is a code sample<a id="id372" class="calibre1"/> of a simple Vertex and Fragment shader written in the Metal Shading Language.</p><div><pre class="programlisting">//Shaders.metal
//(1)
#include &lt;metal_stdlib&gt;
using namespace metal;
//(2)
vertex float4 basic_vertex(                           
//(3)
  const device packed_float3* vertex_array [[ buffer(0) ]], 
//(4)
  unsigned int vertexID [[ vertex_id ]]) {       
//(5)          
  return float4(vertex_array[vertexID], 1.0);              
}
//(6)
fragment half4 basic_fragment() { 
  return half4(1.0);  </pre></div><p class="calibre8">The code here is a bit different than what we've seen throughout the course of the book. Let's go over it line by line.</p><div><ol class="orderedlist"><li class="listitem" value="1">The Metal Shading Language is a C++11-like language, so we see that the Metal Standard Library is imported into the shader file with the line <code class="email">#include &lt;metal_stdlib&gt;</code> in addition to <code class="email">using namespace metal;</code>.</li><li class="listitem" value="2">The next line is the creation of our Vertex shader using the keyword <code class="email">vertex</code>. This shader is a vertex of four floats. Why four floats when 3D space only deals with <em class="calibre10">x</em>, <em class="calibre10">y</em>, and <em class="calibre10">z</em> coordinates? To summarize, 3D matrix math involves a fourth component, <em class="calibre10">w</em>, to accurately handle the math calculations of 3D space. In short if <em class="calibre10">w= 0</em>, the <em class="calibre10">x</em>, <em class="calibre10">y</em>, and <em class="calibre10">z</em> coordinates are vectors; if <em class="calibre10">w = 1</em>, then those coordinates are points. The purpose of this shader will be to draw simple points to the screen, so <em class="calibre10">w</em> will be 1.0.</li><li class="listitem" value="3">Here, we create a pointer to an array of float3 type (holders for our <em class="calibre10">x</em>, <em class="calibre10">y</em>, and <em class="calibre10">z</em> coordinates) and set it to the very first buffer with the <code class="email">[[ buffer(0) ]]</code> declaration. The <code class="email">[[ ]]</code> syntax is used to declare inputs/attributes for our shaders.</li><li class="listitem" value="4">The <a id="id373" class="calibre1"/>unsigned integer <code class="email">vertexID</code> is what we name the <code class="email">vertex_id</code> attribute of this particular array of vertices.</li><li class="listitem" value="5">This is where the float4 type is returned, or in this case, the final position of this vertex array. We see that it returns two sections of the output: the first being the reference to this vertex array, identified by the <code class="email">vertex_id</code> attribute and the <code class="email">w</code> value of <code class="email">1.0</code>, to represent that these are points in space.</li><li class="listitem" value="6">This line is where we create the fragment shader, using the <code class="email">fragment</code> keyword. This shader is of the data type <code class="email">half4</code>, which is an array of [4,4] 16-bit floats. This is, in this case, ultimately to create 16-bit colored pixels. The data in this [4,4]-component vector type saves 16 bits to R, G, B, and alpha channels. This shader is going to simply show pure white pixel shading with no transparency, so we simply write <code class="email">return half4(1.0);</code>. This sets all of the bits to 1, which is equivalent to <code class="email">rgba(1,1,1,1)</code>.</li></ol><div></div><p class="calibre8">When we create a Buffer Object, which can just be a Struct of floating points on the screen, we pass that data through these shaders and out would pop up a white triangle or set of triangle shapes on the screen.</p><p class="calibre8">Looking back at the <em class="calibre10">Graphics pipeline</em> diagram, we see that after the vertex shader is calculated, the GPU does what's known as <a id="id374" class="calibre1"/><strong class="calibre9">Primitive Assembly</strong>. This is essentially where the points and vectors defined in the vertex shader are mapped to coordinates in screen space. The Rasterizer step, in simple terms, then figures from the vertex data where and how we can and can't color that pixel data onto the screen using the fragment shader information. After taking in the fragment shader information, the GPU then uses that information for the blending of that pixel data. Finally, that output is sent to or committed to the frame buffer where the player sees that output. This all happens in a single draw call in the render cycle. Having all of your game's lights, pixels, effects, physics, and other graphics cycle through this in .016666 seconds is the name of the game.</p><p class="calibre8">We'll go over some more Metal code later but understand for now that shaders are like little instruction factories for data input we send to them in our Swift/Object-C code. Other shader types that have arisen over the years are Geometry Shaders and Tessellation Shaders.</p><div><h3 class="title2"><a id="note39" class="calibre1"/>Note</h3><p class="calibre8">Both the Vertex and Fragment shaders are present in this single <code class="email">.metal</code> file, but typically shaders are written in separate files. Xcode and Metal will combine all <code class="email">.metal</code> files in your project, so it doesn't matter if the shaders are in one file or not. OpenGL's GLSL for the most part forces the separation of shader types.</p></div><p class="calibre8">For years, OpenGL worked well for many different GPUs but as we all see, Apple Metal allows us to perform draw calls up to 10x times faster than OpenGL ES.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec46" class="calibre1"/>Why is Metal faster than OpenGL ES?</h1></div></div></div><p class="calibre8">In late 2013, Apple announced the <a id="id375" class="calibre1"/><strong class="calibre9">iPhone 5s</strong>. Built into the 5s was the <strong class="calibre9">A7 Processor</strong>, the<a id="id376" class="calibre1"/> first 64 bit GPU for the iOS device family. It provided a decent <a id="id377" class="calibre1"/>graphical boost compared with prior devices and reflected how GPUs in mobile devices were quickly catching up to gaming consoles released just a few years prior. OpenGL, though a staple in low-level graphics APIs, didn't squeeze the most out of the A7 chip. </p><p class="calibre8">Seen in the next diagram, the interaction between the CPU and GPU doesn't always perform the optimal way we'd want it to for our games.</p><div><img src="img/00076.jpeg" alt="Why is Metal faster than OpenGL ES?" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">Be it textures, shaders, or render targets, draw calls use their own state vector. The CPU via the low-level API uses much of that time verifying the state of the draw call. This process is very expensive for the CPU. What happens is that in many cycles, the GPU is sitting idle, waiting for the CPU to finish its past instruction. Here's what's taking up all of that time in the API:</p><div><ul class="itemizedlist"><li class="listitem"><strong class="calibre9">State validation</strong>: Confirming<a id="id378" class="calibre1"/> API usage is valid. This encodes API state to the hardware state.</li><li class="listitem"><strong class="calibre9">Shader compilation</strong>: Runtime <a id="id379" class="calibre1"/>generation of the shader machine code. This deals with interactions between the state and shaders.</li><li class="listitem"><strong class="calibre9">Sending work to the GPU</strong>: Managing<a id="id380" class="calibre1"/> resource residency batching commands.</li></ul></div><p class="calibre8">What Apple did with <a id="id381" class="calibre1"/>their Metal API is do these steps in a smarter fashion. Shader compilation is done during the application's load time. There's no need to reload the shaders every cycle; this was simply a relic of older hardware limitations. This is why in our previous code example, we can build more than one shader in one Metal file, while this was prohibited in OpenGL ES. State validation, though important, doesn't need to be checked every cycle. Checking the state validation can be set to happen only when new content is loaded.</p><div><h3 class="title2"><a id="note40" class="calibre1"/>Note</h3><p class="calibre8">Even with Metal's advantages, this is why it's recommended to store 2D animations in <a id="id382" class="calibre1"/><strong class="calibre9">SpriteSheets</strong>. We mentioned SpriteSheets back in our discussion of on SpritKit. They are a collection of sprites fitted onto one texture. The graphics pipeline then only has to deal with one version of that content. Internally under the hood of SpriteKit, the GPU then doesn't have to do as many state vector calls compared to having each character animation being placed on its own separate texture.</p></div><p class="calibre8">The last process for the CPU is when it sends the information out to the GPU for processing. This is going to be done during each draw call, and in either Metal or Open GL ES, it will still be this process that will happen the most frequently. Here is the result of this internal, low-level restructuring done in the Metal API:</p><div><img src="img/00077.jpeg" alt="Why is Metal faster than OpenGL ES?" class="calibre11"/></div><p class="calibre12"> </p><p class="calibre8">As we see in the diagram from <em class="calibre10">WWDC14</em>, there are up to 10 extra draw calls that can be added during the render cycle! We can use that time saved for other processes instead of extra draw calls, such as more physics or AI in our games.</p><div><h3 class="title2"><a id="note41" class="calibre1"/>Note</h3><p class="calibre8">The cycle diagrams shown are from the original Metal API announcement at <em class="calibre10">WWDC2014</em> and used a frame rate of 30 fps. If developing for VR where 60 fps or greater is necessary for a working game, these numbers are halved. Either way this is rather impressive for mobile device GPUs. Search online for games made in Metal and you'd be impressed. With this much room to add more to our game during each render cycle, there's no reason not to have an impressive game at the full 60 fps. Additionally, as of iOS 9, the SpriteKit and SceneKit frameworks by default are backed by Metal. Even if the Metal API is too much to understand, we can still utilize these render saving benefits from what we already learned about these frameworks.</p></div></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec47" class="calibre1"/>The basic Metal object/code structure</h1></div></div></div><p class="calibre8">To <a id="id383" class="calibre1"/>finish off our talk about Apple Metal, let's look at an overview of the API's object and code structuring. We already briefly saw some shader code in the Metal Shading Language, so let's see how we can work with this API in our projects.</p><div><table border="1" class="calibre20"><colgroup class="calibre21"><col class="calibre22"/><col class="calibre22"/></colgroup><thead class="calibre23"><tr class="calibre24"><th valign="bottom" class="calibre25">
<p class="calibre26">Objects</p>
</th><th valign="bottom" class="calibre25">
<p class="calibre26">Purpose</p>
</th></tr></thead><tbody class="calibre27"><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">Device</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Reference to the GPU</p>
</td></tr><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">Command queue</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Serial sequence of command buffers</p>
</td></tr><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">Command buffer</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Contains GPU hardware commands</p>
</td></tr><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">Command encoder</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Translates API commands to GPU hardware commands</p>
</td></tr><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">State</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Framebuffer configuration, depth, samplers, blend, and so on</p>
</td></tr><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">Code</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Shaders (vertex, fragment, geometry, and tessellation)</p>
</td></tr><tr class="calibre24"><td valign="top" class="calibre28">
<p class="calibre26">Resources</p>
</td><td valign="top" class="calibre28">
<p class="calibre26">Textures and Data Buffer Objects (vertices, constants, and so on)</p>
</td></tr></tbody></table></div><p class="calibre8">The preceding table represents the various types of objects that we'd work with if writing a game directly in the Metal API. They are the Device, the State, the Command Buffer, our Shaders, Textures, and many more.</p><p class="calibre8">We can import the Metal API into <code class="email">ViewController.swift</code> class with the following:</p><div><pre class="programlisting">import Metal
import QuartzCore</pre></div><p class="calibre8">This imports the Metal API. The QuartzCore API is needed as well since the <code class="email">CAMetalLayer</code> object we will work with is a member of that library. Also, make sure that you set your target device to an actual iOS device as new or newer than the iPhone 5S, the Xcode simulator does not support Metal. Otherwise, Xcode will give you the <em class="calibre10">Could Not Build</em> Objective-C model <code class="email">Metal</code> error. This is true as of the writing of this book with the Xcode 7 Beta. Over time and probably after the official public release of the El Capitan OS, this will no longer be needed. For now, to test your own custom Metal code, you will have to test on an actual device. Doing so will involve having to pay for your own Apple Development account. More on this is given in the next chapter.</p><p class="calibre8">Here's the order in <a id="id384" class="calibre1"/>which we'd have to work with the objects in the table shown previously as well as some code samples in Swift that accomplish these steps:</p><div><ol class="orderedlist"><li class="listitem" value="1">Create the reference to the Device with the <code class="email">MTLDevice</code> class as:<div><pre class="programlisting">let device: MTLDevice = MTLCreateSystemDefaultDevice()</pre></div></li><li class="listitem" value="2">Create a <code class="email">CAMetalLayer</code> object for these objects to be placed on the screen as:<div><pre class="programlisting">let metalLayer = CAMetalLayer()</pre></div></li><li class="listitem" value="3">Create Vertex Data/Buffer Object(s) (VBOs) to send data to shaders as follows:<div><pre class="programlisting">/*Simple Vertex Data object, an array of floats that draws a simple triangle to the screen */
let vertexData:[Float] = [
  0.0, 1.0, 0.0,
  -1.0, -1.0, 0.0,
  1.0, -1.0, 0.0]</pre></div></li><li class="listitem" value="4">Create our shaders that will work with these VBOs.<p class="calibre14">We did this in our previous shader code samples. The vertex data combined with our previously made shaders together create a simple white triangle to the screen.</p></li><li class="listitem" value="5">Set up a Render Pipeline as follows:<div><pre class="programlisting">//Library objects that reference our shaders we created
let library = device.newDefaultLibrary()!
//constant where we pass the vertex shader function
let vertexFunction = library.newFunctionWithName("basic_vertex")
//now the fragment shader 
let fragmentFunction = library.newFunctionWithName("basic_fragment")

/*Describes the Render Pipeline and sets the vertex and fragment shaders of the Render Pipeine*/
let pipelineStateDescriptor = MTLRenderPipelineDescriptor()
//initiates the descriptor's vertex and fragment shader function properties with the constants we created prior
pipelineStateDescriptor.vertexFunction = vertexFunction
pipelineStateDescriptor.fragmentFunction = fragmentFunction

//Makes the pixel format an 8bit color format
pipelineStateDescriptor.colorAttachments.objectAtIndexedSubscript(0).
pixelFormat = .BGRA8Unorm
 
/*Checks if we described the Render Pipeline correctly, otherwise, throws an error. */
var pipelineError : NSError?
pipelineState = device.newRenderPipelineStateWithDescriptor(pipelineStateDescriptor, error: &amp;pipelineError)
if pipelineState == nil {
  println("Pipeline state not created, error \(pipelineError)")</pre></div></li><li class="listitem" value="6">Create a command queue as follows:<div><pre class="programlisting">var commandQueue = device.newCommandQueue()</pre></div></li></ol><div></div><p class="calibre8">To actually<a id="id385" class="calibre1"/> render these objects in our game, we'd have to do the following processes in our view controller:</p><div><ol class="orderedlist"><li class="listitem" value="1">Create a display link. This is a timer that refreshes every time the screen refreshes. It's a member of the class <code class="email">CADisplayLink</code> and at every screen refresh, we call the <code class="email">gameRenderLoop</code> function.<div><pre class="programlisting">var timer = CADisplayLink(target: self, selector: Selector("gameRenderLoop"))
timer.addToRunLoop(NSRunLoop.mainRunLoop(), forMode: NSDefaultRunLoopMode)</pre></div><p class="calibre14">The <code class="email">gameRenderLoop</code> function can look like the following. It calls the soon-to-be filled in function, <code class="email">render()</code>:</p><div><pre class="programlisting">func gameRenderloop() {
  autoreleasepool {
    self.render()
  }</pre></div></li><li class="listitem" value="2">Create a Render Pass Descriptor. For this example, a mostly red texture is to be created around our white triangle as shown here:<div><pre class="programlisting">let passDescriptor = MTLRenderPassDescriptor() 
passDescriptor.colorAttachments[0].texture = drawable.texture
passDescriptor.colorAttachments[0].loadAction = .Clear
passDescriptor.colorAttachments[0].storeAction = .Store
passDescriptor.colorAttachments[0].clearColor = MTLClearColorMake(0.8, 0.0, 0.0, 1.0)</pre></div></li><li class="listitem" value="3">Create a Command Buffer in our <code class="email">render()</code> function:<div><pre class="programlisting">let commandBuffer = commandQueue.commandBuffer()</pre></div></li><li class="listitem" value="4">Create a Render Command Encoder. In other words, a set of commands for <code class="email">commandQueue</code>. In the code example given later, this tells the GPU to draw triangles with the VBO we created earlier. This is placed (in this example) in the <code class="email">render()</code> function.<div><pre class="programlisting">let renderEncoderOpt = commandBuffer.renderCommandEncoderWithDescriptor(renderPassDescriptor)
if let renderEncoder = renderEncoderOpt {
  renderEncoder.setRenderPipelineState(pipelineState)
  renderEncoder.setVertexBuffer(vertexBuffer, offset: 0, atIndex: 0)
  renderEncoder.drawPrimitives(.Triangle, vertexStart: 0, vertexCount: 3, instanceCount: 1)
  renderEncoder.endEncoding()
}</pre></div></li><li class="listitem" value="5">Commit your Command Buffer. This essentially tells the GPU to do its draw call based on the commands that have been packed into the <code class="email">commandBuffer</code> object. Place this after the past code's <code class="email">if</code> statement in the <code class="email">render()</code> function.<div><pre class="programlisting">commandBuffer.presentDrawable(drawable) 
commandBuffer.commit()</pre></div></li></ol><div></div><p class="calibre8">That is <a id="id386" class="calibre1"/>the short of it. That's the general process of drawing a simple triangle to the screen and manually creating the render loop on the GPU.</p><p class="calibre8">Should you rather opt for SpriteKit and SceneKit to do all of this manual work for you? That would be understandable. Remember though, like when playing a game on hard mode, it comes with its rewards to take the harder route. Yes, as of iOS 9, the SpriteKit and SceneKit frameworks are default to Metal. Game engines, such as Unity and Unreal Engine, even implement Metal when converting projects to the platform. However, knowing how to build your games in a low-level graphics API, such as Metal or OpenGL, will give the developer the ability to have the potential for most lean/fast performing game for the device family. Be sure to check out some of the games created with Metal next time you search online. They can really give your players a great experience. At the same time, this can challenge your skills as a game developer since being a game developer is the combination of an artist, engineer, and computer scientist. Working directly in the GPU's basic functions will challenge all of that.</p><p class="calibre8">To dive more into the rabbit hole that is low-level graphics development with Metal, check out these links:</p><div><ul class="itemizedlist"><li class="listitem"><a class="calibre1" href="https://developer.apple.com/metal/">https://developer.apple.com/metal/</a></li><li class="listitem"><a class="calibre1" href="https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/data-types/data-types.html">https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/data-types/data-types.html</a></li><li class="listitem"><a class="calibre1" href="http://www.raywenderlich.com/77488/ios-8-metal-tutorial-swift-getting-started">http://www.raywenderlich.com/77488/ios-8-metal-tutorial-swift-getting-started</a></li><li class="listitem"><a class="calibre1" href="https://realm.io/news/3d-graphics-metal-swift/">https://realm.io/news/3d-graphics-metal-swift/</a></li></ul></div><p class="calibre8">The first <a id="id387" class="calibre1"/>link is to the official Apple Developer page for Metal. The next link is Apple's list of data types used in the Metal API. The last two links are two separate tutorials to make simple Metal scenes in Swift. Some of the code we used can be found in these tutorials as well as full Xcode projects. The first of these two links are to the iOS tutorial site <a class="calibre1" href="http://www.raywenderlich.com">www.raywenderlich.com</a>. The last link is to a page that has a great video presentation and full instructions on Swift and Metal 3D graphics by former Apple Engineer, <em class="calibre10">Warren Moore</em>.</p></div>
<div><div><div><div><h1 class="title" id="calibre_pb_0"><a id="ch06lvl1sec48" class="calibre1"/>Summary</h1></div></div></div><p class="calibre8">Congrats on getting this far. If this book were a game, we'd probably have earned an achievement for this chapter alone. As we saw, working with a low-level API such as Metal can be a bit daunting. We first reviewed what it means when developers and engineers mention lower and upper level frameworks and code. On the CPU side, we saw that the lowest level is the machine's code with Swift and Objective-C in the middle, and above C/C++ and Assembly code. Next, we spoke about the GPU side and where the visual graphics APIs we've gone over in the past chapters stand in the hierarchy. We then got an understanding of the history of lower-level graphics APIs such as OpenGL ES, how the graphic pipeline generally works under the hood, and how to make basic shaders. Finally, we reviewed why Metal is faster during the render cycle than OpenGL, the general structure behind Metal, and some of the code/objects used to manually set up the render loop. This chapter merely scratched the surface on this topic, so if you are up to the challenge, it's highly recommended to continue reading documentation on how Metal can make your games stand out from the rest.</p><p class="calibre8">At this point, you should now have all that it takes to make a game on the iOS platform. The last essential lesson for iOS game development is learning how to test, publish, and update your published game in the app store. In the next chapter, let's learn how to get that game on the Apple app store.</p></div></body></html>