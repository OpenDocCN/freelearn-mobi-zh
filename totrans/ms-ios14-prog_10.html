<html><head></head><body>
		<div><h1 id="_idParaDest-172"><em class="italic"><a id="_idTextAnchor314"/>Chapter 10</em>: Making Smarter Apps with Core ML</h1>
			<p>Over the past few years, machine learning has gained in popularity. However, it has never been easy to implement in mobile applications—that is, until Apple released the <strong class="bold">Core ML</strong> framework as part of iOS 11. Core ML is Apple's solution to all of the problems that developers at the company have run into themselves while implementing machine learning for iOS. As a result, Core ML should have the fastest, most efficient implementations for working with sophisticated machine learning models, through an interface that is as simple and flexible as possible.</p>
			<p>In this chapter, you will learn what machine learning is, how it works, and how you can use trained machine learning models in your apps. You will also learn how you can use Apple's <strong class="bold">Vision framework</strong> to analyze images, and you'll see how it integrates with Core ML for powerful image detection. Lastly, you'll learn how to use the new <strong class="bold">Create ML</strong> tool to train your models, how to deploy your models to the cloud, and how to encrypt them for security. You will learn about these topics in the following sections:</p>
			<ul>
				<li>Understanding machine learning and Core ML </li>
				<li>Combining Core ML and computer vision </li>
				<li>Training your own models with Create ML</li>
				<li>Updating models remotely with Model Deployment</li>
				<li>Encrypting Core ML models</li>
			</ul>
			<p>By the end of this chapter, you will be able to train and use your Core ML models to make the apps you build more intelligent and compelling.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor315"/>Technical requirements</h1>
			<p>The code bundle for this chapter includes three starter projects called <code>TextAnalyzer</code>, <code>ImageAnalyzer</code>, and <code>TextAnalyzerCloud</code>. It also includes a playground file named <code>Create ML.playground</code>. </p>
			<p>The code for this chapter can be found here: <a href="https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition/tree/master/Chapter%2010%20-%20Core%20ML">https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition/tree/master/Chapter%2010%20-%20Core%20ML</a>.</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor316"/><a id="_idTextAnchor317"/><a id="_idTextAnchor318"/>Understanding machine learning and Core ML</h1>
			<p>Machine learning <a id="_idIndexMarker447"/>and<a id="_idIndexMarker448"/> Core ML go hand in hand, but they're not quite the same. Machine learning is all about teaching a machine how it can recognize, analyze, or apply certain things. The result of all this teaching is a trained model that can be used by Core ML to analyze specific inputs and produce an output based on the rules that were established during the training phase.</p>
			<p>Before you learn about Core ML, it's good to obtain some knowledge about machine learning to make sure you're familiar with some of the terms that are used, and <a id="_idTextAnchor319"/>so you know what machine learning is<a id="_idTextAnchor320"/>.</p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor321"/>Understanding what machine learning is</h2>
			<p>A lot of developers will <a id="_idIndexMarker449"/>hear about machine learning, deep learning, or neural networks at some point in their careers. You may have already heard about these topics. If you have, you know that machine learning is a complex field that requires particular domain knowledge. However, machine learning is becoming more prominent and popular by the day, and it is used to improve many different types of applications.</p>
			<p>For instance, machine learning can be used to predict what type of content a particular user might like to see in a music app based on the music that they already have in their library, or to automatically tag faces in photos to connect them to people in the user's contact list. It can even be used to predict costs for specific products or services based on past data. While this might sound like magic, the flow for creating machine learning experiences like these can be split roughly into two phases:</p>
			<ol>
				<li>Training a model</li>
				<li>Using inference to obtain a result from the model</li>
			</ol>
			<p>Large amounts of high-quality data must be collected to perform the first step. If you're going to train a model that should recognize cats, you will need a large number of pictures of cats. You must also collect images that do not contain cats. Each image must then be appropriately tagged to indicate whether the image includes a cat or not<a id="_idTextAnchor322"/>. </p>
			<p>If your dataset only contains images of cats that face towards the camera, the chances are that your model will not be able to recognize cats from a sideways point of view. If your dataset does contain cats from many different sides, but you only collected images for a single breed or with a solid white background, your model might still have a tough time recognizing all cats. Obtaining quality training data is not easy, yet it's essential.</p>
			<p>During the training phase of a model, you must provide a set of inputs that are of the highest quality possible. The smallest mistake could render your entire dataset worthless. Collecting big amounts of high-quality data to train a model is a tedious task. It also takes a lot of time. Certain complex models could take a couple of hours to crunch all the data and<a id="_idIndexMarker450"/> train themselves.</p>
			<p>A trained model comes in several types. Each type of model is suitable for a different kind of task. For instance, if you are working on a model that can classify specific email messages as spam, your model<a id="_idIndexMarker451"/> might be a so-called <strong class="bold">support vector machine</strong>. If you're training a model that recognizes cats in pictures, you are likely <a id="_idIndexMarker452"/>training a <strong class="bold">neural network</strong>.</p>
			<p>Each model comes with its pros and cons, and each model is created and used differently. Understanding all these different models, their implications, and how to train them is extremely hard, and you could likely write a book on each kind of model.</p>
			<p>In part, this is why Core ML is so great. Core ML enables you to make use of pre-trained models in your own apps. On top of this, Core ML standardizes the interface that you use in your own code. This means that you can use complex models w<a id="_idTextAnchor323"/>ithout even realizing it. Let's learn more about Core ML, shal<a id="_idTextAnchor324"/>l we?</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor325"/>Understanding Core ML</h2>
			<p>Due to the complex <a id="_idIndexMarker453"/>nature of machine learning and using trained models, Apple has built Core ML to make incorporating a trained model as straightforward as possible. On top of this, another goal was to ensure that whenever you implement machine learning using Core ML, your implementation is as fast and energy-efficient as possible. Since Apple has been enhancing iOS with machine learning for a couple of years now, they have loads of experience of implementing complex models in<a id="_idTextAnchor326"/> apps.</p>
			<p>If you have ever researched machine learning, you might have come across cloud-based solutions. Typically, you send a bunch of data to a cloud-based solution, and the result is passed back as a response to your request. Core ML is very different, since the trained model lives on the device, instead of in the cloud. This means that your user's data never has to leave the device, which is very good for your user's privacy. Also, having your trained model on the device means that no internet connection is required to use Core ML, which saves both time and precious data. And since there is no potential bottleneck regarding response latency, Core ML is capable of calculating results in real time.</p>
			<p>In the previous section, you learned that there are several types of trained models. Each type of model is used slightly differently, so if you were to implement machine learning in your app manually, you would have to write different wrappers around each of the different models your app uses. Core ML makes sure that you can use each type of model without even being aware of this in your app; they all share the same programming interface. A Core ML model is domain-agnostic.</p>
			<p>To be domain-agnostic, all trained models that you use with Core ML must be in a particular format. Since <a id="_idIndexMarker454"/>machine learning already has a vibrant community with several popular formats, Apple has made sure that the most popular models can be easily converted to Apple's own <code>.mlmodel</code> format. Let's see how to obtain <a id="_idTextAnchor327"/><code>.mlmodel</code> files for you to use in your ow<a id="_idTextAnchor328"/>n apps.</p>
			<h3>Obtaining Core ML models</h3>
			<p>There are two ways <a id="_idIndexMarker455"/>to obtain a model for you to use in your apps. The simplest way is to find an existing <code>.mlmodel</code> file. You can find several ready-to-use <code>.mlmodel</code> files on Apple's machine learning website, at <a href="https://developer.apple.com/machine-learning/">https://developer.apple.com/machine-learning/</a>. This website contains several of the most popular models. At the time of writing, most of these models are focused on recognizing the dominant objects in an image, and chances are that you have different needs for y<a id="_idTextAnchor329"/>our app.</p>
			<p>If you're looking for something that hasn't already been converted by Apple, you can try to look in several places online for a pre-converted <code>.mlmodel</code> file, or you can convert an existing model you have found online. Apple has created converters for several popular machine learning formats, such as <code>.mlmodel</code> file are written in Python, and they ship as part of Xcode. If your needs do not fit the converters that Apple provides, you can extend the <strong class="bold">toolchain</strong>, since the conversion tools are open source. This means that everybody can add their own converters or tweak existing converters.</p>
			<p>Converting Core ML models using Apple's tools can usually be done with a couple of lines of Python. Writing a good conversion script does typically involve a little bit of domain knowledge in the area of machine learning, because you'll need to make sure that the converted model works just as well as the original model.</p>
			<p>Once you have obtained <a id="_idIndexMarker456"/>a Core ML model for your app, either by converting one or finding an existing one, you're ready to add it to your project and begin <a id="_idTextAnchor330"/>using it. Let's see how to do t<a id="_idTextAnchor331"/>his next.</p>
			<h3>Using a Core ML model</h3>
			<p>Applications can utilize<a id="_idIndexMarker457"/> Core ML for many different purposes. One of these purposes is text analysis. You can use a trained model to detect whether a particular piece of text has a positive or negative sentiment. To implement a feature like this, you can use a trained and converted Core ML model.</p>
			<p>The code bundle for this chapter includes a project named <code>@IBAction</code>, named <code>analyze()</code>. The project folder also contains a file called <code>SentimentPolarity.mlmodel</code>. This file is a trained Core ML model that analyzes the sentiment associated with a certain text. Drag this file into Xcode to add the Core ML model to you<a id="_idTextAnchor332"/>r project.</p>
			<p>After adding the model to your project, you can click it in the <strong class="bold">Project Navigator</strong> to see more information about the model, as illustrated in the following screenshot:</p>
			<div><div><img src="img/Figure_10.01_B14717.jpg" alt="Figure 10.1 – Model metadata&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Model metadata</p>
			<p>You can see that this model <a id="_idIndexMarker458"/>was provided by <strong class="bold">Vadym Markov</strong> under the <strong class="bold">MIT</strong> license. If you click the <strong class="bold">Predictions</strong> tab (see the preceding screenshot), you can find out which <strong class="bold">Input</strong> and <strong class="bold">Output</strong> you can expect this model to work with:</p>
			<div><div><img src="img/Figure_10.02_B14717.jpg" alt="Figure 10.2 – Input and output of the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – Input and output of the model</p>
			<p>You can see in this case; the <code>[String: Double]</code> type. This means that we <a id="_idIndexMarker459"/>should feed this model a dictionary of word counts. If you add this model to Xcode, the center section that lists the <strong class="bold">Model Class</strong> might notify you that the model is not part of any targets yet. If this is the case, fix it as you have done previously, by adding this model to your app target in the <strong class="bold">Utilities</strong> sidebar on the right side of the window.</p>
			<p>Now that your model has been implemented, it's time to take it for a spin. First, implement a method that extracts the word count from any given string. You can implement this using the <code>NLTokenizer</code> object from the new <code>NaturalLanguage</code> framework.</p>
			<p><code>NLTokenizer</code> is a text analysis class that is used to split a string into words, sentences, paragraphs, or even whole documents. In this example, the tokenizer is set up to detect individual words. Implement the word count method as follows.</p>
			<p>Add an import to the <code>ViewController.swift</code> file as follows:</p>
			<pre>import NaturalLanguage</pre>
			<p>Now add the following method to the same file:</p>
			<pre>func getWordCounts(from string: String) -&gt; [String: Double] {
  let tokenizer = NLTokenizer(unit: .word)
  tokenizer.string = string
  var wordCount = [String: Double]()
  tokenizer.enumerateTokens(in: 
    string.startIndex..&lt;string.endIndex) { range, attributes in
    let word = String(string[range])
    wordCount[word] = (wordCount[word] ?? 0) + 1
    return true
  }
  return wordCount
}</pre>
			<p>The previous code iterates<a id="_idIndexMarker460"/> over all the words that the tokenizer has recognized and stores it in a dictionary of the <code>[String: Double]</code> type. You might wonder why a <code>Double</code> type is used for the word count, rather than an <code>Int</code> type, since the word counts won't have to deal with decimals. This is true, but the <code>SentimentPolarity</code> model requires its input to be a dictionary of the <code>[String: Double]</code> type, so you must prepare the data accordingly.</p>
			<p>Now that you have the code to prepare the input data for the <code>SentimentPolarity</code> model, let's see how you can use this model to analyze the user's input. Add the following implementation for the <code>analyz<a id="_idTextAnchor333"/><a id="_idTextAnchor334"/>e()</code> method:</p>
			<pre>@IBAction func analyze() {
  let wordCount = getWordCounts(from: textView.text)
  let model = try? SentimentPolarity(configuration: .init())
  guard let prediction = try? model?.prediction(input: 
    wordCount) else { return }
  let alert = UIAlertController(title: nil, message: "Your 
    text is rated: \(prediction.classLabel)", preferredStyle: 
     .alert)
  let okayAction = UIAlertAction(title: "Okay", style: 
    .default, handler: nil)
  alert.addAction(okayAction)
  present(alert, animated: true, completion: nil)
}</pre>
			<p>You might be surprised<a id="_idIndexMarker461"/> that this method is so short, but that's how simple Core ML is! First, we retrieve the <code>wordCount</code> using the method we implemented earlier. Then, an instance of the Core ML model is created. When you added the <code>SentimentPolarity</code> model to the app target, Xcode generated a class interface that abstracted away all complexities involving the model. Because the model is now a simple class, you can obtain a prediction for the sentiment of the text by calling <code>prediction(input:)</code> on the model instance.</p>
			<p>The <code>prediction</code> method returns an object that contains the processed prediction in the <code>classLabel</code> property, as well as an overview of all available predictions and how certain the model is about each option in the <code>classProbability</code> property. You can use this property if you want to be a bit more transparent to the user about the different options that the model suggested and how certain it was about these options.</p>
			<p>Let's see a couple of examples to demonstrate how it works. First, launch the app. Now write <code>I love rainbows</code> in the text area and press <code>I am sad on cloudy days</code>. The result now is <strong class="bold">Your text is rated: Neg</strong>. This time, the sentiment of your sentence is negative! You can try out your own ideas to see how the model behaves in different scenarios.</p>
			<p>In the last section of this chapter, you will learn how you can use <code>Create ML</code> to train your own natural language model to analyze texts that use domain-specific language relevant to your own app.</p>
			<p>Using Core ML to perform<a id="_idIndexMarker462"/> text analysis was quite simple. Now let's see how you can use computer vision together with Core ML to determine the type of obje<a id="_idTextAnchor335"/>ct that exists in a partic<a id="_idTextAnchor336"/>ular picture.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor337"/>Combining Core ML and computer vision</h1>
			<p>When you're developing an<a id="_idIndexMarker463"/> app that works with photos or live camera footage, there are several things you might like to do using computer vision. For instance, it could be desirable to detect faces in an image. Or, maybe you would want to identify certain rectangular areas of photographs, such as traffic signs. You could also be looking for something more sophisticated, such as detecting the dominant object in a picture.</p>
			<p>To work with computer vision in your apps, Apple has created the <strong class="bold">Vision</strong> framework. You can combine Vision and Core ML to perform some pretty sophisticated image recognition. Before you implement a sample app that uses dominant object recognition, let's take a quick look at the Vision fr<a id="_idTextAnchor338"/>amework, so you have an idea of what it's capable of and when you might l<a id="_idTextAnchor339"/>ike to use it.</p>
			<h2 id="_idParaDest-178"><a id="_idTextAnchor340"/>Understanding the Vision framework</h2>
			<p>The Vision framework<a id="_idIndexMarker464"/> is capable of many <a id="_idIndexMarker465"/>different tasks that revolve around computer vision. It is built upon several powerful deep learning techniques that enable state-of-the-art facial recognition, text recognition, barcode detection, and more.</p>
			<p>When you use Vision for facial recognition, you get much more information than just the location of a face in an image. The framework can recognize several facial landmarks, such as eyes, noses, or mouths. All of this is possible due to the extensive use of deep learning behind the scenes at Apple.</p>
			<p>For most tasks, using Vision consists of the following three stages:</p>
			<ol>
				<li value="1">You create a request that specifies what you want; for instance, a <code>VNDetectFaceLandmarksRequest</code> request to detect facial features.</li>
				<li>You set up a handler that can analyze the images.</li>
				<li>The resulting observation contains the information you need.</li>
			</ol>
			<p>The following code sample<a id="_idIndexMarker466"/> illustrates <a id="_idIndexMarker467"/>how you might find facial land<a id="_idTextAnchor341"/>marks in an image:</p>
			<pre>let handler = VNImageRequestHandler(cgImage: image, options: [:])
let request = VNDetectFaceLandmarksRequest(completionHandler: { request, error in
  guard let results = request.results as? [VNFaceObservation]
  else { return }
  for result in results where result.landmarks != nil {
    let landmarks = result.landmarks!
    if let faceContour = landmarks.faceContour {
      print(faceContour.normalizedPoints)
    }
    if let leftEye = landmarks.leftEye {
      print(leftEye.normalizedPoints)
    }
    // etc
  }}
)
try? handler.perform([request])</pre>
			<p>For something as complex as detecting the contour of a face or the exact location of an eye, the code is quite simple. You set up a <code>handler</code> and a <code>request</code>. Next, the <code>handler</code> is asked to <code>perform</code> one or more requests. This means that you can run several requests on a single image.</p>
			<p>In addition to enabling computer vision tasks like this, the Vision framework also tightly integrates with <a id="_idIndexMarker468"/>Core ML. Let's see just how<a id="_idIndexMarker469"/> tight this inte<a id="_idTextAnchor342"/>gration is, by adding an image classifier to the augmented-reality gallery app you h<a id="_idTextAnchor343"/>ave been working on!</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor344"/>Implementing an image classifier</h2>
			<p>The code bundle for this <a id="_idIndexMarker470"/>section contains an<a id="_idIndexMarker471"/> app called <strong class="bold">ImageAnalyzer</strong>. This app uses an image picker to allow a user to select an image from their photo library to use it as an input for the image classifier you will implement. Open the project and explore it for a little bit to see what it does and how it works. Use the starter project if you want to follow along with the rest of this section.</p>
			<p>To add an image classifier, you need to have a Core ML model that can classify images. On Apple's machine learning website (<a href="https://developer.apple.com/machine-learning/build-run-models/">https://developer.apple.com/machine-learning/build-run-models/</a>), there are several models available that can do image classification. An excellent lightweight model you can use is the <strong class="bold">MobileNetV2 </strong>model; go ahead and download it from the machine learning page. Once you have downloaded the model, drag the model into Xcode to add it to the <strong class="bold">ImageAnalyzer </strong>project. Make sure to add it to your app target so that Xcode can generate the class interface for the model.</p>
			<p>After adding the model to Xcode, you can open it to examine the <strong class="bold">Model Predictions </strong>tab. The parameters tell you the different types of inputs and outputs the model will expect and provide. In the case of <strong class="bold">MobileNetV2</strong>, the input should be an image that is <strong class="bold">224</strong> points wide and <strong class="bold">224</strong> points high, as shown in the following screenshot:</p>
			<div><div><img src="img/Figure_10.03_B14717.jpg" alt="Figure 10.3 – Input and output of the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Input an<a id="_idTextAnchor345"/>d output of the model</p>
			<p>After generating the <a id="_idIndexMarker472"/>model, the code to use the model is very similar to the code used to detect facial features with Vision earlier. The most significant difference is that the type of request that is used is a special <code>VNCore MLRequest</code>. This type of request takes the Core ML model you want to use, in addition to a completion handler.</p>
			<p>When combining Core ML and Vision, Vision will take care of image scaling and converting the image to a type that is compatible with the Core ML model. You should make sure that the input image has the correct orientation. If your image is rotated in an unexpected orientation, Core ML might not be able to analyze it correctly.</p>
			<p>First, let's import the <code>Vision</code> framework. Add this statement at the top of the <code>ViewController</code> class in the <strong class="bold">ImageAnalyzer </strong>project:</p>
			<pre>import Vision</pre>
			<p>Now, add the following implementation for <code>analyzeImage(_:)</code> to the <code>ViewController</code> class:</p>
			<pre>func analyzeImage(_ image: UIImage) {
  guard
    let cgImage = image.cgImage,
    let classifier = try? VNCore MLModel(for: 
    MobileNetV2().model)
  else { return }
  let request = VNCore MLRequest(model: classifier,
  completionHandler: { [weak self] request, error in
    guard
      let classifications = request.results as? 
      [VNClassificationObservation],
      let prediction = classifications.first
    else { return }
    DispatchQueue.main.async {
      self?.objectDescription.text = "\(prediction.identifier)
      (\(round(prediction.confidence * 100))% confidence)"
    }
  })
  let handler = VNImageRequestHandler(cgImage: cgImage,
    options: [:])
  try? handle<a id="_idTextAnchor346"/>r.perform([request])
}</pre>
			<p>The previous method takes a <code>UIImage</code> and converts it to a <code>CGImage</code>. Also, a <code>VNCore MLModel</code> is created, based on the <code>MobileNetV2</code> model. This particular model class wraps the Core ML model, so it works seamlessly with <code>Vision</code>. The request is very similar to the request you have seen before. In the <code>completionHandler</code>, the results array and first prediction of the image classifications are extracted and shown to the user. Every prediction made by the classifier will have a label that is stored in the identifier and a confidence rating with a value between <code>0</code> and <code>1</code> stored in the <code>confidence</code> property. Note that the value of the description label is set on the main thread to avoid crashes.</p>
			<p>You have already implemented two different types of Core ML models that were trained for general purposes. Sometimes, these models won't be specific enough for your purposes. For instance, take a look at the following screenshot, where a machine learning model labels a<a id="_idIndexMarker473"/> certain landscape with only 32% confidence:</p>
			<div><div><img src="img/Figure_10.04_B14717.jpg" alt="Figure 10.4 – Photo analysis result&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Photo analysis result</p>
			<p>In the next section, you will learn how to train models for purposes that are specific to you and your<a id="_idIndexMarker474"/> a<a id="_idTextAnchor347"/><a id="_idTextAnchor348"/><a id="_idTextAnchor349"/>pps by using Create ML. </p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor350"/>Training your own models with Create ML</h1>
			<p>As part of Xcode 10 and Apple's version of macOS, <strong class="bold">Mojave</strong>, they have shipped a tool that you can use<a id="_idIndexMarker475"/> to train your own machine learning models by <a id="_idIndexMarker476"/>adding specializations to existing models. This means that you can train your own natural language model that places certain texts in categories that you define. Or, you can train a model that recognizes certain product names or terms in a text that are specific to your application's domain.</p>
			<p>If you're building a news app, you might want to train a Core ML model that can automatically categorize the articles in the app. You can then use this model to keep track of the articles your users read, and present articles that are most likely to fit their interests on a dedicated page in your app.</p>
			<p>In this segment, you will learn how to train natural language models and how you can train an image recognition model based on the Vision framework. In doing so, you will find that creating a large and optimized training set is crucial when you want to train a machine learning model.</p>
			<p>In the code bundle for this chapter, you will find a Playgr<a id="_idTextAnchor351"/>ound called <strong class="bold">Create ML</strong>. This playground contains all the resources used for training<a id="_idTextAnchor352"/> natural language models.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor353"/>Training a Natural Language model</h2>
			<p>The Natural Language<a id="_idIndexMarker477"/> framework has<a id="_idIndexMarker478"/> excellent features to analyze text with. Bundled with the power of machine learning models, you can perform some powerful operations on text. Apple has spent a lot of time training several models with vast amounts of data to ensure that the Natural Language framework can detect names, places, and more.</p>
			<p>However, sometimes you might want to add your own analysis tools. To facilitate this, the Natural Language framework works well with Core ML and Apple's new <strong class="bold">Create ML</strong> framework. With <strong class="bold">Create ML</strong>, you can easily and quickly create your own machine learning models that you can use in your apps straight away.</p>
			<p>You can use several different types of training for a Natural Language model. In this section, you will learn about two different models:</p>
			<ul>
				<li>A tex<a id="_idTextAnchor354"/>t classifier </li>
				<li>A word tagger</li>
			</ul>
			<p>The <strong class="bold">text classifier</strong> will classify a particular piece of text with a label. This is similar to the sentiment analysis you have implemented in the <strong class="bold">TextAnalyzer</strong> sample app. An example of an entry in your training data would look as follows:</p>
			<pre>{
  "text": "We took an exclusive ride in a flying car", 
  "label": "Tech"
}</pre>
			<p>This is a sample of a <a id="_idIndexMarker479"/>news article headline that<a id="_idIndexMarker480"/> belongs in a category labeled <code>Tech</code>. When you feed a large number of samples like this to your model, you could end up with a classifier that can apply labels to news articles based on their headlines. Of course, this assumes that the headlines are specific enough and contain enough information to train the classifier properly. In reality, you will find that short sentences like these will not make the best models. The sample Playground contains a JSON file with training data that attempts to separate news articles into the two categories of politics and tech. Let's see how the model can be trained so you can then see for yourself how accurate the model is.</p>
			<p>The following code trains and stores the custom Core ML model. In the playground file, open the <code>Labeller</code>. Check the code:</p>
			<pre>import Create ML
import Foundation
let trainingData = try! MLDataTable(contentsOf: Bundle.main.url(forResource: "texts", withExtension: "json")!)
let model = try! MLTextClassifier(trainingData: trainingData, textColumn: "text", labelColumn: "label")
try! model.write(to: URL(fileURLWithPath: "/Users/marioeguiluz/Desktop/TextClassifier.mlmodel"))
let techHeadline = try! model.prediction(from: "Snap users drop for first time, but revenue climbs")
let politicsHeadline = try! model.prediction(from: "President Donald Trump is approving a new law")</pre>
			<p>Training the entire model requires only a couple of lines of code. All you need to do is obtain your training<a id="_idIndexMarker481"/> data, create the <a id="_idIndexMarker482"/>classifier, and save it somewhere on your machine. You can even do some quick testing to see whether your model works well, from right inside the playground.</p>
			<p>Note that the preceding code uses a <code>try!</code> statement. This is done to keep the code sample brief and simple. In your own apps, you should always strive for proper error handling to avoid surprising crashes.</p>
			<p>The string passed to the <code>URL(fileURLWithPath:)</code> initializer represents the location where your model will be stored. Make sure to specify the full path here, so, for instance, use <code>/Users/yourUser/Desktop/TextClassifier.mlmodel</code>, and not <code>~/Desktop/TextClassifier.mlmodel</code>. Make sure to substitute <code>yourUser</code> with your own username or folder. </p>
			<p>The following lines of code test two different headlines to see if the model correctly labels them:</p>
			<pre>let techHeadline = try! model.prediction(from: "Snap users drop for first time, but revenue climbs")
let politicsHeadline = try! model.prediction(from: "President Donald Trump is approving a new law")</pre>
			<p>If you're happy with the results of your model, you can grab the trained model from the place where you saved it, and immediately add it to your Xcode project. From there, you can use the model like you would use any other model.</p>
			<p>Let's see another example of a model from the Natural Language framework. In this case, the model should label every word in a text to classify it as a certain type of word. For instance, you could train the model to recognize certain brand names, product names, or other words that have special meanings to your app. An example of some training data that you could use to train a model like this is the following:</p>
			<pre>{
  "tokens": ["Apple", "announced", "iOS 12", "and", "Xcode 
    10", "at", "WWDC 2018"],
  "labels": ["COMPANY", "NONE", "OPERATING_SYSTEM", "NONE",
    "SOFTWARE", "NONE", "EVENT"]
}</pre>
			<p>By collecting many<a id="_idIndexMarker483"/> samples that include the words <a id="_idIndexMarker484"/>that you want to label, your model will be able to not only match tags based on the word itself, but even on the surrounding words. Essentially, the model would be aware of the context in which each word is used to then determine the correct tag. Once you have collected enough sample data, you can train the model in a similar way as the classifier:</p>
			<pre>let labelTrainingData = try! MLDataTable(contentsOf: Bundle.main.url(forResource: "labels", withExtension: "json")!)
let model = try! MLWordTagger(trainingData: labelTrainingData, tokenColumn: "tokens", labelColumn: "labels")
try! model.write(to: URL(fileURLWithPath: "/Users/marioeguiluz/Desktop/TextTagger.mlmodel"))</pre>
			<p>The amount of code to train the model hasn't changed. The only difference is that the previous model was based on the <code>MLTextClassifier</code> class, and the current model is based on <code>MLWordTagger</code>. Again, you can immediately use the trained model to make some predictions that you can then use to validate whether the model was trained properly. Providing good data and testing often are the keys <a id="_idTextAnchor355"/>to building a great Core ML model.</p>
			<p>In addition to text <a id="_idIndexMarker485"/>analysis models, Create ML <a id="_idTextAnchor356"/><a id="_idIndexMarker486"/>can also help you to train your own image recognition mod<a id="_idTextAnchor357"/>els. Let's see how this works next.</p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor358"/>Training a Vision model</h2>
			<p>In the <strong class="bold">ImageAnalyzer</strong> sample<a id="_idIndexMarker487"/> app, you saw that picking<a id="_idIndexMarker488"/> an image of a certain car would be classified as a sports car with a pretty low confidence score. You can train your own vision model that specializes in recognizing certain cars.</p>
			<p>Collecting good training data for image classifiers is tough, because you have to make sure that you gather many pictures of your subjects from all sides and in many different environments. For instance, if all your car images feature cars that are next to trees, or on the road, the model might end up classifying anything with trees or a road next to it as a car. The only way to obtain a perfect training set is to experiment, tweak, and test.</p>
			<p>Training a Vision model works slightly differently from training a Natural Language model. You can't use a JSON file to feed your test data to the classifier. So, instead, you should create folders that contain your images where the folder name is the label you want to apply to each image inside that folder. The following screenshot is an example of a training set that contains two kinds of labels:</p>
			<div><div><img src="img/Figure_10.05_B14717.jpg" alt="Figure 10.5 – Training set of images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Training set of images</p>
			<p>Once you have collected your set of training data, you can store it anywhere on your computer—for instance, on the desktop. You will then pass the path for your training data to your model training code as follows:</p>
			<pre>import Create ML 
import Foundation
let dataUrl = URL(fileURLWithPath: "/path/to/trainingdata")
let source = MLImageClassifier.DataSource.labeledDirectories(at: dataUrl)
let classifier = try! MLImageClassifier(trainingData: source) try! classifier.write(toFile: "/Users/marioegu<a id="_idTextAnchor359"/>iluz/Desktop/CarClassifier.mlmodel")</pre>
			<p>Again, you only need a couple of lines of code to train a model. That's how powerful Create ML is. If you want, you can quickly test your image classifier by dropping the <code>.mlmodel</code> file in the <code>MobileNetV2</code> classifier that you used before.</p>
			<p>Apart from the simple<a id="_idIndexMarker489"/> ways of training models, there are<a id="_idIndexMarker490"/> certain parameters that you can pass to the different Create ML classifiers. If you have trouble training your models properly, you could tweak some of the parameters that are used by Create ML. For instance, you could apply more iterations to your training set, so the model gains a deeper understanding of the training data.</p>
			<p>As mentioned before in this chapter, machine learning is a subject that could span several books on its own, and even though Create ML makes training models straightforward and simple, it's not easy to train a robust model without any prior machine learning experience.</p>
			<p>Now that you have learned how to use your own trained data, in the next section, we are going to learn how to update your models from the cloud, without the need to update the app itself.</p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor360"/>Updating models remotely with Model Deployment</h1>
			<p>One of the new<a id="_idIndexMarker491"/> features of iOS 14 for machine<a id="_idIndexMarker492"/> learning is the ability to keep collections of your models in the cloud, giving you the power to update them at any time without the need to update the app itself.</p>
			<p>We are going to use a project, available in the code bundle of this book, in order to demonstrate this new feature. The project's<a id="_idIndexMarker493"/> name is <strong class="bold">TextAnalyzerCloud</strong>. It is the same project that we used before, but this time, the model will be on the cloud (with a local copy as a fallback).</p>
			<p>There are two steps involved in order to use Model Deployment in our apps:</p>
			<ol>
				<li value="1">Use the Core ML API to retrieve collections of models.</li>
				<li>Prepare and deploy the model.</li>
			</ol>
			<p>Let's implement these steps in the next subsections.</p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor361"/>Using the Core ML API to retrieve collections of models</h2>
			<p>Let's start by learning <a id="_idIndexMarker494"/>how to retrieve models that<a id="_idIndexMarker495"/> are stored in the cloud into your app. Open the <code>ViewController</code> class. At this point, the class just contains an <code>analyze</code> method that counts the words inside a <code>textView</code> and makes a prediction if a model exists. The class also contains some methods to display error and success messages to the user. Note that we have also defined the following property: <code>var model: SentimentPolarity?</code>.<code> </code></p>
			<p>In the <code>analyze</code> method, we are going to download a model from the cloud, and in case of failure, we will use a local modal as a fallback. Let's modify the method to achieve this. Update the implementation of the <code>analyze</code> method, and add the following code where it says <code>//add code</code>:</p>
			<pre>//1
_ = MLModelCollection.beginAccessing(identifier: "SentimentPolarityCollection") { [self] result in
  //2
  var modelURL: URL?
  switch result {
  case .success(let collection):
    modelURL = 
    collection.entries["SentimentPolarity"]?.modelURL
  case .failure(let error):
    handleCollectionError(error)
  }
  //3
  let result = loadSentimentClassifier(url: modelURL)
  //4
  switch result {
  case .success(let sentimentModel):
    model = sentimentModel
    guard let prediction = try? model?.prediction(input: 
      wordCount) else { return }
    showResult(prediction: prediction)
  case .failure(let error):
    handleModelLoadError(error)
  }
}</pre>
			<p>Let's review the preceding code blocks (the following numbers refer to the comments in the preceding code):</p>
			<ul>
				<li>First, in <code>//1</code>, we are accessing the new Core ML API to retrieve a collection of models from our account on the Apple servers. We do that by using the <code>MLModelCollection.beginAccessing</code> method with an identifier for the collection (that has to match the one in the cloud) – in our case, we used <code>SentimentPolarityCollection</code>.</li>
				<li>Next, in <code>//2</code>, we are checking the result of <code>beginAccessing</code>. If it is successful and we get a collection of models, we search for a specific model with an identifier of <code>SentimentPolarity</code> and we extract the <code>modelURL</code> from it. If we get any<a id="_idIndexMarker496"/> errors (such as there<a id="_idIndexMarker497"/> being no network connection), we call the <code>handleCollectionError</code> method to handle it properly (in our case, we inform the user with a modal).</li>
				<li>Now that we have a model URL, in <code>//3</code>, we try to load it. We haven't implemented the <code>loadSentimentClassifier</code> method yet, but we will do it shortly. Just take into account that this method will try to load a model with a given remote URL, and it will wrap it in a <code>Result&lt;SentimentPolarity, Error&gt;</code> enum (to handle errors properly).</li>
				<li>In the last part, under comment <code>//4</code>, we inspect the <code>Result</code> from <code>//3</code>. If we obtained a model, we store it in the <code>model</code> property variable. We store it so we don't need to download the model over and over again. After storing the model, we use it to analyze the text. On the other hand, if we obtained an error, we display a message to the user to inform them about it. </li>
			</ul>
			<p>Now let's add the <code>loadSentimentClassifier</code> method so the class compiles. Add the following method to the <code>ViewController</code>:</p>
			<pre>private func loadSentimentClassifier(url: URL?) -&gt; Result&lt;SentimentPolarity, Error&gt; {
  if let modelUrl = url {
    return Result { try SentimentPolarity(contentsOf: 
    modelUrl)}
  } else {
    return Result { try SentimentPolarity(configuration: 
    .init())}
  }
}</pre>
			<p>This method receives an optional model URL as a param; that is, the URL of our model stored in Apple Servers. It is an optional value because when we try to fetch it, it can fail (for example, if the <a id="_idIndexMarker498"/>user doesn't have an internet <a id="_idIndexMarker499"/>connection). Inside the method, we handle two possibilities:</p>
			<ul>
				<li>If the URL is not nil, we use it to initialize the model with <code>SentimentPolarity(contentsOf:)</code> and return it inside a <code>Result</code>.</li>
				<li>If the URL is nil, we try to initialize the model with a local version and the default configuration with <code>SentimentPolarity(configuration: .init())</code>. Again, we return it inside <code>Result</code>.</li>
			</ul>
			<p>With this method implemented, we have all the code necessary to load a model from the network and use it in our app. However, we still need to perform two important steps to complete the process: Prepare the model to be uploaded to the Apple servers in the proper format, and deploy the model to the cloud.</p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor362"/>Preparing and deploying the model</h2>
			<p>In the previous <a id="_idIndexMarker500"/>section, we created the methods to retrieve a model from the Apple servers<a id="_idIndexMarker501"/> and into our app. Now, we are going to prepare our local model to be deployed into the cloud. </p>
			<p>In the project explorer, click on the file named <code>SentimentPolarity.mlmodel</code>. Now, go to the <strong class="bold">Utilities</strong> tab. You will see the following:</p>
			<div><div><img src="img/Figure_10.06_B14717.jpg" alt="Figure 10.6 – Model Utilities tab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Model Utilities tab</p>
			<p>Click on <strong class="bold">Create Model Archive</strong>. This new option in iOS 14 will help us to deploy our model onto the Apple servers in the cloud. When you click it, this popup will appear:</p>
			<div><div><img src="img/Figure_10.07_B14717.jpg" alt="Figure 10.7 – Generate Model Archive&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Generate Model Archive</p>
			<p>For now, leave <a id="_idIndexMarker502"/>the <strong class="bold">Encrypt Model</strong> checkbox unchecked and click <strong class="bold">Continue</strong> (we <a id="_idIndexMarker503"/>will explore this option later in the chapter). After clicking <strong class="bold">Continue</strong>, Xcode will generate an archive of the model and will display this modal:</p>
			<div><div><img src="img/Figure_10.08_B14717.jpg" alt="Figure 10.8 – The Model Archive Generated dialog&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – The Model Archive Generated dialog</p>
			<p>You can click on the blue arrow to the right of the first option in the preceding screenshot and it will take you to the exact location where the archive of your model is located. You will need to remember this location to upload the archive to the Apple servers. You will see a file with the<code>.mlarchive</code> extension, as in the following screenshot:</p>
			<div><div><img src="img/Figure_10.09_B14717.jpg" alt="Figure 10.9 – Location of the archived model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Location of the archived model</p>
			<p>Now click on the blue<a id="_idIndexMarker504"/> arrow<a id="_idIndexMarker505"/> next to the second option that reads <strong class="bold">You can upload the Model Archive on the Core ML Model Deployment dashboard</strong>. It will open your web browser at the following page:</p>
			<div><div><img src="img/Figure_10.10_B14717.jpg" alt="Figure 10.10 – Core ML Model Collections page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Core ML Model Collections page</p>
			<p>This is your dashboard for managing your model collections on the Apple servers. What we need to do now is to create a new collection with a reference to our model inside, and upload the model archive we just created. Let's do this; click on the blue plus (<strong class="bold">+</strong>) icon next to <strong class="bold">Model Collections</strong>, and fill in the form that appears with the following information:</p>
			<div><div><img src="img/Figure_10.11_B14717.jpg" alt="Figure 10.11 – Create a Model Collection&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.11 – Create a Model Collection</p>
			<p>Let's review the input fields:</p>
			<ul>
				<li><code>//1</code>: <code>MLModelCollection.beginAccessing(identifier: "SentimentPolarityCollection")</code>), we used the identifier <code>SentimentPolarityCollection</code>. Use the same one here (otherwise, you will not be able to download the collection).</li>
				<li><strong class="bold">Description</strong>: Use this field to create a description that will help you to recognize this collection later on. Take into account that if you work in a team, it will need to be useful to the other developers too.</li>
				<li><code>SentimentPolarity</code> (under comment <code>//2</code>: <code>modelURL = collection.entries["SentimentPolarity"]</code>). Again, these identifiers have to match each other. You have the possibility to add more model identifiers by pressing the <strong class="bold">Model ID</strong> blue button, but in our case, we have just one model inside our collection.</li>
			</ul>
			<p>Finally, you can click the <a id="_idIndexMarker508"/>blue <strong class="bold">Create</strong> button, and you will land on the following model <a id="_idIndexMarker509"/>collection page:</p>
			<div><div><img src="img/Figure_10.12_B14717.jpg" alt="Figure 10.12 – Model collection page&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Model collection page</p>
			<p>From this page, you can finally deploy or archive the model into its reference on the cloud. Click on the blue plus (<strong class="bold">+</strong>) button next to <strong class="bold">Deployments</strong>, and fill in the fields as shown here:</p>
			<div><div><img src="img/Figure_10.13_B14717.jpg" alt="Figure 10.13 – Model deployment properties&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.13 – Model deployment properties</p>
			<p>Let's review the fields:</p>
			<ul>
				<li><strong class="bold">Deployment ID</strong>: You can specify any text here that describes why you are deploying this model. It is just a descriptive field; it doesn't need to match anything.</li>
				<li><code>.mlarchive</code> file we created before in Xcode when archiving the model.</li>
			</ul>
			<p>Notice in the bottom part of the form that we can add <strong class="bold">Additional Targeting Rules</strong>. This is another new feature of iOS 14 that allows us to target our models based on device characteristics. For example, we can download certain models only to iPads, or for specific OS versions. To keep this example simple, we are not going to add any rules, but you should try it out in your apps!</p>
			<p>After you upload the <code>.mlarchive</code> file, it should display as follows:</p>
			<div><div><img src="img/Figure_10.14_B14717.jpg" alt="Figure 10.14 – Our first model deployed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Our first model deployed</p>
			<p>When the status is <code>analyze</code> method will give you a verdict.</p>
			<p>In this section, you have<a id="_idIndexMarker512"/> learned how to consume the Core ML API to fetch models from <a id="_idIndexMarker513"/>the cloud to keep your app models up to date. You also learned how to prepare your models and how to deploy them to the Apple servers. Now you are going to learn how to encrypt those models with a new iOS 14 feature to keep your model's data safe on users' devices.</p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor363"/>Encrypting Core ML models</h1>
			<p>One of the new <a id="_idIndexMarker514"/>features of iOS 14 Core ML is the ability to encrypt your machine learning models on users' devices. Xcode 12 has a new tool that will help you to create a private key that you will deploy to the Apple servers. Your app will download that key and store it securely on the users' devices, and will use the key to decrypt the local (encrypted) model, load that decrypted version into memory (so it is not stored insecurely), and have it ready for use in your app.</p>
			<p>The steps to create the key and deploy it to the Apple servers are very straightforward. First, you select your model in the project explorer; in our case, open the <code>SentimentPolarity.mlmodel</code> file. Then, click on the <strong class="bold">Utilities</strong> tab:</p>
			<div><div><img src="img/Figure_10.15_B14717.jpg" alt="Figure 10.15 – Model encryption&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.15 – Model encryption</p>
			<p>Now, click on <strong class="bold">Create Encryption Key</strong>. In the popup that appears, select the proper development <a id="_idIndexMarker515"/>account for your app:</p>
			<div><div><img src="img/Figure_10.16_B14717.jpg" alt="Figure 10.16 – Selecting the development team for the encryption key&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.16 – Selecting the development team for the encryption key</p>
			<p>This will generate a key and <code>.mlmodelkey</code> in your folder:</p>
			<div><div><img src="img/Figure_10.17_B14717.jpg" alt="Figure 10.17 – Generating an encryption key&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.17 – Generating an encryption key</p>
			<p>Clicking the blue arrow will take you to the specific folder where this key is stored. You will need to remember the location if you want to deploy this key to the Apple servers later so your team can use it too. Click <strong class="bold">OK</strong> and close the popup.</p>
			<p>Now if you click on <strong class="bold">Create Model Archive</strong>, you will notice that the <strong class="bold">Encrypt Model</strong> checkbox is active this time:</p>
			<div><div><img src="img/Figure_10.18_B14717.jpg" alt="Figure 10.18 – Generating a model archive with encryption&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.18 – Generating a model archive with encryption</p>
			<p>When you click <strong class="bold">Continue</strong>, Xcode<a id="_idIndexMarker516"/> creates an encrypted archive this time. The steps that follow are exactly the same as the steps we learned in the <em class="italic">Prepare and Deploy the model</em> section. </p>
			<p>However, you can also tell Xcode to encrypt the bundled model (the local copy). To do this, after generating the encryption key (as we just did), you need to click on your project, go to <strong class="bold">Build Phases</strong>, and open the <strong class="bold">Compile Sources</strong> section:</p>
			<div><div><img src="img/Figure_10.19_B14717.jpg" alt="Figure 10.19 – The Build Phases tab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.19 – The Build Phases tab</p>
			<p>Now select the <strong class="bold">SentimentPolarity.mlmodel</strong> model and on the right side of its row, you can double-click to add a flag. Add the route to the encryption key in your project folder:<a id="_idTextAnchor364"/></p>
			<pre>--encrypt "$SRCROOT/SentimentPolarity.mlmodelkey"</pre>
			<p>It should look like this after you have added the flag:</p>
			<div><div><img src="img/Figure_10.20_B14717.jpg" alt="Figure 10.20 – Model with the encryption flag&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.20 – Model with the encryption flag</p>
			<p>Now if you build the app, Xcode will generate an encrypted version of the model inside your app.</p>
			<p>You have learned how to encrypt your model locally (and how to encrypt an archive for the Apple servers). Let's see now how you can load that model at runtime. There is a new class method in ML<a id="_idIndexMarker517"/> Models named <code>load</code> that will decrypt the model for you, downloading the encryption key from the Apple servers. Check out the following example code:</p>
			<pre>SentimentPolarity.load { [self] result in
  switch result {
  case .success(let model):
    self.model = model
    guard let prediction = try? self.model?.prediction(input:
    wordCount) else { return }
    showResult(prediction: prediction)
  case .failure(let error):
    handleDecryptError(error)
  }
}</pre>
			<p>In the preceding code, the <code>class func load</code> will try to download the encryption key from the Apple servers and will decrypt the model with it, storing it in memory. We assign that decrypted model to our variable model, and it is ready to use. We also handle the failure case, displaying an error.</p>
			<p>In this section, you learned how to generate an encryption key, how to encrypt an archived model to upload to the Apple servers and also to encrypt the local copy of it, and finally how to load<a id="_idIndexMarker518"/> and decrypt the model for the app to use.</p>
			<h1 id="_idParaDest-187"><a id="_idTextAnchor365"/>Summary</h1>
			<p>In this chapter, you have seen how you can make use of the machine learning capabilities that iOS provides. You saw that adding a machine learning model to your app is extremely simple since you only have to drag it to Xcode and add it to your target app. You also learned how you can obtain models, and where to look to convert existing models to Core ML models. Creating a machine learning model is not simple, so it's great that Apple has made it so simple to implement machine learning by embedding trained models in your apps.</p>
			<p>In addition to Core ML, you also learned about the Vision and Natural Language frameworks. Vision combines the power of Core ML and smart image analysis to create a compelling framework that can perform a massive amount of work on images. Convenient requests, such as facial landmark detection, text analysis, and more are available out of the box without adding any machine learning models to your app. If you do find that you need more power in the form of custom models, you now know how to use Create ML to train, export, and use your own custom trained Core ML models. You learned that Create ML makes training models simple, but you also learned that the quality of your model is drastically impacted by the quality of your training data.</p>
			<p>Finally, you learned how to deploy your Core ML models in the cloud in order to update them without the need to update the app, and how to encrypt and decrypt them to store your models safely on the user device.</p>
			<p>In the next chapter, you will learn how you can capture, manipulate, and use media files in your apps, including audio, photo, and video elements.</p>
		</div>
	</body></html>