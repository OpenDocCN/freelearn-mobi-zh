["```swift\n@IBOutlet weak var imageView: UIImageView!\n@IBOutlet weak var labelView: UILabel!\n@IBAction func onSelectPhoto(_ sender: Any)\n```", "```swift\n@IBAction func onSelectPhoto(_ sender: Any) {\n let picker = UIImagePickerController()\n picker.delegate = self\n picker.allowsEditing = false\n picker.sourceType = \n      UIImagePickerController.isSourceTypeAvailable(.camera) ? \n        .camera : .photoLibrary\n\n present(picker, animated: true) \n}\n```", "```swift\nextension ViewController: UIImagePickerControllerDelegate,\n  UINavigationControllerDelegate\n```", "```swift\nfunc imagePickerControllerDidCancel(_ picker: \n   UIImagePickerController) {\n  dismiss(animated: true, completion: nil)\n}\n\nfunc imagePickerController(_ picker: UIImagePickerController, \n  didFinishPickingMediaWithInfo info: \n    [UIImagePickerController.InfoKey : Any]) {\n\n    guard let image = info[UIImagePickerController.InfoKey\n      .originalImage] as? UIImage else {\n        return\n    }\n\n    imageView.image = image\n\n    labelView.text = \"This is my image!\"\n\n    dismiss(animated: true, completion: nil)\n}\n```", "```swift\nNSCameraUsageDescription\nNSPhotoLibraryUsageDescription\n```", "```swift\nlet picker = UIImagePickerController() // 1\npicker.delegate = self // 2\npicker.allowsEditing = false // 3\npicker.sourceType = UIImagePickerController.isSourceTypeAvailable\n  (.camera) ? .camera : .photoLibrary // 4\n\npresent(picker, animated: true) // 5\n```", "```swift\nfunc imagePickerControllerDidCancel(_ picker: UIImagePickerController)\nfunc imagePickerController(_ picker: UIImagePickerController, \n  didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey \n    : Any])\n```", "```swift\nvar model: Resnet50!\n\noverride func viewDidLoad() {\n    super.viewDidLoad()\n    model = Resnet50()\n}\n```", "```swift\nguard let image = info[UIImagePickerController.InfoKey.\n  originalImage] as? UIImage else {\n    return\n}\n\nlet (newImage, pixelBuffer) =   \n  ImageHelper.processImageData(capturedImage: image)\n\nimageView.image = newImage\n\nvar imagePredictionText = \"no idea... lol\"\n\n guard let prediction = try? model.prediction(\n      image: pixelBuffer!) else {\n labelView.text = imagePredictionText\n dismiss(animated: true, completion: nil)\n return\n }\n\n imagePredictionText = prediction.classLabel\n\nlabelView.text = \"I think this is a \\(imagePredictionText)\"\n\ndismiss(animated: true, completion: nil)    \n```", "```swift\nlet (newImage, pixelBuffer) = \n  ImageHelper.processImageData(capturedImage: image)\n```", "```swift\nstatic func processImageData(capturedImage: UIImage) -> (UIImage?, \n  CVPixelBuffer?)\nstatic func exifOrientationFromDeviceOrientation() -> \n  CGImagePropertyOrientation\n```", "```swift\nguard let prediction = try? model.prediction(image: pixelBuffer!) else {\n    labelView.text = imagePredictionText\n    dismiss(animated: true, completion: nil)\n    return\n}\n\nimagePredictionText = prediction.classLabel\n```", "```swift\n.classLabel\n.classLabelProbs\n```", "```swift\nimport AVFoundation\n\nprivate var previewLayer: AVCaptureVideoPreviewLayer! = nil\nvar captureSession = AVCaptureSession()\n\nvar bufferSize: CGSize = .zero\nvar rootLayer: CALayer! = nil\n\nprivate let videoDataOutput = AVCaptureVideoDataOutput()\nprivate let videoDataOutputQueue = DispatchQueue(label:\n  \"video.data.output.queue\", qos: .userInitiated, attributes: [], \n    autoreleaseFrequency: .workItem)\n```", "```swift\nfunc setupCaptureSession() {\n\n var deviceInput: AVCaptureDeviceInput!\n\n guard let videoDevice =\n            AVCaptureDevice.DiscoverySession(deviceTypes: \n              [.builtInWideAngleCamera], mediaType: .video,\n                 position: .back).devices.first else {\n        return\n    }\n\n    do {\n        deviceInput = try AVCaptureDeviceInput(device: videoDevice)\n    } catch {\n        print(error.localizedDescription)\n        return\n    }    // More to go here\n\n}\n```", "```swift\ncaptureSession.beginConfiguration()\ncaptureSession.sessionPreset = .medium\n\nguard captureSession.canAddInput(deviceInput) else {\n    captureSession.commitConfiguration()\n    return\n}\ncaptureSession.addInput(deviceInput)\n\nif captureSession.canAddOutput(videoDataOutput) {\n    captureSession.addOutput(videoDataOutput)\n    videoDataOutput.setSampleBufferDelegate(self, queue: \n      videoDataOutputQueue)\n} else {\n    captureSession.commitConfiguration()\n    return\n}\n\ndo {\n    try videoDevice.lockForConfiguration()\n    let dimensions = CMVideoFormatDescriptionGetDimensions(\n      (videoDevice.activeFormat.formatDescription))\n    bufferSize.width = CGFloat(dimensions.width)\n    bufferSize.height = CGFloat(dimensions.height)\n    videoDevice.unlockForConfiguration()\n} catch {\n    print(error)\n}\n```", "```swift\ncaptureSession.commitConfiguration()\n\npreviewLayer = AVCaptureVideoPreviewLayer(session: captureSession)\npreviewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill\nrootLayer = imageView.layer\npreviewLayer.frame = rootLayer.bounds\nrootLayer.addSublayer(previewLayer)\n\n```", "```swift\ncaptureSession.startRunning()\n```", "```swift\nAVCaptureDevice.DiscoverySession(deviceTypes: \n  [.builtInWideAngleCamera], mediaType: .video, position: .back)\n```", "```swift\ncaptureSession.stopRunning()\n```", "```swift\npreviewLayer = AVCaptureVideoPreviewLayer(session: captureSession)\n```", "```swift\nfunc captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: \n  CMSampleBuffer, from connection: AVCaptureConnection) { }\n```", "```swift\nimport Vision\n```", "```swift\nclass VisionViewController: ViewController\n```", "```swift\noverride func setupCaptureSession() {\n\n    super.setupCaptureSession()\n\n    setupDetectionLayer()\n    updateDetectionLayerGeometry()\n\n    startVision()\n\n}\n```", "```swift\nfunc setupDetectionLayer() {\n    detectionlayer = CALayer()\n    detectionlayer.name = \"detection.overlay\"\n    detectionlayer.bounds = CGRect(x: 0.0,\n                                        y: 0.0,\n                                        width: bufferSize.width,\n                                        height: bufferSize.height)\n    detectionlayer.position = CGPoint(x: rootLayer.bounds.midX, y: \n      rootLayer.bounds.midY)\n    rootLayer.addSublayer(detectionlayer)\n}\n```", "```swift\nfunc updateDetectionLayerGeometry() {\n\n    let bounds = rootLayer.bounds\n    var scale: CGFloat\n\n    let xScale: CGFloat = bounds.size.width / bufferSize.height\n    let yScale: CGFloat = bounds.size.height / bufferSize.width\n\n    scale = fmax(xScale, yScale)\n    if scale.isInfinite {\n        scale = 1.0\n    }\n\n    CATransaction.begin()\n    CATransaction.setValue(kCFBooleanTrue, forKey: \n      kCATransactionDisableActions)\n    detectionlayer.setAffineTransform(CGAffineTransform(rotationAngle: \n  CGFloat(.pi / 2.0)).scaledBy(x: scale, y: -scale))\ndetectionlayer.position = CGPoint(x: bounds.midX, y: \n  bounds.midY)\n\n    CATransaction.commit()\n\n}\n```", "```swift\nfunc startVision(){\n\n    guard let localModel = Bundle.main.url(forResource: \"YOLOv3\", \n      withExtension: \"mlmodelc\") else {\n        return\n    }\n\n    do {\n        let visionModel = try VNCoreMLModel(for: MLModel(\n          contentsOf: localModel))\n        let objectRecognition = VNCoreMLRequest(model: visionModel, \n          completionHandler: { (request, error) in\n\n            DispatchQueue.main.async(execute: {\n                if let results = request.results {\n                    self.visionResults(results)\n                }\n            })\n\n        })\n        self.requests = [objectRecognition]\n    } catch let error {\n        print(error.localizedDescription)\n    }\n\n}\n```", "```swift\nfunc visionResults(_ results: [Any]) {\n\n    CATransaction.begin()\n CATransaction.setValue(kCFBooleanTrue, forKey: \n      kCATransactionDisableActions)\n\n detectionlayer?.sublayers = nil\n\n    // Other code to follow    \n}\n```", "```swift\nfor observation in results where observation is \n  VNRecognizedObjectObservation {\n\n    guard let objectObservation = observation as? \n      VNRecognizedObjectObservation else {\n        continue\n    }\n\n    let labelObservation = objectObservation.labels.first\n    let objectBounds = VNImageRectForNormalizedRect(\n      objectObservation.boundingBox, Int(bufferSize.width), Int(bufferSize.height)) \n\n    let shapeLayer = createRoundedRectLayer(with: objectBounds)\n    let textLayer = createTextSubLayer(with: objectBounds,\n      identifier: labelObservation?.identifier ?? \"\",\n        confidence: labelObservation?.confidence ?? 0.0)\n    shapeLayer.addSublayer(textLayer)\n    detectionlayer.addSublayer(shapeLayer)\n\nupdateDetectionLayerGeometry()\nCATransaction.commit()\n\n}\n```", "```swift\ncreateRoundedRectLayer()\ncreateTextSubLayer()\n```", "```swift\noverride func captureOutput(_ output: AVCaptureOutput, didOutput \n  sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n\n    guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) \n      else {\n        return\n    }\n\n    let exifOrientation = \n      ImageHelper.exifOrientationFromDeviceOrientation()\n\n    let imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: \n      pixelBuffer, orientation: exifOrientation, options: [:])\n    do {\n        try imageRequestHandler.perform(self.requests)\n    } catch {\n        print(error)\n    }\n}\n```", "```swift\n@IBAction func onInputTypeSelected(_ sender: UISegmentedControl) {\n\n    switch sender.selectedSegmentIndex {\n    case 0:\n        captureSession.stopRunning()\n    case 1:\n        startLivePreview()\n    default:\n        print(\"Default case\")\n    }\n\n}\n```", "```swift\nfunc startLivePreview() {\n    captureSession.startRunning()\n}\n```", "```swift\nguard let localModel = Bundle.main.url(forResource: \"YOLOv3\", \n  withExtension: \"mlmodelc\") else {\n    return\n}\n```"]