- en: '*Chapter 15*: Recognition with Vision Framework'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Vision framework has been available to developers for a few years now.
    Apple has been introducing better and better features for it, from text recognition
    to image recognition. On iOS 14, Vision comes with more improvements to text recognition
    and other existing functions, but it also allows developers to perform two different
    actions: hand and body pose recognition. The possibilities that these new features
    open up for developers are limitless! Just think about gym apps, yoga apps, health
    apps, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to learn about the basics of the Vision framework
    and how to use the new advancements in text recognition. We will also learn about
    the new hand landmark recognition, building a demo app that can detect the tips
    of the four fingers and the thumb. The chapter code bundle also provides a similar
    example demonstrating body pose recognition. We will discuss these topics in the
    following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Vision framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing text in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recognizing hand landmarks in real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be able to work with the Vision framework
    with total confidence, being able to apply the techniques explained in this chapter
    to implement any type of recognition that Vision provides, from the recognition
    of text in images to the recognition of hand and body poses in videos.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code bundle for this chapter includes a starter project called `HandDetection_start`
    and a couple of playground files named `Vision.playground` and `RecognitionPerformance_start.playground`.
    It also contains a completed example for body pose detection named `BodyPoseDetection_completed`.
    You can find them in the code bundle repository:'
  prefs: []
  type: TYPE_NORMAL
- en: https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the Vision framework
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since the beginning of the App Store, there have been many apps that use the
    camera to build great functionalities using image and video recognition. Think
    of the bank apps that can now scan a check or a credit card so that the user doesn't
    need to input all the numbers. There are networking apps that can take a picture
    of a business card and extract the relevant information. Even the Photos app from
    your iPhone can detect faces in your photographs and classify them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Vision framework provides developers with a robust set of features to make
    it easier than ever to achieve these functionalities: from text and image recognition
    to barcode detection, face landmarks analysis, and now, with iOS 14, hand and
    body pose recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: Vision also allows the use of Core ML models to allow developers to enhance
    object classification and detection in their apps. Vision has been available since
    iOS 11 and macOS 10.13.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several concepts in Vision that are common to any type of detection
    (text detection, image detection, barcode detection, and so on), including the
    `VNRequest`, `VNRequestHandler`, and `VNObservation` entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VNRequest` is the task that we want to perform. For example, `VNDetectAnimalRequest`
    would be used to detect animals in a picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VNRequestHandler` is how we want to detect. It lets us define a completion
    handler where we can play around with the results and shape them in the way that
    we need.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VNObservation` encapsulates the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s look at an example that combines all these concepts and shows how Vision
    can easily help us to detect text inside an image. Open the playground named `Vision.playground`.
    This example code is grabbing an image from a specific URL and trying to extract/detect
    any text on it. The image being used is this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.01 – Example image to extract text with Vision'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_15.01_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.01 – Example image to extract text with Vision
  prefs: []
  type: TYPE_NORMAL
- en: 'If we try to extract text from this image, we should get results such as *Swift
    Data Structure and Algorithms*, or the name of the authors, or the description
    below the title. Let''s review the code in the playground:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go through the numbered comments:'
  prefs: []
  type: TYPE_NORMAL
- en: First, we are creating a `VNImageRequestHandler` instance with a given image
    URL. We instantiate this handler to perform Vision requests on an image. Remember
    that we need to call `perform(_:)` later on to launch the analysis.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we create a `request(VNRecognizeTextRequest)` instance that we will perform
    on the `requestHandler` instance instantiated previously. You can perform multiple
    requests on a `requestHandler` instance. We define a block of code to be executed
    when the request finishes. In this block, we are extracting the observations from
    the request results (`VNRecognizedTextObservation` instances). These observations
    will contain potential outcomes for the analyzed text from the image (`VNRecognizedText`
    instances). We print `topCandidate` from each observation, which should be the
    best match according to the Vision parameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can specify the recognition level for the request. In this example, we are
    using `.accurate` (the alternative is `.fast`). We will see later the results
    with `.fast` and when to use one or the other.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, we are performing the request on the `requestHandler` instance to execute
    everything with the `perform(_:)` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'If you execute the code, the console in the playground will display the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Those seem to be great results, right? If you recheck the image, we are extracting
    the correct text from it! The author names, the title (per line), the description,
    and more! Seems to be a great result! But have you noticed that when you execute
    the playground, it takes a while to finish? This is because we are using the `.accurate`
    option. Let''s see what happens if we use `.fast` instead. Change it in the playground
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This time, the analysis can be done faster, but as you can see, the results
    are far worse for what we wanted (we wanted to detect the text properly!). Why
    should anyone prefer speed over accuracy? Well, for some apps, speed is critical
    and it is fine to sacrifice some accuracy for it. Think of real-time camera-based
    translations or applying real-time filters to take photos. In these scenarios,
    you need fast processing. We will discuss this further later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: This playground example should help you to have a grasp of the incredible potential
    that Vision contains. Just with a few lines of code, we were able to process and
    extract the text of an image with no issues or complex operations. Vision allows
    developers to do amazing things. Let's dive deeper into it in the following sections,
    starting with a more detailed look at text detection for images.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing text in images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Vision framework has been improving its detection of text in images since
    its first iteration. In this section, we are going to learn some state-of-the-art
    techniques to obtain the best results on iOS 14.
  prefs: []
  type: TYPE_NORMAL
- en: 'We saw in the previous section that text detection in Vision can happen in
    two different ways, as defined by the value of `recognitionLevel` that we specify
    in the request: `.fast` and `.accurate`. Let''s see the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '`.accurate`. It doesn''t handle rotated text or different fonts as well as
    the `.accurate` method.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.fast` but is more accurate (of course!). It works in the same way that our
    brain recognizes words. If you read the word "m0untain," your brain can extract
    "mountain" from it, and it knows that the 0 (zero) stands for an o. If you use
    `.fast`, which recognizes character by character, the 0 (zero) would still be
    a 0 (zero) in your results, because no context is taken into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, after the initial recognition phase is finished, results are
    passed into a traditional natural language processor for language processing,
    and the outcome of that is the results (observations). This whole process happens
    exclusively on the device.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, when should anyone use `.fast`, you might wonder. Well, there are scenarios
    in which it is more convenient than `.accurate`:'
  prefs: []
  type: TYPE_NORMAL
- en: To read codes or barcodes quickly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When user interactivity is a crucial aspect, so you want a fast response from
    the text detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To demonstrate the differences between the recognition levels, let''s analyze
    the same image using different techniques. You will also learn some useful tricks
    that you can apply to your projects. Follow the steps given here:'
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and open the playground named `RecognitionPerformance_start.playground`.
    The code is roughly the same as what we tried in the previous section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The only difference is that the image that we are using now contains a 4-digit
    number that represents the serial number of the book:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 15.02 – Book cover with a serial number (1015) below the author names'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/Figure_15.02_B14717.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 15.02 – Book cover with a serial number (1015) below the author names
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you pay close attention to the number font, you will see that it might be
    tricky for a computer to tell whether some digits are numbers or letters. This
    has been done on purpose. In this example, we are going to test the capabilities
    of Vision.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Go ahead and execute the playground code. The console output should look like
    this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We have successfully retrieved the serial number of the book: `1015`. The code
    is also taking a measure of how long it takes to finish the text-recognition process.
    In our case, it was **1.93 seconds** (this can differ from computer to computer
    and also between executions). Can we do better than that? Let''s try out some
    techniques that will help us to improve this processing time while keeping the
    same accuracy. We are going to start with the **region of interest**.'
  prefs: []
  type: TYPE_NORMAL
- en: Region of interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sometimes, when we are analyzing an image with Vision, we don't need to process
    the whole image. For example, if we are processing a specific type of form where
    we know in advance that the first name always goes at the top of the document,
    we may want to just process that area. Processing the whole form would only waste
    time and resources if we just need a specific area.
  prefs: []
  type: TYPE_NORMAL
- en: Let's assume that in the previous example (the book cover), the serial number
    that we want to extract is always in the top-left area. How can we speed up the
    1.93-seconds processing time? We can do so by defining a region of interest. Defining
    a region of interest will tell Vision to only process that area and avoid the
    rest of the image. That will result in a faster processing time.
  prefs: []
  type: TYPE_NORMAL
- en: '`regionOfInterest` is a `CGRect` property of `VNRequest`:'
  prefs: []
  type: TYPE_NORMAL
- en: It defines a rectangular area in which the request will be performed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rectangle is normalized to the dimensions of the image, meaning that the
    width and height of the region of interest go from 0 to 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The origin of the rectangle is in the bottom-left corner of the image, which
    is (0,0). The top-right corner will be (1,1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The default value is `{{0,0},{1,1}}`, which covers everything from the bottom-left
    corner (0,0) to the top-right corner, with width 1 and height 1: the whole image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following figure, you can see the region of interest that we need to
    define to capture the serial number (**1015**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.03 – Region of interest'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_15.03_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.03 – Region of interest
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add that region to the code from the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `ScanPerformance_start.playground` project, add the following code just
    after setting `recoginitionLevel` to `.accurate`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now launch the playground and check the result in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'There are a couple of differences when comparing these results to the previous
    ones:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are no longer extracting that much text. Now that we are defining a region
    of interest, we just extract the words/digits that are contained in that area.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We reduced the processing time from 1.93 seconds to 1.23 seconds. That is 36%
    faster.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s now try to reduce the region of interest to catch just the serial number.
    Modify the region to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Launch the playground. Now the console output is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Modify this line to use `.fast`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save and execute. Check the console output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see how this time, the processing time has been made shorter again,
    but the result is not accurate at all. Instead of detecting `1015`, we have wrongly
    obtained `Iois`.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is a common way to fix this situation in scenarios where we
    have domain knowledge. In our case, we know that the processed characters should
    be numbers only. Therefore, we can adjust the output from Vision to improve the
    results and fix misclassifications. For example, see the following adjustments:'
  prefs: []
  type: TYPE_NORMAL
- en: The character "I" can be "1."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The character "o" can be "0."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The character "s" can be "5."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s implement this in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: 'At the very end of the playground file, add the following method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are extending the `Character` class by adding a new method named `transformToDigit()`.
    This new method is going to help us to improve potential misclassifications. Note
    how in the method itself, we have a table of letter characters that relate to
    a similarly shaped number. What we are doing is just transforming those letters
    into the corresponding digits.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s use it now. Below the `print(recognizedStrings)` line, add the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We are getting the result of the Vision process; in our case, it was `"Iois"`,
    and for each character, we are applying to it our new `transformToDigit()` method.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Execute the code, and you will see the following result in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That looks great! Note how the `"Iois"` result is now looking much better when
    transformed to `"1" "0" "1" "5"`. Also, note how the processing time didn't increase
    that much; this operation is relatively easy to compute.
  prefs: []
  type: TYPE_NORMAL
- en: Now let's summarize what we have done in this section and the improvements we
    made in each step. We started by processing a whole image and using the `.accurate`
    recognition level, and that took us 1.93 seconds. Then, we applied a region of
    interest to just process part of the image that we were interested in, reducing
    the processing time to 1.23 seconds. After that, we changed from `.accurate` to
    `.fast`. This move reduced the processing time to 0.59 seconds, but the results
    were incorrect. Finally, we implemented an easy algorithm to improve the results
    and make them as good as with the `.accurate` level. So, in the end, we got perfect
    results and a processing time of 0.59 seconds rather than 1.93!
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will learn about one of the new features of iOS14,
    hand detection.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing hand landmarks in real time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the additions to Vision in iOS 14 is hand detection. This new feature
    to detect hands in images and video allows developers to find with great detail
    the positions of the wrist and the individual fingers in a video frame or photo.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to explain the basics behind hand detection, and
    we will demonstrate how it works with a sample project. Let's start with the hand
    landmarks that we will be able to recognize.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding hand landmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are 21 landmarks that we will be able to detect in a hand:'
  prefs: []
  type: TYPE_NORMAL
- en: 4 in the thumb
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 4 in each finger (16 in total)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 1 in the wrist
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, Vision differentiates between finger and thumb. In both the
    finger and thumb, there are 4 points of interest. The following figure shows how
    these landmarks are distributed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.04 – Finger and thumb landmarks'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_15.04_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.04 – Finger and thumb landmarks
  prefs: []
  type: TYPE_NORMAL
- en: Note how there is also a landmark in the middle of the wrist.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the four fingers, we can access each of them individually using the following
    keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`littleFinger`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`middleFinger`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ringFinger`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`indexFinger`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside each of them, we can access the four different landmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TIP**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DIP**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PIP**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MCP**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note how for the thumb, these names are slightly different (TIP, IP, PIP, and
    CMC). In the example code that we will build later in this section, we will demonstrate
    how to use these points and each of the fingers plus the thumb.
  prefs: []
  type: TYPE_NORMAL
- en: Vision is capable of detecting more than just one hand at a time. We can specify
    the maximum amount of hands that we want to detect. This parameter will have an
    impact on the performance of our detection. Use `maximumHandCount` to set the
    limit.
  prefs: []
  type: TYPE_NORMAL
- en: For performance and accuracy, it is also better if the hand is not near the
    edges of the frame, if the light conditions are good, and if the hands are perpendicular
    to the camera angle (so the whole hand is visible, not just the edge of it). Also,
    take into account that feet can be recognized as hands sometimes, so avoid mixing
    them.
  prefs: []
  type: TYPE_NORMAL
- en: That is enough theory; let's jump straight into a code example! We will build
    a demo app that will be able to detect hand landmarks using the front video camera
    of a phone and will display an overlay on the detected points.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing hand detection
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to implement a demo app that will be able to detect
    hand landmarks using the front video camera of a phone.
  prefs: []
  type: TYPE_NORMAL
- en: The code bundle of this project contains the initial project and also the final
    result. Go ahead and open the project named `HandDetection_start`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The project contains two main files: A `UIView` instance named `CameraView.swift`
    and a UIViewController instance called `CameraViewController.swift`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The view contains helper methods to draw points on coordinates. It will serve
    as an overlay to draw on top of the camera feed. Just know that the `showPoints(_
    points: [CGPoint], colour: UIColor)` method will allow us to draw an array of
    `CGPoint` structs into the overlay on top of the video camera feed.'
  prefs: []
  type: TYPE_NORMAL
- en: The view controller will be the centerpiece of the example and is where we are
    going to implement the relevant code to perform the hand detection. Go ahead and
    open the `CameraViewController.swift` file. Let's examine the code skeleton that
    we will fill out step by step.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the top of the file, we are defining four properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '`handPoseRequest: VNDetectHumanHandPoseRequest`. We will apply this request
    at the top of the video stream, to detect hand landmarks in each frame. If we
    detect any, we will display some points in the overlay to show them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`videoDataOutputQueue`, `cameraView`, and `cameraFeedSession`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the `viewDidAppear` and `viewWillDisappear` methods, we are starting/creating
    and stopping `AVCaptureSession` for the camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'And finally, in the next four methods, we have four TODO comments, which we
    are going to implement one by one to create this app. Let''s summarize the TODO
    tasks that we are going to perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TODO 1**: Detect one hand only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TODO 2**: Create a video session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TODO 3**: Perform hand detection on the video session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TODO 4**: Process and display detected points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to implement these four tasks in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting hands
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vision can do more than detect one hand at a time. The more hands we ask it
    to detect, the more it will impact performance. In our example, we only want to
    detect one hand. By setting `maximumHandCount` to `1` on the request, we will
    improve performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s start by adding this code below `// TODO 1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's create a video session to capture the video stream from the front
    video camera of the device.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a video session
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the second task, we are going to fill out the code inside the `setupAVSession()`
    method. Go ahead and paste the following code inside the method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we are creating a `videoDevice: AVCaptureDevice` instance by querying
    for the video front camera (if it exists!) with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we use that `videoDevice` to generate a `deviceInput: AVCaptureDeviceInput`
    instance, which will be the video device used for the stream, with the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now add this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After creating the `videoDevice` instance, we are creating a new `session:
    AVCaptureSession` instance. With the session created, we assign `videoDevice`
    as the input and create and configure an output to handle the video stream. We
    assign the class itself as `dataOutput AVCaptureVideoDataOutputSampleBufferDelegate`
    by calling this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This means that when the front video camera captures new frames, our session
    will handle them and send them to our delegate method, which we are going to implement
    in the next step (TODO 3).
  prefs: []
  type: TYPE_NORMAL
- en: Performing hand detection in the video session
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have set up and configured a video session, it is time to handle
    every frame as it comes and tries to detect any hands and their landmarks! We
    need to implement the `captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer:
    CMSampleBuffer, from connection: AVCaptureConnection)` method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the `// TODO 3: Perform hand detection on the video session` line, add
    this code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We want to detect the tip of the four fingers (index, ring, middle, and little)
    and the thumb. So, we are creating five variables of type `CGPoint` to store their
    coordinates, if they are found.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just after these new lines, add the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: With this code, we are asking Vision to execute `handPoseRequest` over `sampleBuffer`
    (the video stream). Then, we guard (using `guard`) against the case in which we
    don't detect any observations (so that if there is no hand in the video frame,
    we just stop at this point).
  prefs: []
  type: TYPE_NORMAL
- en: 'But if the guard doesn''t trigger, it means that we have some hand landmarks
    and we need to process them. Add the following code just after the `// Get observation
    points` line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now we are extracting from the observation any instances of `recognizedPoints()`
    that are related to the thumb and the four fingers. Note that we use `try` to
    do this operation because a result is not guaranteed. With the extracted recognized
    points, we later unwrap the TIP point of each finger and thumb with the `guard`
    statement.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have five variables with the coordinates of the TIP
    point of each finger plus the thumb.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although we already have the five coordinates that we are looking for, we still
    need to perform an extra step. Vision coordinates are different from `AVFoundation`
    ones. Let''s transform them; add the following code just after the last `guard`
    statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the `x` coordinate is the same in both systems, but the `y`
    coordinate is different. In Vision, the bottom-left corner is the (0,0). So, we
    just need to subtract the `y` coordinate of the Vision point to 1 to get a result
    on the `AVFoundation` system.
  prefs: []
  type: TYPE_NORMAL
- en: Great! At this point, we have the hand landmarks detection system up and running,
    with a result in the form of `AVFoundation` CGPoint coordinates. The last step
    is to draw those points!
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following code after the `catch` block (outside of it), just at the
    end of the `func captureOutput(…)` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We are calling the `processPoints(…)` method inside the main thread because
    we want it to work on the UI, so we ensure that everything works perfectly by
    dispatching this work into the correct thread. Let's implement the `processPoints(…)`
    method next.
  prefs: []
  type: TYPE_NORMAL
- en: Processing and displaying detected points
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After the hand landmarks have been detected inside the `captureOutput(…)` method,
    we now want to draw them into the camera overlay. Replace the empty implementation
    of `processPoints(…)` with this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Remember how we are using `CGPoints` converted to `AVFoundation` coordinates?
    Now we want to convert those points into the `UIKit` preview layer. We are performing
    `map` over them, and finally, we are calling the `cameraView` helper method `showPoints`
    to display them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Everything is now in place! It is time to build and run the application. You
    will see the selfie camera triggering, and if you point it at your hand, the tips
    of your fingers and thumb should be overlayed with red dots. Give it a try and
    you should get something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 15.05 – TIP detection'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_15.05_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15.05 – TIP detection
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this approach still has some issues! Try this: let the app detect
    your hand, and then remove the hand from the camera''s view – the red dots are
    still on the overlay! They are not cleaned up when no hand is detected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This has an easy fix. The reason for it is that inside the `captureOutput(…)`
    method, we are not always executing the `processPoints(…)` method. There are times
    (the `guard` statements) where we return without calling it. The solution is to
    wrap the `processPoints(…)` block into a `defer`, moving it to the beginning of
    the code, just after we define the five properties to store the coordinates of
    each tip. It should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The highlighted code is the part that we have wrapped into a `defer` (so it
    will always execute before returning the method). Execute the app again, and you
    will notice that when there is no hand on the screen, the red dots will not be
    there either! We are calling `processPoints` with empty values, so nothing is
    being drawn. With this last step, we have a working example of hand landmark detection
    up and running! Congratulations!
  prefs: []
  type: TYPE_NORMAL
- en: Body pose detection
  prefs: []
  type: TYPE_NORMAL
- en: Vision also provides body pose detection on iOS 14\. Body pose detection is
    quite similar to hand detection, so we are not going to give a step-by-step demo
    of it. But the code bundle of this book contains an example app similar to the
    one in this section but for body pose detection. You can check out the project
    named `BodyPoseDetection_completed` and see the little differences that it has
    from the hand detection project.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have learned about the new Vision methods to detect hand
    landmarks and how to detect hand landmarks using the video stream of a phone as
    input (instead of just detecting a hand in a static image). We also provided a
    similar demo that can be used for body pose detection. Let's jump into the summary
    to finish the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We started this chapter by learning about the basic building blocks of every
    Vision feature: how to use a `VNRequest` instance, its corresponding `VNRequestHandler`
    instances, and the resulting `VNObservation` instances.'
  prefs: []
  type: TYPE_NORMAL
- en: After learning the basics, we applied them to text recognition. We compared
    different recognition levels by using `.fast` and `.accurate`. We also learned
    about regions of interest and how they can affect the performance of Vision requests.
    Finally, we improved our results in text recognition by applying domain knowledge,
    fixing potential errors and misreads from Vision.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we learned about the new hand landmarks recognition capability. But
    this time, we also learned how to apply Vision requests to real-time video streams.
    We were able to detect hand landmarks in a video feed from a device's front camera
    and display an overlay to show the results. This chapter also provided a similar
    example that could be applied to body pose recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, we will learn about a brand new feature of iOS 14: widgets!'
  prefs: []
  type: TYPE_NORMAL
