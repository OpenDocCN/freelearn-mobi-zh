<html><head></head><body><div><div><div><h1 id="_idParaDest-124" class="chapter-number"><a id="_idTextAnchor212"/>6</h1>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor213"/>Microservices, Serverless, and Microfrontends</h1>
			<p>In this chapter, we’ll delve into the architectural styles of microservices, serverless, and microfrontends. They have revolutionized the way we design, develop, and deploy applications. They also empower organizations to build robust, flexible, and scalable systems. We’ll explore the fundamentals, unique features, and tangible benefits of each approach. By the end, you’ll have a comprehensive understanding of these architectures and their application in modern software engineering.</p>
			<p>Our exploration begins with describing the traditional monolith, where the entire system is designed and developed as a single unit. Then, we’ll discuss the challenges of this approach that are faced by many developers to bring about the need to break down this monolith into smaller and loosely coupled components.</p>
			<p>After, we’ll turn our attention to microservices architecture and examine how microservices solve the challenges of monoliths. We’ll cover how a monolith can be transformed into microservices before discussing the benefits, challenges, and trade-offs that come with this distributed architectural style.</p>
			<p>Next, we’ll discuss how serverless computing helps developers focus on writing code without the concerns of infrastructure. We’ll discuss the optimal use cases for serverless architectures and address the challenges associated with this paradigm.</p>
			<p>Finally, we’ll look at how a monolithic application can be transformed into microservices and microfrontends. We’ll discuss how self-contained components benefit the developers and integrate with microservices and serverless backends.</p>
			<p>We’ll cover the following topics in this chapter:</p>
			<ul>
				<li>Monoliths</li>
				<li>Microservices</li>
				<li>Nanoservices</li>
				<li>Serverless</li>
				<li>Microfrontends</li>
			</ul>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor214"/>Technical requirements</h1>
			<p>You can find all the code files used in this chapter on GitHub: <a href="https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-6%20">https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-6</a></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor215"/>Monoliths</h1>
			<p><strong class="bold">Monolith</strong> means <em class="italic">made of one stone</em>. In the <a id="_idIndexMarker447"/>context of software architecture, a monolith refers to a large system designed and developed as a single unit. In a monolithic architecture, there’s typically a single code base, a unified database, and one deployable artifact.</p>
			<p>Having a single code base means it usually relies on individual developers’ efforts to keep the code tidy and clean. There’s little room to enforce separation of concerns by design since all code is hosted in a single place. This often results in all the components, modules, and functionalities of the application being tightly coupled and interdependent.</p>
			<p>A unified database, typically relational only, in a monolithic application is likely to produce a major – if not only one – schema that contains all the entities from all functionalities. Moreover, each entity table will contain all the columns that address all business concerns related to the entity. There may also be a spiderweb of foreign key constraints among tables. It often combines the concerns of a database used for transactions and reporting into one.</p>
			<p>While monoliths have a wide range of code quality and database designs, they have one thing in common: there’s one deployable artifact for the monolithic application, which contains the entire system. It’s big and takes a long time to release. The release procedure usually requires all monolithic application instances to shut down first, after which infrastructure changes are made, the new deployable artifact replaces the old one, and the updated version of the application starts. Each release also often requires intensive pre-planning and coordination among teams to identify dependencies. The release plan can look like a Gantt chart or a project plan, as shown in <em class="italic">Figure 6</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21737_06_1.jpg" alt="Figure 6.1 – Release plan as a Gantt chart" width="1524" height="483"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Release plan as a Gantt chart</p>
			<p>In this release plan, the dev team shuts down the monolithic application first. Then, the database team backs up the database. Afterward, the dev team starts deploying the application, while the infrastructure team applies changes such as network or middleware upgrades. When this is finished, the dev team starts the application. Finally, the QA team verifies the environment and signs off the release. Some organizations may have a dedicated release team or operation team to formulate a playbook for each release and execute the plan as one team.</p>
			<p>The release plan is visualized as a Gantt chart to show the dependencies among teams and show the timeline of the release from left to right.</p>
			<p>Next, we’ll cover the<a id="_idIndexMarker448"/> benefits and challenges of the monolithic approach as a background for the upcoming three architectural styles.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor216"/>Benefits</h2>
			<p>Nowadays, few organizations <a id="_idIndexMarker449"/>would openly advocate monolithic architecture as an optimal style. However, there are still a few benefits that allow it to be considered a choice of architecture:</p>
			<ul>
				<li><strong class="bold">Simplicity</strong>: Monolithic applications have the simplicity of one code base, one database, and one deployable artifact. Once the routines for build, deploy, and run are in place, developers can follow that one pattern that repeats over time. As we mentioned in <a href="B21737_02.xhtml#_idTextAnchor045"><em class="italic">Chapter 2</em></a>, this is known<a id="_idIndexMarker450"/> as the <strong class="bold">You aren’t gonna need it</strong> (<strong class="bold">YAGNI</strong>) principle. If this simplicity is all we need now, then it’s not a bad idea to run with the monolithic approach initially.</li>
				<li><strong class="bold">Short time to market</strong>: This situation typically resonates with developers if they work in a startup company, where time to market is the number one priority and nothing else matters. Sometimes, it’s even a do-it-now-or-quit situation. It could also go well with experimental applications where sophisticated architecture may not be necessary.</li>
				<li><strong class="bold">Build first, optimize later</strong>: Another benefit of monolithic architecture is situational. If the area of business is new and everyone is finding their paths on how to build the system as a product, then it’s beneficial to delay any refactoring or optimization – that is, until everyone, including technical and non-technical stakeholders, has more experience with the subject matter and recognizes the need to break down the monolithic application. Also, it’s better to understand the business ecosystem before trying to break it down by reasonable boundaries <a id="_idIndexMarker451"/>and which features naturally go together.</li>
			</ul>
			<p class="callout-heading">Monolith-first</p>
			<p class="callout">This approach of making a conscious choice toward monolithic architecture is called <strong class="bold">monolith-first</strong>. This <a id="_idIndexMarker452"/>term was made popular by Martin Fowler in his engineering blog in 2015.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor217"/>Challenges</h2>
			<p>While monolithic <a id="_idIndexMarker453"/>architecture has been widely used, it has some drawbacks. The tight coupling between components makes it difficult to update specific parts of the application independently. In other words, changing one part of the system would affect other parts unintentionally:</p>
			<ul>
				<li><strong class="bold">Slow development</strong>: This makes any change bigger than it needs to be, and therefore increases the risk of release. Since every change is bigger, the probability of having code conflicts among engineers is significantly higher. Engineers would spend more time reviewing code and resolving code conflicts. More areas of testing are required to ensure the system is functional. Combined, these factors lead to slower development cycles and reduced flexibility.</li>
				<li><strong class="bold">Difficult to scale and tune performance</strong>: Additionally, scaling resources in a monolith can be inefficient since the entire application needs to be replicated instead of individual components that require more resources being scaled. It’s also harder to tune performance precisely since there could be other processes contending with the same resource, which affect the performance to be tuned intentionally.</li>
				<li><strong class="bold">Time-consuming test suite</strong>: In a monolithic application, the different components and modules are tightly coupled, meaning that changes in one part of the application can have unintended consequences in other parts. Tests need to verify no unintended changes were made for business cases, leading to more complex test scenarios and a longer test execution time. The inter-dependencies also make it difficult to isolate and run tests independently, limiting the potential for parallel execution and shorter test execution times. Even a minor change in a monolithic application would need regression testing. This requires a comprehensive suite of test cases, which can be time-consuming.</li>
				<li><strong class="bold">Risky, long, and big releases</strong>: Releasing a monolithic application usually takes a long time because the entire system is deployed as a unit. Even a minor change would result <a id="_idIndexMarker454"/>in the entire monolith having to be redeployed, which means it becomes harder to continuously deliver the system. Worse, it could end up accumulating more changes to release periodically since the monolith can’t be deployed quickly and continuously. Engineers could be spending a long night aiming to release a monolith application, and these long and late hours may introduce more human errors due to fatigue.<p class="list-inset">On the contrary, releasing a small application in a distributed system can be equally challenging. However, due to the smaller scope of change, it’s possible to use modern strategies such as rolling<a id="_idIndexMarker455"/> releases, <strong class="bold">blue-green releases</strong>, or <strong class="bold">canary releases</strong>. These <a id="_idIndexMarker456"/>release strategies can be performed during business hours where most help is available, and thus reduce the risk of human errors.</p></li>
				<li><strong class="bold">Technology lock-in</strong>: A monolithic application usually has a long lifespan. That means it may have chosen a technology stack a long time ago. Developers are faced with either upgrading the technology, which results in a substantial change in the code base or adopting a different technology in some parts of the codebase, which results in multiple tools doing the same job. This also means the space for experimenting with technology is severely limited.</li>
				<li><strong class="bold">Total system failure</strong>: A failure in a monolithic application easily results in total system failure. Even the operational part may shut down since it’s part of one monolith unit. It’s harder to isolate and contain failures because there’s no clear separation between components.</li>
				<li><strong class="bold">Team dependencies</strong>: Lastly, multiple teams sharing one deployable artifact and probably one code base create a lot of dependencies. One team may have completed a feature that needs to be released as soon as possible, and the other team may still be working on a feature that isn’t ready yet.</li>
				<li><strong class="bold">Slow time to market</strong>: As they share one deployable artifact and one monolith, the first team may not be able to have their feature land on the market in time<a id="_idIndexMarker457"/> until other teams have completed theirs. This slow time to market could mean competitors may have taken opportunities and customers by the time the monolith is released. It hurts the business if the system is constantly catching up with competitors.</li>
			</ul>
			<p>With that, we’ve set the context of the challenges that are faced by engineers when implementing monolithic applications. Next, we’re going to look at architectural styles that aim to conquer these challenges.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor218"/>Microservices</h1>
			<p>Before the term <a id="_idIndexMarker458"/>microservice architecture was coined, the concept of <strong class="bold">service-oriented architecture</strong> (<strong class="bold">SOA</strong>) became<a id="_idIndexMarker459"/> popular in the 2000s as an early response to the challenges posed by monolithic architectures.</p>
			<p>SOA emphasizes encapsulating business functionality into independent services. Each service has a well-defined interface and communicates with other services. Standard protocols such as <strong class="bold">Enterprise Service Bus</strong> (<strong class="bold">ESB</strong>) are <a id="_idIndexMarker460"/>used for communication. The principles and concepts formalized in SOA to break down monoliths provided a basis for the future development of microservices.</p>
			<p>In 2011, the term microservices was brought up in software architecture workshops as the participants increasingly became aware of the emergence of a new architecture. In 2012, the term microservices was officially decided. <em class="italic">James Lewis</em> and <em class="italic">Fred George</em> were the major initial contributors to this style.</p>
			<p>Around the same time, companies such as <em class="italic">Netflix</em> and <em class="italic">Amazon</em> were also experimenting with similar architectural patterns. Netflix played a significant role in popularizing microservices through their adoption of the architecture for their scalable streaming platform. They shared their experiences and insights at various conferences and through blog posts, contributing to the growing interest and understanding of microservices.</p>
			<p>Next, we’re going to cover the key principles of microservices that shape their design and implementation.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor219"/>Key principles</h2>
			<p>Primarily, a microservice <a id="_idIndexMarker461"/>should conform to the <strong class="bold">single responsibility principle</strong> (<strong class="bold">SRP</strong>) at the<a id="_idIndexMarker462"/> business capability or functionality level.</p>
			<p>Let’s consider the example we’ve used throughout this book about village households exchanging services with a contract. Functions such as record-keeping household information, negotiating the contract, exercising the contract, and notifying households are all microservice candidates.</p>
			<p>A microservice should have a well-defined responsibility that handles a single concern or business domain. For this example, we can define each microservice like so:</p>
			<ul>
				<li><strong class="bold">Household service</strong>: Masters the records of households</li>
				<li><strong class="bold">Contract service</strong>: Maintains the workflow of contract negotiation from drafted to fully exercised</li>
				<li><strong class="bold">Notification service</strong>: Sends proxy notification requests to email service providers</li>
			</ul>
			<p>The details of how to break down a system into appropriate business domains will be discussed in depth in <a href="B21737_08.xhtml#_idTextAnchor289"><em class="italic">Chapter 8</em></a>, where we’ll cover <strong class="bold">domain-driven development</strong> (<strong class="bold">DDD</strong>). However, some <a id="_idIndexMarker463"/>architectural smells indicate whether microservices have well-defined responsibilities:</p>
			<ul>
				<li>Each microservice <a id="_idIndexMarker464"/>shouldn’t be developed by more than one team of engineers. However, there could be exceptions, such as if responsibilities aren’t delegated to the teams. This takes us back to <a href="B21737_01.xhtml#_idTextAnchor013"><em class="italic">Chapter 1</em></a> when <strong class="bold">Conway’s law</strong> was<a id="_idIndexMarker465"/> mentioned. It’s recommended to re-organize the teams so that each team has well-defined responsibilities and those responsibilities don’t overlap with other teams. The approach of re-organizing teams to aim for a better architecture is called <a id="_idIndexMarker466"/>the <strong class="bold">Inverse </strong><strong class="bold">Conway Maneuver</strong>.</li>
				<li>A microservice shouldn’t communicate too often with another to complete its feature. What’s worse is if it invokes another microservice’s endpoints iteratively. This is likely to indicate a “leak” of the service boundary. Perhaps the part that this microservice needs from another should be brought back and owned by the microservice.</li>
				<li>A microservice shouldn’t depend on another microservice during a release. If a microservice is unavailable because other microservices are unavailable, this indicates a possible technical dependency.</li>
				<li>Two microservices having inter-dependency in terms of exchanging messages could indicate that responsibilities haven’t been defined well enough. This is more of a problem if the communication is synchronous rather than asynchronous. If service A calls service B synchronously, when service B handles the call from service A, service B calls service A synchronously. This case would easily exhaust all threads in the request handler pool.</li>
				<li>Microservices shouldn’t share code repositories, build processes, database schemas, and deployable artifacts with other microservices. Sharing them may hint at potential dependencies among microservices during build and release. The last thing you want is that your microservice can’t be deployed until another microservice is deployed.</li>
			</ul>
			<p>Having any of the symptoms discussed so far might suggest these microservices have emerged as <strong class="bold">distributed monoliths</strong>, which <a id="_idIndexMarker467"/>is worse than a<a id="_idIndexMarker468"/> traditional monolithic application.</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor220"/>Communication and integration</h2>
			<p>Microservices communicate <a id="_idIndexMarker469"/>with each other through well-defined interfaces, namely <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>). The <a id="_idIndexMarker470"/>communication is either synchronous or asynchronous.</p>
			<p>The synchronous and asynchronous<a id="_idIndexMarker471"/> communications that occur via APIs are specified by <a id="_idIndexMarker472"/>the popular <strong class="bold">Open API</strong> and <strong class="bold">Async API</strong> standards, respectively:</p>
			<ul>
				<li>Synchronous communication is often achieved by one microservice sending a request to another microservice and waiting for a response before continuing its execution. The APIs<a id="_idIndexMarker473"/> can be <a id="_idIndexMarker474"/>exposed<a id="_idIndexMarker475"/> as <strong class="bold">Hypertext Transfer Protocol/Secure</strong> (<strong class="bold">HTTP/HTTPS)</strong>, <strong class="bold">Remote Procedure Call</strong> (<strong class="bold">RPC</strong>), <strong class="bold">Simple Object Access Protocol</strong> (<strong class="bold">SOAP</strong>), and so on.</li>
				<li>Asynchronous communication usually involves messaging systems so that a microservice can send a message and immediately continue its execution. The other microservices receive the message when it’s available.</li>
			</ul>
			<p><strong class="bold">Webhooks</strong> are an<a id="_idIndexMarker476"/> alternative and popular way to communicate among microservices asynchronously without the need for messaging systems. Instead of responding to a request, a microservice usually sends a message to another microservice via the HTTP/REST protocol but doesn’t require a response to continue its execution. Webhooks usually require static configuration to be used with the target endpoints.</p>
			<p>Considering the example of the four potential microservices, they could communicate in the following way:</p>
			<div><div><img src="img/B21737_06_2.jpg" alt="Figure 6.2 – An example of microservice communication" width="1133" height="692"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – An example of microservice communication</p>
			<p>In the preceding diagram, <strong class="bold">Contract service</strong> needs to get household data from <strong class="bold">Household service</strong> to validate requests and manage their workflows. Then, <strong class="bold">Contract service</strong> sends requests to <strong class="bold">Notification service</strong> so that emails can be sent to households to <a id="_idIndexMarker477"/>inform them of any changes in their workflows.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor221"/>Scalability and resilience</h2>
			<p>Microservice <a id="_idIndexMarker478"/>architecture provides inherent scalability due to its modular nature. Each microservice can be scaled independently based on its specific resource requirements.</p>
			<p>In our example of microservices, the usage patterns can be hugely different. The following may apply:</p>
			<ul>
				<li><strong class="bold">Household service</strong>: Low traffic as there’s only a limited number of households</li>
				<li><strong class="bold">Contract service</strong>: Most frequently used by users</li>
				<li><strong class="bold">Notification service</strong>: Medium throughput of email requests but no strict latency requirements</li>
			</ul>
			<p>Assuming this is a true reflection of its usage, we might need more instances of the <strong class="bold">contract service</strong> than any other service. Alternatively, other microservices can send asynchronous messages to request notifications, combined with queuing and batching mechanisms on the receiver side. Then, the <strong class="bold">contract service</strong> will be able to process a large volume of email requests in batches; other microservices don’t need to wait to get a response from the <strong class="bold">contract service</strong> to continue their processes.</p>
			<p>The <strong class="bold">household service</strong> is requested often in getting household data from the <strong class="bold">contract service</strong>. In other words, the availability of the <strong class="bold">household service</strong> has become more important than that of other services. However, we can also consider having a local cache of household data in the <strong class="bold">contract service</strong>. The <strong class="bold">household service</strong> would need to send asynchronous messages when household data is created, updated, and deleted. It can broadcast messages to all interested microservices, though the messages can be kept in a <strong class="bold">last-value queue</strong> messaging structure so that other microservices can copy the data to their local storage.</p>
			<p>By combining these changes to address the concerns of scalability, performance, availability, and resilience, these <a id="_idIndexMarker479"/>services may communicate like so:</p>
			<div><div><img src="img/B21737_06_3.jpg" alt="Figure 6.3 – Example of updated microservice communication" width="1133" height="693"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Example of updated microservice communication</p>
			<p>Note that there are no synchronous request-response messages in the system anymore. This implies that each microservice can operate on its own without other microservices being available, increasing the resilience of the system. For instance, if <strong class="bold">Household service</strong> is down, maintenance operations regarding household data are unavailable, but all other microservices are still operational as they use the last known household kept in their local storage.</p>
			<p>However, it does rely more on messaging systems to provide features such as queuing, batching, and last-value queues. Messaging systems are typically configured and deployed as infrastructure so that they’re more resilient than microservices.</p>
			<p>Even with synchronous communication, there are techniques such as circuit breakers, bulkheads, and graceful degradation that can be employed to respond to failures gracefully.</p>
			<p>Imagine that it was just a monolithic application of four modules. In that case, these concerns can’t be addressed separately. Moreover, if the monolithic application is down, instead of <a id="_idIndexMarker480"/>having a partial system failure occur, we end up with a total system failure. No operation will be available.</p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor222"/>Maintainability and technology choices</h2>
			<p>Each microservice<a id="_idIndexMarker481"/> should have a code repository. A microservice focuses on a single responsibility at the business capability level, so it’s unlikely that a code change in one microservice would change something in another microservice. Moreover, keeping code changes small creates fewer chances of code conflicts among engineers, which also reduces the time required to review pull requests. The productivity of engineers increases as the code using the microservice style is more maintainable.</p>
			<p>As each microservice has its own project, build script, and code repository, any choice that’s made regarding the use of technology and libraries is confined within the project. When it comes to using a new library as a dependency or a newer version of the library, engineers can try it in one microservice to start with, learn and get familiar with it, prove it’s working with one microservice, and then apply the proven approach to other microservices. A dedicated code repository for a microservice reduces many risks when experimenting with recent technologies. It also increases the chance of them being used in the system and keeps the system modern.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor223"/>Testing and quality assurance</h2>
			<p>Testing microservices <a id="_idIndexMarker482"/>as black boxes has a smaller scope than testing a monolithic application. End-to-end test cases may involve a combination of behaviors from different modules.</p>
			<p>However, in microservice architecture, these modules would have become microservices. With well-defined APIs, it’s now possible to mock the behaviors of external interfaces so that the test case focuses on testing how it uses and responds to external APIs. This simplifies the test suite and each microservice focuses on testing its own behaviors instead. The exhaustive combination of behaviors from different microservices’ communication can be inferred and thus remove the need for comprehensively testing all business cases via end-to-end testing.</p>
			<p>Having said that, end-to-end testing may still be required to validate the overall communication among microservices while considering other factors such as URL routing, security controls, and API compatibility. Usually, end-to-end testing only contains critical business cases and focuses on overall system correctness.</p>
			<p>As microservice communication using APIs is vital to ensure the overall system is functional, <strong class="bold">contract testing</strong> can be <a id="_idIndexMarker483"/>brought in to verify that these APIs are as per their specifications and that the microservices conform to them. This involves testing on both the consumer and provider sides of the APIs.</p>
			<p>Consumers create contract tests based on their expectations, simulating the interactions with the microservice via APIs. These tests verify that the consumer’s requirements are met. On the other hand, providers execute the consumer’s contract tests against their implementation. These tests ensure that the provider meets the API’s requirements and doesn’t introduce any breaking changes.</p>
			<p>These improvements allow engineers to verify the quality of the system faster and thus reduce the time to market of changes. We’ll cover software testing in depth in <a href="B21737_13.xhtml#_idTextAnchor418"><em class="italic">Chapter 13</em></a>.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor224"/>Deployment and infrastructure</h2>
			<p>One of the key <a id="_idIndexMarker484"/>characteristics of monolithic applications is the single major deployable artifact. In contrast, one microservice should have its own deployable artifact. This allows us to implement the practice of <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) and, as <a id="_idIndexMarker485"/>a result, reduce or even eliminate downtime during release.</p>
			<p>In our real-life example, and with the updated communications among microservices shown previously, each microservice can be released in isolation. Together with a rolling deployment procedure, it’s possible to keep the system operating while the release is ongoing.</p>
			<p>Moreover, each microservice can be deployed at its own cadence. There’s no need to wait for other microservices to be deployed like in the days of monolithic applications. This encourages engineers to deploy microservices if they’re ready to go, and thus speed up the time to market of software as products.</p>
			<p>Microservice architecture often uses containerization technologies<a id="_idIndexMarker486"/> such as <strong class="bold">Docker</strong> and container orchestration platforms <a id="_idIndexMarker487"/>such as <strong class="bold">Kubernetes</strong>. Typically, building a microservice would generate its own Docker image, in which the dependencies and configurations are already set up. This results in consistent, repeatable, and predictable deployment, and each microservice has an isolated environment.</p>
			<p>Kubernetes provides a declarative approach to managing how microservices are deployed. The desired state of each microservice is defined using Kubernetes manifest files and includes the Docker image to be used as a running microservice. The diverse needs of each microservice, as illustrated in our real-life example, can be realized by declaring the number of replicas and resource requirements in these manifest files.</p>
			<p>The replicas setting defines the desirable number of instances of the <strong class="bold">household service</strong> that are running. Kubernetes’ <strong class="bold">Horizontal Pod Autoscalers</strong> (<strong class="bold">HPAs</strong>) use this number to scale <a id="_idIndexMarker488"/>up and down based on resource utilization.</p>
			<p>Kubernetes also provides mechanisms for microservices to discover and communicate with each other, such<a id="_idIndexMarker489"/> as <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) or environment variables.</p>
			<p>Overall, these <a id="_idIndexMarker490"/>tools allow you to provision, scale, and manage the infrastructure needed to deploy and run microservices in an automated fashion.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor225"/>Team organization</h2>
			<p>Microservice <a id="_idIndexMarker491"/>architecture goes hand in hand with the modern organization of teams. The system is broken down into microservices, and so should the teams.</p>
			<p>The golden rule is that a microservice should be owned by one – and only one – team. This team is responsible and accountable for the full development cycle of the designated microservice.</p>
			<p>The team should be given the autonomy to make small technical decisions within their scope while adhering to broader guidelines on the choice of technology to be used in the system.</p>
			<p>Broader guidelines are there to maintain a certain degree of consistency among teams, such as the choice of commercial messaging, so that the firm reduces the complexity and cost of having too many technologies. Also, these guidelines provide agreed-upon principles and<a id="_idIndexMarker492"/> conventions to all teams, but each team has the power to decide how to execute and adhere to those guidelines.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor226"/>The drawbacks of microservices</h2>
			<p>While microservices<a id="_idIndexMarker493"/> offer several benefits, such as modularity, maintainability, and testability, they also come with some drawbacks that must be considered:</p>
			<ul>
				<li><strong class="bold">Increased complexity</strong>: Microservices architecture introduces a distributed system with more moving parts. Communication among microservices leads to additional complexity in development, testing, and deployment.<p class="list-inset">Engineers will have to consider API versioning and compatibility. Introducing a breaking change in an API would break other microservices that stay in the older API versions. Maintaining a backward-compatible API or transitioning to a new major API version can be a challenge.</p><p class="list-inset">Multiple microservices are used in the end-to-end test suite, which means these services need to be operational before the end-to-end test cases can be run. To make things worse, each microservice is usually managed by its own team, which means there are multiple streams of changes happening at the same time. There are more reasons to fail end-to-end tests now. It could be that one of the microservices failed to start, a change in one microservice ended up not being compatible with other microservices, and so on. It may end up that the engineering teams spend a lot of energy fixing the end-to-end tests.</p><p class="list-inset">Microservices architecture requires a more complex infrastructure, including service routing, load balancing, and container orchestration. This infrastructure as configuration can also act as a boilerplate as each microservice may have similar configurations but only differ in a few sections. The overhead of managing and maintaining this infrastructure can be significant, especially for smaller organizations or teams.</p></li>
				<li><strong class="bold">Network latency and overhead</strong>: Breaking down a monolithic application into microservices implies local function invocations become remote. This can introduce latency and performance issues, especially if the services are geographically distributed.<p class="list-inset">The overhead of network communication, including protocols, serialization, and deserialization, can impact the system’s overall performance.</p></li>
				<li><strong class="bold">Distributed data management</strong>: In a microservices architecture, data is often distributed across multiple microservices, which brings the challenges of keeping data manageable and consistent. Data could be split between two microservices, and there could be data that’s represented differently in multiple microservices. Worse, there could be data among microservices that isn’t consistent, so it’s hard to understand the overall picture.</li>
				<li><strong class="bold">Monitoring and observability</strong>: Tracing a business journey that travels through multiple microservices can be more challenging compared to monolithic<a id="_idIndexMarker494"/> applications. There are techniques to overcome this problem, but they require additional tooling and effort. These techniques will be covered in <a href="B21737_11.xhtml#_idTextAnchor358"><em class="italic">Chapter 11</em></a>.</li>
				<li><strong class="bold">Fine-grained, infrequent, on-demand, or small tasks</strong>: The overhead of microservices architecture could outweigh the benefits if you’re running specific processes. For example, if a summary report of user activities needs to be exported as a file and uploaded to an SFTP folder monthly, it can hardly justify standing up a long-running microservice that’s only used once per month. Similar situations apply to small tasks that are triggered upon request.</li>
			</ul>
			<p>There’s an alternative approach to microservices architecture when the scope is too small to justify the<a id="_idIndexMarker495"/> overhead. We’ll discuss this in the next section.</p>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor227"/>Nanoservices</h1>
			<p>Nanoservices architecture, as <a id="_idIndexMarker496"/>the name suggests, takes the principles of microservices and makes them even more fine-grained. Microservices focus on breaking down monolithic applications into small, independent, and loosely coupled services, with one service for one business capability. Nanoservices take this concept even further by decomposing the system into extremely fine-grained, single-purpose components.</p>
			<p>Nanoservices are designed to handle highly specific, autonomous, independent, and atomic functionalities. A nanoservice is often responsible for a single task or a tiny piece of logic within the overall system. Each nanoservice has a deployable artifact and can be deployed independently. Some of these nanoservices, when combined, can be seen as microservices.</p>
			<p>Using the previous example regarding microservices, the <strong class="bold">household service</strong> can be broken down into several nanoservices:</p>
			<ol>
				<li>Get a Household record by name.</li>
				<li>Create or update a Household record.</li>
			</ol>
			<p>These two nanoservices share the same database schema, while only one of them focuses on returning a Household record by name. The other nanoservice focuses on performing a create/update operation and sending asynchronous messages after a Household <a id="_idIndexMarker497"/>record is updated.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor228"/>Benefits</h2>
			<p>Nanoservices have<a id="_idIndexMarker498"/> much smaller code bases, fewer dependencies, and less resource utilization (CPU, memory, and disk) compared to microservices. This reduced footprint results in simpler deployment and configurations. Scaling nanoservices is efficient as it only concerns the needs of one function.</p>
			<p>Since nanoservices require less coordination and communication between each other, a nanoservice can be seen as a plumbing unit that only focuses on inputs, processes, and output.</p>
			<p>The reduced complexity and overhead of nanoservices can make them particularly well-suited for resource-constrained environments, real-time systems, or scenarios where fault tolerance and rapid scaling are critical.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor229"/>Drawbacks</h2>
			<p>Even though managing a <a id="_idIndexMarker499"/>nanoservice is easier than managing a microservice, a system that’s been broken down into nanoservices has a significantly higher number of services to manage. This high number may outweigh the ease of managing one nanoservice.</p>
			<p>The high number of nanoservices may bring significant network communication and collaboration challenges. The small resource footprint of one nanoservice may not translate well regarding overall resource consumption (CPU, memory, and disks).</p>
			<p>Moreover, some nanoservices may share the same database schema for different operations. In our example of the read and update nanoservices for the <strong class="bold">household service</strong>, they share the same database schema. If the schema needs to evolve, there’s additional complexity regarding how to apply the change in the database schema, and then the changes in both nanoservices. It’s due to these fragmental concerns that some people would consider nanoservices architecture an anti-pattern.</p>
			<p>Maintaining data consistency and coherence across a large number of highly autonomous nanoservices can be a significant challenge that requires careful design and coordination mechanisms. Imagine that there’s one nanoservice for the create operation and another for the update operation; keeping the validation logic of the two nanoservices <a id="_idIndexMarker500"/>consistent can be a challenge.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor230"/>Deciding between microservices and nanoservices</h2>
			<p>The decision of <a id="_idIndexMarker501"/>whether to<a id="_idIndexMarker502"/> have one microservice or multiple nanoservices is a balancing act on different overheads. In general, nanoservices work better if the function is simple and isolated. Any effort to coordinate with other nanoservices should be considered carefully.</p>
			<p>In our previous example, the <strong class="bold">notification service</strong> has a simple objective compared to the other services. It merely translates an internal event into an email that contains an address and requests an email service provider to send the email. This is a suitable candidate to be a nanoservice as its task is simple. If we tweak the <strong class="bold">notification service</strong> so that<a id="_idIndexMarker503"/> it accepts email requests of a universal structure, then it can be an autonomous and independent nanoservice that doesn’t depend on other services.</p>
			<p>The overhead of managing a high number of nanoservices is still a concern to many engineers. However, the emergence of serverless architecture may have addressed this concern by having nanoservices managed by cloud providers. Next, we’ll consider serverless architecture.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor231"/>Serverless</h1>
			<p>Serverless architecture<a id="_idIndexMarker504"/> is a computing style in which engineers are no longer concerned with capacity planning, configuration, management, maintenance, resilience, scalability, physical servers, or virtual servers. There are still servers running but they’re abstracted away.</p>
			<p>Deploying web services used to be an expensive process. Physical server machines (the <strong class="bold">bare-metal</strong> aspect), network <a id="_idIndexMarker505"/>cables, and other accessories need to be purchased with the correct amount of storage, memory, and bandwidth. Operational engineers need to install and keep them on-site in a data center. Physical servers need to be set up correctly and connected to the network. Only then can web services be deployed to and hosted on these servers.</p>
			<p>Bare-metal servers come with not only the initial cost of purchase but also ongoing costs such as electricity, renting from data centers, and visits by engineers to keep your server up and online every time. There are also security concerns as these physical machines are at risk of being damaged or stolen.</p>
			<p>Most engineers aren’t server specialists. Companies may need to either train their engineers to become system administrators, use hardware and network specialist contractors, or hire these specialist engineers to work with the application.</p>
			<p>Every time the system needs to scale, it requires buying new machines or upgrading existing ones. They need to be configured so that they fit the other machines and be used by the application. Sometimes, purchasing and delivering the new machines takes time, so scaling doesn’t happen when it’s needed the most by the system.</p>
			<p>Today, bare-metal servers are still the main choice for systems that require ultra-low latency and high-frequency processing, such as trading systems.</p>
			<p>Serverless architecture aims to solve the issues from the days of bare-metal servers. Let’s explore how.</p>
			<p>The concepts of serverless architecture are deeply rooted in distributed computing. The history of evolution can be<a id="_idIndexMarker506"/> traced back to <strong class="bold">grid computing</strong>, in which computing tasks were distributed<a id="_idIndexMarker507"/> across a network of machines.</p>
			<p>Serverless architecture wasn’t popular in commercial systems until <em class="italic">Amazon</em> launched <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) in 2006. AWS <a id="_idIndexMarker508"/>provides a set of services for businesses to access computing resources over the internet (the <strong class="bold">cloud</strong>). Initially, AWS<a id="_idIndexMarker509"/> offered <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) as virtual<a id="_idIndexMarker510"/> servers to run computation and <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) as<a id="_idIndexMarker511"/> distributed file storage.</p>
			<p>In 2010, <em class="italic">Microsoft</em> launched <strong class="bold">Azure</strong> and <a id="_idIndexMarker512"/>offered cloud services such as AWS, including virtual servers and storage. In 2011, <em class="italic">Google</em> launched <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) to compete <a id="_idIndexMarker513"/>with Microsoft and Amazon when it came to cloud services. AWS, Azure, and GCP remain the three most popular cloud services nowadays, and cloud services are also provided by big companies such as <em class="italic">IBM</em>, <em class="italic">Oracle</em>, <em class="italic">Alibaba</em>, and <em class="italic">Tencent</em>. With the variety of cloud service offerings available, serverless architecture has come to fruition and is still evolving.</p>
			<p>By using cloud services to run applications, the<a id="_idIndexMarker514"/> users are the <strong class="bold">tenants</strong> who subscribe to the service. Tenants rent and use the computing resources in the cloud on an on-demand basis, and hence the cost is arguably more flexible. Cloud services replace the need to procure and provision computer hardware and host it in a data center.</p>
			<p>There are four <a id="_idIndexMarker515"/>major categories of serverless services provided by cloud service providers. Let’s take a closer look.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor232"/>Infrastructure-as-a-Service (IaaS)</h2>
			<p>IaaS offers computing<a id="_idIndexMarker516"/> resources in the cloud <a id="_idIndexMarker517"/>such as virtual servers, storage, and networking on an on-demand basis. It’s like renting an empty space where tenants must configure everything in it.</p>
			<p>Users are given an administrator account so that they can set up the infrastructure via a management console graphical<a id="_idIndexMarker518"/> interface, <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>), or declarative configuration tools<a id="_idIndexMarker519"/> such as <strong class="bold">Terraform</strong>.</p>
			<p>The following typical infrastructures are offered:</p>
			<ul>
				<li><strong class="bold">Virtual servers</strong>: These are virtualized machines that can run anything set up by tenants. Tenants need to specify basic requirements such as CPU, RAM, disk spaces, and network addresses to reach the servers.</li>
				<li><strong class="bold">Secrets management</strong>: There are various situations where we need to keep sensitive data as configuration for an application. This varies from encryption keys and API keys to foreign systems or credentials to access a database. With IaaS, these secrets can be managed separately and are injected into the runtime of the application that runs in, for example, virtual servers. As a result, these secrets can be viewed and managed by fewer people and be abstracted out of the code base. The cloud providers also offered advanced features such as key rotation and expiration for extra security.</li>
				<li><strong class="bold">Distributed file storage</strong>: Cloud service providers offer scalable and durable storage services that can be accessed by applications. They allow tenants to store files of almost any size, and their storage scales as needed. They can replicate the files to multiple locations for redundancy and recovery purposes. They also support file versioning, so it’s possible to retrieve previous versions of the same file object. Finally, they support fine-grained access control to the file and grant time-limited access to specific files for download purposes.</li>
				<li><strong class="bold">Databases</strong>: Managed database services are a big category since there’s a diverse range of choices. Most cloud providers offer relational and NoSQL databases, while some of them offer special types of databases, such as data warehouses. There’s also a list of vendors and versions available that provides a smooth path for applications to move from bare metal to the cloud. They provide managed services that handle infrastructure provisioning, upgrades, scaling, replication, failover, and monitoring. Some of them provide advanced features such as data encryption for handling sensitive information.</li>
				<li><strong class="bold">Messaging</strong>: Like databases, managed messaging services also have a big category of services per cloud provider. There are four main types of messaging available. The first type is a simple queuing service where a message is sent from a sender to a recipient. The second type is the <strong class="bold">Publish/Subscribe</strong> (<strong class="bold">Pub/Sub</strong>) pattern, where <a id="_idIndexMarker520"/>a message is published to a topic via a broker, and all subscribers to the topic receive the message. The third type is streaming, where messages are consumed as a continuous flow as they’re sent. The last type is specialized messaging services, which target specific use cases such as emails and mobile application notifications.<p class="list-inset">Cloud providers abstract<a id="_idIndexMarker521"/> away the complexity of setting up and managing the messaging infrastructure required. These managed services scale up and down on demand and take care of replication, security, and monitoring concerns.</p></li>
			</ul>
			<p>In the next section, we’ll <a id="_idIndexMarker522"/>discuss another big category of services that serve as a platform instead of infrastructure.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor233"/>Platform-as-a-Service (PaaS)</h2>
			<p>PaaS provides engineers <a id="_idIndexMarker523"/>with a cloud platform <a id="_idIndexMarker524"/>so that they can develop, run, and manage applications without setting up the infrastructure themselves. Engineers still need to configure and manage their applications, runtimes, data, and services. The details of the configuration are abstracted away and specified in a declarative way. The cloud provider takes care of the lower-level concerns, such as hardware, operating systems, and network settings.</p>
			<p>These services support specific programming languages and frameworks that enable engineers to focus on the application itself. The service handles the details of provisioning servers, load balancing, scaling, and monitoring.</p>
			<p>Going further with this, if we don’t want to build systems on a platform, perhaps we can simply use and integrate<a id="_idIndexMarker525"/> with the existing software available. This is the topic of the next section.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor234"/>Software-as-a-Service (SaaS)</h2>
			<p>SaaS offers software <a id="_idIndexMarker526"/>that’s ready to be used by<a id="_idIndexMarker527"/> end users or integrated with applications. This service involves tenants just using some software without having to code, manage environments, or even possess technical knowledge. Services in this category range from complete usable software solutions in the cloud and no-code application building to headless systems that can integrate with applications via APIs. In the latter case, tenants are still expected to run their applications via other means and set up a network connection to the SaaS service.</p>
			<p>This category has the largest variety of software applications. Most companies would use at least one SaaS service, and a lot of companies aim to provide SaaS services in this open space.</p>
			<p>Using SaaS gives us a holistic package of business functionalities. This is particularly popular when the business functionalities are necessities but not at the core of the organization.</p>
			<p>Now that we’ve discussed bigger units, such as software, we’re going to look at smaller units, which are functions in the serverless architecture.</p>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor235"/>Function-as-a-Service (FaaS)</h2>
			<p>FaaS allows engineers to<a id="_idIndexMarker528"/> write code and deplo<a id="_idIndexMarker529"/>y it as a function, typically when reacting to events or triggers. These functions don’t store states themselves, but they can make use of other resources, such as file storage and databases. Engineers don’t need to manage any infrastructure. They’re intended to be reusable functions so that higher-order functions can be built on top of other functions.</p>
			<p>Cloud providers scale the runtime environment that’s executing a function based on workload. They also charge based on the usage of the function, which is optimized for cost. Note that some of the services have restrictions, such as maximum execution time, memory usage, and the number of concurrent processes.</p>
			<p>With these services, we can drop our functions as code into the cloud environment for execution. The cloud providers will do the rest for us.</p>
			<p>With that, we’ve covered the four categories of cloud computing services that enable serverless architecture. Next, we’re going to delve into how to build systems using these services. We’ll discuss the benefits of these services and explain how to use them to create a modern, scalable, and <a id="_idIndexMarker530"/>easy-to-maintain system.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor236"/>Benefits</h2>
			<p>The core value of serverless <a id="_idIndexMarker531"/>architecture is infrastructure concerns being abstracted and implemented by cloud providers. Here are the benefits that come with it:</p>
			<ul>
				<li><strong class="bold">Scalability</strong>: Serverless architecture can automatically scale resources based on demand. When the application is under a heavy workload, the cloud provider dynamically allocates resources to handle the increased load and to ensure optimal performance. When the workload decreases or becomes idle, resources are scaled down, resulting in optimized cost and efficient resource utilization.</li>
				<li><strong class="bold">Cost efficiency</strong>: Cloud providers offer a pay-per-use pricing model where tenants are billed by their actual usage. This pricing model optimizes costs and eliminates the need to purchase and maintain idle resources. This is attractive for organizations looking for cost-efficient solutions, particularly startups and small companies.</li>
				<li><strong class="bold">Time to market</strong>: Since it’s a lot quicker to spin up infrastructure to host applications, engineers can focus their time on developing business functions and specific functions. Plus, infrastructure settings now have a more declarative configuration than them having to work on the details of each infrastructure component. This results in a faster development and deployment cycle, the ability to continuously deploy changes, and a shorter time to market.</li>
				<li><strong class="bold">Adaptability and migration</strong>: The range of available services allows engineers to host from big applications to small functions.<p class="list-inset">A lot of companies migrate their systems from bare-metal monolithic applications to virtual servers in the cloud as the first step is to break them down into microservices and functions. This is more cost-efficient and quicker than breaking down monolithic applications first and moving to virtual servers afterward due to the comprehensive support cloud providers offer regarding infrastructure services.</p><p class="list-inset">On the other side of the spectrum, there’s a lot of FaaS support to just write a small function to perform a small task. The wide range of support from cloud providers concerning the size of the application makes it quite easy for engineers to adapt <a id="_idIndexMarker532"/>and migrate existing systems toward serverless architectures.</p></li>
				<li><strong class="bold">Diverse support for business domains</strong>: Serverless architecture is suitable for event-driven and highly scalable systems. It’s commonly used for building microservices, real-time processing systems, web and mobile backends, <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) applications, and<a id="_idIndexMarker533"/> more.<p class="list-inset">There are many ready-to-use SaaS services that engineers could focus on in their business domains. For example, Amazon <strong class="bold">Simple Email Service</strong> (<strong class="bold">SES</strong>) can be used<a id="_idIndexMarker534"/> to send emails to customers, Azure Notification Hub can be used to send push notifications to mobile devices, and Google Cloud IAM can<a id="_idIndexMarker535"/> provide <strong class="bold">multi-factor authentication</strong> (<strong class="bold">MFA</strong>) and reCAPTCHA to verify users’ identities.</p></li>
			</ul>
			<p>However, it’s important to note that serverless architecture might not be suitable for all use cases. Also, many services are provided by each cloud provider, so you need to err on the side of caution to ensure a suitable service is chosen to meet the requirements.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor237"/>Cautions</h2>
			<p>While serverless <a id="_idIndexMarker536"/>architecture offers numerous benefits, there are important drawbacks to consider when adopting this approach:</p>
			<ul>
				<li><strong class="bold">Cold start latency</strong>: FaaS functions are initiated on-demand, meaning that when a function is triggered for the first time or after a period of being idle, it might take a while to start up. This delay is known as a “cold start” and occurs while the cloud provider provisions the necessary resources on the fly.<p class="list-inset">If the function is triggered infrequently, which often causes cold starts, latency is increased for normal requests. If the application is expected to respond without noticeable delays, then PaaS or virtual servers should be used instead.</p></li>
				<li><strong class="bold">Vendor lock-in</strong>: Cloud providers offer a wide range of services, while a lot of them provide proprietary APIs, frameworks, runtime environments, and even languages. While it’s handy to have support from cloud providers in various areas, it’s easy to rely too heavily on a specific cloud provider. This causes vendor lock-in, which makes it challenging to migrate to another provider.<p class="list-inset">This creates barriers to migrating to another cloud provider or switching back to bare-metal infrastructure. While most cloud providers keep their pricing competitive, many companies find it crucial to have the ability to migrate if the situation arises. In response to this, some companies choose to use multi-cloud architecture with data synchronization processes between different cloud platforms.</p></li>
				<li><strong class="bold">Function granularity</strong>: Decomposing an application into smaller functions appropriately is a key aspect of serverless architecture. However, breaking down functionality into excessively fine-grained FaaS functions can result in increased overhead due to the invocation and coordination of numerous functions, leading to release dependency, higher costs, higher latency, and more complex systems.<p class="list-inset">Dividing applications and grouping functions so that they’re the right size is the key factor for a scalable and cost-efficient system. We’re going to explore this aspect in detail while considering our real-life example shortly.</p></li>
				<li><strong class="bold">State management</strong>: FaaS functions are typically designed to be stateless, meaning they don’t retain their memory of previous executions. When there’s a chain of functions and triggers working together with data to share, it poses a challenge to how the state is shared across multiple function invocations. This pattern often involves other IaaS services, such as queues, databases, or in-memory caches. State management must be addressed carefully as it involves concerns such as concurrency, data housekeeping, and compatibility while evolving.</li>
				<li><strong class="bold">Monitoring and debugging</strong>: Troubleshooting and monitoring serverless applications can be more complex than traditional architectures. When a business workflow is distributed among multiple functions and processes, it becomes challenging to diagnose, reproduce, and resolve issues. We should invest in observability tools such as log aggregation, monitoring, dashboards, and alerting. We also <a id="_idIndexMarker537"/>need to design the system so that it can handle errors gracefully.</li>
				<li><strong class="bold">Cost management</strong>: While serverless architectures often optimize cost due to the pay-per-use model, it’s essential to monitor and optimize resource consumption. Granular billing based on usage can lead to unexpected costs if the applications are designed inefficiently or experience unexpected spikes in traffic. This can be caused by inefficient system design or simply because the pattern of usage has changed over time. Adequate monitoring, performance testing, and optimization strategies need to be in place to control costs effectively. This is also an opportunity to discover system inefficiency so that the system can improve with new findings.</li>
				<li><strong class="bold">Long-running processes</strong>: FaaS functions are typically execution time limits that are imposed by cloud providers. In other words, FaaS functions are meant to be small and executed quickly. If an operation requires significant processing time or must run continuously, it might be better to look for PaaS or IaaS alternatives, such as virtual servers. Careful consideration is needed to decide on the appropriate approach.</li>
				<li><strong class="bold">Security and compliance</strong>: Serverless architectures introduce new security considerations. Ensuring secure function invocations, managing access controls, and protecting sensitive data within the serverless environment are critical. Compliance with regulations and industry standards should be thoroughly evaluated to ensure proper security measures are in place.</li>
				<li><strong class="bold">Non-functional requirements (NFRs)</strong>: Even cloud providers offer a wide range of services that take away the concerns of infrastructure that are often parts of the NFRs. Our choice of serverless service needs to meet these requirements. Sometimes, it’s hard to be in control of meeting these requirements since engineers can only configure the desired resources, and in the end, it’s the platform that provisions the resources to meet the desired resources as configuration.<p class="list-inset">In an extreme case, it might be justified to go back to bare-metal servers to have full control of the hardware and network that could fulfill the high-end NFRs.</p></li>
			</ul>
			<p>By understanding and addressing these cautions, organizations can make informed decisions when adopting serverless architecture and mitigate potential risks associated with its<a id="_idIndexMarker538"/> implementation. Next, we’re going to run through an exercise of adopting serverless architecture while utilizing the real-life example provided in this chapter.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor238"/>Adopting serverless architectures in our real-life example</h2>
			<p>Consider the <a id="_idIndexMarker539"/>same real-life example that we used earlier in this chapter, where households exchange services with one another. Previously, we identified three potential microservices. Let’s recapitulate what they are:</p>
			<ul>
				<li><strong class="bold">Household service</strong>: Masters the records of households</li>
				<li><strong class="bold">Contract service</strong>: Maintains the workflow of contract negotiation from drafted to fully exercised</li>
				<li><strong class="bold">Notification service</strong>: Sends proxy notification requests to email service providers</li>
			</ul>
			<p><em class="italic">Figure 6</em><em class="italic">.4</em> shows how these four microservices communicate:</p>
			<div><div><img src="img/B21737_06_31.jpg" alt="Figure 6.4 – Recap example of updated microservice communication" width="1025" height="627"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Recap example of updated microservice communication</p>
			<p>In this exercise, we’ll assume that we want to have the system hosted in AWS. We need to decide which <a id="_idIndexMarker540"/>cloud service to use and what the desired setup is.</p>
			<h3>Function granularity and choosing the computing service</h3>
			<p>Previously, we mentioned we need to be cautious about function granularity as this will impact system efficiency and cost. We’re going to review how these services can be executed and which computing service we might use.</p>
			<p>The <strong class="bold">household service</strong> provides the classic <strong class="bold">Create, Read, Update, and Delete</strong> (<strong class="bold">CRUD</strong>) operations<a id="_idIndexMarker541"/> and connects to a relational database for persistent storage. These operations are highly cohesive as they cover the life cycle of households. They all assume the same database schema. Moreover, to ensure there’s a way for the schema to evolve reliably, it seems reasonable to have the <strong class="bold">household service</strong> own its schema. This implies that incremental database migration tools such as <strong class="bold">Flyway</strong> can <a id="_idIndexMarker542"/>be used and that the<a id="_idIndexMarker543"/> incremental <strong class="bold">Data Definition Language</strong> (<strong class="bold">DDL</strong>) files should be hosted together with the source code that translates into the <strong class="bold">Structured Query Language</strong> (<strong class="bold">SQL</strong>) commands <a id="_idIndexMarker544"/>to perform the CRUD operations of households.</p>
			<p>The incremental database migration runs as part of the service startup. The migration tool will check if the latest DDL file is of the same version as the version registered in the migration history records. If it’s the same version, migration will finish as a <strong class="bold">no operation</strong> (<strong class="bold">no-op</strong>); if<a id="_idIndexMarker545"/> the script version is higher than the recorded version, then the tool will run the incremental script until the version matches again. With this setup, any evolution in the database schema is released in one deployment action, with the schema changes and corresponding code changes going in a synchronized manner.</p>
			<p>Moreover, there are three operations (Create, Update, and Delete) that would need an updated household event to be published. These operations would assume a specific message format. If the message format is going to change, it may affect all three operations. This also suggests they should be grouped as one deployable artifact to ensure smooth, reliable changes.</p>
			<p>On the contrary, if the CRUD operations are separated into four FaaS functions or nanoservices, any change in data structure or message structure would require a coordinated release<a id="_idIndexMarker546"/> of these functions. This means function coupling, release dependency, downtime, and the risk of partial deployment failures.</p>
			<p>Therefore, in this example, the <code>GET</code>, <code>PUT</code> (create and update), and <code>DELETE</code> verbs.</p>
			<p>The messaging technology in this example will be Kafka. We intend to publish events on a normal topic and a compacted topic. The normal topic is used to announce creation, updates, and soft deletion, while the compacted topic is used as a last-value queue to keep the last snapshot of the Household records.</p>
			<p>The next consideration is about which serverless computing the <strong class="bold">household service</strong> should use. Here’s an example of using AWS:</p>
			<div><div><img src="img/B21737_06_4.jpg" alt="Figure 6.5 – The household service using AWS" width="1516" height="1133"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – The household service using AWS</p>
			<p>We can use <a id="_idIndexMarker547"/>Amazon <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) to run the <strong class="bold">household service</strong>. This requires <a id="_idIndexMarker548"/>us to implement the following infrastructure setup:</p>
			<ul>
				<li>Specify<a id="_idIndexMarker549"/> the AWS region</li>
				<li>Create a <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) and subnet</li>
				<li>Create a role in<a id="_idIndexMarker550"/> AWS <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) for EKS to assume a role for the EKS cluster</li>
				<li>Define the EKS cluster that makes use of the VPC, subnet, and the IAM role</li>
				<li>Create a security group for the EKS cluster</li>
				<li>Attach the EKS cluster and EKS service policies to the IAM role</li>
				<li>Create an<a id="_idIndexMarker551"/> Amazon <strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) with PostgreSQL and its subnet</li>
				<li>Configure the Kubernetes provider</li>
				<li>Configure the Kubernetes namespace and config map</li>
				<li>Configure the Kubernetes secrets to be imported from AWS Secrets Manager, such as passwords</li>
				<li>Configure the ingress (incoming traffic route) and <strong class="bold">Application Load Balancer</strong> (<strong class="bold">ALB</strong>) so that<a id="_idIndexMarker552"/> the requests can reach the REST endpoints</li>
				<li>Configure <a id="_idIndexMarker553"/>Amazon <strong class="bold">Managed Kafka Service</strong> (<strong class="bold">MSK</strong>) on Kafka topics and security groups to allow an EKS pod to publish and consume messages</li>
			</ul>
			<p>Given that the <strong class="bold">household service</strong> has the infrastructure set up, we can start a Kotlin project. There are many <a id="_idIndexMarker554"/>ready-to-go project creators available on the internet, including the following:</p>
			<ul>
				<li>Spring Boot</li>
				<li>Ktor</li>
				<li>HTTP4K</li>
				<li>Vert.x</li>
			</ul>
			<p>These tools all create a skeleton project that can be built using their respective server frameworks. In this example, we’re using Ktor as the server framework and REST endpoint routing. In Ktor, endpoint routing is defined like so:</p>
			<pre class="source-code">
routing {
    get("/households/{name}") {
     ...
    }
    put("/households/{name}") {
     ...
    }
    delete("/households/{name}") {
     ...
    }
}</pre>			<p>We’re using a declarative configuration as Kotlin code, and we’re expected to define a payload format and set up corresponding serialization by using Ktor content negotiation.</p>
			<p>The Kafka topics can be defined using Terraform, which provides a standard declarative format for specifying infrastructure. The normal topic will need to set a retention policy that determines how long the message should remain on the topic.</p>
			<p>The compacted topic has a different setup. The message in the compacted topic should be retained for as long as possible. The newer message of the same key will replace the older one by compacting the logs. The cleanup policy of compacted topics should be set to <code>"compact"</code>, with the retention period set to <code>–1</code>. Here’s an example of how a compacted topic is specified<a id="_idIndexMarker555"/> in Terraform:</p>
			<pre class="source-code">
config {
    cleanup_policy = "compact"
    retention_bytes = -1
    retention_ms = -1
    ...
}</pre>			<p>The updated Household records are going to be sent to both topics. This is illustrated in the following code using <a id="_idIndexMarker556"/>the <strong class="bold">Apache </strong><strong class="bold">Kafka API</strong>:</p>
			<pre class="source-code">
topicProducer.send(ProducerRecord(topic, household))
compactedTopicProducer.send(ProducerRecord(topic, key, household))</pre>			<p>The message that’s sent to the compacted topic contains a key to identify and remove older messages of the same key. In this instance, household names are used as keys.</p>
			<p>The <strong class="bold">contract service</strong> provides controlled operations on the workflow regarding contract negotiation and contract exercise. It uses some form of persistent storage to keep a local copy of households and to maintain the state of the contract in its workflows. It uses a serverless computing service similar to that of the <strong class="bold">household service</strong>, as shown in <em class="italic">Figure 6</em><em class="italic">.6</em>:</p>
			<div><div><img src="img/B21737_06_5.jpg" alt="Figure 6.6 – The contract service using AWS" width="1536" height="1213"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – The contract service using AWS</p>
			<p>The <strong class="bold">contract service</strong> receives<a id="_idIndexMarker557"/> Household records from the topics published by the <strong class="bold">household service</strong>. Initially, it consumes all messages in the compacted topic to build its local cache of households and subsequently receives updates of Household records from the normal topic. It also sends notification requests to a topic to be consumed by the <strong class="bold">notification service</strong>.</p>
			<p>In this setting, when a household is updated while the <code>household-v1</code> and <code>household-snapshot-v1</code> topics. The event in the queue is now waiting for the <strong class="bold">contract service</strong> to come back up. One instance of the <strong class="bold">contract service</strong> becomes available and takes this event for further processing.</p>
			<p>The <strong class="bold">contract service</strong> uses the same AWS components as the <strong class="bold">household service</strong> in terms of IAM, Secrets Management, ALB, Kubernetes, and database services.</p>
			<p>On the other hand, the <strong class="bold">notification service</strong> is simple in that it takes a notification request and proxies the request to an email service provider. There’s no tight requirement to send emails instantly and it’s OK to have the emails be sent a couple of minutes late. There’s no need to maintain state either as the request message already contains the household email addresses and the content of the messages.</p>
			<p>This is a suitable candidate <a id="_idIndexMarker558"/>for a FaaS service. While we can use AWS Lambda to meet the requirements, a nanoservice is an equally suitable choice. For this reason, the <strong class="bold">notification service</strong> is called <strong class="bold">Email notifier</strong> in physical deployment, as shown in <em class="italic">Figure 6</em><em class="italic">.7</em>:</p>
			<div><div><img src="img/B21737_06_6.jpg" alt="Figure 6.7 – The notification service using AWS" width="1360" height="802"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – The notification service using AWS</p>
			<p>We can configure the function so that it’s triggered by a new message in the Kafka topic. The function converts a request into email format and passes it to Amazon SES so that it can send the email.</p>
			<p>The function must be configured to use a constant Kafka consumer group so that multiple instances of the same function don’t consume the same message and effectively behave as a queue.</p>
			<p>The function is stateless, and AWS provides all the means to connect to SQS and SES. There’s no concern regarding a cold startup occurring as there’s no tight latency requirement. The function also scales automatically based on traffic and is controlled by AWS. Similarly, this function uses IAM and Secrets Management to control accessible resources and secrets.</p>
			<p>So far, we’ve covered the basic principles of serverless architecture, in conjunction with what’s offered by the four major categories of services provided by major cloud providers. We’ve also discussed the benefits and cautions of applying serverless architecture. Finally, we ran an exercise in adopting serverless architecture for the real-life example specified<a id="_idIndexMarker559"/> in this chapter.</p>
			<p>Next, we’re going to briefly cover the microfrontend architecture, which works similarly to microservices in principle.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor239"/>Microfrontends</h1>
			<p>The microfrontend <a id="_idIndexMarker560"/>architecture aims to enhance modularity, scalability, and autonomy by breaking down the UI into smaller, self-contained frontend modules.</p>
			<p>The term <em class="italic">microfrontend</em> first appeared in 2016 under Thoughtworks Technology Radar with the recommendation of <em class="italic">Assess</em>. It’s often compared to the concept of microservices on the backend. The microfrontend architecture promotes decomposing the frontend into independently deployable and maintainable units, each responsible for a specific part of the UI.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor240"/>Same symptoms from the days of monolithic applications</h2>
			<p>A traditional monolithic<a id="_idIndexMarker561"/> frontend application has a single code base that handles the entire UI. Usually. There’s more than one team of engineers working on it, which causes code conflicts, release dependency, slow build time, limited autonomy for each team, and challenges in scaling and mai<a id="_idTextAnchor241"/><a id="_idTextAnchor242"/>ntaining large applications. This situation is the same regarding microservices, except this happens in the frontend.</p>
			<p>The microfrontend architecture addresses these issues by enabling different teams to work independently on distinct parts of the UI, allowing them to choose their own technologies, frameworks, and release cycles.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor243"/>Many small frontend modules as individual applications</h2>
			<p>In a microfrontend<a id="_idIndexMarker562"/> architecture, the UI is composed of multiple frontend modules. Each module is an application that can be developed, tested, deployed, and scaled independently. Like microservices, one frontend module should be owned by one – and only one – team, but as a common library.</p>
			<p>Each module is a grouping of cohesive functionalities. Considering the real-life example provided in this chapter, there should be two frontend applications:</p>
			<ul>
				<li>The <strong class="bold">household application</strong>, which manages household account creation, updates, and deletion. Each household can manage its own account details via this application. This application primarily communicates with the <strong class="bold">household service</strong> in the backend.</li>
				<li>The <strong class="bold">contract application</strong>, which allows two households to progress from a draft contract to an agreement. It supports drafting a contract, similar to the screens we saw in <a href="B21737_05.xhtml#_idTextAnchor125"><em class="italic">Chapter 5</em></a>. The two households that are involved can agree to the contract or amend it until both households agree with the details. It also tracks how the agreed-upon contract is exercised by the two households involved. Regarding the polymorphic nature of the different services that can be mentioned in the contract, as described in <a href="B21737_03.xhtml#_idTextAnchor067"><em class="italic">Chapter 3</em></a>, there may be multiple screens for households to report the status of the services that are exercised based on the contract. This application primarily communicates with the <strong class="bold">contract service</strong> in the backend.</li>
			</ul>
			<p>All these applications are bundled as self-contained artifacts that can be launched on their own, allowing separate teams to focus on their business domains.</p>
			<p>However, there should be one more application that integrates with all other applications. This application doesn’t contain business logic; instead, it’s merely an over-delegation module that typically provides a menu for users to access other applications. This is the application that creates a unified UI at build time or runtime, depending on whether it’s a web or mobile platform.</p>
			<p>Overall, these frontend applications can be illustrated like so, together with the backend services they communicate with:</p>
			<div><div><img src="img/B21737_06_7.jpg" alt="Figure 6.8 – The frontend and backend communication of the real-life example" width="1637" height="636"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – The frontend and backend communication of the real-life example</p>
			<p>In the preceding diagram, each frontend application has a primary microservice in the backend to communicate with. Each frontend application communicates with other frontend applications. Microservices also communicate with each other. The roles and responsibilities of each frontend application and each microservice are well-defined and clear.</p>
			<p>This architecture enables each frontend application and its primary microservice to be owned by only <a id="_idIndexMarker563"/>one team. This is coherent with the idea of organizing teams by business function, as mentioned in <a href="B21737_01.xhtml#_idTextAnchor013"><em class="italic">Chapter 1</em></a>.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor244"/>Communication among frontend modules</h2>
			<p>Having communication<a id="_idIndexMarker564"/> and coordination among the frontend modules is critical in a microfrontend architecture. There are various techniques and patterns to facilitate this, such as asynchronous messaging, event-driven architectures, or shared state management.</p>
			<p>In our example, a shared state managed by the collection of backend microservices is used to serve each frontend module.</p>
			<p>These techniques enable seamless integration and collaboration between different modules while maintaining loose coupling and encapsulation. A well-integrated UI with a microfrontend architecture brings a connected and consistent user experience.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor245"/>Design systems for consistent user experiences</h2>
			<p>Visualization is <a id="_idIndexMarker565"/>an essential element of any frontend application. While having smaller frontend applications autonomously run by their responsible teams, it’s paramount to ensure the integration UI has a consistent look and feel. A design system provides the common UI components, such as buttons, checkboxes, text fields, and interaction styles, that align all frontend modules so that they behave the same way. In that sense, end users have a seamless experience with little learning required when navigating to another frontend module.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor246"/>Benefits</h2>
			<p>The microfrontend <a id="_idIndexMarker566"/>architecture offers several benefits to engineering teams and organizations. By allowing teams to work autonomously, it promotes faster development cycles, easier maintenance, and the ability to adopt new technologies and frameworks without affecting the entire application. It results in higher productivity and quicker time to market. It also enables scalability by allowing individual modules to be scaled independently, providing flexibility in managing traffic and resources.</p>
			<p>Furthermore, the microfrontend architecture advocates code reusability by reusing UI components brought from design systems that can be shared across multiple applications. This can lead to improved consistency, reduced duplication, reduced user learning, and increased productivity in frontend development.</p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor247"/>Challenges</h2>
			<p>While the microfrontend<a id="_idIndexMarker567"/> architecture offers numerous advantages, it also introduces complexities and challenges such as module communication, versioning, and orchestration when integrating all frontend modules into one application. Successful implementation requires careful planning, design considerations, and selecting the appropriate tools and frameworks.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor248"/>Variations among the web and other platforms</h2>
			<p>The web and other<a id="_idIndexMarker568"/> platforms have a sl<a id="_idTextAnchor249"/>ight difference when applying the microfrontend architecture. The web platform can achieve independent release as frontend modules are integrated primarily using hyperlinks.</p>
			<p>Mobile and desktop applications are trickier because they need to generate a monolithic artifact for users to download and install. Releasing a frontend module would require regenerating the monolithic artifact and bumping the build versions. Some organizations might choose to release the application in a specific cadence to avoid excessive updates being required by the application.</p>
			<p>Overall, the microfrontend architecture is a powerful paradigm that empowers engineering teams to create scalable, modular, and maintainable frontend applications by decomposing the frontend into smaller, independent modules. By emb<a id="_idTextAnchor250"/>racing this architectural style, organizations can achieve greater flexibility, agility, and scalability in their frontend development processes.</p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor251"/>An overall perspective</h1>
			<p>So far, we’ve discussed how monolithic applications evolve from a historical perspective. We’ve discussed that the monolithic architecture evolves into the SOA, where one big application is decomposed into chunks of smaller applications. Then, the era of microservice and microfrontend architectures begins as they’re broken down into even smaller applications.</p>
			<p>Finally, the serverless architecture emerges, which allows a single function to be executed as a unit in the cloud infrastructure. Meanwhile, it still supports bigger cloud applications and lets them run.</p>
			<p>Putting them all together into one perspective, we can start to see how the sizes differ in each architecture:</p>
			<div><div><img src="img/B21737_06_8.jpg" alt="Figure 6.9 – A size comparison among covered architecture styles" width="1650" height="469"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – A size comparison among covered architecture styles</p>
			<p>It’s worth noting that serverless architecture can adapt to all sizes. It can even adapt to monolithic applications, though usually, the first step is to break the monolithic application into smaller services. This is a typical example of “<em class="italic">First make the change easy, then make the </em><em class="italic">easy change.</em>”</p>
			<p class="callout-heading">Note</p>
			<p class="callout">“<em class="italic">First make the change easy, then make the easy change</em>” is a quote attributed to Kent Beck, a<a id="_idIndexMarker569"/> pioneer of <strong class="bold">Extreme Programming</strong> (<strong class="bold">XP</strong>) and Agile methodologies.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor252"/>Summary</h1>
			<p>In this chapter, we covered a few cases where monolithic architecture is justified. We discussed the basic principles of how to divide a monolithic application into microservices and nanoservices, as well as how to detect when the division isn’t right. We used a real-life example to delve into the thought process of designing microservices and nanoservices.</p>
			<p>Then, we introduced the serverless architecture and the most popular cloud providers. We covered the four major categories of cloud computing services (IaaS, PaaS, SaaS, and FaaS) and discussed the benefits and cautions when <a id="_idTextAnchor253"/>using serverless architecture. After, we conducted an exercise where we adopted serverless archi<a id="_idTextAnchor254"/>tecture and chose appropriate cloud computing services to meet the requirements.</p>
			<p>Finally, we briefly covered the microfrontend architecture, where a monolithic frontend application is broken down into frontend applications. We used the same real-life example from before to illustrate the decomposition and how each frontend module communicates with the backend components. Finally, we covered the need for design systems to ensure a consistent user experience and briefly mentioned the benefits and challenges of using the microfrontend architecture.</p>
			<p>In the next chapter, we’re going to dive into the practice of separating concerns using selected methodologies to help drive us toward efficient, scalable, and maintainable applications.</p>
		</div>
	</div></div></body></html>