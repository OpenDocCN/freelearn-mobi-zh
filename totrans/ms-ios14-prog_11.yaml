- en: '*Chapter 11*: Adding Media to Your App'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A lot of the apps that people use every day make use of media in some way. Some
    apps show photos and videos in a user's feed. Other apps focus on playing audio
    or video, while there are also apps that allow users to record media and share
    it with their peers. You can probably name at least a couple of very well-known
    apps that make use of such media in one way or the other.
  prefs: []
  type: TYPE_NORMAL
- en: Because media has such a significant presence in people's daily lives, it's
    good to know how you can integrate media into your own apps. iOS has excellent
    support for media playback and offers several different ways to create and consume
    different types of media. Some ways provide less flexibility but are more straightforward
    to implement. Others are more complex but provide significant power to you as
    a developer.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about several ways to play and record media
    on iOS. You will learn how to play and record video, play audio, and take pictures,
    and you''ll even learn how to apply filters to images with Apple''s Core Image
    framework. This chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Playing audio and video
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recording video and taking pictures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Manipulating photos with Core Image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a great foundation that you can build
    on to create engaging experiences for your users, allowing them to not only view
    content but also to create their own content in your app.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The code bundle for this chapter includes two starter projects called `Captured_start`
    and `MediaPlayback_start`. You can find them in the code bundle repository:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition](https://github.com/PacktPublishing/Mastering-iOS-14-Programming-4th-Edition)'
  prefs: []
  type: TYPE_NORMAL
- en: Playing audio and video
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To make playing audio and video files as simple and straightforward as can be,
    Apple has created the `AVFoundation` framework. This framework contains a lot
    of helper classes that provide very low-level control over how iOS plays audio
    and video files. You can use `AVFoundation` to build a rich, custom media player
    with as many features as you need for your purpose.
  prefs: []
  type: TYPE_NORMAL
- en: If you're looking for a simpler way to integrate media into your app, the `AVKit`
    framework might be what you need. `AVKit` contains several helpers that build
    upon the `AVFoundation` components to provide an excellent default player that
    supports many features, such as subtitles, AirPlay, and more.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you will learn how to implement a simple video player with
    `AVPlayerViewController` from the `AVKit` framework. You will also implement a
    more complex audio player with `AVFoundation` components that play audio in the
    background and display, on the lock screen, the audio track currently being played.
  prefs: []
  type: TYPE_NORMAL
- en: To follow along with the examples, you should open the `MediaPlayback_start`
    project in this chapter's code bundle. The starter app contains a straightforward
    interface with a tab bar and two pages. You will implement a video player on one
    page, and the audio player on the other page. The audio page comes with some predefined
    controls and actions that you will implement later.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple video player
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing you need to do to implement a video player is to obtain a video
    file. You can use any video that is encoded in the `h.264` format. A good sample
    video is the **Big Buck Bunny** sample movie that was created by the Blender Foundation.
    You can find this video at the following URL: [http://bbb3d.renderfarming.net/download.html](http://bbb3d.renderfarming.net/download.html).
    If you want to use this video to practice with, make sure to download the 2D version
    of the video.'
  prefs: []
  type: TYPE_NORMAL
- en: As stated before, you will implement the video player using `AVPlayerViewController`.
    This view controller provides a convenient wrapper around several components from
    `AVFoundation`, and also provides default video controls, so you don't have to
    build your entire video player from scratch, as you will do for the audio player
    later.
  prefs: []
  type: TYPE_NORMAL
- en: '`AVPlayerViewController` is highly configurable, which means that you can choose
    whether the player supports AirPlay, shows playback controls, whether it should
    be full screen when a video plays, and more. For a complete list of configurable
    options, you can refer to Apple''s `AVPlayerViewController` documentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have found your test video, you should add it to the `MediaPlayback`
    project and ensure that the video is added to the app target. You can follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on your project.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on your target.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Build Phases**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Expand **Copy Bundle Resources**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click **+** and select your file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'After doing this, open `VideoViewController.swift` and add the following line
    to import `AVKit`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You should also add a property to `VideoViewController` to hold on to your
    video player instance. Add the following line to the `VideoViewController` class
    to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `AVPlayerViewController` is a `UIViewController` subclass, you should
    add it to `VideoViewController` as a child view controller. Doing this will make
    sure that `VideoViewController` forwards any view controller life cycle events,
    such as `viewDidLoad()`, along with any changes in trait collections and more
    to the video player. To do this, add the following code to the `viewDidLoad()`
    method in `VideoViewController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The previous code snippet adds the video player to the video view controller
    as a child view controller. When you add a view controller as a child view controller,
    you must always call `didMove(toParent:)` on the child controller to make sure
    that it knows that it has been added as a child view controller to another view
    controller. After adding the video player as a child view controller, the video
    player's view is added as a subview for the video view controller, and some constraints
    are set up to position the player view.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all you need to do to create an instance of the video player and make
    it appear in your view controller. The last step is to obtain a reference to your
    video file, create an `AVPlayer` that has a reference to the video file, and assign
    it to the player. Add the following code to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code looks for a video file called `samplevideo.mp4` and obtains
    a URL for that file. It then creates an instance of `AVPlayer` that points to
    that video file and assigns it to the video player. The `AVPlayer` object is responsible
    for playing the video file. The `AVPlayerViewController` instance uses the `AVPlayer`
    instance to play the video and manages the actual playback of the video internally.
  prefs: []
  type: TYPE_NORMAL
- en: If you run your app after adding the player this way, you will find that the
    video plays perfectly well, and that you have access to all the controls you might
    need. This is a great demonstration of how simple it is to add basic media integration
    to your app. The next step is a little more complex. You will directly use an
    `AVAudioPlayer` instance to play an audio file that is controlled through several
    custom media controls. The player will even play audio in the background and integrate
    with the lock screen to show information about the current file. In other words,
    you will build a simple audio player that does everything a user would expect
    it to do.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: When launching in the simulator, `AVKit` and large movie files can take some
    time to load up. Try it on a real device.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an audio player
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before you can implement your audio player, you will need to obtain some `.mp3`
    files you wish to use in your player. If you don't have any audio files on your
    computer, you can get some files from The Free Music Archive website, available
    at [https://freemusicarchive.org/](https://freemusicarchive.org/)about, to obtain
    a couple of free songs that you would like to use for playback. Make sure to add
    them to the `MediaPlayer` Xcode project and ensure that they are included in the
    app target.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will build the audio player using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Implement the necessary controls to start and stop the player and navigate to
    the next and previous songs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Implement the time scrubber.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read the file's metadata and show it to the user.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user interface, outlets, and actions are already set up, so make sure to
    familiarize yourself with the existing code before following along with the implementation
    of the audio player.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing basic audio controls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before you implement the audio player code, you will need to do a little bit
    of housekeeping. To be able to play audio, you need a list of the files that the
    player will play. In addition to this list, you also need to keep track of what
    song the user is currently playing, so you can determine the next and previous
    songs. Lastly, you also need to have the audio player itself. Instead of using
    a pre-built component, you will build your own audio player using an `AVAudioPlayer`
    object. `AVAudioPlayer` is perfect for implementing a simple audio player that
    plays a couple of local `.mp3` files. It offers some convenient helper methods
    to easily adjust the player's volume, seek to a specific timestamp in the song,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the following properties in `AudioViewController.swift`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, don''t forget to add the import:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to replace the files array with the filenames that you use for your
    own audio files. `audioPlayer` does not have a value yet at this point. You will
    set up the audio player next.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before you can play audio, you need to obtain a reference to a media file and
    provide this reference to an `AVAudioPlayer` object. Any time you want to load
    a new media file, you will have to create a new instance of the audio player,
    since you can''t change the current file once a file is playing. Add the following
    helper method to `AudioViewController` to load the current track and create an
    `AVAudioPlayer` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This method reads the filename for the current track and retrieves the local
    URL for it. This URL is then used to create and set the `audioPlayer` property
    on `AudioViewController`. The view controller is also assigned as the delegate
    for the audio player. You won''t implement any of the delegate methods just yet,
    but you can add the following extension to make `AudioViewController` conform
    to the `AVAudioPlayerDelegate` protocol to ensure your code compiles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s call `loadTrack()` on `viewDidLoad()` to instantiate `audioPlayer`
    and load the first song. Add the following method to `AudioViewController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: You will implement one of the `AVAudioPlayerDelegate` methods when you add support
    for navigating to the next and previous tracks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following two methods to the audio view controller to add support for
    playing and pausing the current audio file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'These methods are relatively straightforward. They call the audio player''s
    `play()` and `pause()` methods and update the button''s label, so it reflects
    the current player state. Add the following implementation for `playPauseTapped()`
    so that the play and pause methods get called when the user taps the play/pause
    button:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the app now, you can tap the play/pause button to start and stop
    the currently playing file. Make sure your device is not in silent mode, because
    the audio for your app is muted when the device is in silent mode. You will learn
    how to fix this when you implement the ability to play audio in the background.
    The next step is to add support for playing the next and previous tracks. Add
    the following two implementations to `AudioViewController` to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code adjusts the current track index, loads the new track, and
    immediately plays it. Note that every time the user taps on the next or previous
    button, a fresh audio player has to be created by calling `loadTrack()`. If you
    run the app now, you can play audio, pause it, and skip to the next or previous
    tracks.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you allow a full song to play, it will not yet advance to the next song
    afterward. To implement this, you need to add an implementation for the `audioPlayerDidFinishPlaying(_:successfully:)`
    method from `AVAudioPlayerDelegate`. Add the following implementation to call
    `nextTapped()`, so the next song automatically plays when the current song finishes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that the first features are implemented, the next step is to implement the
    time scrubber that shows the current song's progress and allows the user to adjust
    the playhead's position.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the time scrubber
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The user interface for the audio player app already contains a scrubber that
    is hooked up to the following three actions in the view controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sliderDragStart()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sliderDragEnd()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sliderChanged()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When an audio file is playing, the scrubber should automatically update to reflect
    the current position in the song. However, when a user starts dragging the scrubber,
    it should not update its position until the user has chosen the scrubber's new
    position. When the user is done dragging the scrubber, it should adjust itself
    based on the song's progress again. Any time the value for the slider changes,
    the audio player should adjust the playhead, so the song's progress matches that
    of the scrubber.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unfortunately, the `AVAudioPlayer` object does not expose any delegate methods
    to observe the progress of the current audio file. To update the scrubber regularly,
    you can implement a timer that updates the scrubber to the audio player''s current
    position every second. Add the following property to `AudioViewController`, so
    you can hold on to the timer after you have created it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, add the following two methods to `AudioViewController` as a convenient
    way to start the timer when the user starts dragging the scrubber, or when a file
    starts playing, and stop it when a user stops dragging the scrubber or to preserve
    resources when the playback is paused:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Add a call to `startTimer()` in the `startPlayback()` method and a call to
    `stopTimer()` in the `pausePlayback()` method. If you run the app after doing
    this, the scrubber will immediately begin updating its position when a song starts
    playing. However, scrubbing does not work yet. Add the following implementations
    for the scrubber actions to enable manual scrubbing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The preceding methods are relatively simple, but they provide a very powerful
    feature that immediately makes your homemade audio player feel like an audio player
    you might use every day. The final step for implementing the audio player's functionality
    is to display metadata about the current song.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying song metadata
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most `.mp3` files contain metadata in the form of ID3 tags. These metadata tags
    are used by applications such as iTunes to extract information about a song and
    display it to the user, as well as to categorize a music library or filter it.
    You can gain access to an audio file's metadata through code by loading the audio
    file into an `AVPlayerItem` object and extracting the metadata for its internal
    `AVAsset` instance. An `AVAsset` object contains information about a media item,
    such as its type, location, and more. When you load a file using an `AVPlayerItem`
    object, it will automatically create a corresponding `AVAsset` object for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'A single asset can contain loads of metadata in the metadata dictionary. Luckily,
    Apple has captured all of the valid ID3 metadata tags in the `AVMetadataIdentifier`
    object, so once you have extracted the metadata for an `AVAsset` instance, you
    can loop over all of its metadata to filter out the data you need. The following
    method does this, and sets the extracted values on the `titleLabel` variable of
    `AudioViewController`, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Make sure to add a call to this method from `loadTrack()`, and pass the audio
    file's URL that you obtain in `loadTrack()` to `showMetadataForURL(_:)`. If you
    run your app now, your basic functionality should be all there. The metadata should
    be shown correctly, the scrubber should work, and you should be able to skip songs
    or pause the playback.
  prefs: []
  type: TYPE_NORMAL
- en: Even though your media player seems to be pretty much done at this point, did
    you notice that the music pauses when you send the app to the background? To make
    your app feel more like a real audio player, you should implement background audio
    playback and make sure that the currently playing song is presented on the user's
    lock screen, similar to how the native music app for iOS works. This is precisely
    the functionality you will add next.
  prefs: []
  type: TYPE_NORMAL
- en: Playing media in the background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'On iOS, playing audio in the background requires special permissions that you
    can enable in your app''s **Capabilities** tab. If you enable the **Background
    Modes** capability, you can select the **Audio, AirPlay, and Picture in Picture**
    option to make your app eligible for playing audio in the background. The following
    screenshot shows the enabled capability for playing audio in the background:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 − Background Modes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.1_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 − Background Modes
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to add proper support for background audio playback, there are
    three features you need to implement:'
  prefs: []
  type: TYPE_NORMAL
- en: Set up an audio session, so audio continues playing in the background.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Submit metadata to the "now playing" info center.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Respond to playback actions from remote sources, such as the lock screen.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can set up the audio session for your app with just two lines of code.
    When you create an audio session, iOS will treat the audio played by your app
    slightly differently; for instance, your songs will play even if the device is
    set to silent. It also makes sure that your audio is played when your app is in
    the background, if you have the proper capabilities set up. Add the following
    code to `viewDidLoad()` to set up an audio session for the app:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The second feature to add is to supply information about the currently playing
    track. All information about the currently playing media file should be passed
    to the `MPNowPlayingInfoCenter` object. This object is part of the `MediaPlayer`
    framework and is responsible for showing the user information about the currently
    playing media file on the lock screen and in the command center. Before you pass
    information to the "now playing" info center, make sure to import the `MediaPlayer`
    framework at the top of the `AudioViewController.swift` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the following line of code to `viewDidLoad()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the documentation for `MPNowPlayingInfoCenter`, Apple states that you should
    always pass the most recent "now playing" information to the info center when
    the app goes to the background. To do this, the audio view controller should listen
    to the `UIApplication.didEnterBackgroundNotification` notification, so it can
    respond to the app going to the background. Add the following implementation for
    the `updateNowPlaying()` method to `AudioVideoController`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code configures a dictionary with metadata about the currently
    playing file and passes it to the "now playing" info center. This method is called
    automatically when the app goes to the background, but you should also update
    the "now playing" information when a new song begins playing. Add a call to `updateNowPlaying()`
    in the `loadTrack()` method to make sure the "now playing" information is updated
    whenever a new track is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next and final step is to respond to remote commands. When the user taps
    the play/pause button, next button, or previous button on the lock screen, this
    is sent to your app as a remote command. You should explicitly define the handlers
    that should be called by iOS when a remote command occurs. Add the following method
    to `AudioViewController` to add support for remote commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code obtains a reference to the remote command center and registers
    several handlers. It also calls `beginReceivingRemoteControlEvents()` on the application
    object to make sure it receives remote commands. Add a call to `configureRemoteCommands()`
    in `viewDidLoad()` to make sure that the app begins receiving remote commands
    as soon as the audio player is configured. As an exercise to practice, try implementing
    the commands to control the time scrubber and `+15` and `-15` from the lock screen
    yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Try to run your app and send it to the background. You should be able to control
    media playback from both the control center and the lock screen. The visible metadata
    should correctly update when you skip to the next or previous song, and the scrubber
    should accurately represent the current position of playback in the song.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have implemented a reasonably complete audio player that
    has pretty sophisticated behaviors. The next step in your exploration of media
    on iOS is to discover how you can take pictures and record video.
  prefs: []
  type: TYPE_NORMAL
- en: Recording video and taking pictures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In addition to playing existing media, you can also make apps that allow users
    to create their own content. In this section, you will learn how you can use a
    built-in component to enable users to take a picture. You will also learn how
    you can use a raw video feed to record a video. If you want to follow along with
    the samples in this section, make sure to grab the starter project for `Captured`
    from this chapter's code bundle.
  prefs: []
  type: TYPE_NORMAL
- en: The starter project contains a couple of view controllers and some connected
    outlets and actions. Note that there is a `UIViewController` extension in the
    project, too.
  prefs: []
  type: TYPE_NORMAL
- en: This extension includes a helper method that makes displaying an alert to the
    user a little bit simpler. This extension will be used to show an alert that informs
    the user when their photo or video is stored in the camera roll.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since a user''s camera and photo library are considered very privacy-sensitive,
    you need to make sure that you add the following privacy-related keys to the app''s
    `Info.plist`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Privacy - Camera Usage Description**: This property is required in order
    to access the camera so that you can take pictures and record video.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy - Microphone Usage Description**: You must add this property so that
    your videos record audio, as well as images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy - Photo Library Additions Usage Description**: This property allows
    you to write photos to the user''s photo library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure to provide a good description for the privacy keys, so the user knows
    why you need access to their camera, microphone, and photo library. The better
    your description is, the more likely the user is to allow your app to access the
    associated privacy-sensitive information. After adding the keys, you are ready
    to see how you can take a picture using the built-in `UIImagePickerController`
    component of UIKit.
  prefs: []
  type: TYPE_NORMAL
- en: Taking and storing a picture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you need a user to supply an image, they can do this by either selecting
    an image from their photo library or by taking a picture with the camera. `UIImagePickerController`
    supports both ways of picking an image. In this section, you will learn how you
    can allow users to take an image using the camera. Changing the example to allow
    users to select an image from their photo library should be straightforward, as
    long as you remember to add the `Info.plist`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following implementation for `viewDidLoad()` to the `ImageViewController`
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The previous implementation creates an instance of the `UIImagePickerController`
    object and configures it so that it uses the camera as the image source and presents
    it to the user. Note that the view controller is set as a delegate for the image
    picker.
  prefs: []
  type: TYPE_NORMAL
- en: When the user has taken a picture, the image picker will notify its delegate
    about this so that it can extract the image and use it. In this case, the image
    should be given the `selectedImage` label in the view controller so that it can
    be shown in the image view, and saved when the user taps on the save button, and
    the `saveImage()` method is called as a result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following extension to make `ImageViewController` conform to `UIImagePickerControllerDelegate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note that this extension also makes the image view controller conform to `UINavigationControllerDelegate`.
    The delegate property on the image picker controller requires all delegates to
    conform to both `UINavigationControllerDelegate` and `UIImagePickerControllerDelegate`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the user has taken a picture with the camera, `imagePickerController(_:
    didFinishPickingMediaWithInfo)` is called to notify the delegate about the photo
    that the user took. The first thing that the preceding code does is dismiss the
    picker, as it''s no longer needed. The picture that the user just took is stored
    in the `info` dictionary as the original image. When the image is extracted from
    the dictionary, it is set as `selectedImage`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To store the image, add the following implementation of `saveImage()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code calls `UIImageWriteToSavedPhotosAlbum(_:_:_:_)` to store
    the image in the user's photo library. When the save operation completes, the
    `didSaveImage(_:withError:contextInfo:)` method will be called. If this method
    does not receive any errors, then the photo was successfully stored in the photo
    library and an alert is shown.
  prefs: []
  type: TYPE_NORMAL
- en: Allowing the user to take a picture by implementing `UIImagePickerController`
    is relatively straightforward, and it is a great way to implement a camera feature
    in your app without too much effort. Sometimes, you need more advanced access
    to the camera. In these cases, you can use `AVFoundation` to gain access to the
    raw video feed from the camera, as you will see next.
  prefs: []
  type: TYPE_NORMAL
- en: Recording and storing video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you used `AVFoundation` to build a simple audio player
    app. You will now use `AVFoundation` again, except instead of playing video or
    audio, you will now record video and store it in the user's photo library. When
    using `AVFoundation` to record a video feed, you do so with an `AVCaptureSession`
    object. A capture session is responsible for taking the input from one or more
    `AVCaptureDeviceInput` objects and writing it to an `AVCaptureOutput` subclass.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the objects that are involved with recording media
    through `AVCaptureSession`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 − AVCaptureSession entities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.2_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 − AVCaptureSession entities
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started on implementing the video recorder, make sure to import `AVFoundation`
    in `RecordVideoViewController.swift`. Also, add the following properties to the
    `RecordVideoViewController` class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Most of the preceding properties should look familiar because they were also
    shown in the screenshot that outlined the components that are involved with `AVCaptureSession`.
    Note that `AVCaptureMovieFileOutput` is a subclass of `AVCaptureOutput`, specialized
    in capturing video. The preview layer will be used to render the video feed at
    runtime and present it to the user so that they can see what they are capturing
    with the camera.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to set up the `AVCaptureDevice` objects for the camera and
    microphone and associate them with `AVCaptureSession`. Add the following code
    to the `viewDidLoad()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code first obtains a reference to the camera and microphone that
    will be used to record the video and audio. The second step is to create the `AVCaptureDeviceInput`
    objects that are associated with the camera and microphone and associate them
    with the capture session. The video output is also added to the video capture
    session. If you examine the screenshot that you saw earlier and compare it with
    the preceding code snippet, you will find that all four components are present
    in this implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to provide the user with a view that shows the current camera
    feed so that they can see what they are recording. Add the following code to `viewDidLoad()`
    after the capture session setup code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code sets up the preview layer and associates it with the video
    capture session. The preview layer will directly use the capture session to render
    the camera feed. The capture session is then started. This does not mean that
    the recording session starts; rather, only that the capture session will begin
    processing the data from its camera and microphone inputs.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preview layer is added to the view at this point, but it doesn''t cover
    the video view yet. Add the following implementation for `viewDidLayoutSubviews()`
    to `RecordVideoViewController` to set the preview layer''s size and position,
    so it matches the size and position of `videoView`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the app now will already show you the camera feed. However, tapping
    the record button doesn''t work yet, because you haven''t yet implemented the
    `startStopRecording()` method. Add the following implementation for this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go over the preceding snippet step by step to see what exactly is going
    on:'
  prefs: []
  type: TYPE_NORMAL
- en: First, the `isRecording` property for the video output is checked. If a recording
    is currently active, the recording should be stopped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If no recording is currently active, a new path is created to store the video
    temporarily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since the video output cannot overwrite an existing file, the `FileManager`
    object should attempt to remove any existing files at the temporary video file
    path.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The video output will start recording to the temporary file. The view controller
    itself is passed as a delegate to be notified when the recording has begun and
    is stopped.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Since `RecordVideoViewController` does not conform to `AVCaptureFileOutputRecordingDelegate`
    yet, you should add the following extension to add conformance to `AVCaptureFileOutputRecordingDelegate`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The preceding extension contains three methods. The first is a delegate method,
    called when the video output has begun recording. When the recording has started,
    the title of the `startStopButton` button is updated to reflect the current state.
    The second method is also a delegate method. This method is called when the recording
    has completed. If no errors occur, the video is stored at the temporary location
    you set up earlier. `UISaveVideoAtPathToSavedPhotosAlbum(_:_:_:_:)` is then called,
    to move the video from the temporary location to the user's photo library. This
    method is very similar to the `UIImageWriteToSavedPhotosAlbum(_:_:_:_:)` method
    that you used to store a picture. The third and final method in the extension
    is called when the video is stored in the user's photo library. When the video
    has been successfully stored, an alert is shown, and the title of the `startStopButton`
    button is updated again.
  prefs: []
  type: TYPE_NORMAL
- en: You can now run the app and record some videos! Even though you have done a
    lot of manual work by implementing the video recording logic directly with `AVCaptureSession`,
    most of the hard work is done inside of the `AVFoundation` framework. One final
    media-related feature to explore is applying visual filters to images using **Core
    Image**. Applying filters to images is a very popular functionality in lots of
    apps and it can make your photo app more appealing.
  prefs: []
  type: TYPE_NORMAL
- en: Manipulating photos with Core Image
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have already seen that iOS has powerful capabilities for
    recording and playing media. In this section, you will learn how you can manipulate
    images with Core Image. The Core Image framework provides many different filters
    that you can use to process both images and videos. You will expand on the photo-taking
    capabilities that you implemented in the `Captured` app so that users can grayscale
    and crop images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every Core Image filter you apply to images is an instance of the `CIFilter`
    class. You can create instances of filters as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The `name` parameter in the filter's initializer is expected to be a string
    that refers to a specific filter. You can refer to Apple's documentation on Core
    Image and the Core Image Filter Reference guide to see an overview of all the
    filters that you can use in your apps.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every filter has a certain set of parameters that you need to set on the `CIFilter`
    instance to use the filter; for instance, a grayscale filter requires you to provide
    an input image. Other filters might take an intensity, location, or other properties.
    The best way to see how you can apply a filter to an image is through an example.
    Add the following implementation for `applyGrayScale()` to `ImageViewController.swift`
    to implement a grayscale filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code has a lot of small, interesting details, highlighted with
    numbered comments. Let''s go over the comments one by one to see how the grayscale
    filter is applied:'
  prefs: []
  type: TYPE_NORMAL
- en: The `UIImage` instance that is stored in `selectedImage` is converted into a
    `CGImage` instance. Strictly speaking, this conversion isn't required, but it
    does make applying other filters to the `UIImage` instance later a bit easier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: One downside of using `CGImage`, instead of `UIImage`, is that the orientation
    information that is stored in the image is lost. To make sure the final image
    maintains its orientation, the initial orientation is stored.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This step creates an instance of the grayscale filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since Core Image does not directly support `CGImage` instances, the `CGImage`
    instance is converted into a `CIImage` instance that can be used with Core Image.
    The `CIImage` instance is then assigned as the input image for the grayscale filter,
    by calling `setValue(_:forKey:)` on the filter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The fifth step extracts the new image from the filter and uses a `CIContext`
    object to export the `CIImage` output to a `CGImage` instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The sixth and final step is to create a new `UIImage` instance, based on the
    `CGImage` output. The initial orientation is passed to the new `UIImage` instance
    to make sure it has the same orientation as the original image.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Even though there are a lot of steps involved, and you need to convert between
    different image types quite a bit, applying the filter is relatively simple. Most
    of the preceding code takes care of switching between image types, while the filter
    itself is set up in just a couple of lines. Try running the app now and taking
    a picture. The initial picture will be in full color. After you apply the grayscale
    filter, the image is automatically replaced with a grayscale version of the image,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 − Grayscale'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.3_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.3 − Grayscale
  prefs: []
  type: TYPE_NORMAL
- en: 'The next filter you will implement is a crop filter. The crop filter will crop
    the image so that it''s a square, rather than a portrait or landscape picture.
    The process for implementing the crop filter is mostly the same as for the grayscale
    filter, except for the values that need to be passed to the crop filter. Add the
    following implementation for `cropSquare()` to implement the crop filter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code performs several calculations to figure out the best way
    to crop the image into a square. The `CGRect` instance specifies the crop coordinates
    and size, which are then used to create a `CIVector` object. This object is then
    passed to the filter as the value for the `inputRectangle` key. Apart from specifying
    the crop values, the process of applying the filter is identical, so the code
    should look familiar to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run the app now and tap the crop button, the image will be cropped,
    as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 − Cropping the image'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.4_B14717.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.4 − Cropping the image
  prefs: []
  type: TYPE_NORMAL
- en: There are many more filters available in Core Image, which you can play around
    with to build pretty advanced filters. You can even apply multiple filters to
    a single image to create elaborate effects for the pictures in your apps. Because
    all filters work in very similar ways, it's relatively easy to apply any filter
    to your images once you understand how the general process of applying a filter
    works. You can always use the code from the preceding examples if you need a reminder
    about how to apply Core Image filters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned a lot about media in iOS. You saw how you
    can implement a video player with just a couple of lines of code. After that,
    you learned how to use `AVFoundation` directly to build an audio player that supports
    features such as stopping and resuming playback, skipping songs, and scrubbing
    forward or backward in a song. You even learned how you can keep playing audio
    when the app goes to the background or when the phone is set to silent mode. To
    apply the finishing touches to the audio player, you learned how you can use the
    `MediaPlayer` framework to show the currently playing file on the user's lock
    screen, and how to respond to control events that are sent to the app remotely.
  prefs: []
  type: TYPE_NORMAL
- en: After implementing media playback, you learned how you can build apps that help
    users to create media. You saw that `UIImagePickerController` provides a quick
    and simple interface to allow users to take a picture with the camera. You also
    learned how you can use `AVFoundation` and an `AVCaptureSession` object to implement
    a custom video recording experience. To wrap it all up, you learned about the
    Core Image framework, and how you can use it to apply filters to images.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next chapter, you will learn everything you need to know about location
    services and how to use Core Location in your apps. Depending on the use case
    of your app, handling the user location properly can be a critical task for your
    app to be successful. Examples are well known by now: food delivery apps, map
    apps, sport tracker apps, and so on.'
  prefs: []
  type: TYPE_NORMAL
