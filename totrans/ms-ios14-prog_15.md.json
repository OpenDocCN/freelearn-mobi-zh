["```swift\nimport Vision\nlet imageUrl = URL(string: \"http://marioeguiluz.com/img/portfolio/Swift%20Data%20Structures%20and%20Algorithms%20Mario%20Eguiluz.jpg\")!\n// 1\\. Create a new image-request handler.\nlet requestHandler = VNImageRequestHandler(url: imageUrl, options: [:])\n// 2\\. Create a new request to recognize text.\nlet request = VNRecognizeTextRequest { (request, error) in\n  guard let observations = request.results as? [VNRecognizedTextObservation] else { return }\n  let recognizedStrings = observations.compactMap { observation in\n    // Return the string of the top VNRecognizedText instance.\n    return observation.topCandidates(1).first?.string\n  }\n  // Process the recognized strings.\n  print(recognizedStrings)\n}\n// 3\\. Select .accurate or .fast levels\nrequest.recognitionLevel = .accurate\ndo {\n  // 4\\. Perform the text-recognition request.\n  try requestHandler.perform([request])\n} catch {\n  print(\"Unable to perform the requests: \\(error).\")\n}\n```", "```swift\n[\"Erik Azar, Mario Eguiluz\", \"Swift Data\", \"Structure and\", \"Algorithms\", \"Master the most common algorithms and data structures,\", \"and learn how to implement them efficiently using the most\", \"up-to-date features of Swift\", \"Packt>\"]\n```", "```swift\n// 3\\. Select .accurate or .fast levels\nrequest.recognitionLevel = .fast\n```", "```swift\n[\"Swift Data\", \"Structure and\", \"Algorithms\", \"upto4atefeaturesofSwift3\", \"Packt>\", \"ErfkAz•r. M•rb Eguluz\", \"ml5tertket(w4VIthMsarodats5tr&KtUre\", \"learnItolpIettmeffK1WttIY5lt1fft\", \"LIJJ\"]\n```", "```swift\n    [\"Erik Azar, Mario Eguiluz\", \"1015\", \"Swift Data\", \"Structure and\", \"Algorithms\", \"Master the most common algorithms and data structures,\", \"and learn how to implement them efficiently using the most\", \"up-to-date features of Swift\", \"Packt>\"]\n    1.9300079345703125 seconds\n    ```", "```swift\n    request.regionOfInterest = CGRect(x: 0, y: 0.8, width: 0.7, height: 0.2)\n    ```", "```swift\n    [\"Erik Azar, Mario Eguiluz\", \"1015\"]\n    1.2314139604568481 seconds\n    ```", "```swift\n    request.regionOfInterest = CGRect(x: 0, y: 0.8, width: 0.3, height: 0.1)\n    ```", "```swift\n    .fast for recognitionLevel instead of .accurate, if what we want is speed? Let's see what happens. \n    ```", "```swift\n    request.recognitionLevel = .fast\n    ```", "```swift\n    [\"Iois\"]\n    0.5968900661468506 seconds\n    ```", "```swift\n    extension Character {\n      func transformToDigit() -> Character {\n        let conversionTable = [\n          \"s\": \"5\",\n          \"S\": \"5\",\n          \"o\": \"0\",\n          \"O\": \"0\",\n          \"i\": \"1\",\n          \"I\": \"1\"\n        ]\n        var current = String(self)\n        if let alternativeChar = conversionTable[current] {\n          current = alternativeChar\n        }\n        return current.first!\n      }\n    }\n    ```", "```swift\n    if let serialNumber = recognizedStrings.first {\n      let serialNumberDigits = serialNumber.map { $0.transformToDigit() }\n      print(serialNumberDigits)\n    }\n    ```", "```swift\n    [\"Iois\"]\n    [\"1\", \"0\", \"1\", \"5\"]\n    0.5978780269622803 seconds\n    ```", "```swift\n// TODO 1: Detect one hand only.\nhandPoseRequest.maximumHandCount = 1\n```", "```swift\n// TODO 2: Create video session\n// 1 - Front camera as input\nguard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {\n  fatalError(\"No front camera.\")\n}\n// 2- Capture input from the camera\nguard let deviceInput = try? AVCaptureDeviceInput(device: videoDevice) else {\n  fatalError(\"No video device input.\")\n}\n```", "```swift\nguard let videoDevice = AVCaptureDevice.default(.builtInWideAngleCamera, for: .video, position: .front) else {\n  fatalError(\"No front camera.\")\n}\n```", "```swift\nguard let deviceInput = try? AVCaptureDeviceInput(device: videoDevice) else {\n  fatalError(\"No video device input.\")\n}\n```", "```swift\nlet session = AVCaptureSession()\nsession.beginConfiguration()\nsession.sessionPreset = AVCaptureSession.Preset.high\n// Add video input to session\nguard session.canAddInput(deviceInput) else {\n  fatalError(\"Could not add video device input to the session\")\n}\nsession.addInput(deviceInput)\nlet dataOutput = AVCaptureVideoDataOutput()\nif session.canAddOutput(dataOutput) {\n  session.addOutput(dataOutput)\n  // Add a video data output.\n  dataOutput.alwaysDiscardsLateVideoFrames = true\n  dataOutput.videoSettings = [kCVPixelBufferPixelFormatTypeKey as String: Int(kCVPixelFormatType_420YpCbCr8BiPlanarFullRange)]\n  dataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)\n} else {\n  fatalError(\"Could not add video data output to the session\")\n}\nsession.commitConfiguration()\ncameraFeedSession = session\n```", "```swift\ndataOutput.setSampleBufferDelegate(self, queue: videoDataOutputQueue)\n```", "```swift\nvar thumbTip: CGPoint?\nvar indexTip: CGPoint?\nvar ringTip: CGPoint?\nvar middleTip: CGPoint?\nvar littleTip: CGPoint?\n```", "```swift\nlet handler = VNImageRequestHandler(cmSampleBuffer: sampleBuffer, orientation: .up, options: [:])\ndo {\n  try handler.perform([handPoseRequest])\n  guard let observation = handPoseRequest.results?.first else {\n    return\n  }\n\t// Get observation points\n} catch {\n  cameraFeedSession?.stopRunning()\n  fatalError(error.localizedDescription)\n}\n```", "```swift\nlet thumbPoints = try observation.recognizedPoints(.thumb)\nlet indexFingerPoints = try observation.recognizedPoints(.indexFinger)\nlet ringFingerPoints = try observation.recognizedPoints(.ringFinger)\nlet middleFingerPoints = try observation.recognizedPoints(.middleFinger)\nlet littleFingerPoints = try observation.recognizedPoints(.littleFinger)\nguard let littleTipPoint = littleFingerPoints[.littleTip], let middleTipPoint = middleFingerPoints[.middleTip], let ringTipPoint = ringFingerPoints[.ringTip], let indexTipPoint = indexFingerPoints[.indexTip], let thumbTipPoint = thumbPoints[.thumbTip] else {\n  return\n}\n```", "```swift\nthumbTip = CGPoint(x: thumbTipPoint.location.x, y: 1 - thumbTipPoint.location.y)\nindexTip = CGPoint(x: indexTipPoint.location.x, y: 1 - indexTipPoint.location.y)\nringTip = CGPoint(x: ringTipPoint.location.x, y: 1 - ringTipPoint.location.y)\nmiddleTip = CGPoint(x: middleTipPoint.location.x, y: 1 - middleTipPoint.location.y)\nlittleTip = CGPoint(x: littleTipPoint.location.x, y: 1 - littleTipPoint.location.y)\n```", "```swift\nDispatchQueue.main.sync {\n  self.processPoints([thumbTip, indexTip, ringTip, middleTip, littleTip])\n}\n```", "```swift\nfunc processPoints(_ fingerTips: [CGPoint?]) {\n  // Convert points from AVFoundation coordinates to UIKit // coordinates.\n  let previewLayer = cameraView.previewLayer\n  let convertedPoints = fingerTips\n    .compactMap {$0}\n    .compactMap {previewLayer.layerPointConverted(fromCaptureDevicePoint: $0)}\n  // Display converted points in the overlay\n  cameraView.showPoints(convertedPoints, color: .red)\n}\n```", "```swift\npublic func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {\n  var thumbTip: CGPoint?\n  var indexTip: CGPoint?\n  var ringTip: CGPoint?\n  var middleTip: CGPoint?\n  var littleTip: CGPoint?\n  defer {\n    DispatchQueue.main.sync {\n      self.processPoints([thumbTip, indexTip, ringTip, middleTip, littleTip])\n    }\n  }\n…\n}\n```"]