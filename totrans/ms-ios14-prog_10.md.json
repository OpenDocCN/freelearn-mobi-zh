["```swift\nimport NaturalLanguage\n```", "```swift\nfunc getWordCounts(from string: String) -> [String: Double] {\n  let tokenizer = NLTokenizer(unit: .word)\n  tokenizer.string = string\n  var wordCount = [String: Double]()\n  tokenizer.enumerateTokens(in: \n    string.startIndex..<string.endIndex) { range, attributes in\n    let word = String(string[range])\n    wordCount[word] = (wordCount[word] ?? 0) + 1\n    return true\n  }\n  return wordCount\n}\n```", "```swift\n@IBAction func analyze() {\n  let wordCount = getWordCounts(from: textView.text)\n  let model = try? SentimentPolarity(configuration: .init())\n  guard let prediction = try? model?.prediction(input: \n    wordCount) else { return }\n  let alert = UIAlertController(title: nil, message: \"Your \n    text is rated: \\(prediction.classLabel)\", preferredStyle: \n     .alert)\n  let okayAction = UIAlertAction(title: \"Okay\", style: \n    .default, handler: nil)\n  alert.addAction(okayAction)\n  present(alert, animated: true, completion: nil)\n}\n```", "```swift\nlet handler = VNImageRequestHandler(cgImage: image, options: [:])\nlet request = VNDetectFaceLandmarksRequest(completionHandler: { request, error in\n  guard let results = request.results as? [VNFaceObservation]\n  else { return }\n  for result in results where result.landmarks != nil {\n    let landmarks = result.landmarks!\n    if let faceContour = landmarks.faceContour {\n      print(faceContour.normalizedPoints)\n    }\n    if let leftEye = landmarks.leftEye {\n      print(leftEye.normalizedPoints)\n    }\n    // etc\n  }}\n)\ntry? handler.perform([request])\n```", "```swift\nimport Vision\n```", "```swift\nfunc analyzeImage(_ image: UIImage) {\n  guard\n    let cgImage = image.cgImage,\n    let classifier = try? VNCore MLModel(for: \n    MobileNetV2().model)\n  else { return }\n  let request = VNCore MLRequest(model: classifier,\n  completionHandler: { [weak self] request, error in\n    guard\n      let classifications = request.results as? \n      [VNClassificationObservation],\n      let prediction = classifications.first\n    else { return }\n    DispatchQueue.main.async {\n      self?.objectDescription.text = \"\\(prediction.identifier)\n      (\\(round(prediction.confidence * 100))% confidence)\"\n    }\n  })\n  let handler = VNImageRequestHandler(cgImage: cgImage,\n    options: [:])\n  try? handler.perform([request])\n}\n```", "```swift\n{\n  \"text\": \"We took an exclusive ride in a flying car\", \n  \"label\": \"Tech\"\n}\n```", "```swift\nimport Create ML\nimport Foundation\nlet trainingData = try! MLDataTable(contentsOf: Bundle.main.url(forResource: \"texts\", withExtension: \"json\")!)\nlet model = try! MLTextClassifier(trainingData: trainingData, textColumn: \"text\", labelColumn: \"label\")\ntry! model.write(to: URL(fileURLWithPath: \"/Users/marioeguiluz/Desktop/TextClassifier.mlmodel\"))\nlet techHeadline = try! model.prediction(from: \"Snap users drop for first time, but revenue climbs\")\nlet politicsHeadline = try! model.prediction(from: \"President Donald Trump is approving a new law\")\n```", "```swift\nlet techHeadline = try! model.prediction(from: \"Snap users drop for first time, but revenue climbs\")\nlet politicsHeadline = try! model.prediction(from: \"President Donald Trump is approving a new law\")\n```", "```swift\n{\n  \"tokens\": [\"Apple\", \"announced\", \"iOS 12\", \"and\", \"Xcode \n    10\", \"at\", \"WWDC 2018\"],\n  \"labels\": [\"COMPANY\", \"NONE\", \"OPERATING_SYSTEM\", \"NONE\",\n    \"SOFTWARE\", \"NONE\", \"EVENT\"]\n}\n```", "```swift\nlet labelTrainingData = try! MLDataTable(contentsOf: Bundle.main.url(forResource: \"labels\", withExtension: \"json\")!)\nlet model = try! MLWordTagger(trainingData: labelTrainingData, tokenColumn: \"tokens\", labelColumn: \"labels\")\ntry! model.write(to: URL(fileURLWithPath: \"/Users/marioeguiluz/Desktop/TextTagger.mlmodel\"))\n```", "```swift\nimport Create ML \nimport Foundation\nlet dataUrl = URL(fileURLWithPath: \"/path/to/trainingdata\")\nlet source = MLImageClassifier.DataSource.labeledDirectories(at: dataUrl)\nlet classifier = try! MLImageClassifier(trainingData: source) try! classifier.write(toFile: \"/Users/marioeguiluz/Desktop/CarClassifier.mlmodel\")\n```", "```swift\n//1\n_ = MLModelCollection.beginAccessing(identifier: \"SentimentPolarityCollection\") { [self] result in\n  //2\n  var modelURL: URL?\n  switch result {\n  case .success(let collection):\n    modelURL = \n    collection.entries[\"SentimentPolarity\"]?.modelURL\n  case .failure(let error):\n    handleCollectionError(error)\n  }\n  //3\n  let result = loadSentimentClassifier(url: modelURL)\n  //4\n  switch result {\n  case .success(let sentimentModel):\n    model = sentimentModel\n    guard let prediction = try? model?.prediction(input: \n      wordCount) else { return }\n    showResult(prediction: prediction)\n  case .failure(let error):\n    handleModelLoadError(error)\n  }\n}\n```", "```swift\nprivate func loadSentimentClassifier(url: URL?) -> Result<SentimentPolarity, Error> {\n  if let modelUrl = url {\n    return Result { try SentimentPolarity(contentsOf: \n    modelUrl)}\n  } else {\n    return Result { try SentimentPolarity(configuration: \n    .init())}\n  }\n}\n```", "```swift\n--encrypt \"$SRCROOT/SentimentPolarity.mlmodelkey\"\n```", "```swift\nSentimentPolarity.load { [self] result in\n  switch result {\n  case .success(let model):\n    self.model = model\n    guard let prediction = try? self.model?.prediction(input:\n    wordCount) else { return }\n    showResult(prediction: prediction)\n  case .failure(let error):\n    handleDecryptError(error)\n  }\n}\n```"]