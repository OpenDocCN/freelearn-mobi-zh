<html><head></head><body>
        

                            
                    <h1 class="header-title">A/B Testing Your App</h1>
                
            
            
                
<p>All successful app developers learn from the feedback they get from their users. They investigate it and determine if they need to improve, to remove, or to add features in order to better support of the user's workflow. In this chapter, we will investigate what tools we can use to get feedback from our users if we cannot ask them in person. Multiple iterations of testing and optimizing are required to build an app that solves your customer's problem. Split testing (or A/B testing) is an ongoing process that can help you find the workflow, which will lead to the highest conversions. Using split testing, you can, for example, find the best registration flow for your app. In <a href="b81516a3-47fe-4318-a8e6-2bc8f2f34a04.xhtml" rel="noopener noreferrer" target="_blank">Chapter 10</a>, <em>There is an API For That!</em>, we have seen some good suggestions of what you can do to improve the onboarding process. Now you can also run some experiments and measure what works best for your app. It can also give you feedback about other topics, such as user retainment, engagement, or in-app purchases. We will see why obtaining statistics matters and what we could learn from them.</p>
<p>Pragmatic as we are, we will investigate what tools we can use for this purpose. We will have a quick look on how Firebase, remote config and analytics, could work for us. Split testing is a methodology that you can use any time, even when your app is already in the store. Finally, we will see what we can do for split testing our App or Play Store listing.</p>
<p>Specifically, in this chapter, we will cover the following topics:</p>
<ul>
<li>See why statistics matter</li>
<li>Learn what actionable metrics are</li>
<li>Check out what split testing is and how it can help us to improve our apps</li>
<li>Investigate what tools we can use for testing</li>
<li>Figure out how to use Firebase Remote Config and Firebase Analytics</li>
</ul>


            

            
        
    

        

                            
                    <h1 class="header-title">Why do statistics matter?</h1>
                
            
            
                
<p>Without statistics, you will have little to no feedback. You will be blind to all insights you could otherwise have obtained from your users and their behavior. Do not release your app without any implementation required for obtaining analytical data:</p>
<div><img class=" image-border" height="473" src="img/b0a974ee-4d71-4bef-9cef-16a2da9199aa.png" width="893"/></div>
<p>In general, statistics could inform us about the following:</p>
<ul>
<li>User acquisition performance</li>
<li>User behavior and conversion</li>
<li>User demographics</li>
<li>User behavior by segment or cohort</li>
<li>Financial insights</li>
</ul>
<p>So, the right statistics tell us something about app usage. It gives an answer to questions such as: How well is the app doing and what exactly does "doing well" mean? Is this about the number of downloads? The number of active users? The number of daily new users? It is important not just to have statistics, but to have actionable metrics. It is easy to gather a lot of data. It is more difficult to determine what numbers really matter. Do not get drowned in numbers. Determine what your business objectives are so you know what to measure. It is important to have concrete numbers so you can instantly act upon them by doing the right things.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">About actionable metrics</h1>
                
            
            
                
<p>In <a href="b81516a3-47fe-4318-a8e6-2bc8f2f34a04.xhtml" rel="noopener noreferrer" target="_blank">Chapter 10</a>, <em>There is an API For That!</em>, we already had a preview on the concept of conversions and metrics. Gathering statistics about your mobile app usage matters, as it often is the only way to get feedback from your users. If we want to learn something about this feedback, it is important to realize that the quality of the statics you obtain is more important than the quantity. While it may be tempting to gather as much data as possible, the opposite is actually true. Focus on what really matters. Actionable metrics is what we want. Ash Maurya writes about this in his books <em>Running Lean and Scaling Lean</em>. He claims user growth is more important than your total user base, and he certainly has a point there.</p>
<p>Acquisition and engagement are important metric categories. Acquisition numbers tell you something about your app's downloads, the number of new users, and the number of active users. Engagement is about how often your users are opening your app (and keep using it), retention, and churn rates (the users that have abandoned your app). It is interesting to learn how many of the users that have downloaded your app will stick with your app? And will they still be using your app after 1 week or 1 month? The other important metrics are customer lifetime value and key funnel behavior, but let's start with acquisition and engagement first.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Acquisition</h1>
                
            
            
                
<p>Before users will download your app, they need to be aware of its existence. You need to promote it on social media, on your website, or in some other way. How else would one know that it exists and that your app is really awesome? Getting new users every day is important, as your number of active users will drop otherwise. No matter how cool your app is, it will not work for some people. That does not have to be an issue. As long as the numbers for acquisition is higher than the churn rate, your app will grow.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Engagement</h1>
                
            
            
                
<p>User engagement metrics are all about your app's stickiness. People spend more and more time on mobile devices, which of course is a good thing for your app. But people also have little more than the attention span of a goldfish so average churn rates (app users that no longer use your app) are often higher for mobile apps.</p>
<p>You might wonder how much time a user spends on the app during a session or during a certain period. A session is any kind of interaction until the attention of the user gets interrupted by something else, such as an incoming phone call. To improve your app's retention rate, you should often remind users about the app or you should provide them with a good reason to visit the app on a regular basis. Email and push notifications can be used to get the user's attention again, thus maintaining app awareness.</p>
<p>The retention rate is about the number of users that remain active after a certain period of time, let's say, after 2 months or more. The churn rate is about the users that no longer use your app after the same period of time. To grow, the retention rate needs to be higher than the churn rate. To do so, your app constantly needs to deliver value by providing relevant content, incentives, and new or improved features. In short, you continuously need to give your users reason to come back to your app.</p>
<p>Daily or weekly active users are the most valuable ones as they will be the easiest ones to convert later. The higher the engagement rate, the more valuable the app is to your users. They could become an ambassador of your app by making referrals, or contribute to your app's monetization by clicking on advertisements or by making in-app purchases (revenue).</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Conversions and pirate metrics</h1>
                
            
            
                
<p>Pirate metrics, as we have seen in <a href="b81516a3-47fe-4318-a8e6-2bc8f2f34a04.xhtml" rel="noopener noreferrer" target="_blank">Chapter 10</a>, <em>There is an API For That!</em>, are about the conversion of your app users. Here the conversion steps are shown from acquisition to revenue:</p>
<div><img class=" image-border" height="65" src="img/100d500d-807a-40fc-b50e-f80adb118e45.png" width="568"/></div>
<p>For our app, in particular, it comes down to these steps: <strong>Awareness</strong>, <strong>Visit and search store</strong>, <strong>Download app</strong>, <strong>Open app</strong>, <strong>Activation (Register),</strong> and finally <strong>Retention</strong>. To keep things simple, for now, the ad income or in-app purchase (revenue) is not displayed here:</p>
<div><img class=" image-border" height="253" src="img/f6141e2c-a433-4f5b-9685-0f962fecf626.png" width="481"/></div>
<p>With each step, you lose a number of users. That is a completely normal phenomenon, but you need to make sure that you will not lose too many of them on the way. Let's say 1,000 people learn about the existence of your app by reading about it on a website or on Twitter. 800 of them click on the link to view the app in the store.</p>
<p>They see the app's icon, some screenshots, a description, and some feedback from other users. About 300 users think, "Hmm, this is not for me". So, only 500 users will download the app. 100 of them forget about it while downloading (on their way home, something else is asking for their attention: A call? A whatsapp message?). Eventually, 400 users will open the app. They see an onboarding story with a clear call to action. The app asks them to register using Facebook or Twitter. Probably 200 of them will do this. The remainder of the users have the intention to do this later (but they probably will forget about it). 200 users start exploring the app and, if they are not often reminded about the app, and if the app does not give them sufficient reasons to return to it, they will forget about it within a couple of days. After 1 week only 50 users are still using the app, and after 1 month only 25 of them are still active.</p>
<p>Is this a negative scenario? Not at all. It is a very realistic one for a lot of apps. If you want to make a difference between a failing and a successful app, then you need to think of this. Also, we did not even discuss the monetization part yet. In <a href="0efea3a6-95f2-42d3-9a36-a34bdbef1014.xhtml" rel="noopener noreferrer" target="_blank">Chapter 17</a>, <em>Monetization and Pricing Strategy</em>, we will have a look at that part specifically.</p>
<p>Fortunately, we have tools to improve the conversion rate. It is important to learn what the exact conversion percentages are. If, of all the users that have downloaded the app, only a small percentage sign up, then you will know you have work to do. There probably is something in your onboarding process that is preventing people from signing up. In that particular case, you need to find out if the on-boarding barrier is too high and what you can do to change this. Another example is the conversion number for in-app purchases. It also is an interesting pattern if you notice that they visit that part of your app where they can make such a purchase without ever converting to customers (actually buying something). There is something that needs to be changed there. Perhaps the added value for the products are unclear or maybe the pricing level is just too high.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Get to know your audience</h1>
                
            
            
                
<p>But what is it that you need to change? If you have a small number of beta users, you can just text them to ask. There are also tools available to include some sort of survey, but most people consider them as annoying. It might help if you offer them specific incentives (a free purchase, for example). They could be digital incentives, such as the well-known badges (gamification) or real-life incentives. If you are interested in the latter, checkout Kiip at <a href="http://www.kiip.me/developers">http://www.kiip.me/developers</a> for some examples. They have a great SDK that you can add to your app. For example, it enables you to offer a free cup of coffee to your user if he has fully completed his profile.</p>
<p>If you want to know who your app users are, you need to have additional information about them, such as their locations and what they expect from your app. It is also interesting to know something about their age, gender, the types of devices they use, and at what moments or in what situations they use your app. Knowing your audience well is vital in order to be able to create an app that fully meets the user's expectations. And, in the end, it also leads to better monetization of your app. In fact, this is why Facebook ads have way better conversions than Google ads. Facebook knows much more about their audience and about each individual, so advertisements can be targeted more specifically, thus making the ad <strong>Click Through Rate</strong> (<strong>CTR</strong>) higher. We will learn more about this in <a href="d9145149-0f4e-47b6-bc9d-ff46e5e63304.xhtml" rel="noopener noreferrer" target="_blank">Chapter 14</a>, <em>Growing Traction and Improving Retention</em>, about traction and retention. First, let's see what we need to do to learn more about our app's audience:</p>
<div><img class=" image-border" height="206" src="img/0934ebf1-de0a-4715-9e72-196159c58b5f.png" width="222"/></div>


            

            
        
    

        

                            
                    <h1 class="header-title">Split testing can help us to improve our apps</h1>
                
            
            
                
<p>A/B testing, also known as split testing, in its most basic form comes down to two different implementations shown at random to different kinds of people. A small number, say 5%, are shown the new feature, A, which could be something like a new feature or a new view, and another 5% will see feature B. The remainder of the users will not see the new feature yet. The feature that will prove to be most popular (by conversion or otherwise, depending on the objectives) will be fully implemented and offered to the complete audience of your app.</p>
<p>In case you want to find out what works best for signing up users, you can set up a split test like this:</p>
<div><img class=" image-border" height="210" src="img/03c4be9c-4714-4f58-b00f-b27f34d2c1d5.png" width="356"/></div>
<p>So 50% of your test audience sees variation A, showing a button that says Sign up, which will lead 26% of the visiting users to sign up. The other 74% might think "Hmm, this is not for me", or decide to sign-up later: something they probably will forget about. What happens with the other 50% of the test audience? They will see variation B. It displays a Get started! button. If we look at this variant, we see that 63% of the audience decides to sign up:</p>
<div><img class=" image-border" height="175" src="img/8da6789f-4ff8-43b9-9003-b7384a65f7ed.png" width="338"/></div>
<p>In theory, this proves that variation B is the one that should be implemented as it leads to the highest conversions. The reality is somewhat different. If we have little to no knowledge of our audience the preceding conclusion may be true, but if we do know a little more about our audience we might not accept the results on face value and might consider other questions. Is the audience that sees variation A comparable to the audience that sees variation B? There may be specific customer segments that specifically prefer one feature above another.</p>
<p>We will never find out if we will just do random tests. As stated earlier, we can increase our success on our app monetization only if we know what our audience is and what they want. Step 1 is getting to know our audience (by gathering user data) and step 2 is to take this knowledge into account when doing A/B tests. What if we could choose our target audience and see what works best for them? There are tools available that could help us to do a little bit more sophisticated split testing. We will look at them in the next section.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Keep the differences between variations subtle</h1>
                
            
            
                
<p>The difference between A and B in our example is very subtle, and that is for a reason. If the difference between the two variations is too big, you will not know what it is that you are testing:</p>
<div><img class=" image-border" height="278" src="img/0a7c9d7c-6314-4ff2-96f6-7cbee58af55b.png" width="293"/></div>
<p>The preceding is a brilliant example of what not to do. If your onboarding split test shows that variant A leads to a 61% conversion and variant B leads to a 66% conversion rate, then what does the outcome prove? Not only is the difference in conversion percentages not very convincing, it also is not clear what has led to a slightly better conversion. Was it the background color that did the trick? Or the text (call to action)? Or maybe was it the color of the Sign up button? We will never know. This test has too many parameters.</p>
<p>Think of what the objective of the test is. What is your hypothesis and how can you prove it using a split test? Test one element at a time, so you know what change was responsible for the improved conversion. Run multiple split tests, as a single test will typically not provide sufficient information to fully understand what works best. Remember, it is not important what you think that your users will do. It is important what your users do. And you better find out as early as possible.</p>
<p>The other things that you should take into account are events that may influence your tests. Running a test around holidays or particular events may have a different outcome. Also, conversion rates may be different on different days of the week. For these reasons, always make sure you are running tests for at least a couple of weeks.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Tools for split testing and getting actionable metrics</h1>
                
            
            
                
<p>From a technical perspective, it is pretty easy to do web split-testing experiments. Mobile-optimizing experiments, on the other hand, are more difficult to accomplish. The Play Store or the App Store are the most important reasons for this. A web browser always is connected, but apps live on a device which is not always connected.</p>
<p>Although mobile-app split testing is not as mature yet as for website A/B testing, there are many tools available that can help you test your users. Once you have decided what metrics you want to measure, you can pick the tool that is most convenient for that purpose:</p>
<div><img class=" image-border" height="151" src="img/1a307f5e-eab0-4af7-beb8-bd1a53f6a62e.png" width="150"/></div>
<p>Among others, you can use some of the following tools:</p>
<ul>
<li><strong>Firebase:</strong> This comes with many options, such as remote configurations and analytics. Firebase is a good candidate for split testing your app if you combine these two features. Remote configurations allow to make instant updates to the appearance of your app. Perhaps you are using Firebase already for data storage, real-time data sharing or for onboarding purposes.</li>
<li><strong>Taplytics</strong>: This is a split-testing tool that you can use to make changes that do not require an update in the Play Store or App Store. Without even changing code, you can have multiple fast-test iterations, which makes it one of the most suitable solutions for mobile split-testing purposes.</li>
<li><strong>Fabric</strong>: The Fabric SDK comes with many handy tools, about onboarding. It is a platform that makes it easy to install and maintain SDKs, including, for example, Optimizely. In addition to a Fabric account, you also need to set up an Optimizely account. Optimizely can help to easily integrate split testing into apps. It is a well-known testing tool and is available for both iOS and Android. Just as is the case with Firebase and Taplytics, there are no App Store or Play Store updates required to run A/B tests.</li>
</ul>
<p>The other interesting tools are SplitForce, Flurry Analytics, Amazing A/B testing, Arise, Switchboard, Leanplum, and Apptimize. They all support both iOS and Android. Customer segments are supported by most of them. This functionality allows you to run tests for a particular type of audience. Depending on your objectives, you need to pick the tools that suit your needs best. As an example, we will take a look at Firebase remote config and Firebase analytics specifically to see how this works.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Using Firebase for split testing</h1>
                
            
            
                
<p>You can use Firebase for split testing your Android or iOS app. Tutorials on how to set things up for Firebase and remote configurations specifically can be found at <a href="https://firebase.google.com/docs/remote-config/">https://firebase.google.com/docs/remote-config/</a>.</p>
<p>For this example, we will be looking at an Android implementation. Download the sample project from <a href="https://github.com/mikerworks/packt-lean-firebase-split-testing">https://github.com/mikerworks/packt-lean-firebase-split-testing</a>. The Android Kotlin app that you will find there is to demonstrate how you can run split tests for the onboarding flow of an app. It uses Firebase remote configurations and Firebase analytics.</p>
<p>The project has been setup using the Firebase option of the Tools menu of Android Studio. The Firebase assistant can help you to configure your project for <kbd>Analytics</kbd> and <kbd>Remote Config</kbd>:</p>
<div><img height="346" src="img/c0d7cc1d-2d45-46d4-9755-935c55e1549c.png" width="293"/></div>
<div><p>For the sample project, it has been set up already. In the <kbd>build.gradle</kbd> file within the <kbd>app</kbd> folder, you will find these dependencies for Firebase:</p>
</div>
<pre>dependencies {<br/> ...<br/>   implementation "org.jetbrains.kotlin:kotlin-stdlib-jre7:$kotlin_version"<br/>   implementation 'com.android.support:appcompat-v7:25.4.0'<br/>   implementation 'com.android.support.constraint:constraint-layout:1.0.2'<br/>   implementation 'com.google.firebase:firebase-config:11.6.0'<br/>}<br/>apply plugin: 'com.google.gms.google-services'</pre>
<p>The <kbd>google-services.json</kbd> file in the project should be replaced by your own file. You can download it from Firebase as soon as you have configured your app. (Choose settings in the project overview.) You can use the Firebase assistant to do so or you can go to the developers console of Firebase at <a href="https://console.firebase.google.com">https://console.firebase.google.com</a>:</p>
<div><img height="161" src="img/57838c6c-8362-435f-9bdf-ff26e9f070f4.png" width="485"/></div>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">If you do not have a Firebase account yet, you need to create one first. In the console, you can add and configure your project:</p>
<div><img height="202" src="img/fed2165b-e79e-4837-b9d0-16498be56b67.png" width="797"/></div>
<p>In the project overview, you can go to the Grow section and choose the Remote config option. If you choose the A/B testing on the right, you can determine what variants you want to split test:</p>
<div><img height="366" src="img/8b875e57-8abc-494a-b2ba-270149e7489c.png" width="245"/></div>
<p>Let’s say the example project is an app that has already been published. And let’s say we want to test a new onboarding experience. By clicking on the Create experiment button, we can test what works best. We want to figure out how which variant leads to the highest conversion for sign-ups.</p>
<p>There are two variants: Variant A and Variant B. The Control group will see the app as is; they will not see any variations:</p>
<div><img height="179" src="img/20fe5e18-e002-4505-a3e3-b4c57a721462.png" width="443"/></div>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">You can define one or more parameters for the experiment. Each variant has its own values for these parameters. Things that we could test are for example: the background color of the Sign up button (Blue or green), the sign up text or the background image (strawberries or oranges). As you can see, you can set up multiple parameters, but it is best practice to limit them to two or three.</p>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">You can define a user segment for your split test. In this example, we will just target 5% of the user base to keep it simple. More sophisticated segmentation options are also available. For example, you can target a specific country or users in the age group of 18 to 36. You can create very specific segments if you have obtained a lot of information about your users.</p>
<div><img height="285" src="img/0f3af1c4-f420-4d1f-b2e1-24c0cf334c68.png" width="484"/></div>
<p>The app can read any of these values that you define here. The default ones can be found in the project in the <kbd>remote_config_defaults.xml</kbd> file (in the <kbd>res</kbd>/<kbd>xml</kbd> folder). We need them to let the app function properly in case the remote config values cannot be retrieved (because there is no internet connection, for example).</p>
<p>In the MainActivity app, you can see how it is done. The Firebase remote configuration and analytics are initialized here. The developer's mode is enabled for the debug variant. This will ensure that there is no caching of data, which allows us to test the variants first.</p>
<p>We are also telling the <kbd>firebaseRemoteConfig</kbd> instance that it should use the variables for the <kbd>remote_config_defaults.xml</kbd> file as a fall-back option:</p>
<p> </p>
<pre>val firebaseRemoteConfig = FirebaseRemoteConfig.getInstance()<br/>var firebaseAnalytics: FirebaseAnalytics? = null<br/>override fun onCreate(savedInstanceState: Bundle?) {<br/> ...<br/>   val configSettings = FirebaseRemoteConfigSettings.Builder()<br/>           .setDeveloperModeEnabled(BuildConfig.DEBUG)<br/>           .build()<br/>   firebaseAnalytics = FirebaseAnalytics.getInstance(this)<br/>   firebaseRemoteConfig.setConfigSettings(configSettings)<br/>   firebaseRemoteConfig.setDefaults(R.xml.remote_config_defaults)<br/>   val token = FirebaseInstanceId.getInstance().getToken()<br/>   Log.i(javaClass.simpleName, "token = ${token}")<br/>   val cacheExpiration = 1L<br/>   Log.i(javaClass.simpleName,"fetch")<br/>firebaseRemoteConfig.fetch(cacheExpiration).addOnCompleteListener(this)<br/>}</pre>
<p>Finally, we are logging the device token. Later, we need this token to test a specific variant on our test device. At the end of this code snippet, we fetch the data and listen for the results.</p>
<p>If all the parameters and values have been retrieved, we tell the <kbd>firebaseRemoteConfig</kbd> object to apply these values. The call to the <kbd>applyRemoteConfiguration</kbd> method ensures that the UI will be updated:</p>
<p> </p>
<pre>override fun onComplete(task: Task&lt;Void&gt;) {<br/>   if (task.<em>isSuccessful</em>){<br/>       Log.i(<em>javaClass</em>.<em>simpleName</em>, "complete success")<br/>       firebaseRemoteConfig.activateFetched()<br/>   }<br/>   else{<br/>       Log.i(<em>javaClass</em>.<em>simpleName</em>, "complete no success")<br/>   }<br/>   applyRemoteConfiguration()<br/>}</pre>
<p>Here, we set all the colors and texts that are applicable to the current variant:</p>
<pre>private fun applyRemoteConfiguration(){<br/>   val variant = firebaseRemoteConfig.getString("experiment_variant")<br/>   Log.i(javaClass.simpleName, "experiment = ${variant}")<br/>   firebaseAnalytics?.setUserProperty("Experiment", variant)<br/>   val onboardingColor = firebaseRemoteConfig.getString("onboarding_color")<br/>   Log.i(javaClass.simpleName, "onboarding color= ${onboardingColor}")<br/>   if (onboardingColor=="blue") {<br/>findViewById(R.id.sign_up_button).setBackgroundColor(Color.parseColor("#0000ff"))<br/>   }<br/>   else{<br/>       findViewById(R.id.sign_up_button).setBackgroundColor(Color.parseColor("#00ff00"))<br/>   }<br/>   val onboardingText = firebaseRemoteConfig.getString("onboarding_text")<br/>   Log.i(javaClass.simpleName, "onboarding text= ${onboardingText}")<br/>   (findViewById(R.id.sign_up_text) as TextView).text = onboardingText<br/>   val onboardingBackground = firebaseRemoteConfig.getString("onboarding_background")<br/>   Log.i(javaClass.simpleName, "onboarding bg= ${onboardingBackground}")<br/>   if (onboardingBackground=="strawberry") {<br/>       (findViewById(R.id.image).setBackgroundResource(R.drawable.strawberry))<br/>   }<br/>   else{<br/>       (findViewById(R.id.image).setBackgroundResource(R.drawable.oranges))<br/>   }<br/>}</pre>
<p>This will result in displaying variant A or variant B for the onboarding flow. Since we want to measure the differences in conversion between these two variants, we set a user property for the <kbd>fireBaseAnalytics</kbd> object, and if the user clicks on the sign up button, we log the event like this:</p>
<pre>private fun onSignup(){<br/>   logEvent("signUp")<br/>   Log.i(javaClass.simpleName, "sign up button clicked")<br/>}<br/>private fun logEvent(eventName: String){<br/>   firebaseAnalytics?.logEvent(eventName, Bundle())<br/>}</pre>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">With this approach, we can measure the number of clicks on the sign up button and we can see the results in the Firebase analytics dashboard console for each variant.</p>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">First, we need to test both variants. If you run the app for the first time and everything goes well, you will find something like this in the log output (filter on: token):</p>
<pre class="CodeEndPACKT">11-10 11:22:09.856 27547-27547/com.packt.splittestdemo I/MainActivity: token = cG-QulinNq0:APA91bH2lOQThh57qNseb3PDoBRDy-mPXvE_vezn1nNFBiDrWd0a…</pre>
<p>Copy the token value and go back to the Firebase console. There you can set up a test device. Paste the token at the field Instance ID token and choose Variant A or Variant B:</p>
<div><img height="272" src="img/83a87294-fc9d-4868-8e58-22984db46075.png" width="397"/></div>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">If you choose Variant A and run the app now, it will look like the screenshot shown on the left. It has a background filled with strawberries and it has a blue SIGN UP button. However, if you choose Variant B at the Firebase console and run the app again, it will suddenly show oranges in the background and it has a green SIGN UP button. Variant B is shown on the right:</p>
<div><img height="327" src="img/b8b900d6-9266-4249-88aa-0d32abd01c04.png" width="396"/></div>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">Will this onboarding screen with the blue Call to Action button and a background of strawberries be the winner? Or will we see the highest conversion (sign up) for the onboarding view that uses a green button and a background of oranges? Only time will tell what the outcome will be.</p>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">If we run this split test live for a couple of weeks, we will know which of the two results provide the highest conversion. The winning variant will be the one that we are going to roll out for all users.</p>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">This was just a brief example. There are many other options available to be discovered that are not covered in this chapter, as it is just an introduction to split testing. However, you have an idea of the possibilities now.</p>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;">To learn more about Firebase split testing specifically, have a look at:</p>
<p class="Normal1" style="margin: 0in 0in 6.0pt 0in;"><a href="https://developer.android.com/distribute/best-practices/develop/in-app-a-b-testing.html">https://developer.android.com/distribute/best-practices/develop/in-app-a-b-testing.html</a> or <a href="https://techcrunch.com/2017/10/31/google-firebase-gets-predictions-crashlytics-integration-and-a-new-ab-testing-service/">https://techcrunch.com/2017/10/31/google-firebase-gets-predictions-crashlytics-integration-and-a-new-ab-testing-service/</a> or <a href="https://firebase.google.com/docs/remote-config/use-config-ios">https://firebase.google.com/docs/remote-config/use-config-ios</a>.</p>


            

            
        
    

        

                            
                    <h1 class="header-title">Summary</h1>
                
            
            
                
<p>In this chapter, we have seen why statistics matter and which statistics matter. We have learned about split testing and what the do’s and don’ts are. We have some idea of what tools are available for it and what we need to do to set up a split test of our own apps. We learned something about metrics and about the importance of acquisition and retention.</p>
<p>In the next chapter, we will learn more about retention and how we can further improve it. We are going to improve traction and examine some practical approaches to accomplish that for our app. Let's get started.</p>


            

            
        
    </body></html>