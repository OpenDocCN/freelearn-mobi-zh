- en: Chapter 6. Exhibit the Metal in Your Game
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up to this point, we have learned quite a bit. We looked into Apple's Swift
    programming language, got an idea of the general flow of an iOS app, and how to
    control that through code and/or storyboards. We got an understanding of how 2D
    games and 2D overlays can be made with **SpriteKit** and how 3D games can be designed
    even in the **Xcode** editor with SceneKit. Finally, we reviewed how to create
    reusable game logic, components, and AI with the various aspects of **GameplayKit**.
  prefs: []
  type: TYPE_NORMAL
- en: Essentially, this is all that is needed to get right to planning, coding, and
    building your own games. If there's a game idea that has come to your mind at
    this time, go right ahead and start planning it out. The frameworks and Xcode
    features from the past chapters can help take your abstract ideas and start turning
    them into what could soon be a playable application.
  prefs: []
  type: TYPE_NORMAL
- en: However, before moving forward, we'd like to take this time to go over a few
    more tips, tricks, and topics that we either briefly mentioned or have yet to
    go over. These topics mainly cover the ways we can optimize our games and get
    more out of the Apple hardware. In this chapter, we shall review a bit on the
    rather advanced topic of the Apple Metal low-level graphics API.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just a warning that the topic of low-level graphics APIs can get rather advanced.
    This won't be an all-encompassing tutorial on the subject; more of an upper-level
    summary and a way to appreciate all that SpriteKit and SceneKit do in the background
    for us. We hope that, at the very least, it makes you wish to pursue how to build
    your own custom rendering objects that might potentially allow the development
    of extremely performant and detailed games.
  prefs: []
  type: TYPE_NORMAL
- en: The Apple Metal API and the graphics pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the rules, if not *the golden rule* of modern video game development,
    is to keep our games running constantly at 60 frames per second or greater. If
    developing for VR devices and applications, this is of even more importance as
    dropped frame rates could lead to a sickening and game ending experience for the
    player.
  prefs: []
  type: TYPE_NORMAL
- en: In the past, being lean was the name of the game; hardware limitations prevented
    much from not only being written to the screen but how much memory storage a game
    could hold. This limited the number of scenes, characters, effects, and levels.
    In the past, game development was built more with an engineering mindset, so the
    developers made the things work with what little they had. Many of the games on
    8-bit systems and earlier had levels and characters that were only different because
    of elaborate sprite slicing and recoloring.
  prefs: []
  type: TYPE_NORMAL
- en: Over time, advances in hardware, particularly that of GPUs allowed for richer
    graphical experiences. This leads to the advent of computation-heavy 3D models,
    real-time lighting, robust shaders, and other effects that we can use to make
    our games present an even greater player experience; this while trying to stuff
    it all in that precious .016666 second/60 Hz window.
  prefs: []
  type: TYPE_NORMAL
- en: To get everything out of the hardware and combat the clash between a designer's
    need to make the best looking experience and the engineering reality of hardware
    limitations in even today's CPU/GPUs, Apple developed the Metal API.
  prefs: []
  type: TYPE_NORMAL
- en: CPU/GPU framework levels
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metal is what's known as a low-level GPU API. When we build our games on the
    iOS platform, there are different levels between the machine code in our GPU/CPU
    hardware and what we use to design our games. This goes for any piece of computer
    hardware we work with, be it Apple or others. For example, on the CPU side of
    things, at the very base of it all is the **machine code**. The next level up
    is the **assembly language** of the chipset. Assembly language differs based on
    the CPU chipset and allows the programmer to be as detailed as determining the
    individual registers to swap data in and out of in the processor. Just a few lines
    of a for-loop in C/C++ would take up a decent number of lines to code in assembly.
    The benefit of working in the lower levels of code is that we could make our games
    run much faster. However, most of the mid-upper level languages/APIs are made
    to work well enough so that this isn't a necessity anymore.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Game developers have coded in assembly even after the very early days of game
    development. In the late 1990's, the game developer Chris Sawyer created his game,
    **Rollercoster Tycoon™**, almost entirely in the x86 assembly language! Assembly
    can be a great challenge for any enthusiastic developer who loves to tinker with
    the inner workings of computer hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Moving up the chain we have where C/C++ code would be and just above that is
    where we'd find Swift and Objective-C code. Languages such as Ruby and JavaScript,
    which some developers can use in Xcode, are yet another level up.
  prefs: []
  type: TYPE_NORMAL
- en: That was about the CPU, now on to the GPU. The **Graphics Processing Unit**
    (**GPU**) is the coprocessor that works with the CPU to make the calculations
    for the visuals we see on the screen. The following diagram shows the GPU, the
    APIs that work with the GPU, and possible iOS games that can be made based on
    which framework/API is chosen.
  prefs: []
  type: TYPE_NORMAL
- en: '![CPU/GPU framework levels](img/00074.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: Like the CPU, the lowest level is the processor's machine code. To work as close
    to the GPU's machine code as possible, many developers would use Silicon Graphics'
    **OpenGL API**. For mobile devices, such as the iPhone and iPad, it would be the
    OpenGL subset, **OpenGL ES**. Apple provides a helper framework/library to OpenGL
    ES named **GLKit**. GLKit helps simplify some of the shader logic and lessen the
    manual work that goes into working with the GPU at this level. For many game developers,
    this was practically the only option to make 3D games on the iOS device family
    originally; though some use of iOS's Core Graphics, Core Animation and UIKit frameworks
    were perfectly fine for simpler games.
  prefs: []
  type: TYPE_NORMAL
- en: Not too long into the lifespan of the iOS device family, third-party frameworks
    came into play, which were aimed at game development. Using OpenGL ES as its base,
    thus sitting directly one level above it, is the **Cocos2D framework**. This was
    actually the framework used in the original release of Rovio's Angry Birds™ series
    of games back in 2009\. Eventually, Apple realized how important gaming was for
    the success of the platform and made their own game-centric frameworks, that is,
    the SpriteKit and SceneKit frameworks. They too, like Cocos2D/3D, sat directly
    above OpenGL ES. When we made SKSprite nodes or SCNNodes in our Xcode projects,
    up until the introduction of Metal, OpenGL operations were being used to draw
    these objects in the update/render cycle behind the scenes. As of iOS 9, SpriteKit
    and SceneKit use Metal's rendering pipeline to process graphics to the screen.
    If the device is older, they revert to OpenGL ES as the underlying graphics API.
  prefs: []
  type: TYPE_NORMAL
- en: Graphics pipeline overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This topic can be a book all on its own, but let''s take a look at the graphics
    pipeline to get an idea, at least on an upper level, of what the GPU is doing
    during a single rendered frame. We can imagine the graphical data of our games
    being divided in two main categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertex data**: This is the position information of where on the screen this
    data can be rendered. Vector/vertex data can be expressed as points, lines, or
    triangles. Remember the old saying about video game graphics, "everything is a
    triangle." All of those polygons in a game are just a collection of triangles
    via their point/vector positions. The GPU''s **Vertex Processing Unit** (**VPU**)
    handles this data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rendering/pixel data**: Controlled by the GPU''s Rasterizer, this is the
    data that tells the GPU how the objects, positioned by the vertex data, will be
    colored/shaded on the screen. For example, this is where color channels, such
    as RGB and alpha, are handled. In short, it''s the pixel data and what we actually
    see on the screen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here''s a diagram showing the graphics pipeline overview:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphics pipeline overview](img/00075.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'The graphics pipeline is the sequence of steps it takes to have our data rendered
    to the screen. The previous diagram is a simplified example of this process. Here
    are the main sections that can make up the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Buffer objects**: These are known as **Vertex Buffer Objects** in OpenGL
    and are of the class `MTLBuffer` in the Metal API. These are the objects we create
    in our code that are sent from the CPU to the GPU for **primitive processing**.
    These objects contain data, such as the positions, normal vectors, alphas, colors,
    and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Primitive processing**: These are the steps in the GPU that take our Buffer
    Objects, break down the various vertex and rendering data in those objects, and
    then draw this information to the frame buffer, which is the screen output we
    see on the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before we go over the steps of primitive processing done in Metal, we should
    first understand the history and basics of shaders.
  prefs: []
  type: TYPE_NORMAL
- en: What are shaders?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GPUs first came into use because of none other than the video game industry.
    Arcade cabinets in the 1970's had GPU chips separate from the main CPU to handle
    the specialized visual needs of the games compared with other computing applications
    at the time. Eventually, the need to draw 3D graphics in games in the mid-1990's
    led to the modern GPU architecture we have now. Shaders were actually first introduced
    in 1988 by Pixar back when the company was run by Apple's cofounder Steve Jobs.
    Shaders are little programs we can write directly to the GPU to process the vertex
    and pixel data. Originally, APIs such as OpenGL ES 1.0 didn't make use of shader
    processing but instead were what's known as fixed-function APIs. In fixed-function
    APIs, programmers just referenced simple set rendering commands to the GPU. As
    GPUs evolved and took more work away from the CPU, the use of shaders increased.
    Although a rather more advanced way to traverse the graphics pipeline than the
    fixed-function methodology, shaders allow for even deeper customization of what
    the GPU displays to the screen. Game developers and 3D artists continue to push
    visual effects in games with them.
  prefs: []
  type: TYPE_NORMAL
- en: From OpenGL 2.0 and onwards, shaders were built in the API's C-like language
    named GLSL. In the Apple Metal API, we build shaders with the Metal Shading Language,
    which is a subset of C++11 of the file type `.metal` and can run the pipeline
    in either Objective-C or Swift with our view controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Types of shaders
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Shaders come in a number of types that continue to grow as 3D games and art
    animation continues to progress. The most commonly used are Vertex shaders and
    Fragment shaders. Vertex shaders are used to transform 3D coordinates into 2D
    coordinates for the screen to display, in short, the positioning data of our graphics.
    Fragment shaders, also known as Pixel shaders, are what are used to convert colors
    and other visual attributes of pixels on the screen. These other attributes of
    Fragment Shaders can include bump mapping, shadows, and specific highlights as
    well. We emphasized the word *attributes* because that's usually the name given
    for the properties or input of our shader programs.
  prefs: []
  type: TYPE_NORMAL
- en: Here is a code sample of a simple Vertex and Fragment shader written in the
    Metal Shading Language.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The code here is a bit different than what we've seen throughout the course
    of the book. Let's go over it line by line.
  prefs: []
  type: TYPE_NORMAL
- en: The Metal Shading Language is a C++11-like language, so we see that the Metal
    Standard Library is imported into the shader file with the line `#include <metal_stdlib>`
    in addition to `using namespace metal;`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next line is the creation of our Vertex shader using the keyword `vertex`.
    This shader is a vertex of four floats. Why four floats when 3D space only deals
    with *x*, *y*, and *z* coordinates? To summarize, 3D matrix math involves a fourth
    component, *w*, to accurately handle the math calculations of 3D space. In short
    if *w= 0*, the *x*, *y*, and *z* coordinates are vectors; if *w = 1*, then those
    coordinates are points. The purpose of this shader will be to draw simple points
    to the screen, so *w* will be 1.0.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Here, we create a pointer to an array of float3 type (holders for our *x*, *y*,
    and *z* coordinates) and set it to the very first buffer with the `[[ buffer(0)
    ]]` declaration. The `[[ ]]` syntax is used to declare inputs/attributes for our
    shaders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The unsigned integer `vertexID` is what we name the `vertex_id` attribute of
    this particular array of vertices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is where the float4 type is returned, or in this case, the final position
    of this vertex array. We see that it returns two sections of the output: the first
    being the reference to this vertex array, identified by the `vertex_id` attribute
    and the `w` value of `1.0`, to represent that these are points in space.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This line is where we create the fragment shader, using the `fragment` keyword.
    This shader is of the data type `half4`, which is an array of [4,4] 16-bit floats.
    This is, in this case, ultimately to create 16-bit colored pixels. The data in
    this [4,4]-component vector type saves 16 bits to R, G, B, and alpha channels.
    This shader is going to simply show pure white pixel shading with no transparency,
    so we simply write `return half4(1.0);`. This sets all of the bits to 1, which
    is equivalent to `rgba(1,1,1,1)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When we create a Buffer Object, which can just be a Struct of floating points
    on the screen, we pass that data through these shaders and out would pop up a
    white triangle or set of triangle shapes on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: Looking back at the *Graphics pipeline* diagram, we see that after the vertex
    shader is calculated, the GPU does what's known as **Primitive Assembly**. This
    is essentially where the points and vectors defined in the vertex shader are mapped
    to coordinates in screen space. The Rasterizer step, in simple terms, then figures
    from the vertex data where and how we can and can't color that pixel data onto
    the screen using the fragment shader information. After taking in the fragment
    shader information, the GPU then uses that information for the blending of that
    pixel data. Finally, that output is sent to or committed to the frame buffer where
    the player sees that output. This all happens in a single draw call in the render
    cycle. Having all of your game's lights, pixels, effects, physics, and other graphics
    cycle through this in .016666 seconds is the name of the game.
  prefs: []
  type: TYPE_NORMAL
- en: We'll go over some more Metal code later but understand for now that shaders
    are like little instruction factories for data input we send to them in our Swift/Object-C
    code. Other shader types that have arisen over the years are Geometry Shaders
    and Tessellation Shaders.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Both the Vertex and Fragment shaders are present in this single `.metal` file,
    but typically shaders are written in separate files. Xcode and Metal will combine
    all `.metal` files in your project, so it doesn't matter if the shaders are in
    one file or not. OpenGL's GLSL for the most part forces the separation of shader
    types.
  prefs: []
  type: TYPE_NORMAL
- en: For years, OpenGL worked well for many different GPUs but as we all see, Apple
    Metal allows us to perform draw calls up to 10x times faster than OpenGL ES.
  prefs: []
  type: TYPE_NORMAL
- en: Why is Metal faster than OpenGL ES?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In late 2013, Apple announced the **iPhone 5s**. Built into the 5s was the **A7
    Processor**, the first 64 bit GPU for the iOS device family. It provided a decent
    graphical boost compared with prior devices and reflected how GPUs in mobile devices
    were quickly catching up to gaming consoles released just a few years prior. OpenGL,
    though a staple in low-level graphics APIs, didn't squeeze the most out of the
    A7 chip.
  prefs: []
  type: TYPE_NORMAL
- en: Seen in the next diagram, the interaction between the CPU and GPU doesn't always
    perform the optimal way we'd want it to for our games.
  prefs: []
  type: TYPE_NORMAL
- en: '![Why is Metal faster than OpenGL ES?](img/00076.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: 'Be it textures, shaders, or render targets, draw calls use their own state
    vector. The CPU via the low-level API uses much of that time verifying the state
    of the draw call. This process is very expensive for the CPU. What happens is
    that in many cycles, the GPU is sitting idle, waiting for the CPU to finish its
    past instruction. Here''s what''s taking up all of that time in the API:'
  prefs: []
  type: TYPE_NORMAL
- en: '**State validation**: Confirming API usage is valid. This encodes API state
    to the hardware state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Shader compilation**: Runtime generation of the shader machine code. This
    deals with interactions between the state and shaders.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sending work to the GPU**: Managing resource residency batching commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What Apple did with their Metal API is do these steps in a smarter fashion.
    Shader compilation is done during the application's load time. There's no need
    to reload the shaders every cycle; this was simply a relic of older hardware limitations.
    This is why in our previous code example, we can build more than one shader in
    one Metal file, while this was prohibited in OpenGL ES. State validation, though
    important, doesn't need to be checked every cycle. Checking the state validation
    can be set to happen only when new content is loaded.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even with Metal's advantages, this is why it's recommended to store 2D animations
    in **SpriteSheets**. We mentioned SpriteSheets back in our discussion of on SpritKit.
    They are a collection of sprites fitted onto one texture. The graphics pipeline
    then only has to deal with one version of that content. Internally under the hood
    of SpriteKit, the GPU then doesn't have to do as many state vector calls compared
    to having each character animation being placed on its own separate texture.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last process for the CPU is when it sends the information out to the GPU
    for processing. This is going to be done during each draw call, and in either
    Metal or Open GL ES, it will still be this process that will happen the most frequently.
    Here is the result of this internal, low-level restructuring done in the Metal
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Why is Metal faster than OpenGL ES?](img/00077.jpeg)'
  prefs: []
  type: TYPE_IMG
- en: As we see in the diagram from *WWDC14*, there are up to 10 extra draw calls
    that can be added during the render cycle! We can use that time saved for other
    processes instead of extra draw calls, such as more physics or AI in our games.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The cycle diagrams shown are from the original Metal API announcement at *WWDC2014*
    and used a frame rate of 30 fps. If developing for VR where 60 fps or greater
    is necessary for a working game, these numbers are halved. Either way this is
    rather impressive for mobile device GPUs. Search online for games made in Metal
    and you'd be impressed. With this much room to add more to our game during each
    render cycle, there's no reason not to have an impressive game at the full 60
    fps. Additionally, as of iOS 9, the SpriteKit and SceneKit frameworks by default
    are backed by Metal. Even if the Metal API is too much to understand, we can still
    utilize these render saving benefits from what we already learned about these
    frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: The basic Metal object/code structure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To finish off our talk about Apple Metal, let's look at an overview of the API's
    object and code structuring. We already briefly saw some shader code in the Metal
    Shading Language, so let's see how we can work with this API in our projects.
  prefs: []
  type: TYPE_NORMAL
- en: '| Objects | Purpose |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Device | Reference to the GPU |'
  prefs: []
  type: TYPE_TB
- en: '| Command queue | Serial sequence of command buffers |'
  prefs: []
  type: TYPE_TB
- en: '| Command buffer | Contains GPU hardware commands |'
  prefs: []
  type: TYPE_TB
- en: '| Command encoder | Translates API commands to GPU hardware commands |'
  prefs: []
  type: TYPE_TB
- en: '| State | Framebuffer configuration, depth, samplers, blend, and so on |'
  prefs: []
  type: TYPE_TB
- en: '| Code | Shaders (vertex, fragment, geometry, and tessellation) |'
  prefs: []
  type: TYPE_TB
- en: '| Resources | Textures and Data Buffer Objects (vertices, constants, and so
    on) |'
  prefs: []
  type: TYPE_TB
- en: The preceding table represents the various types of objects that we'd work with
    if writing a game directly in the Metal API. They are the Device, the State, the
    Command Buffer, our Shaders, Textures, and many more.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can import the Metal API into `ViewController.swift` class with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This imports the Metal API. The QuartzCore API is needed as well since the `CAMetalLayer`
    object we will work with is a member of that library. Also, make sure that you
    set your target device to an actual iOS device as new or newer than the iPhone
    5S, the Xcode simulator does not support Metal. Otherwise, Xcode will give you
    the *Could Not Build* Objective-C model `Metal` error. This is true as of the
    writing of this book with the Xcode 7 Beta. Over time and probably after the official
    public release of the El Capitan OS, this will no longer be needed. For now, to
    test your own custom Metal code, you will have to test on an actual device. Doing
    so will involve having to pay for your own Apple Development account. More on
    this is given in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the order in which we''d have to work with the objects in the table
    shown previously as well as some code samples in Swift that accomplish these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the reference to the Device with the `MTLDevice` class as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `CAMetalLayer` object for these objects to be placed on the screen
    as:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create Vertex Data/Buffer Object(s) (VBOs) to send data to shaders as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create our shaders that will work with these VBOs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We did this in our previous shader code samples. The vertex data combined with
    our previously made shaders together create a simple white triangle to the screen.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Set up a Render Pipeline as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a command queue as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To actually render these objects in our game, we''d have to do the following
    processes in our view controller:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a display link. This is a timer that refreshes every time the screen
    refreshes. It's a member of the class `CADisplayLink` and at every screen refresh,
    we call the `gameRenderLoop` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `gameRenderLoop` function can look like the following. It calls the soon-to-be
    filled in function, `render()`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a Render Pass Descriptor. For this example, a mostly red texture is
    to be created around our white triangle as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a Command Buffer in our `render()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Create a Render Command Encoder. In other words, a set of commands for `commandQueue`.
    In the code example given later, this tells the GPU to draw triangles with the
    VBO we created earlier. This is placed (in this example) in the `render()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Commit your Command Buffer. This essentially tells the GPU to do its draw call
    based on the commands that have been packed into the `commandBuffer` object. Place
    this after the past code's `if` statement in the `render()` function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That is the short of it. That's the general process of drawing a simple triangle
    to the screen and manually creating the render loop on the GPU.
  prefs: []
  type: TYPE_NORMAL
- en: Should you rather opt for SpriteKit and SceneKit to do all of this manual work
    for you? That would be understandable. Remember though, like when playing a game
    on hard mode, it comes with its rewards to take the harder route. Yes, as of iOS
    9, the SpriteKit and SceneKit frameworks are default to Metal. Game engines, such
    as Unity and Unreal Engine, even implement Metal when converting projects to the
    platform. However, knowing how to build your games in a low-level graphics API,
    such as Metal or OpenGL, will give the developer the ability to have the potential
    for most lean/fast performing game for the device family. Be sure to check out
    some of the games created with Metal next time you search online. They can really
    give your players a great experience. At the same time, this can challenge your
    skills as a game developer since being a game developer is the combination of
    an artist, engineer, and computer scientist. Working directly in the GPU's basic
    functions will challenge all of that.
  prefs: []
  type: TYPE_NORMAL
- en: 'To dive more into the rabbit hole that is low-level graphics development with
    Metal, check out these links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://developer.apple.com/metal/](https://developer.apple.com/metal/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/data-types/data-types.html](https://developer.apple.com/library/ios/documentation/Metal/Reference/MetalShadingLanguageGuide/data-types/data-types.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://www.raywenderlich.com/77488/ios-8-metal-tutorial-swift-getting-started](http://www.raywenderlich.com/77488/ios-8-metal-tutorial-swift-getting-started)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://realm.io/news/3d-graphics-metal-swift/](https://realm.io/news/3d-graphics-metal-swift/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first link is to the official Apple Developer page for Metal. The next link
    is Apple's list of data types used in the Metal API. The last two links are two
    separate tutorials to make simple Metal scenes in Swift. Some of the code we used
    can be found in these tutorials as well as full Xcode projects. The first of these
    two links are to the iOS tutorial site [www.raywenderlich.com](http://www.raywenderlich.com).
    The last link is to a page that has a great video presentation and full instructions
    on Swift and Metal 3D graphics by former Apple Engineer, *Warren Moore*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congrats on getting this far. If this book were a game, we'd probably have earned
    an achievement for this chapter alone. As we saw, working with a low-level API
    such as Metal can be a bit daunting. We first reviewed what it means when developers
    and engineers mention lower and upper level frameworks and code. On the CPU side,
    we saw that the lowest level is the machine's code with Swift and Objective-C
    in the middle, and above C/C++ and Assembly code. Next, we spoke about the GPU
    side and where the visual graphics APIs we've gone over in the past chapters stand
    in the hierarchy. We then got an understanding of the history of lower-level graphics
    APIs such as OpenGL ES, how the graphic pipeline generally works under the hood,
    and how to make basic shaders. Finally, we reviewed why Metal is faster during
    the render cycle than OpenGL, the general structure behind Metal, and some of
    the code/objects used to manually set up the render loop. This chapter merely
    scratched the surface on this topic, so if you are up to the challenge, it's highly
    recommended to continue reading documentation on how Metal can make your games
    stand out from the rest.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should now have all that it takes to make a game on the iOS
    platform. The last essential lesson for iOS game development is learning how to
    test, publish, and update your published game in the app store. In the next chapter,
    let's learn how to get that game on the Apple app store.
  prefs: []
  type: TYPE_NORMAL
