- en: Chapter 9. Performance Testing and Profiling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we studied and developed tests for our Android application.
    Those tests let us evaluate the compliance to a certain number of specifications
    and allow us to determine if the software is behaving correctly or according to
    these rules by taking a binary verdict, whether it complies or not. If it does
    the software is correct; if it does not we have to fix it until it does.
  prefs: []
  type: TYPE_NORMAL
- en: In many other cases, mainly after we have verified that the software conforms
    to all these specifications, we want to move forward and know how or in what manner
    they are satisfied, and at the same time how the system performs under different
    situations to analyze other attributes such as usability, speed, response time,
    and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to Android Developer''s Guide ([http://developer.android.com/guide/index.html](http://developer.android.com/guide/index.html)),
    these are the best practices when it comes to designing our application:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing for performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing for responsiveness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing for seamlessness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It's extremely important to follow these best practices and start thinking mainly
    in terms of performance and responsiveness from the very beginning of the design.
    Since our application will run on mobile devices with limited computer power,
    the bigger gains are obtained by identifying the targets for the optimization
    once our application is built, at least partially, and applying the performance
    testing that we will be discussing soon.
  prefs: []
  type: TYPE_NORMAL
- en: 'As Donald Knuth popularized years ago:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Premature *optimization is the root of all evil"*.'
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: These optimizations, which are based on guesses, intuition, and even superstition
    often interfere with the design over short term periods, and with readability
    and maintainability over long term periods. On the contrary, *micro-optimizations*
    are based on identifying the bottlenecks or hot-spots that require optimization,
    apply the changes, and then benchmark again to evaluate the improvements of the
    optimization. So the point we are concentrating on here is on measuring the existing
    performance and the optimization alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will introduce a series of concepts related to benchmarking and
    profiles as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional logging statement methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating Android performance tests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using profiling tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microbenchmarks using Caliper
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye Olde Logge method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sometimes this is too simplistic for real scenarios but I'm not going to say
    that it could not help in some cases mainly because its implementation takes minutes
    and you only need the `logcat` text output to analyze the case, which comes in
    handy during situations as described in previous chapters where you want to automate
    procedures or apply Continuous Integration.
  prefs: []
  type: TYPE_NORMAL
- en: 'This method consists in timing a method, and or a part of it, surrounding it
    by two time measures and logging the difference at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This is very straightforward. We take the times and log the difference. For
    this we are using the `Log.i()` method and we can see the output in `logcat` while
    we run the application. You can control the execution of this benchmark by setting
    `true` or `false` to the `BENCHMARK_TEMPERATURE_CONVERSION` constant that you
    should have defined elsewhere.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we launch the activity with the `BENCHMARK_TEMPERATURE_CONVERSION` constant
    set to true in the `logcat`, we will receive messages like these every time the
    conversion takes place:'
  prefs: []
  type: TYPE_NORMAL
- en: '**INFO/TemperatureConverterActivity(392): TemperatureConversion took 55 ms
    to complete**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**INFO/TemperatureConverterActivity(392): TemperatureConversion took 11 ms
    to complete**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**INFO/TemperatureConverterActivity(392): TemperatureConversion took 5 ms to
    complete**.'
  prefs: []
  type: TYPE_NORMAL
- en: Something you should take into account is that these benchmark-enabling constants
    should not be enabled in the production build, as other common constants are used
    like DEBUG or LOGD. To avoid this mistake you should integrate the verification
    of these constants values in the build process you are using for automated builds
    such as Ant or Make.
  prefs: []
  type: TYPE_NORMAL
- en: Pretty simple, but this would not apply for more complex cases.
  prefs: []
  type: TYPE_NORMAL
- en: Performance tests in Android SDK
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If the previous method of adding log statements does not suit you, there is
    a different method of getting performance test results from our application.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, performance tests in Android SDK are half baked (at least up
    to Android 2.3 Gingerbread, the latest version available at the time this book
    was written). There is no reasonable way of getting performance test results from
    an Android SDK application as the classes used by Android tests are hidden in
    the Android SDK and only available to system applications, that is to applications
    that are built as part of the main build or system image. This strategy is not
    available for SDK applications so we are not digging deeper in that direction
    and we will focus on other available choices.
  prefs: []
  type: TYPE_NORMAL
- en: Launching the performance test
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: These tests are based on a similar approach like the one used by Android to
    test system applications. The idea is to extend `android.app.Instrumentation`
    to provide performance snapshots, automatically creating a framework that we can
    even extend to satisfy other needs. We are presenting a simple case here due to
    the limitations imposed by this medium.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the LaunchPerformanceBase instrumentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our first step is to extend `Instrumentation` to provide the functionality
    we need. We are using a new package named `com.example.aatg.tc.test.launchperf`
    to keep our tests organized:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We are extending `Instrumentation` here. The constructor initialized the two
    fields in this class: `mResults` and `mIntent`. At the end we invoke the method
    `setAutomaticPerformanceSnapshots()` which is the key here to create this performance
    test.'
  prefs: []
  type: TYPE_NORMAL
- en: The method `LaunchApp()` is in charge of starting the desired `Activity` and
    waiting before returning.
  prefs: []
  type: TYPE_NORMAL
- en: The `finish()` method logs the results received and then invokes `Instrumentation's
    finish()`.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the TemperatureConverterActivityLaunchPerformance class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This class sets up the `Intent` to invoke the `TemperatureConverterActivity`
    and furnish the infrastructure provided by the `LaunchPerformanceBase` class to
    test the performance of launching our `Activity:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Here, `onCreate()` calls `super.onCreate()` as the Android lifecycle dictates.
    Then the `Intent` is set, specifying the class name and the package. Then one
    of the `Instrumentation's` methods is called, `start()`, which creates and starts
    a new thread in which to run instrumentation. This new thread will make a call
    to `onStart()`, where you can implement the instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Then `onStart()` implementation follows, invoking `LaunchApp()` and `finish()`.
  prefs: []
  type: TYPE_NORMAL
- en: Running the tests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To be able to run this test we need to define the specific `Instrumentation`
    in the `AndroidManifest.xml` of the `TemperatureConverterTest` project.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the snippet of code we have to add to the manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once everything is in place we are ready to start running the test.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, install the APK that includes these changes. Then, we have several options
    to run the tests as we have reviewed in previous chapters. In this case we are
    using the command line as it is the easiest way of getting all the details. Replace
    the serial number for what is applicable in your case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We receive the set of results for this test in the standard output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We have highlighted two of the values we are interested in: **execution_time**
    and **cpu_time**. They account for the total execution time and the CPU time used
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Running this test on an emulator increases the potential for mis-measurement,
    because the host computer is running other processes that also take on the CPU,
    and the emulator does not necessarily represent the performance of a real piece
    of hardware.
  prefs: []
  type: TYPE_NORMAL
- en: Because of this we are taking these two measures into account. The `execution_time`
    gives us the real time and `cpu_time` the total time used by the CPU to compute
    our code.
  prefs: []
  type: TYPE_NORMAL
- en: Needless to say, that in this and any other case where you measure something
    that is variable over time, you should use a measurement strategy and run the
    test several times to obtain different statistical values, such as average or
    standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, the current implementation of Android ADT does not allow using
    an instrumentation that does not extend `android.test.InstrumentationTestRunner`,
    though `.launchperf.TemperatureConverterActivityLaunchPerformance` extends `LaunchPerformaceBase`
    that extends `Instrumentation`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This screenshot shows the error trying to define this Instrumentation in Eclipse
    Run Configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Running the tests](img/3500_09_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using the Traceview and dmtracedump platform tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Android SDK includes among its various tools two that are specially intended
    to analyze performance problems and potentially determine the target to apply
    optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'These tools have an advantage over other alternatives: usually no modification
    to the source code is needed for simpler tasks. However, for more complex cases
    some additions are needed, but they are very simple as we will see shortly.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you don''t need precision about starting and stopping tracing, you can drive
    it from the command line or Eclipse. For example, to start tracing from the command
    line you can use the following command. Remember to replace the serial number
    for what is applicable in your case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Do something, for example enter a temperature in the Celsius field to force
    a conversion.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, if you need more precision about when profiling starts, you can
    add this piece of code instead of the previous one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This will create a trace file, using the default name `dmtrace.trace` in the
    Sdcard by invoking `Debug.startMethodTracing()`, which starts method tracing with
    the default log name and buffer size. When we are done, we call `Debug.stopMethodTracing()`
    to stop the profiling.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To be able to write to the Sdcard the application requires a `android.permission.WRITE_EXTERNAL_STORAGE`
    permission added in the manifest.
  prefs: []
  type: TYPE_NORMAL
- en: For VMs earlier than Android 2.2, the permission is required even for doing
    this from Eclipse as the file is also generated. Starting with Android 2.2, the
    stream is sent through the JDWP connection and the permission is not needed anymore.
  prefs: []
  type: TYPE_NORMAL
- en: You need to exercise the application in order to obtain the trace file. This
    file needs to be pulled to the development computer to be further analyzed using
    `traceview:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this command `traceview''s` window appears displaying all the
    information collected:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Using the Traceview and dmtracedump platform tools](img/3500_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Remember that enabling profiling really slows down the application execution
    so the measure should be interpreted by its relative weight not by their absolute
    values.
  prefs: []
  type: TYPE_NORMAL
- en: The top part of the window shows the **timeline panel** and a colored area for
    every method. Time increases to the right. There are also small lines under the
    colored row displaying the extent of all the calls to the selected method.
  prefs: []
  type: TYPE_NORMAL
- en: We profiled a small segment of our application so only the main thread was running.
    In other cases where other threads were running during the profiling, this information
    will also be displayed.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom part shows the **profile panel** and every method executed and its
    parent-child relationship. We refer to calling methods as *parents* and called
    methods as *children*. When clicked, a method expands to show its parents and
    children. Parents are shown with a purple background and children with a yellow
    background.
  prefs: []
  type: TYPE_NORMAL
- en: Also the color selected for the method, done in a round-robin fashion, is displayed
    before the method name.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, at the bottom there's a **Find:** field where we can enter a filter
    to reduce the amount of information displayed. For example if we are only interested
    in displaying methods in the `com.example.aatg.tc` package, we should enter **com/example/aatg/tc**.
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on a column will set the order of the list according to this column
    in ascending or descending order.
  prefs: []
  type: TYPE_NORMAL
- en: 'This table describes the available columns and their descriptions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Column | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| name | The name of the method including its package name in the form described
    above, which is using "/" (slash) as the delimiter. Also the parameters and the
    return type are displayed. |'
  prefs: []
  type: TYPE_TB
- en: '| Incl% | The inclusive time, as a percentage of the total time, used by the
    method. That is including all of its children. |'
  prefs: []
  type: TYPE_TB
- en: '| Inclusive | The inclusive time, in milliseconds, used by this method. That
    is including this method and all of its children. |'
  prefs: []
  type: TYPE_TB
- en: '| Excl% | The exclusive time, as a percentage of the total time, used by the
    method. That is excluding all of its children. |'
  prefs: []
  type: TYPE_TB
- en: '| Exclusive | The exclusive time, in milliseconds, this is the total time spent
    in this method. That is excluding all of its children. |'
  prefs: []
  type: TYPE_TB
- en: '| Calls+RecurCalls/Total | This column shows the number of calls for this method
    and the number of recursive calls.The number of calls compared with the total
    number of calls made to this method. |'
  prefs: []
  type: TYPE_TB
- en: '| Time/Call | The time in milliseconds of every call.That is Inclusive/Calls.
    |'
  prefs: []
  type: TYPE_TB
- en: Microbenchmarks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Benchmarking is the act of running a computer program or operation in order
    to compare operations in a way that produces quantitative results, normally by
    running a set of tests and trials against them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmarks can be organized in two big categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Macrobenchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microbenchmarks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Macrobenchmarks** exist as a means to compare different platforms in specific
    areas such as processor speed, number of floating point operations per unit of
    time, graphics and 3D performance, and so on. They are normally used against hardware
    components but can also be used to test software specific areas, such as compiler
    optimization or algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: As opposed to these traditional macrobenchmarks, a **microbenchmark** attempts
    to measure the performance of a very small piece of code, often a single method.
    The results obtained are used to choose between competing implementations that
    provide the same functionality deciding the optimization path.
  prefs: []
  type: TYPE_NORMAL
- en: The risk here is to microbenchmark something different than what you think you
    are measuring. This is something to take into account mainly in the case of JIT
    compilers as used by Android starting with version 2.2 Froyo. The JIT compiler
    may compile and optimize your microbenchmark differently than the same code in
    your application. So, be cautious when taking your decision.
  prefs: []
  type: TYPE_NORMAL
- en: This is different from the profiling tactic introduced in the previous section
    as this approach does not consider the entire application but a single method
    or algorithm at a time.
  prefs: []
  type: TYPE_NORMAL
- en: Caliper microbenchmarks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Caliper** is Google''s Open Source framework for writing, running, and viewing
    results of microbenchkmarks. There are many examples and tutorials on its website
    at [http://code.google.com/p/caliper/](http://code.google.com/p/caliper/).'
  prefs: []
  type: TYPE_NORMAL
- en: It's a work in progress but still useful in many circumstances. We are exploring
    its essential use here and will introduce more Android related usage in the next
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Its central idea is to benchmark methods, mainly to understand how efficient
    they are; we may decide that this is the target for our optimization, perhaps
    after analyzing the results provided by profiling via traveview.
  prefs: []
  type: TYPE_NORMAL
- en: Caliper benchmark extends normally `com.google.caliper.SimpleBenchmark` which
    implements the `Benchmark` interface. Benchmarks are structured in a similar fashion
    as JUnit 3 tests and maintain the same structure with the difference that here
    benchmarks start with the prefix **time** as opposed to **test**. Every benchmark
    then accepts an `int` parameter usually named `reps`, indicates the number of
    repetitions to benchmark the code that sits inside the method surrounded by a
    loop counting the repetitions.
  prefs: []
  type: TYPE_NORMAL
- en: The `setUp()` method is also present.
  prefs: []
  type: TYPE_NORMAL
- en: We need caliper installed in our computer. At the time of this writing, caliper
    is not distributed as binary but as source code that you can download and build
    yourself. Follow the instructions provided in its website which basically is getting
    the source code and building yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Put in a very simple way, you can do it using these command lines. You need
    Subversion and Ant installed to do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The `calliper-0.0.jar` and `allocation.jar` will be found in the `build/caliper-0.0/lib`
    subdirectory.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the TemperatureConverterBenchmark project
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let's start by creating a new Java project in Eclipse. Yes, this time is not
    an Android project, just Java.
  prefs: []
  type: TYPE_NORMAL
- en: For consistency use the package `com.example.aatg.tc.benchmark` as the main
    package.
  prefs: []
  type: TYPE_NORMAL
- en: Add the `caliper` library and the existing `TemperatureConverter` project to
    the **Java Build Path** in the project's properties.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then create the `TemperatureConverterBenchmark` class that is containing our
    benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We have a `setUp()` method that, similar to JUnit tests, is run before the benchmarks
    are run. This method initializes an array of random temperatures used in the conversion
    benchmark. The size of this array is passed as a parameter to caliper and annotated
    here with the `@Param` annotation. Caliper will provide the value of this parameter
    automatically.
  prefs: []
  type: TYPE_NORMAL
- en: We use a Gaussian distribution for the pseudo-random temperatures as this could
    be a good model of the reality.
  prefs: []
  type: TYPE_NORMAL
- en: Then the benchmark itself. As we noted before it should start with the prefix
    time, as in this instance `timeCelsiusToFahrenheit()`. Inside this method we loop
    for the repetitions and invoke the conversion `TemperatureConverter.celsiusToFahrenheit()`
    which is the method we wish to benchmark.
  prefs: []
  type: TYPE_NORMAL
- en: Running caliper
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To run caliper we use a script which is based on the script that comes with
    the distribution. Be sure to place it in a directory included in the `PATH` or
    use the correct path to invoke it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Adapt it to your needs. Before running it, remember that we still need to set
    our `CLASSPATH` so caliper can find the `TemperatureConverter` and the benchmarks
    themselves. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterwards we can run caliper as:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run the benchmarks and if everything goes well we will be presented
    with the results:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0% Scenario{vm=java, benchmark=CelsiusToFahrenheit, size=1} 8.95ns; σ=0.11ns
    @ 10 trials**'
  prefs: []
  type: TYPE_NORMAL
- en: '**.caliperrc found, reading properties..**.'
  prefs: []
  type: TYPE_NORMAL
- en: '**ns logarithmic runtime**'
  prefs: []
  type: TYPE_NORMAL
- en: '**9 XXXXXXXXXXXXXXXXXXXXXXXXXXXXXX**'
  prefs: []
  type: TYPE_NORMAL
- en: '**vm: java**'
  prefs: []
  type: TYPE_NORMAL
- en: '**benchmark: CelsiusToFahrenheit**'
  prefs: []
  type: TYPE_NORMAL
- en: '**size: 1**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively we can repeat the benchmark for different number of temperatures
    to find out if the values itself affect the performance of the conversion. In
    such cases we run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Here we added different sizes for the temperatures array and the results obtained
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**0% Scenario{vm=java, trial=0, benchmark=CelsiusToFahrenheit, size=1} 3.47
    ns; σ=0.19 ns @ 10 trials**'
  prefs: []
  type: TYPE_NORMAL
- en: '**33% Scenario{vm=java, trial=0, benchmark=CelsiusToFahrenheit, size=10} 11.67
    ns; σ=1.20 ns @ 10 trials**'
  prefs: []
  type: TYPE_NORMAL
- en: '**67% Scenario{vm=java, trial=0, benchmark=CelsiusToFahrenheit, size=100} 63.06
    ns; σ=3.83 ns @ 10 trials**'
  prefs: []
  type: TYPE_NORMAL
- en: '**67% Scenario{vm=java, trial=0, benchmark=CelsiusToFahrenheit, size=100} 63.06
    ns; σ=3.83 ns @ 10 trials**'
  prefs: []
  type: TYPE_NORMAL
- en: '**size ns linear runtime**'
  prefs: []
  type: TYPE_NORMAL
- en: '**1 3.47 =**'
  prefs: []
  type: TYPE_NORMAL
- en: '**10 11.67 =====**'
  prefs: []
  type: TYPE_NORMAL
- en: '**100 63.06 ==============================**'
  prefs: []
  type: TYPE_NORMAL
- en: '**vm: java**'
  prefs: []
  type: TYPE_NORMAL
- en: '**trial: 0**'
  prefs: []
  type: TYPE_NORMAL
- en: '**benchmark: CelsiusToFahrenheit**'
  prefs: []
  type: TYPE_NORMAL
- en: To help visualize these results there is a service hosted in the Google AppEngine
    ([http://microbenchmarks.appspot.com](http://microbenchmarks.appspot.com)) that
    accepts your result's data and lets you visualize them in a much better way. To
    access this service you should obtain an API key providing your Google login.
    Once obtained this key is placed in the `.caliperrc` file in your home directory
    and next time you run the benchmarks the results will be uploaded.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.caliperrc` would look like this snippet after you pasted the obtained
    API key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the benchmarks again using the same command line as before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition to the text output, you will receive the instructions to access
    the results online. You can view current and previous benchmark results online
    at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://microbenchmarks.appspot.com/run/user@gmail.com/com.example.aatg.tc.benchmark.TemperatureConverterBenchmark.CelsiusToFahrenheitBenchmark](http://microbenchmarks.appspot.com/run/user@gmail.com/com.example.aatg.tc.benchmark.TemperatureConverterBenchmark.CelsiusToFahrenheitBenchmark).'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous URL replace `user@gmail.com` with your real Google login username
    that you used to generate the API key.
  prefs: []
  type: TYPE_NORMAL
- en: '![Running caliper](img/3500_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter we dissected the available alternatives to test the performance
    measures of our application benchmarking and profiling our code.
  prefs: []
  type: TYPE_NORMAL
- en: While some options that should be provided by the Android SDK are not yet completed
    by the time of this writing, and there is no possibility to implement Android
    `PerformanceTestCases` because some code is hidden in the SDK, we visited and
    analyzed some other valid alternatives.
  prefs: []
  type: TYPE_NORMAL
- en: Among these alternatives we found that we can use simple log statements to more
    sophisticated code extending Instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: Subsequently we analyzed profiling alternatives and described and exemplified
    the use of `traceview` and `dmtracedump`.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we discovered caliper—a microbenchmarking tool that has native support
    for Android. However, we introduced its most basic usage and postponed more specific
    Android and Dalvik VM usage for the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter we will be building Android from source code to obtain an
    EMMA instrumented build and we will be executing coverage report on our code.
    We will also introduce alternative tactics and tools by the end of the chapter.
  prefs: []
  type: TYPE_NORMAL
