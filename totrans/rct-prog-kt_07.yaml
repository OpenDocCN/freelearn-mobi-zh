- en: Concurrency and Parallel Processing in RxKotlin with Schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So, up until now, you have learned the basics of reactive programming. You learned
    about Observable, Observers, and Subjects, as well as backpressure, Flowable,
    processors, and operators. Now, it's time for us to learn some other new topics
    in reactive programming, probably the most important ones—concurrency and parallel
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: A popular misconception regarding reactive programming is that reactive programming
    is multi-threaded by default. The truth is actually that RxKotlin works on a single
    thread by default, although it provides us with loads of operators to implement
    multi-threading as per our business logic and requirements with ease.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to concurrency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `subscribeOn()` and `observeOn()` operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parallelization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The definition of concurrency can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: As a programming paradigm, concurrent computing is a form of modular programming,
    namely factoring an overall computation into subcomputations that may be executed
    concurrently.
  prefs: []
  type: TYPE_NORMAL
- en: – Wikipedia
  prefs: []
  type: TYPE_NORMAL
- en: As the definition says, concurrency is all about breaking the entire task into
    small parts and then executing them concurrently (there's a small difference between
    concurrent execution and parallel execution, which we will discuss shortly).
  prefs: []
  type: TYPE_NORMAL
- en: So, what does it mean to execute subcomputations concurrently? Let's look at
    a real-life example. Think of a situation where you're cooking a new dish at your
    home and you have three chores—bring the spices, cut the vegetables, and also
    marinate something. Now, if you're doing it all alone, you have to do them one
    by one, but if you have a family member at your disposal, then you can distribute
    the tasks between the two of you. You can cut the vegetables while the other person
    is bringing the spices, and whoever between you two completes early can continue
    on the third task—marinating the food.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of you and the family member (who helped you) as two threads,
    or, to be more specific, you're the main thread of the program (here, cooking)
    as you're the responsible person for the entire job, and you'll be distributing
    tasks between you and the family member, who is a worker thread. Together, you
    and your family member form a thread pool.
  prefs: []
  type: TYPE_NORMAL
- en: The entire program will execute faster if there are more threads and the complete
    task is divided properly among them.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel execution versus concurrency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The concepts of concurrency and parallelization are not only related, but they
    are deeply connected to each other; you may think of them as identical twin brothers.
    They look almost the same, but there are differences. Let's try to discover.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous example, we discussed concurrency, but it seemed to execute
    in parallel. Now, let's take a better example, which will not only help us understand
    parallelization, but will allow us to understand the differences between concurrency
    and parallelization as well.
  prefs: []
  type: TYPE_NORMAL
- en: Think of a hotel with 5 customers who ordered 15 dishes. These 15 dishes represent
    identical tasks, and each of them require to be cooked by a chef. Now, as with
    the previous example, think of the cooks as threads (in the previous example,
    you and your family member were playing the role of a cook in your home), but
    rather than sharing sub-parts of a dish, they will cook each dish at a time (because,
    obviously, there are 15 orders!).
  prefs: []
  type: TYPE_NORMAL
- en: Now, if you get 15 cooks at your disposal (along with 15 ovens and other resources),
    then you can get all the dishes to be cooked in one go, but that's not quite economical.
    You cannot infinitely increase your cooks and resources with the number of orders.
    The more economical solution would be to hire 5 cooks and make a pool (or you
    may say a queue) of orders and execute orders one after another. So, each cook
    has to make three dishes (or iterations of tasks). If there are more orders, then
    the pool would grow bigger.
  prefs: []
  type: TYPE_NORMAL
- en: Parallelization says to wisely divide tasks in a pool; instead of creating threads
    for each task, create a pool of tasks, and assign them to an existing thread,
    and reuse them.
  prefs: []
  type: TYPE_NORMAL
- en: The conclusion is, parallelization is achieved with concurrency, but it is not
    the same thing; rather, it is about how to use concurrency.
  prefs: []
  type: TYPE_NORMAL
- en: Now, why is it so important? Or rather, why is it required at all? I think you
    already got the answer, but let's inspect.
  prefs: []
  type: TYPE_NORMAL
- en: Think of a situation where you're working with a large dataset, and also have
    a long chain of operations to be performed on them before being displayed to the
    user. If you're an application developer, you'd probably want to perform all the
    operations in the background and pass the resultant data to the foreground for
    displaying it to the user. Concurrency is useful for this same scenario.
  prefs: []
  type: TYPE_NORMAL
- en: As I mentioned earlier, RxKotlin doesn't perform actions concurrently, but provides
    you with loads of options to perform the selected operations concurrently, leaving
    the choice to you.
  prefs: []
  type: TYPE_NORMAL
- en: You're probably wondering if RxKotlin really is single threaded by default,
    then how is the subscription handled by it? Should the subscription be concurrent?
    Let's find the answers before we proceed further with concurrent computing with
    RxKotlin.
  prefs: []
  type: TYPE_NORMAL
- en: So, whenever you subscribe to an Observable and/or Flowable, the current thread
    is blocked until all the items are emitted and received by the Observer chain
    (except for the cases with interval and timer factory methods). Surprising, right?
    However, it's actually good, because, for an Observable chain, if a separate thread
    is assigned to each operator (any operator generally subscribes to the source
    Observable and performs operations on the emissions, the next operator subscribes
    to the emissions by the current one), then it would be totally messy.
  prefs: []
  type: TYPE_NORMAL
- en: To resolve this scenario, ReactiveX provided us with scheduler and scheduling
    operators. By using them, thread management becomes easy, as the synchronization
    is almost automatic and there's no shared data between threads (as a basic property
    of functional programming, thus functional reactive programming).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have got some hands on the ideas behind concurrency, we can move
    forward with implementing concurrency using RxKotlin.
  prefs: []
  type: TYPE_NORMAL
- en: What is a scheduler?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In ReactiveX, the heart of concurrency lies in schedulers. As I have already
    mentioned, by default, the Observable and the chain of operators applied to it
    will do the work on the same thread where subscribe is called, and the thread
    will be blocked until Observer receives the `onComplete` or `onError` notification.
    We can use schedulers to change this behavior.
  prefs: []
  type: TYPE_NORMAL
- en: A scheduler can be thought of as a thread pool, from which ReactiveX can pool
    a thread and execute its task on it. It's basically an abstraction over multithreading
    and concurrency, making the implementation of concurrency a lot easier in ReactiveX.
  prefs: []
  type: TYPE_NORMAL
- en: Types of scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As an abstraction layer for thread pool management, the scheduler API provides
    you with some pre-composed scheduler. It also allows you to create a new user-defined
    scheduler. Let''s take a look at the available scheduler types:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Schedulers.io()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Schedulers.computation()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Schedulers.newThread()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Schedulers.single()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Schedulers.trampoline()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Schedulers.from()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will look into their definitions and their prescribed use-cases, but first,
    let's get started with some code.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start with a usual example without a scheduler, and then we will implement
    a scheduler in the same example to observe the difference, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this program, we used two `Observable`; we used delay inside their subscription
    to simulate long running tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output displays the expected result. The Observers run one after
    another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5c3babe4-c0e1-48dd-a586-a38386fcf3af.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The total execution time of this program would be around 3,100 milliseconds
    (as the delay is performed before printing), while the thread pool was sitting
    idle in between. Using scheduler, this time can be significantly reduced. Let''s
    get it done:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This program contains three new lines as compared to the previous one. On comment
    `(1)` and `(2)`, `subscribeOn(Schedulers.computation())`, and `runBlocking { delay(2100)
    }` on comment `(3)`. We will inspect the significance of those lines after taking
    a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46462863-a4a6-4db2-a7d4-eb39eb80ba97.png)'
  prefs: []
  type: TYPE_IMG
- en: As the output shows, `Observable` in this example is emitted concurrently. The
    line of the `subscribeOn(Schedulers.computation())` code enabled both downstreams
    to subscribe to the `Observable` in a different (background) thread, which influenced
    concurrency. You should already be used to it with using it `runBlocking { delay(2100)
    }` on comment `(3)`; we use it to keep the program alive. As all the operations
    are being performed in different threads, we need to block the main thread to
    keep the program alive. However, notice the time duration of the delay we passed;
    it's only 2,100 milliseconds, and the output confirms both the subscriptions processed
    all the emissions. So, it's clear, we saved 1,000 milliseconds right away.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now continue discussions on different types of schedulers available—we
    will then dive into different ways to use them.
  prefs: []
  type: TYPE_NORMAL
- en: Schedulers.io() - I/O bound scheduler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Schedulers.io()` provides us with I/O bound threads. To be more accurate,
    `Schedulers.io()` provides you with `ThreadPool`, which can create an unbounded
    number of worker threads that are meant to be performing I/O bounded tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, what exactly does the I/O bounded thread mean? And why are we calling it
    I/O bounded? Let's inspect.
  prefs: []
  type: TYPE_NORMAL
- en: All the threads in this pool are blocking and are meant to perform more I/O
    operations than computationally intense tasks, giving less load to CPUs, but may
    take longer due to waiting for I/O. By I/O operations, we mean interactions with
    file systems, databases, services, or I/O devices.
  prefs: []
  type: TYPE_NORMAL
- en: We should be cautious about using this scheduler as it can create an infinite
    number of threads (until the memory lasts) and can cause `OutOfMemory` errors.
  prefs: []
  type: TYPE_NORMAL
- en: Schedulers.computation() - CPU bound schedulers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Schedulers.computation()` is probably the most useful scheduler for programmers.
    It provides us with a bounded thread-pool, which can contain a number of threads
    equal to the number of available CPU cores. As the name suggests, this scheduler
    is meant for CPU intense works.
  prefs: []
  type: TYPE_NORMAL
- en: We should use this scheduler only for CPU—intense tasks and not for any other
    cause. The reason is that the threads in this scheduler keeps the CPU cores busy,
    and may slow down the entire application if it is used for I/O bound or any other
    tasks that involves non-computational tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason why we should consider `Schedulers.io()` for I/O bound tasks
    and `Schedulers.computation()` for computational purposes is that `computation()`
    threads utilize the processors better and create no more threads than the available
    CPU cores, and reuses them. While `Schedulers.io()` is unbounded, and if you schedule
    10,000 computational tasks on `io()` in parallel, then each of those 10,000 tasks
    each have their own thread and be competing for CPU incurring context switching
    costs.
  prefs: []
  type: TYPE_NORMAL
- en: Schedulers.newThread()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Schedulers.newThread()` provides us with a scheduler that creates a new
    thread for each task provided. While at first glance it may seem similar to `Schedulers.io()`,
    there's actually a huge difference.
  prefs: []
  type: TYPE_NORMAL
- en: The `Schedulers.io()` uses a thread pool, and whenever it gets a new unit of
    work, it first looks into the thread pool to see if any idle thread is available
    to take up the task; it proceeds to create a new thread if no pre-existing thread
    is available to take up the work.
  prefs: []
  type: TYPE_NORMAL
- en: However, `Schedulers.newThread()` doesn't even use a thread pool; instead, it
    creates a new thread for every request and forgets them forever.
  prefs: []
  type: TYPE_NORMAL
- en: In most of the cases, when you're not using `Schedulers.computation()`, you
    should consider `Schedulers.io()` and should predominantly avoid using `Schedulers.newThread()`;
    threads are very expensive resources, you should try to avoid the creation of
    new threads as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Schedulers.single()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `Schedulers.single()` provides us with a scheduler that contains only one
    thread and returns the single instance for every call. Confused? Let's make it
    clear. Think of a situation where you need to execute tasks that are strongly
    sequential—`Schedulers.single()` is the best available option for you here. As
    it provides you with only one thread, every task that you enqueue here is bound
    to be executed sequentially.
  prefs: []
  type: TYPE_NORMAL
- en: Schedulers.trampoline()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`Schedulers.single()` and `Schedulers.trampoline()` sound somewhat similar,
    both the schedulers are for sequential execution. While `Schedulers.single()`
    guarantees that all its task will run sequentially, it may run parallel to the
    thread it was called upon (if not, that thread is from `Schedulers.single()` as
    well); the `Schedulers.trampoline()` is different in that sector.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike maintaining a thread to its disposal like `Schedulers.single()`, `Schedulers.trampoline()`
    queues up the task on the thread it was called on.
  prefs: []
  type: TYPE_NORMAL
- en: So, it'll be sequential with the thread it was called upon.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at some examples of `Schedulers.single()` and `Schedulers.trampoline()`
    to understand them better:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cfd79afb-d432-441b-8621-cc726d151517.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The output clearly shows that despite the fact that both the subscriptions run
    sequentially, they run in parallel to the calling thread.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement the same code with `Schedulers.trampoline()` and observe
    the difference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that the scheduler ran sequentially to the calling
    thread:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d045d0b4-1881-49ec-b7a2-784db8c0e416.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Schedulers.from
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've seen the default/predefined schedulers available within RxKotlin.
    However, while developing applications, you may need to define your custom scheduler.
    Keeping that scenario in mind, ReactiveX has provided you with `Schedulers.from(executor:Executor)`,
    which lets you convert any executor into a scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we've created a custom `Scheduler` from an `Executor` (for
    the sake of simplicity, we've used a standard Thread Pool Executor; you're free
    to use your own custom executor).
  prefs: []
  type: TYPE_NORMAL
- en: On comment `(1)`, we created the executor with the `Executors.newFixedThreadPool()`
    method, on comment `(2)`, we created the `scheduler` instance with the help of
    `Schedulers.from(executor:Executor)`. We used the `scheduler` instance on comment
    `(3)`, comment `(4)`, and comment `(5)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/44fd479d-be13-44bf-ad1c-3fc16bd4f185.jpg)'
  prefs: []
  type: TYPE_IMG
- en: How to use schedulers – subscribeOn and observeOn operators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have gained some grip on what schedulers are, how many types of
    schedulers are available, and how to create a `scheduler` instance, we will focus
    on how to use schedulers.
  prefs: []
  type: TYPE_NORMAL
- en: There are basically two operators that help us implement schedulers. Up until
    now, in this chapter, we've used the `subscribeOn` operator in all the examples
    with a scheduler; however, there's another operator—`observeOn`. We will now focus
    on these two operators, learning how they work, and how they differ.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start with the `subscribeOn` operator.
  prefs: []
  type: TYPE_NORMAL
- en: Changing thread on subscription – subscribeOn operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need to understand how the `Observable` works before delving any further
    in how to use scheduler. Let''s take a look at the following graphics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6dd696f1-ad49-4bcf-92a0-13d1d2b4c943.jpg)'
  prefs: []
  type: TYPE_IMG
- en: As the preceding image depicts, it's the threads that are responsible for carrying
    items from the source all the way to the Subscriber through operators. It may
    be a single thread throughout the subscription, or it may even be different threads
    at different levels.
  prefs: []
  type: TYPE_NORMAL
- en: By default, the thread in which we perform the subscription is the responsible
    of bringing all the emissions down to the Subscriber, unless we instruct it otherwise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the code example first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: It's a simple RxKotlin code example; we are creating `Observable`, mapping it,
    and then subscribing to it. The only difference here is that I've printed the
    `Thread` name inside both the `map` and the `subscribe` lambdas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/baa99960-4bda-454e-be6d-66a998433fd7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: From the output, we can determine that the main thread executes the entire subscription.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `subscribeOn` operator, as the name suggests, helps us change the thread
    of a subscription. Let''s modify the program once and take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The entire program remains the same, except that, in between `map` and `subscribe`,
    we used the `subscribeOn` operator at comment `(1)`. Let''s check the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/440f9c65-9f1a-4eaf-b124-21e413b55ab2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The `subscribeOn` operator changes the thread for the entire subscription; you
    can use it wherever you want in the subscription flow. It will change the thread
    once and for all.
  prefs: []
  type: TYPE_NORMAL
- en: Observing on a different thread – observeOn operator
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While `subscribeOn` looks like an awesome gift from heaven, it may not be suited
    in some cases. For example, you may want to do computations on the `computation`
    threads and display the results from the `io` threads, which actually you should
    do. The `subscribeOn` operator requires a companion for all these things; while
    it'll specify the thread for the entire subscription, it requires its companion
    to specify threads for specific operators.
  prefs: []
  type: TYPE_NORMAL
- en: The perfect companion to the `subscribeOn` operator is the `observeOn` operator.
    The `observeOn` operator specifies the scheduler for all the operators called
    after it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify our program with `observeOn` to perform the `map` operation in
    the `Schedulers.computation()` and receive the result of the subscription (`onNext`)
    in the `Schedulers.io()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output clearly shows we''re successful in achieving our objective:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/10acaff9-5525-4572-8318-425df7c4a0de.jpg)'
  prefs: []
  type: TYPE_IMG
- en: So, what did we do? We specified the `computation` threads for the `map` operator
    by calling `observeOn(Schedulers.computation())` just before it, and called `observeOn(Schedulers.io())`
    before subscribe to switch to `io` threads to receive the results.
  prefs: []
  type: TYPE_NORMAL
- en: In this program, we did a context switch; we exchanged data with threads and
    implemented communication in between threads with such an ease, with merely 7-8
    lines of code—that's the abstraction schedulers provides us with.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned about concurrent execution and parallelism and
    how to achieve multithreading in RxKotlin. Multithreading is a necessity in today's
    app driven era, as modern users don't like to wait, or, to be blocked, you need
    to constantly switch threads to perform computations and UX operations.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you learned how schedulers in RxKotlin can help you, or, rather,
    how schedulers abstract the complexities of multithreading.
  prefs: []
  type: TYPE_NORMAL
- en: While concurrent execution and parallelism is an essential part of modern application
    development, testing is probably the most crucial part. We cannot deliver any
    app without testing it. Agile methodology (though we are not discussing agile
    here) says we should perform testing repeatedly and with every iteration of our
    product (application) development.
  prefs: []
  type: TYPE_NORMAL
- en: In the [Chapter 8](08299ee7-7cdc-4700-ae32-362b3145d26d.xhtml), *Testing RxKotlin
    Applications*, we will discuss testing. Don't dare miss it out, turn the page
    right now!
  prefs: []
  type: TYPE_NORMAL
