<html><head></head><body><p>&#13;
&#13;
                            &#13;
                    Using CoreML and Vision in Swift&#13;
                &#13;
            &#13;
            &#13;
                &#13;
<p>The Swift programming language has come a long way since its first introduction, and in comparison to many other programming languages, it's still well within its infancy.</p>&#13;
<p>However, with this in mind, with every release of Swift and its place in the open source community, we've seen it grow from strength to strength over such a short period of time. We already covered server-side Swift back in <a href="9ce1feb3-8fca-4656-91dc-796ba77c3d07.xhtml">Chapter 8</a>, <em>Server-Side Swift</em>, another evolution that was again fueled by the open source community.</p>&#13;
<p>Another fast-moving train is that of machine learning, once again driven by the power of the community, and recognized giants in the industry, such as TensorFlow, now support the Swift programming language.</p>&#13;
<p>In this chapter, we're going to look at Apple's offering for machine learning – CoreML – and how we can build an app using Swift to read and process machine learning models, giving us intelligent image recognition.</p>&#13;
<p>We'll also take a look at Apple's Vision Framework and how it works alongside CoreML to allow us to process video being streamed to our devices in real time, recognizing objects on the fly.</p>&#13;
<p class="mce-root">In this chapter, we will cover the following recipes:</p>&#13;
<ul>&#13;
<li>Building an image capture app</li>&#13;
<li>Using CoreML models to detect objects in images</li>&#13;
<li>Building a video capture app</li>&#13;
<li>Using CoreML and the Vision Framework to detect objects in real time</li>&#13;
</ul>&#13;
<h1 id="uuid-7dac93c8-efa3-4ae4-ba21-1ff439180c9a">Technical requirements</h1>&#13;
<p>You can find the code files present in this chapter on GitHub at <a href="https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11">https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11</a></p>&#13;
<p>Check out the following video to see the Code in Action: <a href="https://bit.ly/2NmP961" target="_blank">https://bit.ly/2NmP961</a></p>&#13;
<h1 id="uuid-32c706fa-9b8a-47f8-b866-55642ec26291">Building an image capture app</h1>&#13;
<p>In this first recipe, we're going to create an app that captures either an image from your camera roll or an image taken from your camera. This will set up our iOS app ready for us to incorporate CoreML to detect objects in our photos.</p>&#13;
<h2 id="uuid-28f32240-f778-4f23-a9a8-17bb86a86ada">Getting ready</h2>&#13;
<p class="mce-root">For this recipe, you'll need the latest version of Xcode available from the Mac App Store.</p>&#13;
<h2 id="uuid-fe870e97-6601-42f2-b972-133266ccd1fa" class="mce-root">How to do it...</h2>&#13;
<p>With Xcode open, let's get started:</p>&#13;
<ol>&#13;
<li>Create a new project in Xcode. Go to <strong>File</strong> | <strong>New</strong> | <strong>Project</strong> | <strong>iOS App</strong>.</li>&#13;
<li>In <kbd>Main.storyboard</kbd>, add the following:&#13;
<ol>&#13;
<li>Add <kbd>UISegmentedControl</kbd> with two options (<strong>Photo / Camera Roll</strong> and <strong>Live Camera</strong>).</li>&#13;
<li>Next, add a <kbd>UILabel</kbd> view just underneath. </li>&#13;
<li>Add a <kbd>UIImageView</kbd> view beneath that.</li>&#13;
<li>Finally, add a <kbd>UIButton</kbd> component.</li>&#13;
</ol>&#13;
</li>&#13;
<li>Space these accordingly using AutoLayout constraints with <kbd>UIImageView</kbd> being the prominent object:</li>&#13;
</ol>&#13;
<div><img src="img/a5c4c4b3-3373-46d5-ac77-73cc7b895406.png" style="" width="526" height="1170"/></p>&#13;
<p>Figure 11.1 – Camera/photo app</p>&#13;
<ol start="4">&#13;
<li>Once we have this in place, let's hook these up to our <kbd>ViewController.swift</kbd> file:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">@IBOutlet weak var imageView: UIImageView!<br/>@IBOutlet weak var labelView: UILabel!<br/><strong>@IBAction func onSelectPhoto(_ sender: Any)</strong></pre>&#13;
<p>Take note that in the preceding, we have two <kbd>IBOutlet</kbd> and one <kbd>IBAction</kbd> (we don't need an outlet for <kbd>UIButton</kbd>, we just care about its action).</p>&#13;
<ol start="5">&#13;
<li>Next, populate <kbd>IBAction</kbd> with the following code:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">@IBAction func onSelectPhoto(_ sender: Any) {<br/><strong>    let picker = UIImagePickerController()</strong><br/><strong>    picker.delegate = self</strong><br/><strong>    picker.allowsEditing = false</strong><br/><strong>    picker.sourceType = <br/>      UIImagePickerController.isSourceTypeAvailable(.camera) ? <br/>        .camera : .photoLibrary</strong><br/><strong>        </strong><br/><strong>    present(picker, animated: true)</strong> <br/>}</pre>&#13;
<ol start="6">&#13;
<li> Now, let's create an extension of <kbd>UIViewController</kbd>. You can do this at the bottom of the <kbd>ViewController</kbd> class if you like:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">extension ViewController: UIImagePickerControllerDelegate,<br/>  UINavigationControllerDelegate</pre>&#13;
<ol start="7">&#13;
<li>Our extension needs to conform to the <kbd>UIImagePickerControllerDelegate</kbd> and <strong><kbd>UINavigationControllerDelegate</kbd></strong> protocols. We can now go ahead and populate our extension with the following delegate method:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func imagePickerControllerDidCancel(_ picker: <br/>   UIImagePickerController) {<br/>  dismiss(animated: true, completion: nil)<br/>}<br/><br/>func imagePickerController(_ picker: UIImagePickerController, <br/>  didFinishPickingMediaWithInfo info: <br/>    [UIImagePickerController.InfoKey : Any]) {<br/>    <br/><br/>    guard let image = info[UIImagePickerController.InfoKey<br/>      .originalImage] as? UIImage else {<br/>        return<br/>    }<br/>    <br/>    imageView.image = image<br/>        <br/>    labelView.text = "This is my image!"<br/>    <br/>    dismiss(animated: true, completion: nil)<br/>}</pre>&#13;
<ol start="8">&#13;
<li>Before we go any further, we'll need to add a couple of lines to our <kbd>info.plist</kbd>:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">NSCameraUsageDescription<br/>NSPhotoLibraryUsageDescription</pre>&#13;
<p style="padding-left: 60px">9. Add these in with the following string description: <kbd>Chapter 11 wants to detect cook Stuff</kbd>. This is an iOS security feature that will prompt the user when any app/code tries to access the camera, photo library, or location services. Failure to add this in could result in an app crash.</p>&#13;
<p>For our app, we can add whatever we want, but for a production app, make sure the text you enter is useful and informative to the user. Apple will check this when reviewing your app and has been known to potentially block a release until this is resolved.</p>&#13;
<p>Go ahead and run your code, and then launch the app. One of the following things should happen:</p>&#13;
<ul>&#13;
<li>If you are running the app from the simulator, our <kbd>UIButton</kbd> press should present the photo picker (along with the default images supplied by the iOS simulator).</li>&#13;
<li>If you are running from a device, then you should be presented with the camera view, allowing you to capture a photo.</li>&#13;
</ul>&#13;
<p>Either way, whether a photo was selected or a picture was taken, the resulting image should show in <kbd>UIImageView</kbd>!</p>&#13;
<h2 id="uuid-d3d3262d-cb86-49d3-b547-6f611acfbdcc" class="mce-root">How it works...</h2>&#13;
<p>Let's step through what we've just done. We'll begin at <kbd>IBAction</kbd> and have a look at the <kbd>UIPickerView</kbd> view we've created:</p>&#13;
<pre>let picker = UIImagePickerController() // 1<br/>picker.delegate = self // 2<br/>picker.allowsEditing = false // 3<br/>picker.sourceType = UIImagePickerController.isSourceTypeAvailable<br/>  (.camera) ? .camera : .photoLibrary // 4<br/>    <br/>present(picker, animated: true) // 5</pre>&#13;
<p>Let's go through this one line at a time:</p>&#13;
<ol>&#13;
<li>We instantiate an instance of <kbd>UIImagePickerController</kbd> – an available API that will allow us to choose an image based on a specific source.</li>&#13;
<li>We set the delegate as <kbd>self</kbd>, so we can harness any results or actions caused by <kbd>UIImagePickerController</kbd>.</li>&#13;
<li>We set <kbd>allowEditing</kbd> to <kbd>false</kbd>, which is used to hide controls when the camera is our source.</li>&#13;
<li>In this instance, we set the source type based on whether the camera is available or not (so it works well with the simulator).</li>&#13;
<li>Finally, we present our view controller.</li>&#13;
</ol>&#13;
<p>Now, let's take a look at our delegate methods:</p>&#13;
<pre>func imagePickerControllerDidCancel(_ picker: UIImagePickerController)<br/>func imagePickerController(_ picker: UIImagePickerController, <br/>  didFinishPickingMediaWithInfo info: [UIImagePickerController.InfoKey <br/>    : Any])</pre>&#13;
<p class="mce-root">The first is pretty self-explanatory; <kbd>imagePickerControllerDidCancel</kbd> handles any instances where <kbd>UIImagePickerController</kbd> is canceled by the users. In our case, we just dismiss the instance returned – job done!</p>&#13;
<p><kbd>didFinishPickingMediaWithInfo</kbd> is where interesting things happen. Notice how we are given a dictionary of <strong>info</strong> in our response. Here, we have various segments of information. The one we are looking for is under the <kbd>UIImagePickerController.InfoKey.originalImage</kbd> key. This gives us a <kbd>UIImage</kbd> of what we've just selected, allowing us to assign this straight back to <kbd>UIImageView</kbd>.</p>&#13;
<p>Now that we've got an app that allows us to take or choose a photo, we can apply it to some real work with the power of CoreML and object detection.</p>&#13;
<h2 id="uuid-76bd4d40-5bd1-4803-9a2c-d8e0ad9c1d0c" class="mce-root">There's more...</h2>&#13;
<p class="mce-root">A quick note to mention: you'll also have noticed that we were required to conform our extension to <kbd>UINavigationControllerDelegate</kbd>. This is required by iOS to allow <kbd>UIImageContoller</kbd> to be handled and presented correctly from its "presenting" stack (<kbd>ViewController</kbd> in our instance).</p>&#13;
<h2 id="uuid-1dde618e-f1d2-410c-80a4-cfbc11f2c3ef" class="mce-root">See also</h2>&#13;
<p class="mce-root">For more information on <kbd>UIImagePickerController</kbd>, refer to <a href="https://developer.apple.com/documentation/uikit/uiimagepickercontroller">https://developer.apple.com/documentation/uikit/uiimagepickercontroller</a>.</p>&#13;
<h1 id="uuid-bc97bf76-b2ce-48cc-9dba-2039a9b8fcd6">Using CoreML models to detect objects in images</h1>&#13;
<p>In this recipe, we'll take the app we just built and incorporate the CoreML framework in order to detect objects in our images.</p>&#13;
<p>We'll also take a look at the generated CoreML models available for us to use and download directly from Apple's Developer portal.</p>&#13;
<h2 id="uuid-1ea9f663-5451-44cf-a230-e65e0ce28e67">Getting ready</h2>&#13;
<p>For this recipe, you'll need the latest version of Xcode available from the Mac App Store. </p>&#13;
<p>Next, head on over to the Apple Developer portal at the following address: <a href="https://developer.apple.com/machine-learning/models/">https://developer.apple.com/machine-learning/models/</a>.</p>&#13;
<p>Here, you will find out a little bit more about the models available for us to download and use in our Xcode project.</p>&#13;
<p>You'll notice there are options for image models and text models. For this recipe, we're going to be using image models, specifically one called Resnet50, which uses a residual neural network that attempts to identify and classify what it perceives to be the dominant object in an image.</p>&#13;
<p>For more information on the different types of machine learning models, see the links in the <em>See also</em> section at the end of this recipe. </p>&#13;
<p>From here, download the Resnet50.mlmodel (32-bit) model. If you are having trouble downloading the file, you can just take a copy from the sample project in our GitHub repository.</p>&#13;
<p>Once downloaded, add this to your Xcode project by simply dragging it into the file explorer tree in our previous app.</p>&#13;
<h2 id="uuid-fef51a13-061c-4f5f-a742-31caf2a9ae84" class="mce-root">How to do it...</h2>&#13;
<p>Let's make a start where we left off in our previous project:</p>&#13;
<ol start="1">&#13;
<li>With everything in place, head back into <kbd>ViewController.swift</kbd> and add the following global variable and addition to our <kbd>viewDidLoad()</kbd> function:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px"><strong>var model: Resnet50!</strong><br/><br/>override func viewDidLoad() {<br/>    super.viewDidLoad()<br/>    <strong>model = Resnet50()</strong><br/>}</pre>&#13;
<ol start="2">&#13;
<li class="mce-root">Now, head on over to the sample project and obtain a file called <kbd>ImageHelpers.swift</kbd>; add this to our project. Once this has been added, we'll head on back over to our <kbd>didFinishPickingMediaWithInfo</kbd> delegate and expand on this a little further.</li>&#13;
<li>Add in the following highlighted changes:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">guard let image = info[UIImagePickerController.InfoKey.<br/>  originalImage] as? UIImage else {<br/>    return<br/>}<br/><br/><strong>let (newImage, pixelBuffer) =   <br/>  ImageHelper.processImageData(capturedImage: image)</strong><br/><br/><strong>imageView.image = newImage</strong><br/><br/><strong>var imagePredictionText = "no idea... lol"</strong><br/><br/><strong>    guard let prediction = try? model.prediction(<br/>      image: pixelBuffer!) else {</strong><br/><strong>        labelView.text = imagePredictionText</strong><br/><strong>        dismiss(animated: true, completion: nil)</strong><br/><strong>        return</strong><br/><strong>    }</strong><br/><strong>    </strong><br/><strong>    imagePredictionText = prediction.classLabel</strong><br/><strong>    </strong><br/><strong>labelView.text = "I think this is a \(imagePredictionText)"</strong><br/><br/>dismiss(animated: true, completion: nil)    </pre>&#13;
<p>With everything in place, run the app and select a photo. As long as you didn't point it at a blank wall, you should be seeing some interesting feedback.</p>&#13;
<p>With all that in place, let's break down the changes we just made to understand what just happened a little more.</p>&#13;
<h2 id="uuid-adcbfe8e-6ecf-4708-b2c8-f12b6d4ed2e7" class="mce-root">How it works...</h2>&#13;
<p>The first thing is to take a look at the following line we added in:</p>&#13;
<pre><strong>let (newImage, pixelBuffer) = <br/>  ImageHelper.processImageData(capturedImage: image)</strong></pre>&#13;
<p>Here, we added in a call to a helper method we took from our sample project. This helper contains the following two functions:</p>&#13;
<pre>static func processImageData(capturedImage: UIImage) -&gt; (UIImage?, <br/>  CVPixelBuffer?)<br/>static func exifOrientationFromDeviceOrientation() -&gt; <br/>  CGImagePropertyOrientation</pre>&#13;
<p>These functions and what they do are a little out of the scope of this book, and this chapter in particular. However, at a very high level, the first function, <kbd>processImageData()</kbd>, takes an instance of <kbd>UIImage</kbd> and transforms this to <kbd>CVPixelBuffer</kbd> format.</p>&#13;
<p>This essentially returns the <kbd>UIImage</kbd> object back to its raw format that it was captured in (<kbd>UIImage</kbd> is merely a UIKit wrapper for our true raw image).</p>&#13;
<p>During this process, we need to flip the orientation too as with all captured images. This is almost certainly in landscape mode (and more often than not, you've taken a picture or selected a photo in portrait mode).</p>&#13;
<p>Another reason for performing this is that our ResNet50 model is trained to observe images at only 224 x 224. So, we need to readjust the captured image to this size.</p>&#13;
<p>If you need more information on the model you have in your project, simply select the file in the file explorer and view the details in the main window. From here, the Predictions tab will give you all the details you need about the input file required.</p>&#13;
<p>So, with our helper function implemented, we receive a new <kbd>UIImage</kbd> object (modified to our new spec) and the image in <kbd>CVPixelBuffer</kbd> format, all ready to pass over to CoreML for processing.</p>&#13;
<p>Now, let's take a look at the following code:</p>&#13;
<pre>guard let prediction = try? <strong>model.prediction(image: pixelBuffer!</strong>) else {<br/>    labelView.text = imagePredictionText<br/>    dismiss(animated: true, completion: nil)<br/>    return<br/>}<br/><br/>imagePredictionText = <strong>prediction.classLabel</strong></pre>&#13;
<p>In the preceding code, I've highlighted some areas of interest. First is our <kbd>prediction()</kbd> function call on our <kbd>model</kbd> object. Here, we pass in our image in the <kbd>CVPixelBuffer</kbd> format we got back from our helper method earlier. From this, wrapped in a <kbd>try</kbd> statement, CoreML will now attempt to detect an object in the photo. If successful, we'll exit our <kbd>guard</kbd> statement gracefully and be able to access the properties available in our <kbd>prediction</kbd> variable.</p>&#13;
<p>If you take a look at the properties available in our ResNet50 model, you'll see the various options we have:</p>&#13;
<pre>.classLabel<br/>.classLabelProbs</pre>&#13;
<p>The class label we've already seen, but the class label probability will return us a dictionary of the most likely category for our image with a value based around a confidence score.</p>&#13;
<p>Each model will have its own set of properties based on its desired intention and how it's been built.</p>&#13;
<h2 id="uuid-362c8dd7-9860-4f6e-97e5-14b5fd2db235" class="mce-root">There's more...</h2>&#13;
<p>At the beginning of this section, we obtained a model that allowed us to detect objects in our images. Touching on this subject a little more, models are a set of data that has been trained to identify a pattern or characteristics of a certain description.</p>&#13;
<p>For example, we want a model that detects cats; so, we train our model by feeding it images of around 10,000 various pictures of cats. Our model training will identify features and shapes common to each other and categorize them accordingly.</p>&#13;
<p>When we then feed our model an image of a cat, we hope that it is able to pick up those categorized features within our image and successfully identify the cat.</p>&#13;
<p>The more images you train with, the greater the performance; however, that still depends on the integrity of the images too. Training with the same image of a cat (just in a different pose) 1,000 times might give you the same results as if you take 10,000 images of the same cat (again in a different pose).</p>&#13;
<p>The same goes the other way too; if you train with 500,000 images of a panther and then 500,000 images of a kitten, it's just not going to work.</p>&#13;
<p>Away from CoreML, you are now able to train a model using Swift with TensorFlow. TensorFlow is a Google product that is leading the way in terms of machine learning and with an ever-growing community of developers behind it coupled with Swift's own open source community. Advancement in this particular technology is certainly looking bright.</p>&#13;
<h2 id="uuid-951eef20-f041-494a-9557-32826be0cad4" class="mce-root">See also</h2>&#13;
<p>For more information, please refer to the following links:</p>&#13;
<ul>&#13;
<li>Apple CoreML documentation: <a href="https://developer.apple.com/documentation/coreml">https://developer.apple.com/documentation/coreml</a></li>&#13;
<li>TensorFlow Swift: <a href="https://www.tensorflow.org/swift/tutorials/model_training_walkthrough">https://www.tensorflow.org/swift/tutorials/model_training_walkthrough</a></li>&#13;
</ul>&#13;
<h1 id="uuid-be9d6eee-3e24-46cb-a84b-37bbf1af26c0">Building a video capture app</h1>&#13;
<p>So, what we have seen so far of CoreML is pretty neat, to say the least. But taking a look back over this chapter so far, we have probably spent more time building our app to harness the power of CoreML than actually implementing it.</p>&#13;
<p>In this section, we're going to take our app a little further by streaming a live camera feed that in turn will allow us to intercept each frame and detect objects in real time.</p>&#13;
<h2 id="uuid-ed36d495-2e73-4161-b0f5-d77da46b1429">Getting ready</h2>&#13;
<p>For this section, you'll need the latest version of Xcode available from the Mac App Store.</p>&#13;
<p>Please note that for this section, you'll need to be connected to a real device for this to work. Currently, the iOS simulator does not have a way to emulate the front or back camera.</p>&#13;
<h2 id="uuid-07fe02c0-42bb-462d-8d4c-c02642a13c99" class="mce-root">How to do it...</h2>&#13;
<p>Let's begin:</p>&#13;
<ol>&#13;
<li>Head over to our <kbd>ViewContoller.swift</kbd> file and make the following amendments:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">import AVFoundation<br/><br/>private var previewLayer: AVCaptureVideoPreviewLayer! = nil<br/>var captureSession = AVCaptureSession()<br/><br/>var bufferSize: CGSize = .zero<br/>var rootLayer: CALayer! = nil<br/><br/>private let videoDataOutput = AVCaptureVideoDataOutput()<br/>private let videoDataOutputQueue = DispatchQueue(label:<br/>  "video.data.output.queue", qos: .userInitiated, attributes: [], <br/>    autoreleaseFrequency: .workItem)</pre>&#13;
<ol start="2">&#13;
<li>Now, create a function called <kbd>setupCaptureSession()</kbd> and we'll start by adding in the following:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func setupCaptureSession() {<br/>    <br/><strong>    var deviceInput: AVCaptureDeviceInput!</strong><br/><strong>    </strong><br/><strong>    guard let videoDevice =<br/>            AVCaptureDevice.DiscoverySession(deviceTypes: <br/>              [.builtInWideAngleCamera], mediaType: .video,<br/>                 position: .back).devices.first else {<br/>        return<br/>    }<br/><br/>    do {<br/>        deviceInput = try AVCaptureDeviceInput(device: videoDevice)<br/>    } catch {<br/>        print(error.localizedDescription)<br/>        return<br/>    }<br/><br/></strong>    // More to go here<br/><br/>}</pre>&#13;
<p style="padding-left: 60px">In the preceding code, we are checking our device for an available camera, specifically <kbd>.builtInWideAngleCamera</kbd> at the back.<strong> </strong>If no device can be found, our guard will fail.</p>&#13;
<ol start="3">&#13;
<li>Next, we initialize <kbd>AVCaptureDeviceInput</kbd> with our new <kbd>videoDevice</kbd> object.</li>&#13;
<li>Now, continuing in our function, add the following code:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">captureSession.beginConfiguration()<br/>captureSession.sessionPreset = .medium<br/><br/>guard captureSession.canAddInput(deviceInput) else {<br/>    captureSession.commitConfiguration()<br/>    return<br/>}<br/>captureSession.addInput(deviceInput)<br/><br/>if captureSession.canAddOutput(videoDataOutput) {<br/>    captureSession.addOutput(videoDataOutput)<br/>    videoDataOutput.setSampleBufferDelegate(self, queue: <br/>      videoDataOutputQueue)<br/>} else {<br/>    captureSession.commitConfiguration()<br/>    return<br/>}<br/><br/>do {<br/>    try videoDevice.lockForConfiguration()<br/>    let dimensions = CMVideoFormatDescriptionGetDimensions(<br/>      (videoDevice.activeFormat.formatDescription))<br/>    bufferSize.width = CGFloat(dimensions.width)<br/>    bufferSize.height = CGFloat(dimensions.height)<br/>    videoDevice.unlockForConfiguration()<br/>} catch {<br/>    print(error)<br/>}</pre>&#13;
<p style="padding-left: 60px">Essentially, here we are attaching our device to a capture session, allowing us to stream what the device input (camera) is processing programmatically straight into our code. Now we just to point this at our view so that we can see the output.</p>&#13;
<ol start="5">&#13;
<li>Add the following additional code to our function:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">captureSession.commitConfiguration()<br/><br/>previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)<br/>previewLayer.videoGravity = AVLayerVideoGravity.resizeAspectFill<br/>rootLayer = imageView.layer<br/>previewLayer.frame = rootLayer.bounds<br/>rootLayer.addSublayer(previewLayer)<br/><br/></pre>&#13;
<p style="padding-left: 60px">With the code we've just added, we are essentially creating a visible layer from our current capture session. In order for us to process this on our screen, we need to assign this to a <kbd>rootLayer</kbd> (our <kbd>CALayer</kbd> variable we added earlier). While this seems a little overkill and we could just add this to the layer of our <kbd>UIImageView</kbd>, we're prepping for something we need to do in our next recipe.</p>&#13;
<ol start="6">&#13;
<li>Finally, with our camera and device all set up, it's time to set the camera rolling:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">captureSession.startRunning()</pre>&#13;
<p>Go ahead and run the app. Note again that this will only work on a real device and not a simulator. All going well, you should have a live stream from your camera.</p>&#13;
<h2 id="uuid-ff06b355-4735-4f41-821d-7cd800487b9d" class="mce-root">How it works...</h2>&#13;
<p>The best way to explain this would be to think of the capture session as a wrapper or a configuration between the device's hardware and software. The camera hardware has a lot of options, so we configure our capture session to pick out what we want for our particular instance.</p>&#13;
<p>Let's look back at this line of code:</p>&#13;
<pre><strong>AVCaptureDevice.DiscoverySession(deviceTypes: <br/>  [.builtInWideAngleCamera], mediaType: .video, position: .back)</strong></pre>&#13;
<p>Here, you could control the enum bases on a UI toggle, allowing the user to specify which camera to use. You could even use the following:</p>&#13;
<pre>captureSession.stopRunning()</pre>&#13;
<p>Re-configure the session and then <kbd>startRunning()</kbd> again. Essentially (albeit at a much more complex level), this is what happens when you switch from the front to the back camera when taking a photo.</p>&#13;
<p>With the session captured, we can now stream the output directly to any view we like just like we did here:</p>&#13;
<pre>previewLayer = AVCaptureVideoPreviewLayer(session: captureSession)</pre>&#13;
<p class="mce-root">But the fun comes when we want to manipulate the image that is being streamed, by capturing them one frame at a time. We do this by implementing the <kbd>AVCaptureVideoDataOutputSampleBufferDelegate</kbd> protocol, which allows us to override the following delegate methods:</p>&#13;
<pre>func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: <br/>  CMSampleBuffer, from connection: AVCaptureConnection) { }</pre>&#13;
<p>Notice something familiar here... we're being given <kbd>sampleBuffer</kbd>, just like we got in <kbd>UIImagePickerDelegate</kbd>. The difference here is that this will be called with every frame, not just when one is selected.</p>&#13;
<h2 id="uuid-83dc4019-bec1-43e7-9a7b-011a3a71903a" class="mce-root">There's more...</h2>&#13;
<p>Playing around with capture sessions and <kbd>AVCaptureOutputs</kbd> is an expensive operation. Always make sure you stop your session from running when it's not needed, and make sure your delegates are not unnecessarily processing data when they don't need to be.</p>&#13;
<p>Another thing to note is that the initialization of a capture device can in some instances be slow, so make sure you have the appropriate UI to handle the potential blocking it may cause.</p>&#13;
<p>Final note: if you are struggling with memory leaks and high CPU times, take a look at a suite of tools called Instruments. Bundles of Xcode Instruments can offer a wide range of performance tracing tools that can really help you to get the most out of your Swift code.</p>&#13;
<h2 id="uuid-2aa26ccc-9d56-4bdc-90a1-9c3100671a82" class="mce-root">See also</h2>&#13;
<p>For more information, refer to the following links:</p>&#13;
<ul>&#13;
<li>Instruments overview: <a href="https://help.apple.com/instruments/mac/current/#/dev7b09c84f5">https://help.apple.com/instruments/mac/current/#/dev7b09c84f5</a></li>&#13;
<li>AVFoundation: <a href="https://developer.apple.com/documentation/avfoundation">https://developer.apple.com/documentation/avfoundation</a></li>&#13;
</ul>&#13;
<h1 id="uuid-34578c44-7d17-465b-8104-692aed8cb2d8">Using CoreML and the Vision Framework to detect objects in real time</h1>&#13;
<p>We've seen what CoreML can do in terms of object detection, but taking everything we've done so far into account, we can certainly go a step further. Apple's Vision Framework offers a unique set of detection tools from landmark detection and face detection in images to tracking recognition.</p>&#13;
<p>With the latter, tracking recognition, the Vision Framework allows us to take models built with CoreML and use them in conjunction with CoreML's object detection to identify and track the object in question.</p>&#13;
<p>In this section, we'll take everything we've learned so far, from how AVFoundation works to implementing CoreML, and build a real-time object detection app using a device camera.</p>&#13;
<h2 id="uuid-c0e09cf4-b56d-4986-8d43-d8316f7e8562">Getting ready</h2>&#13;
<p>For this section, you'll need the latest version of Xcode available from the Mac App Store.</p>&#13;
<p class="mce-root">Next, head on over to the Apple Developer portal at the following address: <a href="https://developer.apple.com/machine-learning/models/">https://developer.apple.com/machine-learning/models/</a>.<a href="https://developer.apple.com/machine-learning/models/"/></p>&#13;
<p>Here, you will find out a little bit more about the models available for us to download and use in our Xcode project. You'll notice there are options for image models or text models. For this recipe, we're going to be using image models, specifically one called <strong>YOLOv3</strong>, which uses a residual neural network that attempts to identify and classify what it perceives to be the dominant object in the image.</p>&#13;
<p>For more information on the different types of machine learning models, see the links in the <em>See also</em> section at the end of this recipe. </p>&#13;
<p>From here, download the YOLOv3.mlmodel (32-bit) model. If you are having trouble downloading the file, you can just take a copy from the sample project in our GitHub repository.</p>&#13;
<p>Once downloaded, add this to your Xcode project by simply dragging it into the file explorer tree in our previous app.</p>&#13;
<h2 id="uuid-55c803a2-bb04-4b3c-ab88-02f3f2e05e73" class="mce-root">How to do it...</h2>&#13;
<p>We'll start by creating a new <kbd>UIViewController</kbd> for all our vision work, in Xcode:</p>&#13;
<ol>&#13;
<li>Go to File | New | File.</li>&#13;
<li>Choose <strong>Cocoa Touch Class</strong>.</li>&#13;
<li>Name this <kbd>VisionViewController</kbd>.</li>&#13;
<li>Make this a subclass of <kbd>UIViewController</kbd>.</li>&#13;
</ol>&#13;
<p>With that done, we can now head on over to our new <kbd>VisionViewController</kbd> and add in the following highlighted code. We'll start by importing the Vision Framework:</p>&#13;
<pre><strong>import Vision</strong></pre>&#13;
<p>Now, we'll subclass our existing <kbd>ViewController</kbd> so that we can get the best of both worlds (without the need for copious amounts of code duplication):</p>&#13;
<pre>class VisionViewController: <strong>ViewController</strong></pre>&#13;
<p>With that done, we can now override some of our functions in <kbd>ViewContoller.swift</kbd>. We'll start with <kbd>setupCaptureSession()</kbd>:</p>&#13;
<pre>override func setupCaptureSession() {<br/>    <br/>    <strong>super.setupCaptureSession()</strong><br/>    <br/>    setupDetectionLayer()<br/>    updateDetectionLayerGeometry()<br/>    <br/>    startVision()<br/>    <br/>}</pre>&#13;
<p>When overriding from another class, always remember to call the base function first. In the case of the preceding code, this can be done by calling <kbd>super.setupCaptureSession()</kbd> as highlighted.</p>&#13;
<p>You'll notice some functions in the ViewControler.swift file that we've not yet created. Let's go through these now one by one:</p>&#13;
<ol>&#13;
<li>First, we'll add a detection layer to our <kbd>rootLayer</kbd> that we created earlier. This <kbd>CALayer</kbd> will be used as the drawing plane for our detected object area:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func setupDetectionLayer() {<br/>    detectionlayer = CALayer()<br/>    detectionlayer.name = "detection.overlay"<br/>    detectionlayer.bounds = CGRect(x: 0.0,<br/>                                        y: 0.0,<br/>                                        width: bufferSize.width,<br/>                                        height: bufferSize.height)<br/>    detectionlayer.position = CGPoint(x: rootLayer.bounds.midX, y: <br/>      rootLayer.bounds.midY)<br/>    rootLayer.addSublayer(detectionlayer)<br/>}</pre>&#13;
<p style="padding-left: 60px">As you can see from the code, we create its bounds based on the height and width taken from our <kbd>bufferSize</kbd> property (which is being shared back over in our <kbd>ViewController</kbd> class).</p>&#13;
<ol start="2">&#13;
<li>Next, we need to add some geometry to <kbd>detectionLayer()</kbd>. This will re-adjust and scale the detection layer based on the device's current geometry:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func updateDetectionLayerGeometry() {<br/>    <br/>    let bounds = rootLayer.bounds<br/>    var scale: CGFloat<br/>    <br/>    let xScale: CGFloat = bounds.size.width / bufferSize.height<br/>    let yScale: CGFloat = bounds.size.height / bufferSize.width<br/>    <br/>    scale = fmax(xScale, yScale)<br/>    if scale.isInfinite {<br/>        scale = 1.0<br/>    }<br/>    <br/>    CATransaction.begin()<br/>    CATransaction.setValue(kCFBooleanTrue, forKey: <br/>      kCATransactionDisableActions)<br/>    <strong>detectionlayer</strong>.setAffineTransform(CGAffineTransform(rotationAngle: <br/>  CGFloat(.pi / 2.0)).scaledBy(x: scale, y: -scale))<br/><strong>detectionlayer</strong>.position = CGPoint(x: bounds.midX, y: <br/>  bounds.midY)<br/>    <br/>    CATransaction.commit()<br/>    <br/>}</pre>&#13;
<ol start="3">&#13;
<li>Finally, let's hook up our <kbd>startVision()</kbd> function:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func startVision(){<br/>    <br/>    guard let localModel = Bundle.main.url(forResource: "YOLOv3", <br/>      withExtension: "mlmodelc") else {<br/>        return<br/>    }<br/>    <br/>    do {<br/>        let visionModel = try VNCoreMLModel(for: MLModel(<br/>          contentsOf: localModel))<br/>        let objectRecognition = VNCoreMLRequest(model: visionModel, <br/>          completionHandler: { (request, error) in<br/>            <br/>            DispatchQueue.main.async(execute: {<br/>                if let results = request.results {<br/>                    self.visionResults(results)<br/>                }<br/>            })<br/>            <br/>        })<br/>        self.requests = [objectRecognition]<br/>    } catch let error {<br/>        print(error.localizedDescription)<br/>    }<br/>    <br/>}</pre>&#13;
<ol start="4">&#13;
<li class="mce-root">With this comes a new function, <kbd>visionResults()</kbd>. Go ahead and create this function in <kbd>VisionViewController</kbd> too.</li>&#13;
</ol>&#13;
<p>We could have simply used an extension in our original <kbd>ViewController</kbd> to house all these new functions, but we'd run the risk of overloading our view controller to the point where it could become too unmaintainable. Also, our logic and extension for <kbd>UIImagePicker</kbd> was in here, so the separation is nice.</p>&#13;
<ol start="5">&#13;
<li>With this, let's build out <kbd>visionResults()</kbd> function. We'll do this a section at a time so it all makes sense:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func visionResults(_ results: [Any]) {<br/>    <br/>    <strong>CATransaction.begin()</strong><br/><strong>    CATransaction.setValue(kCFBooleanTrue, forKey: <br/>      kCATransactionDisableActions)</strong><br/><strong>    </strong><br/><strong>    detectionlayer?.sublayers = nil</strong><br/>    <br/>    // Other code to follow    <br/>}</pre>&#13;
<p style="padding-left: 60px">We start with some basic housekeeping; performing <kbd>CATransaction</kbd> locks in memory any changes we're going to make to <kbd>CALayer</kbd>, before we finally commit them for use. In this code, we'll be modifying <kbd>detectionLayer</kbd>.</p>&#13;
<ol start="6">&#13;
<li>Next, we'll iterate around our <kbd>results</kbd> parameter to pull out anything that is of class type <kbd>VNRecognizedObjectObservation</kbd>:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">for observation in results where observation is <br/>  VNRecognizedObjectObservation {<br/>    <br/>    guard let objectObservation = observation as? <br/>      VNRecognizedObjectObservation else {<br/>        continue<br/>    }<br/>    <br/>    let labelObservation = objectObservation.labels.first<br/>    let objectBounds = <strong>VNImageRectForNormalizedRect(<br/>      objectObservation.boundingBox, </strong><strong>Int(bufferSize.width),<br/>        </strong><strong>Int(bufferSize.height))                                                                    </strong><br/>    <br/>    let shapeLayer = createRoundedRectLayer(with: objectBounds)<br/>    let textLayer = createTextSubLayer(with: objectBounds,<br/>      identifier: labelObservation?.identifier ?? "",<br/>        confidence: labelObservation?.confidence ?? 0.0)<br/>    shapeLayer.addSublayer(textLayer)<br/>    detectionlayer.addSublayer(shapeLayer)<br/><br/>updateDetectionLayerGeometry()<br/>CATransaction.commit()<br/>    <br/>}</pre>&#13;
<p style="padding-left: 60px">From this, we'll continue to use Vision to obtain the <kbd>Rect</kbd> and position of the identified object(s) using <kbd>VNImageRectForNormalizedRect</kbd>. We can also grab some text information about the objects detected and use that too.</p>&#13;
<ol start="7">&#13;
<li>Finally, we'll gracefully close off any changes to <kbd>detectionLayer</kbd> and update the geometry to match the detected objects. You'll notice there are two new functions we've just introduced:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px"><strong>createRoundedRectLayer()</strong><br/><strong>createTextSubLayer()</strong></pre>&#13;
<p>These again are helper functions, one to draw the rectangle of the detected object and the other to write the text. These functions are generic boilerplate sample code that can be obtained from Apple's documentation. Feel free to have a play around with these to suit your needs. One thing I will mention: you'll notice how we do all this again using layers rather than adding <kbd>UIView</kbd> and <kbd>UILabel</kbd>. This again is because UIKit is a wrapper around a lot of core functionality. But adding a UIKit component on top of another component is unnecessary and with what is already an intense program, this could be performed much more efficiently by updating and manipulating the layers directly on a UIKit object.</p>&#13;
<p>These objects can be found in the sample project on GitHub; just copy them into your project (either in <kbd>VisionViewController</kbd> or your own helper file).</p>&#13;
<p>With our AV Foundation camera streaming in place and Vison and CoreML ready to do their magic, there is one final override we need to add to our <kbd>VisionViewController</kbd>:</p>&#13;
<pre>override func captureOutput(_ output: AVCaptureOutput, didOutput <br/>  sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {<br/>    <br/>    guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) <br/>      else {<br/>        return<br/>    }<br/>    <br/>    let exifOrientation = <br/>      ImageHelper.exifOrientationFromDeviceOrientation()<br/>    <br/>    let imageRequestHandler = VNImageRequestHandler(cvPixelBuffer: <br/>      pixelBuffer, orientation: exifOrientation, options: [:])<br/>    do {<br/>        try imageRequestHandler.perform(self.requests)<br/>    } catch {<br/>        print(error)<br/>    }<br/>}</pre>&#13;
<p>Using the delegate for AV Foundation, we grab each frame again, converting this to <kbd>CVPixelBuffer</kbd> in order to create <kbd>VNImageRequestHander</kbd>. This now kicks off the requests in our <kbd>startVision()</kbd> function, stitching everything together nicely.</p>&#13;
<p>We're almost done; let's finish off with some bits and pieces to tie all this together now:</p>&#13;
<ol>&#13;
<li>Head on over to <kbd>ViewController.swift</kbd> and add the following <kbd>IBAction</kbd> and logic from <kbd>UISegmentedControl</kbd> that we created earlier:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">@IBAction func onInputTypeSelected(_ sender: UISegmentedControl) {<br/>    <br/>    switch sender.selectedSegmentIndex {<br/>    case 0:<br/>        captureSession.stopRunning()<br/>    case 1:<br/>        startLivePreview()<br/>    default:<br/>        print("Default case")<br/>    }<br/>    <br/>}</pre>&#13;
<ol start="2">&#13;
<li>Now, create a function called <kbd>startLivePreview()</kbd>:</li>&#13;
</ol>&#13;
<pre style="padding-left: 60px">func startLivePreview() {<br/>    captureSession.startRunning()<br/>}</pre>&#13;
<ol start="3">&#13;
<li>Remove <kbd>captureSession.startRunning()</kbd> from <kbd>setupCaptureSession()</kbd>.</li>&#13;
<li>Finally, in our <kbd>Main.storyboard</kbd> view controller, change the class from <kbd>ViewController</kbd> to <kbd>VisionViewController</kbd>.</li>&#13;
</ol>&#13;
<ol start="5">&#13;
<li>Now, go ahead and run the app. All going well, you should be live-detecting images with an overlay that looks like this:</li>&#13;
</ol>&#13;
<div><img src="img/c44f615f-9831-4031-8041-8e91c52658ee.png" style="" width="1098" height="2417"/></div>&#13;
<p>Figure 11.2 – Vision detection</p>&#13;
<p>As you can see, both Vision and CoreML have successfully detected my cell phone and its location in the image (all in real time).</p>&#13;
<h2 id="uuid-ae38958e-8582-4c1a-b0c3-822d0b6595e9" class="mce-root">How it works...</h2>&#13;
<p>A high-level overview goes something like this:</p>&#13;
<ol>&#13;
<li>Capture a real-time camera feed (using AV Foundation).</li>&#13;
<li>Use a trained CoreML model to detect whether the image contains an object (that it recognizes).</li>&#13;
<li>Use Vision to detect the position of the object in the picture.</li>&#13;
</ol>&#13;
<p>We covered the camera streaming elements in the previous recipe, but let's take a deeper look at how <em>steps 2</em> and <em>3</em> work.</p>&#13;
<p>Let's actually start with <em>step 3</em>. We saw in the last section how we use <kbd>VNImageRequestHander</kbd> to pass back <kbd>CVPixelBuffer</kbd> of each image frame. This now fires off calls in our <kbd>setupVision()</kbd> function, so let's take a closer look in there.</p>&#13;
<p>First, we grab our model from the apps bundle so that we can pass this over to Vision:</p>&#13;
<pre>guard let localModel = Bundle.main.url(forResource: "<strong>YOLOv3</strong>", <br/>  withExtension: "mlmodelc") else {<br/>    return<br/>}</pre>&#13;
<p>Next, we head back to <em>step 2</em>, where we create an instance of <kbd>VNCoreMLModel()</kbd>,<strong> </strong>passing in our <kbd>localModel</kbd>. With this <kbd>visionModel</kbd>, we can now create our <kbd>VNCoreMLRequest</kbd> call, along with its completion handler, which will fire from requests that come in via our AV Foundation delegate.</p>&#13;
<p>This one simple request does the work of both the Vision Framework and CoreML – first detecting whether an object is found, then supplying us with the details on where that object is located inside the image.</p>&#13;
<p>This is where the bulk of our work is done. If you look again at our <kbd>visionResults()</kbd> function and all the helper functions within, these are merely just ways of parsing data that has come back, and in turn, decorating our view.</p>&#13;
<p>In our "results" from the <kbd>VNCoreMLRequest()</kbd> response, we take an instance of <kbd>VNRecognizedObjectObservation</kbd>, which in turn gives us two properties, a label (of what CoreML thinks it has found) along with a confidence score.</p>&#13;
<h2 id="uuid-c4070299-7e83-43c9-bb76-56638280fb3f" class="mce-root">See also</h2>&#13;
<p>For more information on <kbd>CALayer</kbd>, refer to <a href="https://developer.apple.com/documentation/quartzcore/calayer">https://developer.apple.com/documentation/quartzcore/calayer</a>.</p>&#13;
&#13;
&#13;
            &#13;
&#13;
            &#13;
        &#13;
    </div></body></html>