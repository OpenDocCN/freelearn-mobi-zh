<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer106">
			<h1 id="_idParaDest-285" class="chapter-number"><a id="_idTextAnchor379"/>12</h1>
			<h1 id="_idParaDest-286"><a id="_idTextAnchor380"/>Performance and Scalability</h1>
			<p>Software systems grow with business and changing environments manifested in higher complexity, more diverse user demands, and heavier workloads. The ability to maintain high performance and scale under growth becomes critical. Performance refers to how quickly a system can process and respond to requests, while scalability describes a system’s capacity to handle a higher volume of traffic and usage <span class="No-Break">over time.</span></p>
			<p>Poor performance can lead to frustrating user experiences, lost productivity, and even complete system failures. And systems that can’t scale to meet rising demands will quickly become overwhelmed and unusable. Therefore, ensuring optimal performance and scalability is a key challenge for any software <span class="No-Break">engineering project.</span></p>
			<p>In this chapter, we’ll explore the core concepts and principles of performance engineering and scalable system design. We’ll discuss common performance bottlenecks and mitigation strategies, review techniques for load testing and benchmarking, and cover architectural patterns and design choices that enable horizontal and <span class="No-Break">vertical scalability.</span></p>
			<p>By the end of this chapter, you’ll have a solid understanding of how to build highly performant, scalable systems that can withstand the pressure of <span class="No-Break">real-world demands.</span></p>
			<p>We’re going to cover the following topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Dimensions of performance <span class="No-Break">and scalability</span></li>
				<li>Optimize performance now <span class="No-Break">or later?</span></li>
				<li>Performance <span class="No-Break">test planning</span></li>
				<li>Executing a <span class="No-Break">performance test</span></li>
				<li><span class="No-Break">Micro-benchmarking</span></li>
				<li>Strategies for <span class="No-Break">performance improvement</span></li>
				<li>Ultra-low <span class="No-Break">latency systems</span></li>
			</ul>
			<h1 id="_idParaDest-287"><a id="_idTextAnchor381"/>Technical requirements</h1>
			<p>You can find all the code files used in this chapter on <span class="No-Break">GitHub: </span><a href="https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-12"><span class="No-Break">https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-12</span></a></p>
			<h1 id="_idParaDest-288"><a id="_idTextAnchor382"/>Dimensions of performance and scalability</h1>
			<p>Performance is<a id="_idIndexMarker1036"/> the efficiency of a system in executing tasks and responding to requests. It’s measured by <span class="No-Break">various metrics:</span></p>
			<ul>
				<li><strong class="bold">Latency</strong>: The time taken for the system to respond to <span class="No-Break">a request.</span></li>
				<li><strong class="bold">Throughput</strong>: The number of requests processed in a given <span class="No-Break">time frame.</span></li>
				<li><strong class="bold">Resource utilization</strong>: The percentage of resources (for example CPU, memory, network, files, and so on) used <span class="No-Break">during operations.</span></li>
				<li><strong class="bold">Concurrent users</strong>: The number of users effectively served by the system simultaneously without degradation <span class="No-Break">in performance.</span></li>
				<li><strong class="bold">Page load time</strong>: The total time taken for a screen to fully load, including all assets (images, videos, scripts, and <span class="No-Break">so on)</span></li>
				<li><strong class="bold">Queue size</strong>: The number of requests waiting to be processed by <span class="No-Break">the server</span></li>
				<li><strong class="bold">Time to first byte</strong> (<strong class="bold">TTFB</strong>): The time<a id="_idIndexMarker1037"/> that elapsed from when the client initiated a request to the client receiving the first byte from <span class="No-Break">the server.</span></li>
				<li><strong class="bold">Cache hit ratio</strong>: The percentage of requests served from the cache versus those from the slower secondary data source. A higher ratio indicates more <span class="No-Break">efficient caching.</span></li>
				<li><strong class="bold">Error rate</strong>: The percentage of requests resulting in errors (for example, HTTP error statuses). High error rates indicate problems with the application <span class="No-Break">or infrastructure.</span></li>
			</ul>
			<p>Scalability is the capacity of a system to handle increased load without degrading performance. It indicates how well a system can grow and adapt to an increase in user traffic and data volume. Scalability can be<a id="_idIndexMarker1038"/> categorized into <span class="No-Break">two types:</span></p>
			<ul>
				<li><strong class="bold">Vertical scaling</strong>: Adding <a id="_idIndexMarker1039"/>more resources (for example, CPU or RAM) to a single node to increase its capacity. This is also known<a id="_idIndexMarker1040"/> as <span class="No-Break"><strong class="bold">scale up</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Horizontal scaling</strong>: Adding <a id="_idIndexMarker1041"/>more nodes to distribute the load and improve capacity in a distributed system. This<a id="_idIndexMarker1042"/> is also known<a id="_idIndexMarker1043"/> as <span class="No-Break"><strong class="bold">scale out</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>Scalability is also the capacity of a system to downsize when the load is reduced. Scaling down is usually concerned with the flexible use of resources and cost savings. It’s still an important aspect of scalability, but the focus is usually on scaling up and <span class="No-Break">scaling out.</span></p>
			<p>Scalability can be measured by the <span class="No-Break">following metrics:</span></p>
			<ul>
				<li><strong class="bold">Scalability ratio</strong>: The ratio of<a id="_idIndexMarker1044"/> the increase in performance to the increase in resources such as the number <span class="No-Break">of servers</span></li>
				<li><strong class="bold">Time to scale</strong>: The time taken between adding resources to extra resources becoming operational in <span class="No-Break">the system</span></li>
			</ul>
			<p>These metrics are useful in measuring how a change in the system may affect performance <a id="_idIndexMarker1045"/>and scalability. Without them, it’s difficult to decide whether performance should be optimized or not. We’re going to discuss this decision in depth in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor383"/>Optimize performance now or later?</h1>
			<p>Engineers and architects <a id="_idIndexMarker1046"/>often face the question of whether performance should be optimized now or later. This happens from the early stages of system design to already established <span class="No-Break">production systems.</span></p>
			<p>We all know optimizing performance is crucial, but whether to prioritize it from day one isn’t a simple <span class="No-Break">binary question.</span></p>
			<p>You may have heard of someone who said “<em class="italic">Premature optimization is the root of evil.</em>” The statement itself is theatrical, but there are some merits <span class="No-Break">in it.</span></p>
			<p>You may also have heard a quote saying, “<em class="italic">Make it work, make it right, make it fast.</em>” This was coined by software engineer <span class="No-Break"><em class="italic">Kent Beck</em></span><span class="No-Break">.</span></p>
			<p>So, what would be the consequences if a system is optimized prematurely, or if we reverse the order to “make it fast” <span class="No-Break">too early?</span></p>
			<p>Spending too much time on performance improvements before understanding user behaviors and requirements can lead to wasted effort. Moreover, it creates an unnecessarily complex architecture that hinders the team’s productivity. The team may have to simplify the over-engineered system, which also requires effort. In that sense, the team is punished<a id="_idIndexMarker1047"/> twice for improving performance <span class="No-Break">too early.</span></p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor384"/>Considerations for optimizing performance and scalability</h2>
			<p>There are several factors <a id="_idIndexMarker1048"/>to consider regarding whether the <a id="_idIndexMarker1049"/>system should be optimized for performance <span class="No-Break">and scalability:</span></p>
			<ul>
				<li><strong class="bold">Core features completeness</strong>: If the core features of a system are still being developed, then it’s often more important to focus on delivering core features and functionality initially. This is the first step: “<em class="italic">Make </em><span class="No-Break"><em class="italic">it work.</em></span><span class="No-Break">”</span><p class="list-inset">In addition, we must ensure the system behaves as expected as per functional requirements. Correctness should always come before performance. This is the second step: “<em class="italic">Make </em><span class="No-Break"><em class="italic">it right.</em></span><span class="No-Break">”</span></p></li>
				<li><strong class="bold">Performance metrics</strong>: Before optimizing performance or improving scalability, it’s paramount to have current performance metrics as a baseline. A performance baseline provides insights into the current system bottlenecks that help the team prioritize which area should be <span class="No-Break">improved first.</span><p class="list-inset">A performance benchmark enables empirical and objective comparison of whether a change has resulted in better or worse performance, or whether an attempt to improve performance has achieved <span class="No-Break">its goals.</span></p></li>
				<li><strong class="bold">Non-functional requirements</strong>: Non-functional requirements are a useful source of guidance on whether the system needs to be optimized now. Non-functional requirements for performance can be driven by regulatory constraints, external system integration conformance, or principles of <span class="No-Break">user experiences.</span></li>
				<li><strong class="bold">Critical use cases, user experiences, and competitors</strong>: If the application is expected to handle high traffic from the beginning (for example, product launch events, training, or marketing campaigns), then early optimization is essential. If the application’s performance directly impacts user satisfaction, it’s important to address performance concerns early to avoid negative feedback. The current performance metrics of competitors also indicate how much the application’s performance should <span class="No-Break">be optimized.</span></li>
				<li><strong class="bold">Scalability needs</strong>: If rapid growth or scaling needs are anticipated for an application, implementing <a id="_idIndexMarker1050"/>good performance practices<a id="_idIndexMarker1051"/> from the beginning will save time and <span class="No-Break">effort later.</span></li>
			</ul>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor385"/>Best practices for performance</h2>
			<p>Even if it may not be the<a id="_idIndexMarker1052"/> right time to optimize performance, there are some best practices to at least not make <span class="No-Break">performance worse:</span></p>
			<ul>
				<li><strong class="bold">Measure first</strong>: Measure the current performance metrics, ideally all operations, but as a bottom line, measure the core features and most <span class="No-Break">frequent operations.</span></li>
				<li><strong class="bold">Implement basic optimization</strong>: Basic performance best practices such as efficient database queries in the early stage <span class="No-Break">of development.</span></li>
				<li><strong class="bold">Plan for scalability</strong>: Plan and have scalability in mind when designing system architecture to allow for easier optimization later without major refactoring. Sometimes, it’s about not putting restrictions that would <span class="No-Break">limit scalability.</span></li>
			</ul>
			<p>While it isn’t always necessary to optimize performance on day one, incorporating basic performance considerations into your development process can lead to better long-term results. Focus on delivering value first, then iterate on performance as the application evolves. Let’s go through an example of performance measurement to understand <span class="No-Break">this better.</span></p>
			<h1 id="_idParaDest-292"><a id="_idTextAnchor386"/>An example of basic performance measurement</h1>
			<p>Here’s a basic<a id="_idIndexMarker1053"/> example of an operation to <span class="No-Break">be measured:</span></p>
			<pre class="source-code">
fun sampleOperation() {
    Thread.sleep(1)
}</pre>			<p>The goal of this example is to find out the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Throughput</strong>: How many operations can be performed in <span class="No-Break">a second</span></li>
				<li><strong class="bold">Latency</strong>: How long it takes to finish an operation <span class="No-Break">on average</span></li>
			</ul>
			<p>A small function, <strong class="source-inline">measureTotalTimeElapsed</strong>, must be defined to measure the total time elapsed for all iterations of <span class="No-Break">the operation:</span></p>
			<pre class="source-code">
fun measureTotalTimeElapsed(
    iterations: Int,
    operation: (Int) -&gt; Unit,
): Long =
    measureTimeMillis {
        repeat(iterations, operation)
    }</pre>			<p>This function uses the <strong class="source-inline">measureTimeMillis</strong> Kotlin function from Standard Library to capture the time spent in repeating <span class="No-Break">the operation.</span></p>
			<p>Finally, this is the <strong class="source-inline">main</strong> function to launch <span class="No-Break">the test:</span></p>
			<pre class="source-code">
fun main() {
    val iterations = 1_000
    val operationTime = measureTotalTimeElapsed(iterations) { sampleOperation() }
    println("Total time elapsed: ${operationTime / 1000.0} second")
    println("Throughput: ${iterations / (operationTime / 1000.0)} operations per second")
    println("Latency (average): ${operationTime / iterations} ms")
}</pre>			<p>This function defines the operation to be executed 1,000 times. After invoking <span class="No-Break">the </span><span class="No-Break"><strong class="source-inline">measure</strong></span><strong class="source-inline">
TotalTimeElapsed</strong> function with the Lambda expression that runs the <strong class="source-inline">sampleOperation</strong> function, the total time elapsed in milliseconds is returned. Then, the throughput is calculated as the number of iterations divided by the total elapsed time in seconds. The average latency is calculated as the inverse reciprocal of throughput – the total time elapsed divided by the number <span class="No-Break">of iterations.</span></p>
			<p>This is a sample output from running <span class="No-Break">the test:</span></p>
			<pre class="console">
Total time elapsed: 1.264 second
Throughput: 791.1392405063291 operations per second
Latency (average): 1 ms</pre>			<p>Since the sample function, <strong class="source-inline">sampleOperation</strong>, only makes the thread sleep for 1 millisecond, the average latency is 1 millisecond, as expected. The throughput in this run is close to 800, but it varies in <span class="No-Break">every run.</span></p>
			<p>Kotlin Standard Library provides a few functions for <span class="No-Break">time measurement:</span></p>
			<ul>
				<li>Return time elapsed in milliseconds (used in this <span class="No-Break">example): </span><span class="No-Break"><strong class="source-inline">measureTimeMillis</strong></span></li>
				<li>Return time elapsed in <span class="No-Break">nanoseconds: </span><span class="No-Break"><strong class="source-inline">measureNanoTime</strong></span></li>
				<li>Return time elapsed as <span class="No-Break"><strong class="source-inline">Duration</strong></span><span class="No-Break">: </span><span class="No-Break"><strong class="source-inline">measureTime</strong></span></li>
				<li>Return time elapsed and the value returned from the Lambda <span class="No-Break">expression: </span><span class="No-Break"><strong class="source-inline">measureTimedValue</strong></span></li>
			</ul>
			<p>For real-life performance <a id="_idIndexMarker1054"/>critical systems, this is certainly not enough. Due to this, in the next section, we’ll cover the main types of <span class="No-Break">performance tests.</span></p>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor387"/>Performance tests</h1>
			<p>Performance tests are <a id="_idIndexMarker1055"/>a category of test that evaluates the speed, responsiveness, and stability of a system under a given workload. In this section, we’ll look at the main types of <span class="No-Break">performance tests.</span></p>
			<h2 id="_idParaDest-294"><a id="_idTextAnchor388"/>Load testing</h2>
			<p>Load tests aim to assess<a id="_idIndexMarker1056"/> the behaviors of a system under<a id="_idIndexMarker1057"/> expected load conditions, such as a configured number of concurrent requests. The goal is to identify bottlenecks in application or infrastructure where performance may degrade under load. It ensures the system can handle anticipated traffic without <span class="No-Break">performance degradation.</span></p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor389"/>Stress testing</h2>
			<p>Stress tests <a id="_idIndexMarker1058"/>aim to <a id="_idIndexMarker1059"/>evaluate the system’s performance under extreme load conditions beyond its normal operational capacity. They also help us determine the breaking point of the system and how it fails under stress, so proactive monitoring and alerts can be deployed <span class="No-Break">for precautions.</span></p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor390"/>Endurance testing (soak testing)</h2>
			<p>Endurance tests, known <a id="_idIndexMarker1060"/>as soak tests, focus <a id="_idIndexMarker1061"/>on the stability and performance of a system over an extended period. This extended period is used to identify issues that accumulate or emerge over time, such as memory leaks, resource exhaustion, or <span class="No-Break">performance degradation.</span></p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor391"/>Spike testing</h2>
			<p>Spike tests introduce <a id="_idIndexMarker1062"/>a <a id="_idIndexMarker1063"/>sudden increase in load (the “spike”) so that we can observe how the system reacts in this situation. The result illustrates how the system can handle abrupt changes in traffic <span class="No-Break">without failure.</span></p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor392"/>Volume and latency testing</h2>
			<p>Volume tests <a id="_idIndexMarker1064"/>evaluate the system’s performance<a id="_idIndexMarker1065"/> with a large volume of data. Latency tests measure the time delay between a request and the corresponding response. They usually measure metrics such as throughput and latency <a id="_idIndexMarker1066"/>to ensure<a id="_idIndexMarker1067"/> the application can<a id="_idIndexMarker1068"/> meet <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>) or <strong class="bold">service-level </strong><span class="No-Break"><strong class="bold">objectives</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SLOs</strong></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor393"/>Scalability testing</h2>
			<p>Scalability<a id="_idIndexMarker1069"/> tests<a id="_idIndexMarker1070"/> aim to determine how well the system can scale up or down in response to increasing or decreasing loads. It measures the performance of the system as resources are added <span class="No-Break">or removed.</span></p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor394"/>Configuration testing</h2>
			<p>Configuration tests<a id="_idIndexMarker1071"/> aim<a id="_idIndexMarker1072"/> to identify the optimal configuration for performance. They involve running performance tests under different configurations, including hardware, software, and <span class="No-Break">the network.</span></p>
			<h1 id="_idParaDest-301"><a id="_idTextAnchor395"/>Planning a performance test</h1>
			<p>Although there are <a id="_idIndexMarker1073"/>different types of performance tests, planning and executing performance tests are similar. The difference is in the details of each step. In this section, we’ll explore the journey of planning and executing a <span class="No-Break">performance test.</span></p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor396"/>Planning</h2>
			<p>In the planning phase, first, the objectives of the test should be defined. This means we must define the information we want to get out of the tests – for example, can a household record be created within 50 milliseconds? Can the system handle 5,000 concurrent requests without degradation? These objectives are the primary drives to plan and execute a performance test. They also determine which type of performance tests can <span class="No-Break">be used.</span></p>
			<p>Then, business scenarios for performance tests should be defined. Usually, the objectives would have given a great hint at which scenarios would be used, but it’s worth exploring the details of the steps involved in each scenario and formalizing them as a blueprint of the <span class="No-Break">test script.</span></p>
			<p>The last part of planning is to specify the load levels to run, including the number of users and the duration of the test. Sometimes, it isn’t so clear which level to run, especially if we want to find the breaking point of the system. This is OK initially since performance tests are <a id="_idIndexMarker1074"/>meant to <span class="No-Break">run iteratively.</span></p>
			<h2 id="_idParaDest-303"><a id="_idTextAnchor397"/>Preparation and development</h2>
			<p>Once there’s an initial<a id="_idIndexMarker1075"/> plan, the performance test can be prepared and developed. These activities can happen <span class="No-Break">in parallel.</span></p>
			<p>The test script is the core of the test execution. The test needs to be automated to achieve consistent results. This involves a big decision on which tool to use. Here’s a list of commonly <span class="No-Break">used tools:</span></p>
			<ul>
				<li><strong class="bold">Apache JMeter</strong> (<a href="https://jmeter.apache.org/">https://jmeter.apache.org/</a>): Open source, free, GUI support, distributed<a id="_idIndexMarker1076"/> testing, plugin support, <span class="No-Break">and Java-based</span></li>
				<li><strong class="bold">LoadRunner by OpenText</strong> (<a href="https://www.opentext.com/">https://www.opentext.com/</a>): Commercial licenses, GUI, integration <a id="_idIndexMarker1077"/>with CI/CD tools, analytics and reporting support, and support <span class="No-Break">for Java</span></li>
				<li><strong class="bold">Gatling</strong> (<a href="https://docs.gatling.io/">https://docs.gatling.io/</a>): Open <a id="_idIndexMarker1078"/>source, commercial licenses with additional features, and scripts can be written <span class="No-Break">in Kotlin</span></li>
				<li><strong class="bold">K6</strong> (<a href="https://k6.io/">https://k6.io/</a>): Open <a id="_idIndexMarker1079"/>source, subscription-based for cloud features, and can integrate with CI/CD scripts <a id="_idIndexMarker1080"/>written in <span class="No-Break"><strong class="bold">JavaScript</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">JS</strong></span><span class="No-Break">)</span></li>
				<li><strong class="bold">Locust</strong> (<a href="https://locust.io/">https://locust.io/</a>): Open<a id="_idIndexMarker1081"/> source, GUI support, distributed testing, and scripts written <span class="No-Break">in Python</span></li>
				<li><strong class="bold">BlazeMeter</strong> (<a href="https://www.blazemeter.com/">https://www.blazemeter.com/</a>): Free<a id="_idIndexMarker1082"/> with limited features, commercial licenses, cloud-based, GUI support, real-time reporting and analytics, integrated with CI/CD, and supports <span class="No-Break">JMeter scripts</span></li>
			</ul>
			<p>These tools provide comprehensive features such as organizing test scripts, managing multiple test configurations, metrics measurement, analytics, and reporting. You also have the option to build your own drivers of performance tests. This is applicable if your tests are simple and there are sufficient metric measurements without <span class="No-Break">external tools.</span></p>
			<p>Appropriate metric measurement needs to be set up according to what the test script requires. The metrics can be measured by the testing tools, or by the monitoring tools already embedded in the system, as discussed previously in <a href="B21737_11.xhtml#_idTextAnchor358"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>. Any missing metrics need to be set up before executing <span class="No-Break">the tests.</span></p>
			<p>Meanwhile, a test environment needs to be set up for execution. Ideally, the environment should be comparable to the actual production environment where the system runs. If that’s too expensive, an environment of a smaller scale can be used to project expected performance, with a degree of inaccuracy <span class="No-Break">in mind.</span></p>
			<p>The test environment should be an isolated sandbox that does nothing but the performance tests. It can be a challenge for some organizations to replicate a production-like environment for performance testing. Replicating an environment with data alone may already be a challenge for some organizations. In addition, the environment needs to have the necessary data to run the <span class="No-Break">test scenarios.</span></p>
			<p>Sometimes, the<a id="_idIndexMarker1083"/> system has integration with third-party systems. In this case, the external integration would need to be stubbed out <span class="No-Break">with simulators.</span></p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor398"/>Execution and iteration</h2>
			<p>Once we have the test <a id="_idIndexMarker1084"/>scripts, test environments, and corresponding metrics set up, we’re ready to execute the performance test. It’s vital to allow an iterative feedback loop where tests can run multiple times, and there could be changes between each test. Within each iteration, the same operation should be executed numerous times so that we have enough data points to <span class="No-Break">perform analysis.</span></p>
			<p>The tests should be run twice at a minimum, where the initial run identifies a bottleneck, then a change is made with the intent to eliminate the bottleneck, and finally, another run proves the bottleneck no longer exists, as indicated <span class="No-Break">by metrics.</span></p>
			<p>Realistically, another bottleneck will emerge after the biggest one is eliminated. The performance landscape will change for every change that’s made to improve performance. The iteration can end when the objectives are completed, or a new problem may be discovered during <span class="No-Break">the process.</span></p>
			<p>The iterative <a id="_idIndexMarker1085"/>execution of a performance test can be seen in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B21737_12_1.jpg" alt="Figure 12.1 – A sample workflow of performance testing" width="1323" height="1481"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – A sample workflow of performance testing</p>
			<p>In each run, the test is<a id="_idIndexMarker1086"/> executed by running the test script. The test script usually starts with warm-up operations. For example, if we’re going to send 10,000 requests 100 times, the first 10 times can be treated as a warm-up, so the metrics <span class="No-Break">aren’t considered.</span></p>
			<p>Warming up allows the system to reach a stable state before actual performance measurements are taken. Processing initial requests triggers the cache so that it populates with frequently accessed data. It also allows the system to allocate resources such as thread, memory, and database connections effectively. Other transient factors such as just-in-time compilation, garbage collection, and resource contention can be reduced by warming up <span class="No-Break">the system.</span></p>
			<p>After running the test, a report should be generated from the metric data that’s been collected. The report should be constructed in a format that allows iterations to be compared. The raw data is then analyzed to produce statistical figures such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>The mean and median percentiles of the <span class="No-Break">response time</span></li>
				<li>Average and peak throughputs; increase in throughput divided by the increase in resource which indicates <span class="No-Break">scaling efficiency</span></li>
				<li>The overall error rate and error rates <span class="No-Break">by types</span></li>
				<li>Average and <span class="No-Break">maximum latency</span></li>
				<li>The number of concurrent users being handled <span class="No-Break">without degradation</span></li>
				<li>The time for which the load <span class="No-Break">was maintained</span></li>
			</ul>
			<p>From these numbers, some bottlenecks may be identified. Some figures can be below the non-functional requirements, SLAs, or SLOs. Some figures can stand out as particularly slow compared to others. These bottlenecks drive the changes required to improve <span class="No-Break">overall performance.</span></p>
			<p>Especially in early <a id="_idIndexMarker1087"/>iterations, deficiencies in the test scripts may be identified. It isn’t uncommon to realize the test script itself isn’t efficient and causes slowness in the system. The test script may have unnecessary loops or complex logic that slows down the script’s execution time. The test script may have an artificial wait time between requests, which as a result limits the throughput. Other factors, such as error handling, synchronous operations, resource contention, and network performance, can also skew the result of performance tests. These findings lead to the test script being reviewed and updated for <span class="No-Break">future runs.</span></p>
			<p>After changes are made, the performance tests should be executed again to examine whether the target performance statistical figures have improved, and at the same, to ensure the changes don’t deteriorate system performance in <span class="No-Break">other areas.</span></p>
			<p>This repetitive exercise carries on until we’re satisfied with the results. There are several possibilities where the iteration <span class="No-Break">should stop:</span></p>
			<ul>
				<li>The test objectives have been completed – for example, we’ve detected the maximum number of requests the system can handle without <span class="No-Break">performance degradation</span></li>
				<li>The performance metrics have fulfilled the non-functional requirements, SLAs, <span class="No-Break">or SLOs</span></li>
				<li>The time spent on performance testing has exceeded the original <span class="No-Break">time-boxed duration</span></li>
			</ul>
			<p>Performance testing is meant to be a recurring exercise. A successful and satisfactory performance test only supports the assumption that the system is capable of handling requests within the configuration and parameters in the test scripts. The system usage pattern is constantly <a id="_idIndexMarker1088"/>changing due to business growth and new features being introduced <span class="No-Break">over time.</span></p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor399"/>Benefits of performance testing</h2>
			<p>Performance testing<a id="_idIndexMarker1089"/> provides insights into how the system performs under pre-configured loads. It indicates how we can optimize the system to provide a fast and reliable user experience, even under heavy load. It helps stakeholders understand system limits and make informed decisions about scaling and infrastructure. It also identifies potential issues before they impact users, reducing the risk of downtime or <span class="No-Break">service degradation.</span></p>
			<p>Performance tests are essential for ensuring that applications meet user expectations and maintain stability under varying conditions. By conducting different types of performance tests, organizations can identify and address potential issues, optimize performance, and enhance overall <span class="No-Break">user satisfaction.</span></p>
			<p>Next, we’ll consider a technique that’s used iteratively to measure a function’s performance. This technique is <span class="No-Break">called micro-benchmarking.</span></p>
			<h1 id="_idParaDest-306"><a id="_idTextAnchor400"/>Micro-benchmarking</h1>
			<p>While performance<a id="_idIndexMarker1090"/> testing focuses on system-level performance, micro-benchmarking is a performance measurement of a small and isolated piece of code at the function level. Micro-benchmarking is usually applicable to the <span class="No-Break">following areas:</span></p>
			<ul>
				<li>The algorithm that sits in the core of the whole system – for example, a search algorithm for an internet <span class="No-Break">search engine</span></li>
				<li>The function that’s used most frequently by <span class="No-Break">end users</span></li>
				<li>The function that’s exposed as an API to <span class="No-Break">external systems</span></li>
				<li>The code path that’s mission-critical <span class="No-Break">and performance-sensitive</span></li>
				<li>When comparing the implementations of a function, an algorithm, or a <span class="No-Break">code change</span></li>
			</ul>
			<p><strong class="bold">Kotlin benchmarking</strong> (<a href="https://github.com/Kotlin/kotlinx-benchmark">https://github.com/Kotlin/kotlinx-benchmark</a>) is <a id="_idIndexMarker1091"/>the most popular tool <a id="_idIndexMarker1092"/>for running benchmarks for Kotlin code. It wraps the classic <strong class="bold">Java Microbenchmark Harness</strong> (<strong class="bold">JMH</strong>) framework, and<a id="_idIndexMarker1093"/> it supports <a id="_idIndexMarker1094"/>Kotlin<a id="_idIndexMarker1095"/> through <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>), JS, Native, and even <strong class="bold">Web </strong><span class="No-Break"><strong class="bold">Assembly</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">WASM</strong></span><span class="No-Break">).</span></p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor401"/>Setting up micro-benchmarking with a Gradle Kotlin DSL script</h2>
			<p>It’s simple to <a id="_idIndexMarker1096"/>set up benchmarking <a id="_idIndexMarker1097"/>with a Gradle Kotlin DSL script. For example, for JVM, we need the <span class="No-Break">following plugins:</span></p>
			<pre class="source-code">
plugins {
    id("org.jetbrains.kotlinx.benchmark") version "0.4.11"
    kotlin("plugin.allopen") version "2.0.20"
}</pre>			<p>The first plugin is for Kotlin micro-benchmarking, while the second plugin is used to open the final Kotlin classes for instrumentation. Now, we need to make sure the plugins and dependencies can be looked up <span class="No-Break">from repositories:</span></p>
			<pre class="source-code">
repositories {
    mavenCentral()
    gradlePluginPortal()
}</pre>			<p>Next, a code dependency on Kotlin micro-benchmarking needs to <span class="No-Break">be declared:</span></p>
			<pre class="source-code">
    implementation("org.jetbrains.kotlinx:kotlinx-benchmark-runtime:0.4.11")</pre>			<p>Then, we need to configure the <strong class="source-inline">allOpen</strong> plugin so that it only opens Kotlin classes with the <span class="No-Break"><strong class="source-inline">State</strong></span><span class="No-Break"> annotation:</span></p>
			<pre class="source-code">
allOpen {
    annotation("org.openjdk.jmh.annotations.State")
}</pre>			<p>The final part of the setup is setting up <span class="No-Break">micro-benchmarking itself:</span></p>
			<pre class="source-code">
benchmark {
    targets {
        register("main")
    }
    configurations {
        named("main") {
        }
    }
}</pre>			<p>The configuration is called <strong class="source-inline">main</strong> and has been chosen to run. It’s possible to configure the number of warmup iterations, the number of iterations to be measured, and the length of how<a id="_idIndexMarker1098"/> long <a id="_idIndexMarker1099"/>each iteration should last. However, an annotation-based configuration has been used in <span class="No-Break">this example.</span></p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor402"/>The micro-benchmarking test</h2>
			<p>The actual benchmark <a id="_idIndexMarker1100"/>runner code is annotated so that it can be picked by the runner for execution with a specific configuration. Please note that this test should be placed in the <strong class="source-inline">main</strong> source folder (not the <strong class="source-inline">test</strong> source folder) so that it can be picked up by <span class="No-Break">the plugin:</span></p>
			<pre class="source-code">
@State(Scope.Benchmark)
@Fork(1)
@Warmup(iterations = 10)
@Measurement(iterations = 20, time = 1, timeUnit = TimeUnit.MILLISECONDS)
class MicrobenchmarkingTest {
    private var data = emptyList&lt;UUID&gt;()
    @Setup
    fun setUp() {
        data = (1..2).map { UUID.randomUUID() }
    }
    @Benchmark
    fun combineUUIDBenchmark(): UUID = data.reduce { one, two -&gt; one + two }
    private operator fun UUID.plus(another: UUID): UUID {
        val mostSignificant = mostSignificantBits xor another.mostSignificantBits
        val leastSignficant = leastSignificantBits xor another.leastSignificantBits
        return UUID(mostSignificant, leastSignficant)
    }
}</pre>			<p>This micro-benchmarking<a id="_idIndexMarker1101"/> test evaluates the performance of the function that combines<a id="_idIndexMarker1102"/> two <strong class="bold">Universally Unique Identifiers</strong> (<strong class="bold">UUIDs</strong>). The <strong class="source-inline">State</strong> annotation triggers the <strong class="source-inline">allOpen</strong> plugin to open this class for instrumentation. Then, the <strong class="source-inline">Fork</strong> annotation defines how many threads are used for execution. Other annotations specify the number of iterations for warmup, execution, and the duration of <span class="No-Break">each iteration.</span></p>
			<p>For example, the <strong class="source-inline">setup</strong> annotation function is used to create the data required to run the test, while the <strong class="source-inline">combineUUIDBenchmark</strong> function, which has the <strong class="source-inline">Benchmark</strong> annotation, is the<a id="_idIndexMarker1103"/> major function to <span class="No-Break">be measured.</span></p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor403"/>Micro-benchmarking runner</h2>
			<p>To run micro-benchmarking, we <a id="_idIndexMarker1104"/>can use the following <span class="No-Break">Gradle command:</span></p>
			<pre class="console">
./gradlew benchmark</pre>			<p>The summary of the result is printed to the console, while the detailed r<a id="_idTextAnchor404"/>eport is generated unde<a id="_idTextAnchor405"/>r the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">build/reports/benchmarks/main</strong></span><span class="No-Break"> folder:</span></p>
			<pre class="console">
Success: 109349297.194 ±(99.9%) 15493649.408 ops/s [Average]
  (min, avg, max) = (55205844.260, 109349297.194, 132224154.121), stdev = 17842509.699
  CI (99.9%): [93855647.787, 124842946.602] (assumes normal distribution)</pre>			<p>The format of micro-benchmarking is designed to compare runs. Improvements can be made between runs, and the next run should demonstrate whether the changes have made <span class="No-Break">a difference.</span></p>
			<p>Micro-benchmarking is a valuable subset of performance testing that focuses on code implementation. By understanding the performance characteristics of isolated functions, engineers can make targeted optimizations. In contrast, performance testing takes a holistic approach to assess how well the entire system performs under various conditions. Both practices are essential for delivering <span class="No-Break">high-performance systems.</span></p>
			<p>There’s another tool that measures and analyzes the performance of an application, but visually with graphical user interfaces. This tool is called the application profiler, and we’re going to cover it in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-310"><a id="_idTextAnchor406"/>Application profiling</h1>
			<p>Profiling works by<a id="_idIndexMarker1105"/> monitoring and analyzing the performance of an application at runtime. Profilers instrument code and intercept calls to collect performance measurements, such as elapsed time and the number of invocations. It can generate the stack trace of the application to visualize relationships <span class="No-Break">between functions.</span></p>
			<p>The profiler tool also monitors memory allocation and deallocation, analyzes the heap dump, and identifies potential <span class="No-Break">memory leaks.</span></p>
			<p>At the same time, the profiler tool measures CPU cycles that have been consumed by various parts of the code and identifies computing-intensive functions. The profiler tool also monitors the usage of other resources, such as file operations, network activities, and interactions, among threads to provide a comprehensive view of <span class="No-Break">resource utilization.</span></p>
			<p>The profiler tool comes with detailed reports that are visualized in the user interface to assist engineers in pinpointing the areas that <span class="No-Break">require optimization.</span></p>
			<p>However, running an application with the profiler significantly slows down performance due to invasive instrumentation and measurement. The metric data that’s captured should be treated as a magnification of the actual runtime and be used to find areas that are slow, inefficient, <span class="No-Break">or resource-consuming.</span></p>
			<p>There <a id="_idIndexMarker1106"/>are <a id="_idIndexMarker1107"/>several <a id="_idIndexMarker1108"/>popular<a id="_idIndexMarker1109"/> profiler <a id="_idIndexMarker1110"/>tools <a id="_idIndexMarker1111"/>available for <span class="No-Break">Kotlin engineers:</span></p>
			<ul>
				<li><strong class="bold">YourKit Java </strong><span class="No-Break"><strong class="bold">Profiler</strong></span><span class="No-Break"> (</span><a href="https://www.yourkit.com/java/profiler/"><span class="No-Break">https://www.yourkit.com/java/profiler/</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><strong class="bold">VisualVM</strong></span><span class="No-Break"> (</span><a href="https://visualvm.github.io/startupprofiler.html"><span class="No-Break">https://visualvm.github.io/startupprofiler.html</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">IntelliJ IDEA </strong><span class="No-Break"><strong class="bold">Profiler</strong></span><span class="No-Break"> (</span><a href="https://www.jetbrains.com/pages/intellij-idea-profiler/"><span class="No-Break">https://www.jetbrains.com/pages/intellij-idea-profiler/</span></a><span class="No-Break">)</span></li>
				<li><span class="No-Break"><strong class="bold">JProfiler</strong></span><span class="No-Break"> (</span><a href="https://www.ej-technologies.com/jprofiler"><span class="No-Break">https://www.ej-technologies.com/jprofiler</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Async </strong><span class="No-Break"><strong class="bold">Profiler</strong></span><span class="No-Break"> (</span><a href="https://github.com/async-profiler/async-profiler"><span class="No-Break">https://github.com/async-profiler/async-profiler</span></a><span class="No-Break">)</span></li>
				<li><strong class="bold">Java Mission </strong><span class="No-Break"><strong class="bold">Control</strong></span><span class="No-Break"> (</span><a href="https://www.oracle.com/java/technologies/jdk-mission-control.html"><span class="No-Break">https://www.oracle.com/java/technologies/jdk-mission-control.html</span></a><span class="No-Break">)</span></li>
			</ul>
			<p>Application profilers should be used to analyze performance-critical operations. They don’t usually run in production environments due to instrumentation being slowed down significantly. It’s common to run profilers in a lower environment with inputs simulating the <span class="No-Break">production</span><span class="No-Break"><a id="_idIndexMarker1112"/></span><span class="No-Break"> environment.</span></p>
			<p>Next, we’re going to cover a few performance <span class="No-Break">improvement strategies.</span></p>
			<h1 id="_idParaDest-311"><a id="_idTextAnchor407"/>Strategies for performance improvement</h1>
			<p>Improving the <a id="_idIndexMarker1113"/>performance of a system often requires a diverse approach that addresses various aspects. No silver bullet magically boosts performance. However, some common strategies help engineers navigate the problem to meet the <span class="No-Break">non-functional requirements.</span></p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor408"/>Testing, testing, testing</h2>
			<p>Performance tests should be<a id="_idIndexMarker1114"/> conducted continuously and repetitively. When there’s a perceived performance issue, it’s unlikely to know the root cause without running performance tests. Instead of blindly applying “performance fixes,” engineers should execute performance tests to understand the <span class="No-Break">problem first.</span></p>
			<p>Performance tests should be treated as both troubleshooting and discovery tools. There are always bottlenecks in the system that <span class="No-Break">surprise engineers.</span></p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor409"/>Avoiding expensive operations</h2>
			<p>More often than<a id="_idIndexMarker1115"/> not, performance issues are caused by a mismatch between the nature of the operations and the actual implementation. In other words, resources are used in unnecessary areas that would use excessive resources and computation power. If excessive resources are spent on expensive operations, then there will be <span class="No-Break">performance issues.</span></p>
			<p>Let’s consider an example scenario that demonstrates performance optimization by avoiding <span class="No-Break">expensive operations.</span></p>
			<h3>Scenario – iteration on expensive operations</h3>
			<p>Imagine that there’s a function that’s expensive to execute. This expense is high for <span class="No-Break">several reasons:</span></p>
			<ul>
				<li>It’s a remote synchronous call to <span class="No-Break">another application</span></li>
				<li>It’s computationally expensive <span class="No-Break">and/or resource-hungry</span></li>
				<li>It involves files, databases, messaging, networks, or <span class="No-Break">other resources</span></li>
				<li>It may be blocked until a result <span class="No-Break">comes back</span></li>
			</ul>
			<p>We know the following function isn’t expensive, but let’s pretend it is for the sake <span class="No-Break">of discussion:</span></p>
			<pre class="source-code">
fun someExpensiveOp(n: Int): Int = n</pre>			<p>On <a id="_idTextAnchor410"/>top of this<a id="_idIndexMarker1116"/> function, we’d like to run<a id="_idTextAnchor411"/> some filter<a id="_idTextAnchor412"/>ing, mapping, <span class="No-Break">and<a id="_idTextAnchor413"/> selection:</span></p>
			<pre class="source-code">
    val result = listOf(1, 7, 3, 23, 63).filter {
        println("filter:$it"); it &gt; 3
    }.map {
        println("expensive:$it"); someExpensiveOp(it)
    }.take(2)
    println(result)</pre>			<p>First, this piece of code filters to take only numbers greater than 3. Then, it invokes the <strong class="source-inline">expensive</strong> function and gets a new number. At the end, only the first two numbers are selected. The <strong class="source-inline">println</strong> function is called to show which value is evaluated in the <strong class="source-inline">filter</strong>, <strong class="source-inline">map</strong>, or <span class="No-Break"><strong class="source-inline">take</strong></span><span class="No-Break"> function.</span></p>
			<p>Executing this piece of code produces the following <span class="No-Break">console output:</span></p>
			<pre class="console">
filter:1
filter:7
filter:3
filter:23
filter:63
expensive:7
expensive:23
expensive:63
[7, 23]</pre>			<p>All five numbers are evaluated if they’re greater than 3. The numbers <strong class="source-inline">7</strong>, <strong class="source-inline">23</strong>, and <strong class="source-inline">63</strong> are greater than 3, so they’re passed to the <strong class="source-inline">expensive</strong> operation. Finally, only the first two numbers from the expensive operation <span class="No-Break">are returned.</span></p>
			<p>The expensive operation for the third number isn’t necessary because only the first two numbers are selected at the end. In addition, it could have found the first two numbers during filtering and stopped checking the rest of <span class="No-Break">the values.</span></p>
			<p>Optimized with<a id="_idIndexMarker1117"/> the <strong class="source-inline">asSequence</strong> function from Kotlin Standard Library, the code looks <span class="No-Break">as follows:</span></p>
			<pre class="source-code">
    val result = listOf(1, 7, 3, 23, 63)
        .asSequence().filter {
            println("filter:$it"); it &gt; 3
        }.map {
            println("expensive:$it"); someExpensiveOp(it)
        }.take(2)
    println(result)</pre>			<p>However, executing the preceding code prints the following to <span class="No-Break">the console:</span></p>
			<pre class="console">
kotlin.sequences.TakeSequence@246b179d</pre>			<p>No filtering, no expensive operation, or selection was run. This is because the <strong class="source-inline">asSequence</strong> function doesn’t build the list until there’s a terminal function. Let’s update <span class="No-Break">the code:</span></p>
			<pre class="source-code">
    println(result.toList())</pre>			<p>Now, the execution prints the following to <span class="No-Break">the console:</span></p>
			<pre class="console">
filter:1
filter:7
expensive:7
filter:3
filter:23
expensive:23
[7, 23]</pre>			<p>The sequence operation understands it only takes the first two numbers, so it looks for the first two numbers greater than 3 and stops there. The number <strong class="source-inline">63</strong> wasn’t even processed. The first number greater than 3 was <strong class="source-inline">7</strong>, so <strong class="source-inline">7</strong> was passed to the <strong class="source-inline">expensive</strong> operation. The second number greater than 3 was <strong class="source-inline">23</strong>, so <strong class="source-inline">23</strong> was also passed to the <strong class="source-inline">expensive</strong> operation. This<a id="_idIndexMarker1118"/> implementation has saved one <strong class="source-inline">expensive</strong> operation compared to the <span class="No-Break">previous one.</span></p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor414"/>An example of a performance improvement journey</h2>
			<p>The households in <a id="_idIndexMarker1119"/>the village have decided to run a survey to rate each household’s services. A vote consists of a rating from 1 <span class="No-Break">to 3:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold">1</strong></span><span class="No-Break">: Good</span></li>
				<li><span class="No-Break"><strong class="bold">2</strong></span><span class="No-Break">: Average</span></li>
				<li><span class="No-Break"><strong class="bold">3</strong></span><span class="No-Break">: Poor</span></li>
			</ul>
			<p>A household can vote for all other households, but only one vote can be made per household. Households are given 1 day to submit all the votes. Let’s also assume one household provides only <span class="No-Break">one service.</span></p>
			<p>Each household has a “score,” which is the sum of the rank numbers of all votes to that household. The household that has the highest score becomes the household that provides the best service in <span class="No-Break">the village.</span></p>
			<p>So, if there are <em class="italic">n</em> households in the village, the maximum number of votes will be <strong class="source-inline">n x (n- 1</strong>). We need a system that calculates the score of all households being voted for, and that records all votes as audit records. The system also needs to display non-final scores for each household when voting is <span class="No-Break">in progress.</span></p>
			<p>A simplistic architecture of this voting system may look <span class="No-Break">as follows:</span></p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B21737_12_2.jpg" alt="Figure 12.2 – Simulated survey architecture v0.1" width="967" height="912"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Simulated survey architecture v0.1</p>
			<p>All households <a id="_idIndexMarker1120"/>submit their votes to be validated by <strong class="bold">Voting Service</strong>. The service validates the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li>All households involved <span class="No-Break">are valid</span></li>
				<li>A household can’t vote <span class="No-Break">for itself</span></li>
				<li>A household can only vote for another <span class="No-Break">household once</span></li>
				<li>The vote has a <span class="No-Break">valid rank</span></li>
			</ul>
			<p>Since the voting happens in one day for the village, there’s a need for the system to respond quickly (<strong class="bold">latency</strong>) so that it can process many votes within a certain period (<strong class="bold">throughput</strong>) and support a vast number of <span class="No-Break">households (</span><span class="No-Break"><strong class="bold">scalability</strong></span><span class="No-Break">).</span></p>
			<p>The system expects many concurrent requests to <strong class="bold">Voting Service</strong> at a time, and that could cause <span class="No-Break">a spike.</span></p>
			<p>It’s possible to scale up the service vertically by adding more resources (CPU, memory, and so on). However, there are physical limitations regarding the number of CPU sockets or the maximum RAM it can support. Adding more resources also leads to diminishing returns, in which performance doesn’t improve proportionally due to other bottlenecks. The only running instance is also the single point of failure that if this instance fails, the entire system <span class="No-Break">becomes unavailable.</span></p>
			<p>Alternatively, the<a id="_idIndexMarker1121"/> system can scale out horizontally if we add more instances of the service. A <strong class="bold">load balancer</strong> can<a id="_idIndexMarker1122"/> be deployed to distribute load across multiple instances of the service, preventing any single instance from becoming a bottleneck. This significantly increases throughput by enabling <span class="No-Break">parallel processing.</span></p>
			<p>The load balancer has some knowledge of the load of each instance, so it can route the next request to the instance with the least load. This allows us to add more instances to handle increased loads. With that, the architecture has changed, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B21737_12_3.jpg" alt="Figure 12.3 – Simulated survey architecture v0.2" width="943" height="1091"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Simulated survey architecture v0.2</p>
			<p>Now, <strong class="bold">Voting Service</strong> has <a id="_idIndexMarker1123"/>two stateful validation rules. The first is that the households involved must be valid. The second is that each household can only vote for other <span class="No-Break">households once.</span></p>
			<p>The household records are frequently accessed, and they can be queried in the database remotely. <strong class="bold">Caching</strong> all <a id="_idIndexMarker1124"/>household records in each service instance is a sensible strategy to speed <span class="No-Break">up validation.</span></p>
			<p>Enforcing the rule that one household can only vote for another household would benefit from caching. If we cache a list of households that a given household has voted for (the <em class="italic">x-voted-by-y-list</em>), then we can enforce this business rule. However, there’s a complication if any instance can handle any household because it implies sharing <span class="No-Break">the list.</span></p>
			<p>There are two options we can consider. The first option is that we can use a distributed in-memory database such as Redis so the <em class="italic">x-voted-by-y-lists</em> can be shared, at the cost of having a distributed in-memory database and potential <span class="No-Break">resource contention.</span></p>
			<p>The second option is to configure the load balancer so that it supports sticky routing. Requests from one household always go to the one responsible instance. Each instance knows its assignment and can locally cache the <em class="italic">x-voted-by-y-lists</em> from the database during startup. The local cache is also updated as it processes <span class="No-Break">incoming requests.</span></p>
			<p>At this point, the bottleneck has shifted to the database since all the traffic is eventually funneled into it and each request can only be responded to after database operations have finished. This impacts the latency of the response to each <span class="No-Break">voting request.</span></p>
			<p>The score needs to be calculated for each household being voted in. This is an accumulated number <a id="_idIndexMarker1125"/>that leaves little room for parallel processing. Each validated vote also needs to be kept as an audit record in <span class="No-Break">the database.</span></p>
			<p>However, votes validated by <strong class="bold">Voting Service</strong> can be further processed asynchronously. Each vote can be partitioned by the household being voted for, so if <strong class="bold">Household 1</strong> votes for <strong class="bold">Household 2</strong>, the vote goes to the “bucket” for <span class="No-Break"><strong class="bold">Household 2</strong></span><span class="No-Break">.</span></p>
			<p>Resolve a household to a bucket can be as simple as a modulo function, that is, the remainder of a hash number divided by the number <span class="No-Break">of buckets:</span></p>
			<pre class="source-code">
Bucket number = (hash number of household name) mod (number of buckets)</pre>			<p>Each bucket is an event stream. <strong class="bold">Voting Service</strong> can respond to the voting request after an event is published to the corresponding event topic representing the bucket. The vote counting, score calculation, and vote persistence metrics will be processed when downstream components consume the event. This change will significantly reduce the latency of each <span class="No-Break">voting request.</span></p>
			<p>The updated architecture looks <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B21737_12_4.jpg" alt="Figure 12.4 – Simulated survey architecture v0.3" width="1197" height="1339"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Simulated survey architecture v0.3</p>
			<p>This approach has <a id="_idIndexMarker1126"/>a limitation: the number of buckets that are fixed at runtime. Events are already routed to the bucket, and we need to maintain the bucket assignment to calculate the <span class="No-Break">score correctly.</span></p>
			<p>The downstream operations require persisting the result in the database eventually, and we want to avoid overloading the database. Let’s examine the data to <span class="No-Break">be persisted:</span></p>
			<ul>
				<li><strong class="bold">Score for each household</strong>: One accumulative number per household; historical numbers <span class="No-Break">don’t matter</span></li>
				<li><strong class="bold">Vote audit record</strong>: Each record needs to be kept, and each record is independent of <span class="No-Break">each other</span></li>
			</ul>
			<p>The score numbers and vote audit records are different in their natures, so it makes sense for them to be processed differently. It’s better to keep this temporary data in a transient local<a id="_idIndexMarker1127"/> cache to reduce database load but persist the <span class="No-Break">values periodically.</span></p>
			<p>Here, we can introduce <span class="No-Break">two components:</span></p>
			<ul>
				<li>The first one, <strong class="bold">Vote Counter</strong>, consumes the event stream for its one assigned bucket and calculates scores for its responsible households. It doesn’t update the score records in the database immediately. Instead, it flushes the latest scores to the database with a fixed schedule – for example, every 10 minutes. This mechanism “soaks” the spike of votes and turns it into <span class="No-Break">regular updates.</span><p class="list-inset">There are multiple instances of <strong class="bold">Vote Counter</strong>, and there should be at least two instances consuming one bucket to provide availability. Each score record should consist of a household name, the number of votes, the score, and a timestamp. There should be de-duplication rules that only persist newer records and skip the <span class="No-Break">old ones.</span></p></li>
				<li>The second component, <strong class="bold">Vote Journalist</strong>, consumes a batch of events at a time and flushes the update into the database in one transaction. If the transaction fails, the events in the batch aren’t acknowledged and will be processed again later. <strong class="bold">Vote Journalist</strong> instances of the same bucket should be configured so that only one instance receives the batch of events. The batching processing significantly increases the throughput of vote audit record persistence. However, it would require performance testing to discover the optimal batch size that can scale with the number of votes and still be processed within the memory limits of <span class="No-Break">the process.</span></li>
			</ul>
			<p>With all these performance concerns considered, we have the final 1.0 architecture, as shown in <span class="No-Break"><em class="italic">Figure 12</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B21737_12_5.jpg" alt="Figure 12.5 – Simulated survey architecture v1.0" width="1296" height="2026"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Simulated survey architecture v1.0</p>
			<p>In this architecture, we’ve <a id="_idIndexMarker1128"/>optimized the process of load balancing the incoming requests to multiple instances of <strong class="bold">Voting Service</strong> for validation. This increases the throughput and scalability. Then, we introduced local caching of households and the <em class="italic">x-voted-by-y-lists</em> in each service instance to speed up the validation process. It also supports horizontal scaling by adding <span class="No-Break">more instances.</span></p>
			<p>After, we created a few buckets of event streams for each one responsible for several households exclusively. After <strong class="bold">Voting Service</strong> validates a request to be valid, it responds to the original request and publishes an event stream to the corresponding bucket. This reduces the latency for the response to the <span class="No-Break">voting request:</span></p>
			<ul>
				<li><strong class="bold">Vote Counter</strong> is introduced to calculate the scores of households that have been assigned to the given bucket. It sends the latest scores to the database periodically and soaks up <span class="No-Break">the spike.</span></li>
				<li><strong class="bold">Vote Journalist</strong> is introduced to receive a batch of events at a time and to persist them to the database in one transaction. Batch processing increases the throughput of vote audit <span class="No-Break">record persistence.</span></li>
			</ul>
			<p>In this example, we learned how to optimize the throughput, latency, and scalability of a system. Performance improvements are highly situational. We simply shouldn’t copy a pattern to another system and believe it will perform. Performance needs to be measured and tested. A <a id="_idIndexMarker1129"/>change is only considered a performance improvement when the metrics prove it. However, some known best practices of performance can be improved, something we’ll cover in the <span class="No-Break">next section.</span></p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor415"/>Best practices of performance in Kotlin</h2>
			<p>Kotlin has a few<a id="_idIndexMarker1130"/> features that aim to reduce overhead and therefore improve performance. However, these features come with justifications so that engineers can make a conscious decision when to use them. They aren’t expected to be used everywhere <span class="No-Break">without reasoning.</span></p>
			<h3>In-line functions</h3>
			<p>In-line Kotlin functions <a id="_idIndexMarker1131"/>are simply copied to the caller to reduce the need to invoke the function itself. This is particularly useful if there’s a deep stack of functions or if there’s a <span class="No-Break">higher-order function.</span></p>
			<p>In-line functions can be declared by adding a modifier at the function level, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
inline fun &lt;T&gt; measureTime(block: () -&gt; T): T {
    val start = System.nanoTime()
    val result = block()
    val timeTaken = System.nanoTime() - start
    return result.also { println("taken: $timeTaken") }
}</pre>			<h3>The use of immutable and mutable data structures</h3>
			<p>Immutable data eliminates the need for locking in multi-thread environments. In Kotlin, <strong class="source-inline">List</strong>, <strong class="source-inline">Set</strong>, and <strong class="source-inline">Map</strong> collections work with <span class="No-Break">immutable data.</span></p>
			<p>However, if we’re building a bigger object, such as a string, it’s advisable to use mutable collections or the <strong class="source-inline">StringBuilder</strong> class to avoid unnecessary object creation that could trigger garbage collection <span class="No-Break">in Kotlin/JVM.</span></p>
			<h3>The use of coroutines for asynchronous operations</h3>
			<p>Kotlin’s coroutine <a id="_idIndexMarker1132"/>library enables the program to invoke asynchronous operations so that the thread isn’t blocked and can perform other operations while waiting for the asynchronous result to come back. It enables better resource management and quicker response <span class="No-Break">in applications.</span></p>
			<p>For example, imagine that there are two <span class="No-Break">time-consuming functions:</span></p>
			<pre class="source-code">
suspend fun task1(): Int {
    delay(1000)
    println("Task 1 completed")
    return 42
}
suspend fun task2(): Int {
    delay(1500) // Simulate a 1.5-second delay
    println("Task 2 completed")
    return 58
}</pre>			<p>These two functions have the <strong class="source-inline">suspend</strong> modifier to indicate they can be paused and resumed without blocking the thread. The <strong class="source-inline">main</strong> function that uses these two suspend functions is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
fun main() =
    runBlocking {
        val result1 = async { task1() }
        val result2 = async { task2() }
        val combinedResult = result1.await() + result2.await()
        println("Combined Result: $combinedResult")
    }</pre>			<p>The <strong class="source-inline">runBlocking</strong> function starts a coroutine that blocks the current thread until its execution is complete. Within this block, there are two <strong class="source-inline">async</strong> functions to invoke the two time-consuming <strong class="source-inline">suspend</strong> functions. The <strong class="source-inline">async</strong> function returns a <strong class="source-inline">Deferred</strong> object on which we invoke the <strong class="source-inline">await</strong> function to block until the result returns. The two numbers that are returned by the respective time-consuming functions are added and the sum is printed to <span class="No-Break">the console.</span></p>
			<p>Note that Kotlin code written with the best practices of performance in mind would still need to be measured. Empirical results are the <span class="No-Break">only proof.</span></p>
			<p>Next, we’re going <a id="_idIndexMarker1133"/>to briefly mention ultra-low latency systems and how they push performance and scalability to <span class="No-Break">the extreme.</span></p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor416"/>Ultra-low latency systems</h1>
			<p>Ultra-low latency <a id="_idIndexMarker1134"/>systems operate in the microsecond or nanosecond magnitude. They run in an environment where low response time is critical and even essential. They can be seen in financial trading, telecommunications, gaming, and <span class="No-Break">industrial automation.</span></p>
			<p>These systems aim to achieve the lowest possible latency, highest efficiency, high predictability, and high concurrency regarding processing. They involve all aspects of a system to reduce response time, such as network optimization, hardware acceleration, load balancing, and <span class="No-Break">efficient algorithms.</span></p>
			<p>These systems are usually written in system-level programming languages such as C++ and Rust. However, there are a few ultra-low latency systems written in Kotlin or Java that operate in the <span class="No-Break">microsecond magnitude.</span></p>
			<p>The low latency systems in Kotlin or Java employ several technical designs that aren’t <span class="No-Break">as common:</span></p>
			<ul>
				<li>Reuse objects, avoid object creation, and avoid <span class="No-Break">garbage collection.</span></li>
				<li>Use specific JVM vendors for <span class="No-Break">better performance.</span></li>
				<li>Avoid the use of third-party libraries to reduce overhead and ensure you have full control <span class="No-Break">over performance.</span></li>
				<li>Use the <strong class="bold">Disruptor pattern</strong> as it <a id="_idIndexMarker1135"/>provides a large ring buffer for inter-thread lock-free communication and memory barriers for data visibility in <span class="No-Break">a thread.</span></li>
				<li>Use a single-thread model for each JVM process to reduce context switching, lock contention, and the need for synchronous and <span class="No-Break">concurrency processing.</span></li>
				<li>Write code or design systems that are aware of and optimized for the underlying hardware and network infrastructure they run on. This <a id="_idIndexMarker1136"/>is also called <span class="No-Break"><strong class="bold">mechanical sympathy</strong></span><span class="No-Break">.</span></li>
			</ul>
			<p>Ultra-low latency systems have the justification that they can break a few design principles (for example, immutable objects) in exchange for higher performance. They are exceptional cases due to the demanding need for low latency, high throughput, and quick response time. When developing these systems, performance tests are critical and should be part of the normal <span class="No-Break">development activities.</span></p>
			<p>Developing ultra-low latency systems is a specialized topic whose content is beyond the scope of this chapter. However, there <a id="_idIndexMarker1137"/>are a few pieces of reading material that you may <span class="No-Break">find useful:</span></p>
			<ul>
				<li><em class="italic">Mechanical empathy</em>, by Martin <span class="No-Break">Thompson (</span><a href="https://mechanical-sympathy.blogspot.com/"><span class="No-Break">https://mechanical-sympathy.blogspot.com/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">LMAX </em><span class="No-Break"><em class="italic">Disruptor</em></span><span class="No-Break"> (</span><a href="https://lmax-exchange.github.io/disruptor/"><span class="No-Break">https://lmax-exchange.github.io/disruptor/</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Simple Binary </em><span class="No-Break"><em class="italic">Encoding</em></span><span class="No-Break"> (</span><a href="https://github.com/real-logic/simple-binary-encoding"><span class="No-Break">https://github.com/real-logic/simple-binary-encoding</span></a><span class="No-Break">)</span></li>
				<li><em class="italic">Aeron </em><span class="No-Break"><em class="italic">messaging</em></span><span class="No-Break"> (</span><a href="https://github.com/real-logic/aeron"><span class="No-Break">https://github.com/real-logic/aeron</span></a><span class="No-Break">)</span></li>
			</ul>
			<h1 id="_idParaDest-317"><a id="_idTextAnchor417"/>Summary</h1>
			<p>In this chapter, we covered different dimensions of performance and scalability and mentioned a few essential metrics that measure how well a system performs and scales. We emphasized the importance of performance tests, several types of performance tests, and how to plan one. We also provided an example of micro-benchmarking in Kotlin before discussing the use of a profiler to achieve <span class="No-Break">better performance.</span></p>
			<p>Then, we delved into some strategies for performance improvement. We considered a scenario where only necessary expensive operations were executed. We also looked at an example of a journey of performance improvement for a system in a real-life situation. This allowed us to consider a few best practices regarding performance in Kotlin through <span class="No-Break">code examples.</span></p>
			<p>Finally, we briefly introduced ultra-low latency systems and where they can <span class="No-Break">be used.</span></p>
			<p>In the next chapter, we’re going to discuss the topic of <span class="No-Break">software testing.</span></p>
		</div>
	</div></div></body></html>