<html><head></head><body><div><div><div><h1 id="_idParaDest-285" class="chapter-number"><a id="_idTextAnchor379"/>12</h1>
			<h1 id="_idParaDest-286"><a id="_idTextAnchor380"/>Performance and Scalability</h1>
			<p>Software systems grow with business and changing environments manifested in higher complexity, more diverse user demands, and heavier workloads. The ability to maintain high performance and scale under growth becomes critical. Performance refers to how quickly a system can process and respond to requests, while scalability describes a system’s capacity to handle a higher volume of traffic and usage over time.</p>
			<p>Poor performance can lead to frustrating user experiences, lost productivity, and even complete system failures. And systems that can’t scale to meet rising demands will quickly become overwhelmed and unusable. Therefore, ensuring optimal performance and scalability is a key challenge for any software engineering project.</p>
			<p>In this chapter, we’ll explore the core concepts and principles of performance engineering and scalable system design. We’ll discuss common performance bottlenecks and mitigation strategies, review techniques for load testing and benchmarking, and cover architectural patterns and design choices that enable horizontal and vertical scalability.</p>
			<p>By the end of this chapter, you’ll have a solid understanding of how to build highly performant, scalable systems that can withstand the pressure of real-world demands.</p>
			<p>We’re going to cover the following topics in this chapter:</p>
			<ul>
				<li>Dimensions of performance and scalability</li>
				<li>Optimize performance now or later?</li>
				<li>Performance test planning</li>
				<li>Executing a performance test</li>
				<li>Micro-benchmarking</li>
				<li>Strategies for performance improvement</li>
				<li>Ultra-low latency systems</li>
			</ul>
			<h1 id="_idParaDest-287"><a id="_idTextAnchor381"/>Technical requirements</h1>
			<p>You can find all the code files used in this chapter on GitHub: <a href="https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-12">https://github.com/PacktPublishing/Software-Architecture-with-Kotlin/tree/main/chapter-12</a></p>
			<h1 id="_idParaDest-288"><a id="_idTextAnchor382"/>Dimensions of performance and scalability</h1>
			<p>Performance is<a id="_idIndexMarker1036"/> the efficiency of a system in executing tasks and responding to requests. It’s measured by various metrics:</p>
			<ul>
				<li><strong class="bold">Latency</strong>: The time taken for the system to respond to a request.</li>
				<li><strong class="bold">Throughput</strong>: The number of requests processed in a given time frame.</li>
				<li><strong class="bold">Resource utilization</strong>: The percentage of resources (for example CPU, memory, network, files, and so on) used during operations.</li>
				<li><strong class="bold">Concurrent users</strong>: The number of users effectively served by the system simultaneously without degradation in performance.</li>
				<li><strong class="bold">Page load time</strong>: The total time taken for a screen to fully load, including all assets (images, videos, scripts, and so on)</li>
				<li><strong class="bold">Queue size</strong>: The number of requests waiting to be processed by the server</li>
				<li><strong class="bold">Time to first byte</strong> (<strong class="bold">TTFB</strong>): The time<a id="_idIndexMarker1037"/> that elapsed from when the client initiated a request to the client receiving the first byte from the server.</li>
				<li><strong class="bold">Cache hit ratio</strong>: The percentage of requests served from the cache versus those from the slower secondary data source. A higher ratio indicates more efficient caching.</li>
				<li><strong class="bold">Error rate</strong>: The percentage of requests resulting in errors (for example, HTTP error statuses). High error rates indicate problems with the application or infrastructure.</li>
			</ul>
			<p>Scalability is the capacity of a system to handle increased load without degrading performance. It indicates how well a system can grow and adapt to an increase in user traffic and data volume. Scalability can be<a id="_idIndexMarker1038"/> categorized into two types:</p>
			<ul>
				<li><strong class="bold">Vertical scaling</strong>: Adding <a id="_idIndexMarker1039"/>more resources (for example, CPU or RAM) to a single node to increase its capacity. This is also known<a id="_idIndexMarker1040"/> as <strong class="bold">scale up</strong>.</li>
				<li><strong class="bold">Horizontal scaling</strong>: Adding <a id="_idIndexMarker1041"/>more nodes to distribute the load and improve capacity in a distributed system. This<a id="_idIndexMarker1042"/> is also known<a id="_idIndexMarker1043"/> as <strong class="bold">scale out</strong>.</li>
			</ul>
			<p>Scalability is also the capacity of a system to downsize when the load is reduced. Scaling down is usually concerned with the flexible use of resources and cost savings. It’s still an important aspect of scalability, but the focus is usually on scaling up and scaling out.</p>
			<p>Scalability can be measured by the following metrics:</p>
			<ul>
				<li><strong class="bold">Scalability ratio</strong>: The ratio of<a id="_idIndexMarker1044"/> the increase in performance to the increase in resources such as the number of servers</li>
				<li><strong class="bold">Time to scale</strong>: The time taken between adding resources to extra resources becoming operational in the system</li>
			</ul>
			<p>These metrics are useful in measuring how a change in the system may affect performance <a id="_idIndexMarker1045"/>and scalability. Without them, it’s difficult to decide whether performance should be optimized or not. We’re going to discuss this decision in depth in the next section.</p>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor383"/>Optimize performance now or later?</h1>
			<p>Engineers and architects <a id="_idIndexMarker1046"/>often face the question of whether performance should be optimized now or later. This happens from the early stages of system design to already established production systems.</p>
			<p>We all know optimizing performance is crucial, but whether to prioritize it from day one isn’t a simple binary question.</p>
			<p>You may have heard of someone who said “<em class="italic">Premature optimization is the root of evil.</em>” The statement itself is theatrical, but there are some merits in it.</p>
			<p>You may also have heard a quote saying, “<em class="italic">Make it work, make it right, make it fast.</em>” This was coined by software engineer <em class="italic">Kent Beck</em>.</p>
			<p>So, what would be the consequences if a system is optimized prematurely, or if we reverse the order to “make it fast” too early?</p>
			<p>Spending too much time on performance improvements before understanding user behaviors and requirements can lead to wasted effort. Moreover, it creates an unnecessarily complex architecture that hinders the team’s productivity. The team may have to simplify the over-engineered system, which also requires effort. In that sense, the team is punished<a id="_idIndexMarker1047"/> twice for improving performance too early.</p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor384"/>Considerations for optimizing performance and scalability</h2>
			<p>There are several factors <a id="_idIndexMarker1048"/>to consider regarding whether the <a id="_idIndexMarker1049"/>system should be optimized for performance and scalability:</p>
			<ul>
				<li><strong class="bold">Core features completeness</strong>: If the core features of a system are still being developed, then it’s often more important to focus on delivering core features and functionality initially. This is the first step: “<em class="italic">Make </em><em class="italic">it work.</em>”<p class="list-inset">In addition, we must ensure the system behaves as expected as per functional requirements. Correctness should always come before performance. This is the second step: “<em class="italic">Make </em><em class="italic">it right.</em>”</p></li>
				<li><strong class="bold">Performance metrics</strong>: Before optimizing performance or improving scalability, it’s paramount to have current performance metrics as a baseline. A performance baseline provides insights into the current system bottlenecks that help the team prioritize which area should be improved first.<p class="list-inset">A performance benchmark enables empirical and objective comparison of whether a change has resulted in better or worse performance, or whether an attempt to improve performance has achieved its goals.</p></li>
				<li><strong class="bold">Non-functional requirements</strong>: Non-functional requirements are a useful source of guidance on whether the system needs to be optimized now. Non-functional requirements for performance can be driven by regulatory constraints, external system integration conformance, or principles of user experiences.</li>
				<li><strong class="bold">Critical use cases, user experiences, and competitors</strong>: If the application is expected to handle high traffic from the beginning (for example, product launch events, training, or marketing campaigns), then early optimization is essential. If the application’s performance directly impacts user satisfaction, it’s important to address performance concerns early to avoid negative feedback. The current performance metrics of competitors also indicate how much the application’s performance should be optimized.</li>
				<li><strong class="bold">Scalability needs</strong>: If rapid growth or scaling needs are anticipated for an application, implementing <a id="_idIndexMarker1050"/>good performance practices<a id="_idIndexMarker1051"/> from the beginning will save time and effort later.</li>
			</ul>
			<h2 id="_idParaDest-291"><a id="_idTextAnchor385"/>Best practices for performance</h2>
			<p>Even if it may not be the<a id="_idIndexMarker1052"/> right time to optimize performance, there are some best practices to at least not make performance worse:</p>
			<ul>
				<li><strong class="bold">Measure first</strong>: Measure the current performance metrics, ideally all operations, but as a bottom line, measure the core features and most frequent operations.</li>
				<li><strong class="bold">Implement basic optimization</strong>: Basic performance best practices such as efficient database queries in the early stage of development.</li>
				<li><strong class="bold">Plan for scalability</strong>: Plan and have scalability in mind when designing system architecture to allow for easier optimization later without major refactoring. Sometimes, it’s about not putting restrictions that would limit scalability.</li>
			</ul>
			<p>While it isn’t always necessary to optimize performance on day one, incorporating basic performance considerations into your development process can lead to better long-term results. Focus on delivering value first, then iterate on performance as the application evolves. Let’s go through an example of performance measurement to understand this better.</p>
			<h1 id="_idParaDest-292"><a id="_idTextAnchor386"/>An example of basic performance measurement</h1>
			<p>Here’s a basic<a id="_idIndexMarker1053"/> example of an operation to be measured:</p>
			<pre class="source-code">
fun sampleOperation() {
    Thread.sleep(1)
}</pre>			<p>The goal of this example is to find out the following aspects:</p>
			<ul>
				<li><strong class="bold">Throughput</strong>: How many operations can be performed in a second</li>
				<li><strong class="bold">Latency</strong>: How long it takes to finish an operation on average</li>
			</ul>
			<p>A small function, <code>measureTotalTimeElapsed</code>, must be defined to measure the total time elapsed for all iterations of the operation:</p>
			<pre class="source-code">
fun measureTotalTimeElapsed(
    iterations: Int,
    operation: (Int) -&gt; Unit,
): Long =
    measureTimeMillis {
        repeat(iterations, operation)
    }</pre>			<p>This function uses the <code>measureTimeMillis</code> Kotlin function from Standard Library to capture the time spent in repeating the operation.</p>
			<p>Finally, this is the <code>main</code> function to launch the test:</p>
			<pre class="source-code">
fun main() {
    val iterations = 1_000
    val operationTime = measureTotalTimeElapsed(iterations) { sampleOperation() }
    println("Total time elapsed: ${operationTime / 1000.0} second")
    println("Throughput: ${iterations / (operationTime / 1000.0)} operations per second")
    println("Latency (average): ${operationTime / iterations} ms")
}</pre>			<p>This function defines the operation to be executed 1,000 times. After invoking the <code>measure</code><strong class="source-inline">
TotalTimeElapsed</strong> function with the Lambda expression that runs the <code>sampleOperation</code> function, the total time elapsed in milliseconds is returned. Then, the throughput is calculated as the number of iterations divided by the total elapsed time in seconds. The average latency is calculated as the inverse reciprocal of throughput – the total time elapsed divided by the number of iterations.</p>
			<p>This is a sample output from running the test:</p>
			<pre class="console">
Total time elapsed: 1.264 second
Throughput: 791.1392405063291 operations per second
Latency (average): 1 ms</pre>			<p>Since the sample function, <code>sampleOperation</code>, only makes the thread sleep for 1 millisecond, the average latency is 1 millisecond, as expected. The throughput in this run is close to 800, but it varies in every run.</p>
			<p>Kotlin Standard Library provides a few functions for time measurement:</p>
			<ul>
				<li>Return time elapsed in milliseconds (used in this example): <code>measureTimeMillis</code></li>
				<li>Return time elapsed in nanoseconds: <code>measureNanoTime</code></li>
				<li>Return time elapsed as <code>Duration</code>: <code>measureTime</code></li>
				<li>Return time elapsed and the value returned from the Lambda expression: <code>measureTimedValue</code></li>
			</ul>
			<p>For real-life performance <a id="_idIndexMarker1054"/>critical systems, this is certainly not enough. Due to this, in the next section, we’ll cover the main types of performance tests.</p>
			<h1 id="_idParaDest-293"><a id="_idTextAnchor387"/>Performance tests</h1>
			<p>Performance tests are <a id="_idIndexMarker1055"/>a category of test that evaluates the speed, responsiveness, and stability of a system under a given workload. In this section, we’ll look at the main types of performance tests.</p>
			<h2 id="_idParaDest-294"><a id="_idTextAnchor388"/>Load testing</h2>
			<p>Load tests aim to assess<a id="_idIndexMarker1056"/> the behaviors of a system under<a id="_idIndexMarker1057"/> expected load conditions, such as a configured number of concurrent requests. The goal is to identify bottlenecks in application or infrastructure where performance may degrade under load. It ensures the system can handle anticipated traffic without performance degradation.</p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor389"/>Stress testing</h2>
			<p>Stress tests <a id="_idIndexMarker1058"/>aim to <a id="_idIndexMarker1059"/>evaluate the system’s performance under extreme load conditions beyond its normal operational capacity. They also help us determine the breaking point of the system and how it fails under stress, so proactive monitoring and alerts can be deployed for precautions.</p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor390"/>Endurance testing (soak testing)</h2>
			<p>Endurance tests, known <a id="_idIndexMarker1060"/>as soak tests, focus <a id="_idIndexMarker1061"/>on the stability and performance of a system over an extended period. This extended period is used to identify issues that accumulate or emerge over time, such as memory leaks, resource exhaustion, or performance degradation.</p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor391"/>Spike testing</h2>
			<p>Spike tests introduce <a id="_idIndexMarker1062"/>a <a id="_idIndexMarker1063"/>sudden increase in load (the “spike”) so that we can observe how the system reacts in this situation. The result illustrates how the system can handle abrupt changes in traffic without failure.</p>
			<h2 id="_idParaDest-298"><a id="_idTextAnchor392"/>Volume and latency testing</h2>
			<p>Volume tests <a id="_idIndexMarker1064"/>evaluate the system’s performance<a id="_idIndexMarker1065"/> with a large volume of data. Latency tests measure the time delay between a request and the corresponding response. They usually measure metrics such as throughput and latency <a id="_idIndexMarker1066"/>to ensure<a id="_idIndexMarker1067"/> the application can<a id="_idIndexMarker1068"/> meet <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>) or <strong class="bold">service-level </strong><strong class="bold">objectives</strong> (<strong class="bold">SLOs</strong>).</p>
			<h2 id="_idParaDest-299"><a id="_idTextAnchor393"/>Scalability testing</h2>
			<p>Scalability<a id="_idIndexMarker1069"/> tests<a id="_idIndexMarker1070"/> aim to determine how well the system can scale up or down in response to increasing or decreasing loads. It measures the performance of the system as resources are added or removed.</p>
			<h2 id="_idParaDest-300"><a id="_idTextAnchor394"/>Configuration testing</h2>
			<p>Configuration tests<a id="_idIndexMarker1071"/> aim<a id="_idIndexMarker1072"/> to identify the optimal configuration for performance. They involve running performance tests under different configurations, including hardware, software, and the network.</p>
			<h1 id="_idParaDest-301"><a id="_idTextAnchor395"/>Planning a performance test</h1>
			<p>Although there are <a id="_idIndexMarker1073"/>different types of performance tests, planning and executing performance tests are similar. The difference is in the details of each step. In this section, we’ll explore the journey of planning and executing a performance test.</p>
			<h2 id="_idParaDest-302"><a id="_idTextAnchor396"/>Planning</h2>
			<p>In the planning phase, first, the objectives of the test should be defined. This means we must define the information we want to get out of the tests – for example, can a household record be created within 50 milliseconds? Can the system handle 5,000 concurrent requests without degradation? These objectives are the primary drives to plan and execute a performance test. They also determine which type of performance tests can be used.</p>
			<p>Then, business scenarios for performance tests should be defined. Usually, the objectives would have given a great hint at which scenarios would be used, but it’s worth exploring the details of the steps involved in each scenario and formalizing them as a blueprint of the test script.</p>
			<p>The last part of planning is to specify the load levels to run, including the number of users and the duration of the test. Sometimes, it isn’t so clear which level to run, especially if we want to find the breaking point of the system. This is OK initially since performance tests are <a id="_idIndexMarker1074"/>meant to run iteratively.</p>
			<h2 id="_idParaDest-303"><a id="_idTextAnchor397"/>Preparation and development</h2>
			<p>Once there’s an initial<a id="_idIndexMarker1075"/> plan, the performance test can be prepared and developed. These activities can happen in parallel.</p>
			<p>The test script is the core of the test execution. The test needs to be automated to achieve consistent results. This involves a big decision on which tool to use. Here’s a list of commonly used tools:</p>
			<ul>
				<li><strong class="bold">Apache JMeter</strong> (<a href="https://jmeter.apache.org/">https://jmeter.apache.org/</a>): Open source, free, GUI support, distributed<a id="_idIndexMarker1076"/> testing, plugin support, and Java-based</li>
				<li><strong class="bold">LoadRunner by OpenText</strong> (<a href="https://www.opentext.com/">https://www.opentext.com/</a>): Commercial licenses, GUI, integration <a id="_idIndexMarker1077"/>with CI/CD tools, analytics and reporting support, and support for Java</li>
				<li><strong class="bold">Gatling</strong> (<a href="https://docs.gatling.io/">https://docs.gatling.io/</a>): Open <a id="_idIndexMarker1078"/>source, commercial licenses with additional features, and scripts can be written in Kotlin</li>
				<li><strong class="bold">K6</strong> (<a href="https://k6.io/">https://k6.io/</a>): Open <a id="_idIndexMarker1079"/>source, subscription-based for cloud features, and can integrate with CI/CD scripts <a id="_idIndexMarker1080"/>written in <strong class="bold">JavaScript</strong> (<strong class="bold">JS</strong>)</li>
				<li><strong class="bold">Locust</strong> (<a href="https://locust.io/">https://locust.io/</a>): Open<a id="_idIndexMarker1081"/> source, GUI support, distributed testing, and scripts written in Python</li>
				<li><strong class="bold">BlazeMeter</strong> (<a href="https://www.blazemeter.com/">https://www.blazemeter.com/</a>): Free<a id="_idIndexMarker1082"/> with limited features, commercial licenses, cloud-based, GUI support, real-time reporting and analytics, integrated with CI/CD, and supports JMeter scripts</li>
			</ul>
			<p>These tools provide comprehensive features such as organizing test scripts, managing multiple test configurations, metrics measurement, analytics, and reporting. You also have the option to build your own drivers of performance tests. This is applicable if your tests are simple and there are sufficient metric measurements without external tools.</p>
			<p>Appropriate metric measurement needs to be set up according to what the test script requires. The metrics can be measured by the testing tools, or by the monitoring tools already embedded in the system, as discussed previously in <a href="B21737_11.xhtml#_idTextAnchor358"><em class="italic">Chapter 11</em></a>. Any missing metrics need to be set up before executing the tests.</p>
			<p>Meanwhile, a test environment needs to be set up for execution. Ideally, the environment should be comparable to the actual production environment where the system runs. If that’s too expensive, an environment of a smaller scale can be used to project expected performance, with a degree of inaccuracy in mind.</p>
			<p>The test environment should be an isolated sandbox that does nothing but the performance tests. It can be a challenge for some organizations to replicate a production-like environment for performance testing. Replicating an environment with data alone may already be a challenge for some organizations. In addition, the environment needs to have the necessary data to run the test scenarios.</p>
			<p>Sometimes, the<a id="_idIndexMarker1083"/> system has integration with third-party systems. In this case, the external integration would need to be stubbed out with simulators.</p>
			<h2 id="_idParaDest-304"><a id="_idTextAnchor398"/>Execution and iteration</h2>
			<p>Once we have the test <a id="_idIndexMarker1084"/>scripts, test environments, and corresponding metrics set up, we’re ready to execute the performance test. It’s vital to allow an iterative feedback loop where tests can run multiple times, and there could be changes between each test. Within each iteration, the same operation should be executed numerous times so that we have enough data points to perform analysis.</p>
			<p>The tests should be run twice at a minimum, where the initial run identifies a bottleneck, then a change is made with the intent to eliminate the bottleneck, and finally, another run proves the bottleneck no longer exists, as indicated by metrics.</p>
			<p>Realistically, another bottleneck will emerge after the biggest one is eliminated. The performance landscape will change for every change that’s made to improve performance. The iteration can end when the objectives are completed, or a new problem may be discovered during the process.</p>
			<p>The iterative <a id="_idIndexMarker1085"/>execution of a performance test can be seen in <em class="italic">Figure 12</em><em class="italic">.1</em>:</p>
			<div><div><img src="img/B21737_12_1.jpg" alt="Figure 12.1 – A sample workflow of performance testing" width="1323" height="1481"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1 – A sample workflow of performance testing</p>
			<p>In each run, the test is<a id="_idIndexMarker1086"/> executed by running the test script. The test script usually starts with warm-up operations. For example, if we’re going to send 10,000 requests 100 times, the first 10 times can be treated as a warm-up, so the metrics aren’t considered.</p>
			<p>Warming up allows the system to reach a stable state before actual performance measurements are taken. Processing initial requests triggers the cache so that it populates with frequently accessed data. It also allows the system to allocate resources such as thread, memory, and database connections effectively. Other transient factors such as just-in-time compilation, garbage collection, and resource contention can be reduced by warming up the system.</p>
			<p>After running the test, a report should be generated from the metric data that’s been collected. The report should be constructed in a format that allows iterations to be compared. The raw data is then analyzed to produce statistical figures such as the following:</p>
			<ul>
				<li>The mean and median percentiles of the response time</li>
				<li>Average and peak throughputs; increase in throughput divided by the increase in resource which indicates scaling efficiency</li>
				<li>The overall error rate and error rates by types</li>
				<li>Average and maximum latency</li>
				<li>The number of concurrent users being handled without degradation</li>
				<li>The time for which the load was maintained</li>
			</ul>
			<p>From these numbers, some bottlenecks may be identified. Some figures can be below the non-functional requirements, SLAs, or SLOs. Some figures can stand out as particularly slow compared to others. These bottlenecks drive the changes required to improve overall performance.</p>
			<p>Especially in early <a id="_idIndexMarker1087"/>iterations, deficiencies in the test scripts may be identified. It isn’t uncommon to realize the test script itself isn’t efficient and causes slowness in the system. The test script may have unnecessary loops or complex logic that slows down the script’s execution time. The test script may have an artificial wait time between requests, which as a result limits the throughput. Other factors, such as error handling, synchronous operations, resource contention, and network performance, can also skew the result of performance tests. These findings lead to the test script being reviewed and updated for future runs.</p>
			<p>After changes are made, the performance tests should be executed again to examine whether the target performance statistical figures have improved, and at the same, to ensure the changes don’t deteriorate system performance in other areas.</p>
			<p>This repetitive exercise carries on until we’re satisfied with the results. There are several possibilities where the iteration should stop:</p>
			<ul>
				<li>The test objectives have been completed – for example, we’ve detected the maximum number of requests the system can handle without performance degradation</li>
				<li>The performance metrics have fulfilled the non-functional requirements, SLAs, or SLOs</li>
				<li>The time spent on performance testing has exceeded the original time-boxed duration</li>
			</ul>
			<p>Performance testing is meant to be a recurring exercise. A successful and satisfactory performance test only supports the assumption that the system is capable of handling requests within the configuration and parameters in the test scripts. The system usage pattern is constantly <a id="_idIndexMarker1088"/>changing due to business growth and new features being introduced over time.</p>
			<h2 id="_idParaDest-305"><a id="_idTextAnchor399"/>Benefits of performance testing</h2>
			<p>Performance testing<a id="_idIndexMarker1089"/> provides insights into how the system performs under pre-configured loads. It indicates how we can optimize the system to provide a fast and reliable user experience, even under heavy load. It helps stakeholders understand system limits and make informed decisions about scaling and infrastructure. It also identifies potential issues before they impact users, reducing the risk of downtime or service degradation.</p>
			<p>Performance tests are essential for ensuring that applications meet user expectations and maintain stability under varying conditions. By conducting different types of performance tests, organizations can identify and address potential issues, optimize performance, and enhance overall user satisfaction.</p>
			<p>Next, we’ll consider a technique that’s used iteratively to measure a function’s performance. This technique is called micro-benchmarking.</p>
			<h1 id="_idParaDest-306"><a id="_idTextAnchor400"/>Micro-benchmarking</h1>
			<p>While performance<a id="_idIndexMarker1090"/> testing focuses on system-level performance, micro-benchmarking is a performance measurement of a small and isolated piece of code at the function level. Micro-benchmarking is usually applicable to the following areas:</p>
			<ul>
				<li>The algorithm that sits in the core of the whole system – for example, a search algorithm for an internet search engine</li>
				<li>The function that’s used most frequently by end users</li>
				<li>The function that’s exposed as an API to external systems</li>
				<li>The code path that’s mission-critical and performance-sensitive</li>
				<li>When comparing the implementations of a function, an algorithm, or a code change</li>
			</ul>
			<p><strong class="bold">Kotlin benchmarking</strong> (<a href="https://github.com/Kotlin/kotlinx-benchmark">https://github.com/Kotlin/kotlinx-benchmark</a>) is <a id="_idIndexMarker1091"/>the most popular tool <a id="_idIndexMarker1092"/>for running benchmarks for Kotlin code. It wraps the classic <strong class="bold">Java Microbenchmark Harness</strong> (<strong class="bold">JMH</strong>) framework, and<a id="_idIndexMarker1093"/> it supports <a id="_idIndexMarker1094"/>Kotlin<a id="_idIndexMarker1095"/> through <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>), JS, Native, and even <strong class="bold">Web </strong><strong class="bold">Assembly</strong> (<strong class="bold">WASM</strong>).</p>
			<h2 id="_idParaDest-307"><a id="_idTextAnchor401"/>Setting up micro-benchmarking with a Gradle Kotlin DSL script</h2>
			<p>It’s simple to <a id="_idIndexMarker1096"/>set up benchmarking <a id="_idIndexMarker1097"/>with a Gradle Kotlin DSL script. For example, for JVM, we need the following plugins:</p>
			<pre class="source-code">
plugins {
    id("org.jetbrains.kotlinx.benchmark") version "0.4.11"
    kotlin("plugin.allopen") version "2.0.20"
}</pre>			<p>The first plugin is for Kotlin micro-benchmarking, while the second plugin is used to open the final Kotlin classes for instrumentation. Now, we need to make sure the plugins and dependencies can be looked up from repositories:</p>
			<pre class="source-code">
repositories {
    mavenCentral()
    gradlePluginPortal()
}</pre>			<p>Next, a code dependency on Kotlin micro-benchmarking needs to be declared:</p>
			<pre class="source-code">
    implementation("org.jetbrains.kotlinx:kotlinx-benchmark-runtime:0.4.11")</pre>			<p>Then, we need to configure the <code>allOpen</code> plugin so that it only opens Kotlin classes with the <code>State</code> annotation:</p>
			<pre class="source-code">
allOpen {
    annotation("org.openjdk.jmh.annotations.State")
}</pre>			<p>The final part of the setup is setting up micro-benchmarking itself:</p>
			<pre class="source-code">
benchmark {
    targets {
        register("main")
    }
    configurations {
        named("main") {
        }
    }
}</pre>			<p>The configuration is called <code>main</code> and has been chosen to run. It’s possible to configure the number of warmup iterations, the number of iterations to be measured, and the length of how<a id="_idIndexMarker1098"/> long <a id="_idIndexMarker1099"/>each iteration should last. However, an annotation-based configuration has been used in this example.</p>
			<h2 id="_idParaDest-308"><a id="_idTextAnchor402"/>The micro-benchmarking test</h2>
			<p>The actual benchmark <a id="_idIndexMarker1100"/>runner code is annotated so that it can be picked by the runner for execution with a specific configuration. Please note that this test should be placed in the <code>main</code> source folder (not the <code>test</code> source folder) so that it can be picked up by the plugin:</p>
			<pre class="source-code">
@State(Scope.Benchmark)
@Fork(1)
@Warmup(iterations = 10)
@Measurement(iterations = 20, time = 1, timeUnit = TimeUnit.MILLISECONDS)
class MicrobenchmarkingTest {
    private var data = emptyList&lt;UUID&gt;()
    @Setup
    fun setUp() {
        data = (1..2).map { UUID.randomUUID() }
    }
    @Benchmark
    fun combineUUIDBenchmark(): UUID = data.reduce { one, two -&gt; one + two }
    private operator fun UUID.plus(another: UUID): UUID {
        val mostSignificant = mostSignificantBits xor another.mostSignificantBits
        val leastSignficant = leastSignificantBits xor another.leastSignificantBits
        return UUID(mostSignificant, leastSignficant)
    }
}</pre>			<p>This micro-benchmarking<a id="_idIndexMarker1101"/> test evaluates the performance of the function that combines<a id="_idIndexMarker1102"/> two <code>State</code> annotation triggers the <code>allOpen</code> plugin to open this class for instrumentation. Then, the <code>Fork</code> annotation defines how many threads are used for execution. Other annotations specify the number of iterations for warmup, execution, and the duration of each iteration.</p>
			<p>For example, the <code>setup</code> annotation function is used to create the data required to run the test, while the <code>combineUUIDBenchmark</code> function, which has the <code>Benchmark</code> annotation, is the<a id="_idIndexMarker1103"/> major function to be measured.</p>
			<h2 id="_idParaDest-309"><a id="_idTextAnchor403"/>Micro-benchmarking runner</h2>
			<p>To run micro-benchmarking, we <a id="_idIndexMarker1104"/>can use the following Gradle command:</p>
			<pre class="console">
./gradlew benchmark</pre>			<p>The summary of the result is printed to the console, while the detailed r<a id="_idTextAnchor404"/>eport is generated unde<a id="_idTextAnchor405"/>r the <code>/</code><code>build/reports/benchmarks/main</code> folder:</p>
			<pre class="console">
Success: 109349297.194 ±(99.9%) 15493649.408 ops/s [Average]
  (min, avg, max) = (55205844.260, 109349297.194, 132224154.121), stdev = 17842509.699
  CI (99.9%): [93855647.787, 124842946.602] (assumes normal distribution)</pre>			<p>The format of micro-benchmarking is designed to compare runs. Improvements can be made between runs, and the next run should demonstrate whether the changes have made a difference.</p>
			<p>Micro-benchmarking is a valuable subset of performance testing that focuses on code implementation. By understanding the performance characteristics of isolated functions, engineers can make targeted optimizations. In contrast, performance testing takes a holistic approach to assess how well the entire system performs under various conditions. Both practices are essential for delivering high-performance systems.</p>
			<p>There’s another tool that measures and analyzes the performance of an application, but visually with graphical user interfaces. This tool is called the application profiler, and we’re going to cover it in the next section.</p>
			<h1 id="_idParaDest-310"><a id="_idTextAnchor406"/>Application profiling</h1>
			<p>Profiling works by<a id="_idIndexMarker1105"/> monitoring and analyzing the performance of an application at runtime. Profilers instrument code and intercept calls to collect performance measurements, such as elapsed time and the number of invocations. It can generate the stack trace of the application to visualize relationships between functions.</p>
			<p>The profiler tool also monitors memory allocation and deallocation, analyzes the heap dump, and identifies potential memory leaks.</p>
			<p>At the same time, the profiler tool measures CPU cycles that have been consumed by various parts of the code and identifies computing-intensive functions. The profiler tool also monitors the usage of other resources, such as file operations, network activities, and interactions, among threads to provide a comprehensive view of resource utilization.</p>
			<p>The profiler tool comes with detailed reports that are visualized in the user interface to assist engineers in pinpointing the areas that require optimization.</p>
			<p>However, running an application with the profiler significantly slows down performance due to invasive instrumentation and measurement. The metric data that’s captured should be treated as a magnification of the actual runtime and be used to find areas that are slow, inefficient, or resource-consuming.</p>
			<p>There <a id="_idIndexMarker1106"/>are <a id="_idIndexMarker1107"/>several <a id="_idIndexMarker1108"/>popular<a id="_idIndexMarker1109"/> profiler <a id="_idIndexMarker1110"/>tools <a id="_idIndexMarker1111"/>available for Kotlin engineers:</p>
			<ul>
				<li><strong class="bold">YourKit Java </strong><strong class="bold">Profiler</strong> (<a href="https://www.yourkit.com/java/profiler/">https://www.yourkit.com/java/profiler/</a>)</li>
				<li><strong class="bold">VisualVM</strong> (<a href="https://visualvm.github.io/startupprofiler.html">https://visualvm.github.io/startupprofiler.html</a>)</li>
				<li><strong class="bold">IntelliJ IDEA </strong><strong class="bold">Profiler</strong> (<a href="https://www.jetbrains.com/pages/intellij-idea-profiler/">https://www.jetbrains.com/pages/intellij-idea-profiler/</a>)</li>
				<li><strong class="bold">JProfiler</strong> (<a href="https://www.ej-technologies.com/jprofiler">https://www.ej-technologies.com/jprofiler</a>)</li>
				<li><strong class="bold">Async </strong><strong class="bold">Profiler</strong> (<a href="https://github.com/async-profiler/async-profiler">https://github.com/async-profiler/async-profiler</a>)</li>
				<li><strong class="bold">Java Mission </strong><strong class="bold">Control</strong> (<a href="https://www.oracle.com/java/technologies/jdk-mission-control.html">https://www.oracle.com/java/technologies/jdk-mission-control.html</a>)</li>
			</ul>
			<p>Application profilers should be used to analyze performance-critical operations. They don’t usually run in production environments due to instrumentation being slowed down significantly. It’s common to run profilers in a lower environment with inputs simulating the production<a id="_idIndexMarker1112"/> environment.</p>
			<p>Next, we’re going to cover a few performance improvement strategies.</p>
			<h1 id="_idParaDest-311"><a id="_idTextAnchor407"/>Strategies for performance improvement</h1>
			<p>Improving the <a id="_idIndexMarker1113"/>performance of a system often requires a diverse approach that addresses various aspects. No silver bullet magically boosts performance. However, some common strategies help engineers navigate the problem to meet the non-functional requirements.</p>
			<h2 id="_idParaDest-312"><a id="_idTextAnchor408"/>Testing, testing, testing</h2>
			<p>Performance tests should be<a id="_idIndexMarker1114"/> conducted continuously and repetitively. When there’s a perceived performance issue, it’s unlikely to know the root cause without running performance tests. Instead of blindly applying “performance fixes,” engineers should execute performance tests to understand the problem first.</p>
			<p>Performance tests should be treated as both troubleshooting and discovery tools. There are always bottlenecks in the system that surprise engineers.</p>
			<h2 id="_idParaDest-313"><a id="_idTextAnchor409"/>Avoiding expensive operations</h2>
			<p>More often than<a id="_idIndexMarker1115"/> not, performance issues are caused by a mismatch between the nature of the operations and the actual implementation. In other words, resources are used in unnecessary areas that would use excessive resources and computation power. If excessive resources are spent on expensive operations, then there will be performance issues.</p>
			<p>Let’s consider an example scenario that demonstrates performance optimization by avoiding expensive operations.</p>
			<h3>Scenario – iteration on expensive operations</h3>
			<p>Imagine that there’s a function that’s expensive to execute. This expense is high for several reasons:</p>
			<ul>
				<li>It’s a remote synchronous call to another application</li>
				<li>It’s computationally expensive and/or resource-hungry</li>
				<li>It involves files, databases, messaging, networks, or other resources</li>
				<li>It may be blocked until a result comes back</li>
			</ul>
			<p>We know the following function isn’t expensive, but let’s pretend it is for the sake of discussion:</p>
			<pre class="source-code">
fun someExpensiveOp(n: Int): Int = n</pre>			<p>On <a id="_idTextAnchor410"/>top of this<a id="_idIndexMarker1116"/> function, we’d like to run<a id="_idTextAnchor411"/> some filter<a id="_idTextAnchor412"/>ing, mapping, and<a id="_idTextAnchor413"/> selection:</p>
			<pre class="source-code">
    val result = listOf(1, 7, 3, 23, 63).filter {
        println("filter:$it"); it &gt; 3
    }.map {
        println("expensive:$it"); someExpensiveOp(it)
    }.take(2)
    println(result)</pre>			<p>First, this piece of code filters to take only numbers greater than 3. Then, it invokes the <code>expensive</code> function and gets a new number. At the end, only the first two numbers are selected. The <code>println</code> function is called to show which value is evaluated in the <code>filter</code>, <code>map</code>, or <code>take</code> function.</p>
			<p>Executing this piece of code produces the following console output:</p>
			<pre class="console">
filter:1
filter:7
filter:3
filter:23
filter:63
expensive:7
expensive:23
expensive:63
[7, 23]</pre>			<p>All five numbers are evaluated if they’re greater than 3. The numbers <code>7</code>, <code>23</code>, and <code>63</code> are greater than 3, so they’re passed to the <code>expensive</code> operation. Finally, only the first two numbers from the expensive operation are returned.</p>
			<p>The expensive operation for the third number isn’t necessary because only the first two numbers are selected at the end. In addition, it could have found the first two numbers during filtering and stopped checking the rest of the values.</p>
			<p>Optimized with<a id="_idIndexMarker1117"/> the <code>asSequence</code> function from Kotlin Standard Library, the code looks as follows:</p>
			<pre class="source-code">
    val result = listOf(1, 7, 3, 23, 63)
        .asSequence().filter {
            println("filter:$it"); it &gt; 3
        }.map {
            println("expensive:$it"); someExpensiveOp(it)
        }.take(2)
    println(result)</pre>			<p>However, executing the preceding code prints the following to the console:</p>
			<pre class="console">
kotlin.sequences.TakeSequence@246b179d</pre>			<p>No filtering, no expensive operation, or selection was run. This is because the <code>asSequence</code> function doesn’t build the list until there’s a terminal function. Let’s update the code:</p>
			<pre class="source-code">
    println(result.toList())</pre>			<p>Now, the execution prints the following to the console:</p>
			<pre class="console">
filter:1
filter:7
expensive:7
filter:3
filter:23
expensive:23
[7, 23]</pre>			<p>The sequence operation understands it only takes the first two numbers, so it looks for the first two numbers greater than 3 and stops there. The number <code>63</code> wasn’t even processed. The first number greater than 3 was <code>7</code>, so <code>7</code> was passed to the <code>expensive</code> operation. The second number greater than 3 was <code>23</code>, so <code>23</code> was also passed to the <code>expensive</code> operation. This<a id="_idIndexMarker1118"/> implementation has saved one <code>expensive</code> operation compared to the previous one.</p>
			<h2 id="_idParaDest-314"><a id="_idTextAnchor414"/>An example of a performance improvement journey</h2>
			<p>The households in <a id="_idIndexMarker1119"/>the village have decided to run a survey to rate each household’s services. A vote consists of a rating from 1 to 3:</p>
			<ul>
				<li><strong class="bold">1</strong>: Good</li>
				<li><strong class="bold">2</strong>: Average</li>
				<li><strong class="bold">3</strong>: Poor</li>
			</ul>
			<p>A household can vote for all other households, but only one vote can be made per household. Households are given 1 day to submit all the votes. Let’s also assume one household provides only one service.</p>
			<p>Each household has a “score,” which is the sum of the rank numbers of all votes to that household. The household that has the highest score becomes the household that provides the best service in the village.</p>
			<p>So, if there are <em class="italic">n</em> households in the village, the maximum number of votes will be <code>n x (n- 1</code>). We need a system that calculates the score of all households being voted for, and that records all votes as audit records. The system also needs to display non-final scores for each household when voting is in progress.</p>
			<p>A simplistic architecture of this voting system may look as follows:</p>
			<div><div><img src="img/B21737_12_2.jpg" alt="Figure 12.2 – Simulated survey architecture v0.1" width="967" height="912"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2 – Simulated survey architecture v0.1</p>
			<p>All households <a id="_idIndexMarker1120"/>submit their votes to be validated by <strong class="bold">Voting Service</strong>. The service validates the following aspects:</p>
			<ul>
				<li>All households involved are valid</li>
				<li>A household can’t vote for itself</li>
				<li>A household can only vote for another household once</li>
				<li>The vote has a valid rank</li>
			</ul>
			<p>Since the voting happens in one day for the village, there’s a need for the system to respond quickly (<strong class="bold">latency</strong>) so that it can process many votes within a certain period (<strong class="bold">throughput</strong>) and support a vast number of households (<strong class="bold">scalability</strong>).</p>
			<p>The system expects many concurrent requests to <strong class="bold">Voting Service</strong> at a time, and that could cause a spike.</p>
			<p>It’s possible to scale up the service vertically by adding more resources (CPU, memory, and so on). However, there are physical limitations regarding the number of CPU sockets or the maximum RAM it can support. Adding more resources also leads to diminishing returns, in which performance doesn’t improve proportionally due to other bottlenecks. The only running instance is also the single point of failure that if this instance fails, the entire system becomes unavailable.</p>
			<p>Alternatively, the<a id="_idIndexMarker1121"/> system can scale out horizontally if we add more instances of the service. A <strong class="bold">load balancer</strong> can<a id="_idIndexMarker1122"/> be deployed to distribute load across multiple instances of the service, preventing any single instance from becoming a bottleneck. This significantly increases throughput by enabling parallel processing.</p>
			<p>The load balancer has some knowledge of the load of each instance, so it can route the next request to the instance with the least load. This allows us to add more instances to handle increased loads. With that, the architecture has changed, as shown in <em class="italic">Figure 12</em><em class="italic">.3</em>:</p>
			<div><div><img src="img/B21737_12_3.jpg" alt="Figure 12.3 – Simulated survey architecture v0.2" width="943" height="1091"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3 – Simulated survey architecture v0.2</p>
			<p>Now, <strong class="bold">Voting Service</strong> has <a id="_idIndexMarker1123"/>two stateful validation rules. The first is that the households involved must be valid. The second is that each household can only vote for other households once.</p>
			<p>The household records are frequently accessed, and they can be queried in the database remotely. <strong class="bold">Caching</strong> all <a id="_idIndexMarker1124"/>household records in each service instance is a sensible strategy to speed up validation.</p>
			<p>Enforcing the rule that one household can only vote for another household would benefit from caching. If we cache a list of households that a given household has voted for (the <em class="italic">x-voted-by-y-list</em>), then we can enforce this business rule. However, there’s a complication if any instance can handle any household because it implies sharing the list.</p>
			<p>There are two options we can consider. The first option is that we can use a distributed in-memory database such as Redis so the <em class="italic">x-voted-by-y-lists</em> can be shared, at the cost of having a distributed in-memory database and potential resource contention.</p>
			<p>The second option is to configure the load balancer so that it supports sticky routing. Requests from one household always go to the one responsible instance. Each instance knows its assignment and can locally cache the <em class="italic">x-voted-by-y-lists</em> from the database during startup. The local cache is also updated as it processes incoming requests.</p>
			<p>At this point, the bottleneck has shifted to the database since all the traffic is eventually funneled into it and each request can only be responded to after database operations have finished. This impacts the latency of the response to each voting request.</p>
			<p>The score needs to be calculated for each household being voted in. This is an accumulated number <a id="_idIndexMarker1125"/>that leaves little room for parallel processing. Each validated vote also needs to be kept as an audit record in the database.</p>
			<p>However, votes validated by <strong class="bold">Voting Service</strong> can be further processed asynchronously. Each vote can be partitioned by the household being voted for, so if <strong class="bold">Household 1</strong> votes for <strong class="bold">Household 2</strong>, the vote goes to the “bucket” for <strong class="bold">Household 2</strong>.</p>
			<p>Resolve a household to a bucket can be as simple as a modulo function, that is, the remainder of a hash number divided by the number of buckets:</p>
			<pre class="source-code">
Bucket number = (hash number of household name) mod (number of buckets)</pre>			<p>Each bucket is an event stream. <strong class="bold">Voting Service</strong> can respond to the voting request after an event is published to the corresponding event topic representing the bucket. The vote counting, score calculation, and vote persistence metrics will be processed when downstream components consume the event. This change will significantly reduce the latency of each voting request.</p>
			<p>The updated architecture looks like this:</p>
			<div><div><img src="img/B21737_12_4.jpg" alt="Figure 12.4 – Simulated survey architecture v0.3" width="1197" height="1339"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4 – Simulated survey architecture v0.3</p>
			<p>This approach has <a id="_idIndexMarker1126"/>a limitation: the number of buckets that are fixed at runtime. Events are already routed to the bucket, and we need to maintain the bucket assignment to calculate the score correctly.</p>
			<p>The downstream operations require persisting the result in the database eventually, and we want to avoid overloading the database. Let’s examine the data to be persisted:</p>
			<ul>
				<li><strong class="bold">Score for each household</strong>: One accumulative number per household; historical numbers don’t matter</li>
				<li><strong class="bold">Vote audit record</strong>: Each record needs to be kept, and each record is independent of each other</li>
			</ul>
			<p>The score numbers and vote audit records are different in their natures, so it makes sense for them to be processed differently. It’s better to keep this temporary data in a transient local<a id="_idIndexMarker1127"/> cache to reduce database load but persist the values periodically.</p>
			<p>Here, we can introduce two components:</p>
			<ul>
				<li>The first one, <strong class="bold">Vote Counter</strong>, consumes the event stream for its one assigned bucket and calculates scores for its responsible households. It doesn’t update the score records in the database immediately. Instead, it flushes the latest scores to the database with a fixed schedule – for example, every 10 minutes. This mechanism “soaks” the spike of votes and turns it into regular updates.<p class="list-inset">There are multiple instances of <strong class="bold">Vote Counter</strong>, and there should be at least two instances consuming one bucket to provide availability. Each score record should consist of a household name, the number of votes, the score, and a timestamp. There should be de-duplication rules that only persist newer records and skip the old ones.</p></li>
				<li>The second component, <strong class="bold">Vote Journalist</strong>, consumes a batch of events at a time and flushes the update into the database in one transaction. If the transaction fails, the events in the batch aren’t acknowledged and will be processed again later. <strong class="bold">Vote Journalist</strong> instances of the same bucket should be configured so that only one instance receives the batch of events. The batching processing significantly increases the throughput of vote audit record persistence. However, it would require performance testing to discover the optimal batch size that can scale with the number of votes and still be processed within the memory limits of the process.</li>
			</ul>
			<p>With all these performance concerns considered, we have the final 1.0 architecture, as shown in <em class="italic">Figure 12</em><em class="italic">.5</em>:</p>
			<div><div><img src="img/B21737_12_5.jpg" alt="Figure 12.5 – Simulated survey architecture v1.0" width="1296" height="2026"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5 – Simulated survey architecture v1.0</p>
			<p>In this architecture, we’ve <a id="_idIndexMarker1128"/>optimized the process of load balancing the incoming requests to multiple instances of <strong class="bold">Voting Service</strong> for validation. This increases the throughput and scalability. Then, we introduced local caching of households and the <em class="italic">x-voted-by-y-lists</em> in each service instance to speed up the validation process. It also supports horizontal scaling by adding more instances.</p>
			<p>After, we created a few buckets of event streams for each one responsible for several households exclusively. After <strong class="bold">Voting Service</strong> validates a request to be valid, it responds to the original request and publishes an event stream to the corresponding bucket. This reduces the latency for the response to the voting request:</p>
			<ul>
				<li><strong class="bold">Vote Counter</strong> is introduced to calculate the scores of households that have been assigned to the given bucket. It sends the latest scores to the database periodically and soaks up the spike.</li>
				<li><strong class="bold">Vote Journalist</strong> is introduced to receive a batch of events at a time and to persist them to the database in one transaction. Batch processing increases the throughput of vote audit record persistence.</li>
			</ul>
			<p>In this example, we learned how to optimize the throughput, latency, and scalability of a system. Performance improvements are highly situational. We simply shouldn’t copy a pattern to another system and believe it will perform. Performance needs to be measured and tested. A <a id="_idIndexMarker1129"/>change is only considered a performance improvement when the metrics prove it. However, some known best practices of performance can be improved, something we’ll cover in the next section.</p>
			<h2 id="_idParaDest-315"><a id="_idTextAnchor415"/>Best practices of performance in Kotlin</h2>
			<p>Kotlin has a few<a id="_idIndexMarker1130"/> features that aim to reduce overhead and therefore improve performance. However, these features come with justifications so that engineers can make a conscious decision when to use them. They aren’t expected to be used everywhere without reasoning.</p>
			<h3>In-line functions</h3>
			<p>In-line Kotlin functions <a id="_idIndexMarker1131"/>are simply copied to the caller to reduce the need to invoke the function itself. This is particularly useful if there’s a deep stack of functions or if there’s a higher-order function.</p>
			<p>In-line functions can be declared by adding a modifier at the function level, like so:</p>
			<pre class="source-code">
inline fun &lt;T&gt; measureTime(block: () -&gt; T): T {
    val start = System.nanoTime()
    val result = block()
    val timeTaken = System.nanoTime() - start
    return result.also { println("taken: $timeTaken") }
}</pre>			<h3>The use of immutable and mutable data structures</h3>
			<p>Immutable data eliminates the need for locking in multi-thread environments. In Kotlin, <code>List</code>, <code>Set</code>, and <code>Map</code> collections work with immutable data.</p>
			<p>However, if we’re building a bigger object, such as a string, it’s advisable to use mutable collections or the <code>StringBuilder</code> class to avoid unnecessary object creation that could trigger garbage collection in Kotlin/JVM.</p>
			<h3>The use of coroutines for asynchronous operations</h3>
			<p>Kotlin’s coroutine <a id="_idIndexMarker1132"/>library enables the program to invoke asynchronous operations so that the thread isn’t blocked and can perform other operations while waiting for the asynchronous result to come back. It enables better resource management and quicker response in applications.</p>
			<p>For example, imagine that there are two time-consuming functions:</p>
			<pre class="source-code">
suspend fun task1(): Int {
    delay(1000)
    println("Task 1 completed")
    return 42
}
suspend fun task2(): Int {
    delay(1500) // Simulate a 1.5-second delay
    println("Task 2 completed")
    return 58
}</pre>			<p>These two functions have the <code>suspend</code> modifier to indicate they can be paused and resumed without blocking the thread. The <code>main</code> function that uses these two suspend functions is shown here:</p>
			<pre class="source-code">
fun main() =
    runBlocking {
        val result1 = async { task1() }
        val result2 = async { task2() }
        val combinedResult = result1.await() + result2.await()
        println("Combined Result: $combinedResult")
    }</pre>			<p>The <code>runBlocking</code> function starts a coroutine that blocks the current thread until its execution is complete. Within this block, there are two <code>async</code> functions to invoke the two time-consuming <code>suspend</code> functions. The <code>async</code> function returns a <code>Deferred</code> object on which we invoke the <code>await</code> function to block until the result returns. The two numbers that are returned by the respective time-consuming functions are added and the sum is printed to the console.</p>
			<p>Note that Kotlin code written with the best practices of performance in mind would still need to be measured. Empirical results are the only proof.</p>
			<p>Next, we’re going <a id="_idIndexMarker1133"/>to briefly mention ultra-low latency systems and how they push performance and scalability to the extreme.</p>
			<h1 id="_idParaDest-316"><a id="_idTextAnchor416"/>Ultra-low latency systems</h1>
			<p>Ultra-low latency <a id="_idIndexMarker1134"/>systems operate in the microsecond or nanosecond magnitude. They run in an environment where low response time is critical and even essential. They can be seen in financial trading, telecommunications, gaming, and industrial automation.</p>
			<p>These systems aim to achieve the lowest possible latency, highest efficiency, high predictability, and high concurrency regarding processing. They involve all aspects of a system to reduce response time, such as network optimization, hardware acceleration, load balancing, and efficient algorithms.</p>
			<p>These systems are usually written in system-level programming languages such as C++ and Rust. However, there are a few ultra-low latency systems written in Kotlin or Java that operate in the microsecond magnitude.</p>
			<p>The low latency systems in Kotlin or Java employ several technical designs that aren’t as common:</p>
			<ul>
				<li>Reuse objects, avoid object creation, and avoid garbage collection.</li>
				<li>Use specific JVM vendors for better performance.</li>
				<li>Avoid the use of third-party libraries to reduce overhead and ensure you have full control over performance.</li>
				<li>Use the <strong class="bold">Disruptor pattern</strong> as it <a id="_idIndexMarker1135"/>provides a large ring buffer for inter-thread lock-free communication and memory barriers for data visibility in a thread.</li>
				<li>Use a single-thread model for each JVM process to reduce context switching, lock contention, and the need for synchronous and concurrency processing.</li>
				<li>Write code or design systems that are aware of and optimized for the underlying hardware and network infrastructure they run on. This <a id="_idIndexMarker1136"/>is also called <strong class="bold">mechanical sympathy</strong>.</li>
			</ul>
			<p>Ultra-low latency systems have the justification that they can break a few design principles (for example, immutable objects) in exchange for higher performance. They are exceptional cases due to the demanding need for low latency, high throughput, and quick response time. When developing these systems, performance tests are critical and should be part of the normal development activities.</p>
			<p>Developing ultra-low latency systems is a specialized topic whose content is beyond the scope of this chapter. However, there <a id="_idIndexMarker1137"/>are a few pieces of reading material that you may find useful:</p>
			<ul>
				<li><em class="italic">Mechanical empathy</em>, by Martin Thompson (<a href="https://mechanical-sympathy.blogspot.com/">https://mechanical-sympathy.blogspot.com/</a>)</li>
				<li><em class="italic">LMAX </em><em class="italic">Disruptor</em> (<a href="https://lmax-exchange.github.io/disruptor/">https://lmax-exchange.github.io/disruptor/</a>)</li>
				<li><em class="italic">Simple Binary </em><em class="italic">Encoding</em> (<a href="https://github.com/real-logic/simple-binary-encoding">https://github.com/real-logic/simple-binary-encoding</a>)</li>
				<li><em class="italic">Aeron </em><em class="italic">messaging</em> (<a href="https://github.com/real-logic/aeron">https://github.com/real-logic/aeron</a>)</li>
			</ul>
			<h1 id="_idParaDest-317"><a id="_idTextAnchor417"/>Summary</h1>
			<p>In this chapter, we covered different dimensions of performance and scalability and mentioned a few essential metrics that measure how well a system performs and scales. We emphasized the importance of performance tests, several types of performance tests, and how to plan one. We also provided an example of micro-benchmarking in Kotlin before discussing the use of a profiler to achieve better performance.</p>
			<p>Then, we delved into some strategies for performance improvement. We considered a scenario where only necessary expensive operations were executed. We also looked at an example of a journey of performance improvement for a system in a real-life situation. This allowed us to consider a few best practices regarding performance in Kotlin through code examples.</p>
			<p>Finally, we briefly introduced ultra-low latency systems and where they can be used.</p>
			<p>In the next chapter, we’re going to discuss the topic of software testing.</p>
		</div>
	</div></div></body></html>