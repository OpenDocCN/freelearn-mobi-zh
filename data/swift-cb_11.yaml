- en: Using CoreML and Vision in Swift
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Swift中的CoreML和Vision
- en: The Swift programming language has come a long way since its first introduction,
    and in comparison to many other programming languages, it's still well within
    its infancy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Swift编程语言自从首次推出以来已经走了很长的路，与许多其他编程语言相比，它仍然处于婴儿期。
- en: However, with this in mind, with every release of Swift and its place in the
    open source community, we've seen it grow from strength to strength over such
    a short period of time. We already covered server-side Swift back in [Chapter
    8](9ce1feb3-8fca-4656-91dc-796ba77c3d07.xhtml), *Server-Side Swift*, another evolution
    that was again fueled by the open source community.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，考虑到这一点，随着Swift及其在开源社区中的地位每一次发布，我们都看到它在如此短的时间内从强到更强。我们已经在[第8章](9ce1feb3-8fca-4656-91dc-796ba77c3d07.xhtml)中介绍了服务器端Swift，*服务器端Swift*，这是又一次由开源社区推动的演变。
- en: Another fast-moving train is that of machine learning, once again driven by
    the power of the community, and recognized giants in the industry, such as TensorFlow,
    now support the Swift programming language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个快速发展的领域是机器学习，它再次受到社区力量的推动，并且行业中的巨头，如TensorFlow，现在也支持Swift编程语言。
- en: In this chapter, we're going to look at Apple's offering for machine learning
    – CoreML – and how we can build an app using Swift to read and process machine
    learning models, giving us intelligent image recognition.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨苹果的机器学习产品——CoreML——以及我们如何使用Swift构建应用程序来读取和处理机器学习模型，从而实现智能图像识别。
- en: We'll also take a look at Apple's Vision Framework and how it works alongside
    CoreML to allow us to process video being streamed to our devices in real time,
    recognizing objects on the fly.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将探讨苹果的Vision框架以及它是如何与CoreML协同工作，使我们能够实时处理流到我们设备上的视频，并在飞行中识别对象。
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Building an image capture app
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建图像捕获应用程序
- en: Using CoreML models to detect objects in images
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CoreML模型检测图像中的对象
- en: Building a video capture app
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建视频捕获应用程序
- en: Using CoreML and the Vision Framework to detect objects in real time
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CoreML和Vision框架实时检测对象
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files present in this chapter on GitHub at [https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11)
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本章中提供的代码文件，网址为[https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11](https://github.com/PacktPublishing/Swift-Cookbook-Second-Edition/tree/master/Chapter11)
- en: 'Check out the following video to see the Code in Action: [https://bit.ly/2NmP961](https://bit.ly/2NmP961)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频以查看代码的实际操作：[https://bit.ly/2NmP961](https://bit.ly/2NmP961)
- en: Building an image capture app
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建图像捕获应用程序
- en: In this first recipe, we're going to create an app that captures either an image
    from your camera roll or an image taken from your camera. This will set up our
    iOS app ready for us to incorporate CoreML to detect objects in our photos.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个食谱中，我们将创建一个应用程序，它可以捕获来自您的相册或相机拍摄的照片。这将使我们的iOS应用程序准备好，以便我们能够将CoreML集成到我们的照片中检测对象。
- en: Getting ready
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, you'll need the latest version of Xcode available from the
    Mac App Store.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个食谱，您需要从Mac App Store获取的最新版本的Xcode。
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'With Xcode open, let''s get started:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 打开Xcode后，让我们开始吧：
- en: Create a new project in Xcode. Go to **File** | **New** | **Project** | **iOS
    App**.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Xcode中创建一个新的项目。转到**文件** | **新建** | **项目** | **iOS App**。
- en: 'In `Main.storyboard`, add the following:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`Main.storyboard`中添加以下内容：
- en: Add `UISegmentedControl` with two options (**Photo / Camera Roll** and **Live
    Camera**).
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加带有两个选项的`UISegmentedControl`（**照片/相册**和**实时相机**）。
- en: Next, add a `UILabel` view just underneath.
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在下面添加一个`UILabel`视图。
- en: Add a `UIImageView` view beneath that.
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下面添加一个`UIImageView`视图。
- en: Finally, add a `UIButton` component.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，添加一个`UIButton`组件。
- en: 'Space these accordingly using AutoLayout constraints with `UIImageView` being
    the prominent object:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用AutoLayout约束相应地间隔使用`UIImageView`作为突出对象：
- en: '![](img/a5c4c4b3-3373-46d5-ac77-73cc7b895406.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a5c4c4b3-3373-46d5-ac77-73cc7b895406.png)'
- en: Figure 11.1 – Camera/photo app
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – 摄像头/照片应用程序
- en: 'Once we have this in place, let''s hook these up to our `ViewController.swift`
    file:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们有了这个，就让我们将这些连接到我们的`ViewController.swift`文件中：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Take note that in the preceding, we have two `IBOutlet` and one `IBAction` (we
    don't need an outlet for `UIButton`, we just care about its action).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的内容中，我们有两个`IBOutlet`和一个`IBAction`（我们不需要`UIButton`的出口，我们只关心它的动作）。
- en: 'Next, populate `IBAction` with the following code:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，用以下代码填充 `IBAction`：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, let''s create an extension of `UIViewController`. You can do this at the
    bottom of the `ViewController` class if you like:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个 `UIViewController` 的扩展。如果你喜欢，可以在 `ViewController` 类的底部做这件事：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Our extension needs to conform to the `UIImagePickerControllerDelegate` and
    **`UINavigationControllerDelegate`** protocols. We can now go ahead and populate
    our extension with the following delegate method:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的扩展需要遵守 `UIImagePickerControllerDelegate` 和 **`UINavigationControllerDelegate`**
    协议。现在我们可以继续填充我们的扩展，以下是一个委托方法：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before we go any further, we''ll need to add a couple of lines to our `info.plist`:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们继续之前，我们需要在我们的 `info.plist` 中添加几行：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '9\. Add these in with the following string description: `Chapter 11 wants to
    detect cook Stuff`. This is an iOS security feature that will prompt the user
    when any app/code tries to access the camera, photo library, or location services.
    Failure to add this in could result in an app crash.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 9. 使用以下字符串描述添加这些内容：`Chapter 11 wants to detect cook Stuff`。这是一个 iOS 安全功能，当任何应用/代码尝试访问相机、照片库或位置服务时，会提示用户。如果没有添加这个，可能会导致应用崩溃。
- en: For our app, we can add whatever we want, but for a production app, make sure
    the text you enter is useful and informative to the user. Apple will check this
    when reviewing your app and has been known to potentially block a release until
    this is resolved.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的应用，我们可以添加任何我们想要的，但对于一个生产应用，确保你输入的文本对用户有用且信息丰富。苹果会在审查你的应用时检查这一点，并且已知在解决这个问题之前可能会阻止发布。
- en: 'Go ahead and run your code, and then launch the app. One of the following things
    should happen:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 继续运行你的代码，然后启动应用。以下情况之一应该会发生：
- en: If you are running the app from the simulator, our `UIButton` press should present
    the photo picker (along with the default images supplied by the iOS simulator).
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在从模拟器运行应用，我们的 `UIButton` 点击应该会显示照片选择器（以及 iOS 模拟器提供的默认图片）。
- en: If you are running from a device, then you should be presented with the camera
    view, allowing you to capture a photo.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在从设备上运行，那么你应该会看到相机视图，允许你拍照。
- en: Either way, whether a photo was selected or a picture was taken, the resulting
    image should show in `UIImageView`!
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 无论哪种方式，无论是选择了照片还是拍了照片，最终的结果图像都应该显示在 `UIImageView` 中！
- en: How it works...
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Let''s step through what we''ve just done. We''ll begin at `IBAction` and have
    a look at the `UIPickerView` view we''ve created:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们一步一步回顾一下我们刚刚所做的工作。我们从 `IBAction` 开始，看看我们创建的 `UIPickerView` 视图：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s go through this one line at a time:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这一行：
- en: We instantiate an instance of `UIImagePickerController` – an available API that
    will allow us to choose an image based on a specific source.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们实例化一个 `UIImagePickerController` 的实例——这是一个可用的 API，它将允许我们根据特定的来源选择一个图片。
- en: We set the delegate as `self`, so we can harness any results or actions caused
    by `UIImagePickerController`.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将委托设置为 `self`，这样我们就可以利用由 `UIImagePickerController` 引起的任何结果或操作。
- en: We set `allowEditing` to `false`, which is used to hide controls when the camera
    is our source.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将 `allowEditing` 设置为 `false`，这用于在相机是我们来源时隐藏控件。
- en: In this instance, we set the source type based on whether the camera is available
    or not (so it works well with the simulator).
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，我们根据相机是否可用设置源类型（因此它与模拟器配合得很好）。
- en: Finally, we present our view controller.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们展示我们的视图控制器。
- en: 'Now, let''s take a look at our delegate methods:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的委托方法：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The first is pretty self-explanatory; `imagePickerControllerDidCancel` handles
    any instances where `UIImagePickerController` is canceled by the users. In our
    case, we just dismiss the instance returned – job done!
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方法相当直观；`imagePickerControllerDidCancel` 处理用户取消 `UIImagePickerController`
    的任何实例。在我们的情况下，我们只是关闭返回的实例——任务完成！
- en: '`didFinishPickingMediaWithInfo` is where interesting things happen. Notice
    how we are given a dictionary of **info** in our response. Here, we have various
    segments of information. The one we are looking for is under the `UIImagePickerController.InfoKey.originalImage`
    key. This gives us a `UIImage` of what we''ve just selected, allowing us to assign
    this straight back to `UIImageView`.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`didFinishPickingMediaWithInfo` 是有趣的事情发生的地方。注意我们得到的响应中有一个 **info** 字典。在这里，我们有各种信息片段。我们要找的是在
    `UIImagePickerController.InfoKey.originalImage` 键下的。这给我们一个 `UIImage`，显示我们刚刚选择的图片，允许我们直接将其分配给
    `UIImageView`。'
- en: Now that we've got an app that allows us to take or choose a photo, we can apply
    it to some real work with the power of CoreML and object detection.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个允许我们拍照或选择照片的应用程序，我们可以利用 CoreML 和对象检测的力量将其应用于一些实际工作。
- en: There's more...
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: 'A quick note to mention: you''ll also have noticed that we were required to
    conform our extension to `UINavigationControllerDelegate`. This is required by
    iOS to allow `UIImageContoller` to be handled and presented correctly from its
    "presenting" stack (`ViewController` in our instance).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 简要说明：你也会注意到我们被要求使我们的扩展符合 `UINavigationControllerDelegate`。这是 iOS 所要求的，以便正确处理和展示从其“呈现”堆栈（在我们的实例中是
    `ViewController`）的 `UIImageContoller`。
- en: See also
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more information on `UIImagePickerController`, refer to [https://developer.apple.com/documentation/uikit/uiimagepickercontroller](https://developer.apple.com/documentation/uikit/uiimagepickercontroller).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 `UIImagePickerController` 的更多信息，请参阅 [https://developer.apple.com/documentation/uikit/uiimagepickercontroller](https://developer.apple.com/documentation/uikit/uiimagepickercontroller)。
- en: Using CoreML models to detect objects in images
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CoreML 模型检测图像中的对象
- en: In this recipe, we'll take the app we just built and incorporate the CoreML
    framework in order to detect objects in our images.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将使用我们刚刚构建的应用程序并集成 CoreML 框架，以便在我们的图像中检测对象。
- en: We'll also take a look at the generated CoreML models available for us to use
    and download directly from Apple's Developer portal.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将查看可用于我们使用和直接从苹果开发者门户下载的生成的 CoreML 模型。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this recipe, you'll need the latest version of Xcode available from the
    Mac App Store.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个菜谱，你需要从 Mac App Store 获取的最新版本的 Xcode。
- en: 'Next, head on over to the Apple Developer portal at the following address:
    [https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，前往以下地址的 Apple 开发者门户：[https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/)。
- en: Here, you will find out a little bit more about the models available for us
    to download and use in our Xcode project.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你将了解到更多关于我们可以下载和使用在我们 Xcode 项目中的模型的信息。
- en: You'll notice there are options for image models and text models. For this recipe,
    we're going to be using image models, specifically one called Resnet50, which
    uses a residual neural network that attempts to identify and classify what it
    perceives to be the dominant object in an image.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到有图像模型和文本模型的选择。对于这个菜谱，我们将使用图像模型，具体是一个名为 Resnet50 的模型，它使用残差神经网络，试图识别和分类它感知到的图像中的主要对象。
- en: For more information on the different types of machine learning models, see
    the links in the *See also* section at the end of this recipe.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同类型机器学习模型的更多信息，请参阅本菜谱末尾的 *参见* 部分的链接。
- en: From here, download the Resnet50.mlmodel (32-bit) model. If you are having trouble
    downloading the file, you can just take a copy from the sample project in our
    GitHub repository.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，下载 Resnet50.mlmodel（32 位）模型。如果你在下载文件时遇到麻烦，你可以直接从我们的 GitHub 仓库中的示例项目中复制一份。
- en: Once downloaded, add this to your Xcode project by simply dragging it into the
    file explorer tree in our previous app.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，只需将其拖动到我们之前应用程序的文件资源管理器树中即可将其添加到你的 Xcode 项目中。
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s make a start where we left off in our previous project:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们上一个项目中中断的地方开始：
- en: 'With everything in place, head back into `ViewController.swift` and add the
    following global variable and addition to our `viewDidLoad()` function:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一切准备就绪后，返回 `ViewController.swift` 并将以下全局变量添加到我们的 `viewDidLoad()` 函数中：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now, head on over to the sample project and obtain a file called `ImageHelpers.swift`;
    add this to our project. Once this has been added, we'll head on back over to
    our `didFinishPickingMediaWithInfo` delegate and expand on this a little further.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，前往示例项目并获取一个名为 `ImageHelpers.swift` 的文件；将其添加到我们的项目中。一旦添加，我们将回到 `didFinishPickingMediaWithInfo`
    代理并进一步扩展它。
- en: 'Add in the following highlighted changes:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下突出显示的更改：
- en: '[PRE8]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With everything in place, run the app and select a photo. As long as you didn't
    point it at a blank wall, you should be seeing some interesting feedback.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪后，运行应用程序并选择一张照片。只要你没有对着空墙拍照，你应该会看到一些有趣的反馈。
- en: With all that in place, let's break down the changes we just made to understand
    what just happened a little more.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 一切就绪后，让我们分析我们刚刚所做的更改，以便更好地理解刚刚发生了什么。
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The first thing is to take a look at the following line we added in:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 第一件事是查看我们添加的以下行：
- en: '[PRE9]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we added in a call to a helper method we took from our sample project.
    This helper contains the following two functions:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们添加了一个从我们的示例项目中取出的辅助方法的调用。这个辅助方法包含以下两个函数：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: These functions and what they do are a little out of the scope of this book,
    and this chapter in particular. However, at a very high level, the first function,
    `processImageData()`, takes an instance of `UIImage` and transforms this to `CVPixelBuffer`
    format.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数及其功能略超出了本书的范围，尤其是这一章。然而，从非常高的层面来看，第一个函数`processImageData()`接受一个`UIImage`实例并将其转换为`CVPixelBuffer`格式。
- en: This essentially returns the `UIImage` object back to its raw format that it
    was captured in (`UIImage` is merely a UIKit wrapper for our true raw image).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上将`UIImage`对象返回到其捕获的原始格式（`UIImage`仅仅是我们的真实原始图像的UIKit包装器）。
- en: During this process, we need to flip the orientation too as with all captured
    images. This is almost certainly in landscape mode (and more often than not, you've
    taken a picture or selected a photo in portrait mode).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程中，我们还需要翻转方向，就像所有捕获的图像一样。这几乎肯定是在横幅模式下（而且通常情况下，你是在肖像模式下拍照或选择照片）。
- en: Another reason for performing this is that our ResNet50 model is trained to
    observe images at only 224 x 224\. So, we need to readjust the captured image
    to this size.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 进行这一操作的另一个原因是我们的ResNet50模型被训练来观察只有224 x 224像素的图像。因此，我们需要调整捕获的图像到这个大小。
- en: If you need more information on the model you have in your project, simply select
    the file in the file explorer and view the details in the main window. From here,
    the Predictions tab will give you all the details you need about the input file
    required.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要关于项目中模型的更多信息，只需在文件资源管理器中选择文件，然后在主窗口中查看详细信息。从这里，预测标签将提供你需要的所有关于输入文件的信息。
- en: So, with our helper function implemented, we receive a new `UIImage` object
    (modified to our new spec) and the image in `CVPixelBuffer` format, all ready
    to pass over to CoreML for processing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在实现辅助函数之后，我们接收一个新的`UIImage`对象（修改为我们的新规范）和`CVPixelBuffer`格式的图像，所有这些都准备好传递给CoreML进行处理。
- en: 'Now, let''s take a look at the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下下面的代码：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the preceding code, I've highlighted some areas of interest. First is our
    `prediction()` function call on our `model` object. Here, we pass in our image
    in the `CVPixelBuffer` format we got back from our helper method earlier. From
    this, wrapped in a `try` statement, CoreML will now attempt to detect an object
    in the photo. If successful, we'll exit our `guard` statement gracefully and be
    able to access the properties available in our `prediction` variable.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我突出显示了几个感兴趣的区域。首先是我们的`prediction()`函数调用在`model`对象上。在这里，我们传入从之前辅助方法中获取的`CVPixelBuffer`格式的图像。基于此，在`try`语句中包裹，CoreML现在将尝试在照片中检测一个对象。如果成功，我们将优雅地退出`guard`语句，并能够访问`prediction`变量中可用的属性。
- en: 'If you take a look at the properties available in our ResNet50 model, you''ll
    see the various options we have:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看我们ResNet50模型中可用的属性，你会看到我们拥有的各种选项：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The class label we've already seen, but the class label probability will return
    us a dictionary of the most likely category for our image with a value based around
    a confidence score.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了类标签，但类标签概率将返回一个字典，其中包含我们图像最可能的类别及其基于置信度分数的值。
- en: Each model will have its own set of properties based on its desired intention
    and how it's been built.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都将根据其预期的意图和构建方式拥有自己的属性集。
- en: There's more...
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: At the beginning of this section, we obtained a model that allowed us to detect
    objects in our images. Touching on this subject a little more, models are a set
    of data that has been trained to identify a pattern or characteristics of a certain
    description.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开头，我们获得了一个允许我们在图像中检测对象的模型。进一步探讨这个主题，模型是一组经过训练以识别某种描述的模式或特征的数据集。
- en: For example, we want a model that detects cats; so, we train our model by feeding
    it images of around 10,000 various pictures of cats. Our model training will identify
    features and shapes common to each other and categorize them accordingly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们想要一个检测猫的模型；因此，我们通过给它大约10,000张不同猫的图像来训练我们的模型。我们的模型训练将识别共同的特征和形状，并相应地进行分类。
- en: When we then feed our model an image of a cat, we hope that it is able to pick
    up those categorized features within our image and successfully identify the cat.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们给模型提供一个猫的图像时，我们希望它能够识别图像中那些分类特征并成功识别出猫。
- en: The more images you train with, the greater the performance; however, that still
    depends on the integrity of the images too. Training with the same image of a
    cat (just in a different pose) 1,000 times might give you the same results as
    if you take 10,000 images of the same cat (again in a different pose).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 你训练的图片越多，性能就越好；然而，这仍然取决于图片的完整性。用同一只猫的图片（只是不同的姿势）训练1,000次可能给你带来的结果与拍摄10,000张同一只猫的图片（再次是不同的姿势）的结果相同。
- en: The same goes the other way too; if you train with 500,000 images of a panther
    and then 500,000 images of a kitten, it's just not going to work.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的情况也适用于相反的方向；如果你用50,000张豹子的图片和50,000张小猫的图片进行训练，这根本就不会起作用。
- en: Away from CoreML, you are now able to train a model using Swift with TensorFlow.
    TensorFlow is a Google product that is leading the way in terms of machine learning
    and with an ever-growing community of developers behind it coupled with Swift's
    own open source community. Advancement in this particular technology is certainly
    looking bright.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 离开CoreML，你现在可以使用Swift和TensorFlow训练模型。TensorFlow是谷歌的产品，在机器学习领域处于领先地位，拥有不断增长的开发者社区，以及Swift自己的开源社区。这一特定技术的进步前景确实光明。
- en: See also
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'For more information, please refer to the following links:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅以下链接：
- en: 'Apple CoreML documentation: [https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apple CoreML文档：[https://developer.apple.com/documentation/coreml](https://developer.apple.com/documentation/coreml)
- en: 'TensorFlow Swift: [https://www.tensorflow.org/swift/tutorials/model_training_walkthrough](https://www.tensorflow.org/swift/tutorials/model_training_walkthrough)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'TensorFlow Swift: [https://www.tensorflow.org/swift/tutorials/model_training_walkthrough](https://www.tensorflow.org/swift/tutorials/model_training_walkthrough)'
- en: Building a video capture app
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建视频捕获应用
- en: So, what we have seen so far of CoreML is pretty neat, to say the least. But
    taking a look back over this chapter so far, we have probably spent more time
    building our app to harness the power of CoreML than actually implementing it.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，到目前为止，我们所看到的CoreML内容至少是非常不错的。但是回顾一下到目前为止的章节，我们可能花在构建应用以利用CoreML功能上的时间比实际实现它的时间还要多。
- en: In this section, we're going to take our app a little further by streaming a
    live camera feed that in turn will allow us to intercept each frame and detect
    objects in real time.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将通过流式传输实时摄像头视频来进一步扩展我们的应用，这样我们就可以拦截每一帧并在实时中检测对象。
- en: Getting ready
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this section, you'll need the latest version of Xcode available from the
    Mac App Store.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这一部分，你需要从Mac App Store获取最新版本的Xcode。
- en: Please note that for this section, you'll need to be connected to a real device
    for this to work. Currently, the iOS simulator does not have a way to emulate
    the front or back camera.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于这一部分，你需要连接到真实设备才能使其工作。目前，iOS模拟器没有模拟前后摄像头的方法。
- en: How to do it...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'Let''s begin:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始：
- en: 'Head over to our `ViewContoller.swift` file and make the following amendments:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请转到我们的`ViewContoller.swift`文件，并做出以下修改：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, create a function called `setupCaptureSession()` and we''ll start by adding
    in the following:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个名为`setupCaptureSession()`的函数，我们首先添加以下内容：
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we are checking our device for an available camera, specifically
    `.builtInWideAngleCamera` at the back.If no device can be found, our guard will
    fail.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们正在检查设备上是否有可用的摄像头，特别是后置的`.builtInWideAngleCamera`。如果找不到设备，我们的保护措施将会失败。
- en: Next, we initialize `AVCaptureDeviceInput` with our new `videoDevice` object.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用新的`videoDevice`对象初始化`AVCaptureDeviceInput`。
- en: 'Now, continuing in our function, add the following code:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，继续在我们的函数中添加以下代码：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Essentially, here we are attaching our device to a capture session, allowing
    us to stream what the device input (camera) is processing programmatically straight
    into our code. Now we just to point this at our view so that we can see the output.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在这里我们正在将设备连接到一个捕获会话中，这样我们就可以将设备输入（摄像头）处理的结果程序化地直接输入到我们的代码中。现在我们只需将其指向我们的视图，这样我们就可以看到输出。
- en: 'Add the following additional code to our function:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的函数中添加以下附加代码：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With the code we've just added, we are essentially creating a visible layer
    from our current capture session. In order for us to process this on our screen,
    we need to assign this to a `rootLayer` (our `CALayer` variable we added earlier).
    While this seems a little overkill and we could just add this to the layer of
    our `UIImageView`, we're prepping for something we need to do in our next recipe.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们刚刚添加的代码，我们实际上是从当前的捕获会话中创建了一个可见层。为了让我们能够在屏幕上处理它，我们需要将其分配给`rootLayer`（我们之前添加的`CALayer`变量）。虽然这看起来有点过度，我们本可以直接将其添加到`UIImageView`的层中，但我们正在为我们的下一个菜谱做准备。
- en: 'Finally, with our camera and device all set up, it''s time to set the camera
    rolling:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，当我们的摄像头和设备都设置好了，是时候让摄像头开始工作了：
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Go ahead and run the app. Note again that this will only work on a real device
    and not a simulator. All going well, you should have a live stream from your camera.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行应用程序。再次注意，这只能在真实设备上工作，而不是在模拟器上。一切顺利的话，你应该有来自摄像头的实时流。
- en: How it works...
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The best way to explain this would be to think of the capture session as a wrapper
    or a configuration between the device's hardware and software. The camera hardware
    has a lot of options, so we configure our capture session to pick out what we
    want for our particular instance.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的解释方式是将捕获会话想象成设备硬件和软件之间的包装器或配置。摄像头硬件有很多选项，因此我们配置捕获会话以选择我们特定实例所需的内容。
- en: 'Let''s look back at this line of code:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这一行代码：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here, you could control the enum bases on a UI toggle, allowing the user to
    specify which camera to use. You could even use the following:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以根据UI切换控制枚举基，使用户能够指定要使用哪个摄像头。你甚至可以使用以下：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Re-configure the session and then `startRunning()` again. Essentially (albeit
    at a much more complex level), this is what happens when you switch from the front
    to the back camera when taking a photo.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 重新配置会话然后再次调用`startRunning()`。本质上（尽管在更复杂的层面上），这就是当你从前置摄像头切换到后置摄像头拍照时发生的事情。
- en: 'With the session captured, we can now stream the output directly to any view
    we like just like we did here:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在会话捕获后，我们现在可以直接将输出流式传输到我们喜欢的任何视图，就像我们在这里做的那样：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'But the fun comes when we want to manipulate the image that is being streamed,
    by capturing them one frame at a time. We do this by implementing the `AVCaptureVideoDataOutputSampleBufferDelegate`
    protocol, which allows us to override the following delegate methods:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但有趣的是，当我们想要通过逐帧捕获来操作正在流式传输的图像时。我们通过实现`AVCaptureVideoDataOutputSampleBufferDelegate`协议来完成这项工作，它允许我们重写以下代理方法：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Notice something familiar here... we're being given `sampleBuffer`, just like
    we got in `UIImagePickerDelegate`. The difference here is that this will be called
    with every frame, not just when one is selected.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有什么熟悉的地方...我们被提供了`sampleBuffer`，就像我们在`UIImagePickerDelegate`中得到的那样。这里的区别在于，这将在每一帧上被调用，而不仅仅是当选择了一个时。
- en: There's more...
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Playing around with capture sessions and `AVCaptureOutputs` is an expensive
    operation. Always make sure you stop your session from running when it's not needed,
    and make sure your delegates are not unnecessarily processing data when they don't
    need to be.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在捕获会话和`AVCaptureOutputs`上玩耍是一个昂贵的操作。始终确保在不需要时停止会话运行，并确保你的代理在不需要时不要不必要地处理数据。
- en: Another thing to note is that the initialization of a capture device can in
    some instances be slow, so make sure you have the appropriate UI to handle the
    potential blocking it may cause.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要注意的事项是，在某些情况下，捕获设备的初始化可能很慢，所以请确保你有适当的UI来处理它可能引起的潜在阻塞。
- en: 'Final note: if you are struggling with memory leaks and high CPU times, take
    a look at a suite of tools called Instruments. Bundles of Xcode Instruments can
    offer a wide range of performance tracing tools that can really help you to get
    the most out of your Swift code.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的注意事项：如果你在内存泄漏和高CPU时间上遇到困难，请查看一套名为Instruments的工具。Xcode Instruments的工具包可以提供一系列性能跟踪工具，这可以帮助你充分利用Swift代码。
- en: See also
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For more information, refer to the following links:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 更多信息，请参阅以下链接：
- en: 'Instruments overview: [https://help.apple.com/instruments/mac/current/#/dev7b09c84f5](https://help.apple.com/instruments/mac/current/#/dev7b09c84f5)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仪器概述：[https://help.apple.com/instruments/mac/current/#/dev7b09c84f5](https://help.apple.com/instruments/mac/current/#/dev7b09c84f5)
- en: 'AVFoundation: [https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation)'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AVFoundation：[https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation)
- en: Using CoreML and the Vision Framework to detect objects in real time
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CoreML 和 Vision 框架进行实时物体检测
- en: We've seen what CoreML can do in terms of object detection, but taking everything
    we've done so far into account, we can certainly go a step further. Apple's Vision
    Framework offers a unique set of detection tools from landmark detection and face
    detection in images to tracking recognition.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了 CoreML 在物体检测方面的能力，但考虑到我们迄今为止所做的一切，我们当然可以更进一步。苹果的 Vision 框架提供了一套独特的检测工具，从图像中的地标检测和面部检测到跟踪识别。
- en: With the latter, tracking recognition, the Vision Framework allows us to take
    models built with CoreML and use them in conjunction with CoreML's object detection
    to identify and track the object in question.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 对于后者，跟踪识别，Vision 框架允许我们使用用 CoreML 构建的模型，并将其与 CoreML 的物体检测结合使用，以识别和跟踪相关对象。
- en: In this section, we'll take everything we've learned so far, from how AVFoundation
    works to implementing CoreML, and build a real-time object detection app using
    a device camera.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将利用我们迄今为止所学的所有知识，从 AVFoundation 的工作原理到实现 CoreML，并使用设备摄像头构建一个实时物体检测应用程序。
- en: Getting ready
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: For this section, you'll need the latest version of Xcode available from the
    Mac App Store.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，您需要从 Mac App Store 获取的最新版本的 Xcode。
- en: 'Next, head on over to the Apple Developer portal at the following address:
    [https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/).'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，前往以下地址的 Apple 开发者门户：[https://developer.apple.com/machine-learning/models/](https://developer.apple.com/machine-learning/models/)。
- en: Here, you will find out a little bit more about the models available for us
    to download and use in our Xcode project. You'll notice there are options for
    image models or text models. For this recipe, we're going to be using image models,
    specifically one called **YOLOv3**, which uses a residual neural network that
    attempts to identify and classify what it perceives to be the dominant object
    in the image.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您将了解到有关我们可以在 Xcode 项目中下载和使用的一些模型的更多信息。您会注意到有图像模型或文本模型的选择。对于这个食谱，我们将使用图像模型，特别是名为
    **YOLOv3** 的一个模型，它使用残差神经网络，试图识别和分类它感知到的图像中的主要对象。
- en: For more information on the different types of machine learning models, see
    the links in the *See also* section at the end of this recipe.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 关于不同类型机器学习模型的更多信息，请参阅本食谱末尾的 *See also* 部分的链接。
- en: From here, download the YOLOv3.mlmodel (32-bit) model. If you are having trouble
    downloading the file, you can just take a copy from the sample project in our
    GitHub repository.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，下载 YOLOv3.mlmodel（32 位）模型。如果您在下载文件时遇到麻烦，您可以直接从我们的 GitHub 仓库中的示例项目中复制一份。
- en: Once downloaded, add this to your Xcode project by simply dragging it into the
    file explorer tree in our previous app.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，只需将其拖动到我们之前应用程序的文件资源管理器树中，即可将其添加到您的 Xcode 项目中。
- en: How to do it...
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'We''ll start by creating a new `UIViewController` for all our vision work,
    in Xcode:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个新的 `UIViewController` 来处理所有视觉工作，在 Xcode 中：
- en: Go to File | New | File.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 File | New | File。
- en: Choose **Cocoa Touch Class**.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 **Cocoa Touch Class**。
- en: Name this `VisionViewController`.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其命名为 `VisionViewController`。
- en: Make this a subclass of `UIViewController`.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使其成为 `UIViewController` 的子类。
- en: 'With that done, we can now head on over to our new `VisionViewController` and
    add in the following highlighted code. We''ll start by importing the Vision Framework:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们现在可以前往我们的新 `VisionViewController` 并添加以下高亮代码。我们首先导入 Vision 框架：
- en: '[PRE22]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, we''ll subclass our existing `ViewController` so that we can get the best
    of both worlds (without the need for copious amounts of code duplication):'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将对现有的 `ViewController` 进行子类化，以便我们可以兼得两者之长（无需大量代码重复）：
- en: '[PRE23]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'With that done, we can now override some of our functions in `ViewContoller.swift`.
    We''ll start with `setupCaptureSession()`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这些后，我们现在可以覆盖一些我们在 `ViewContoller.swift` 中的函数。我们首先从 `setupCaptureSession()`
    开始：
- en: '[PRE24]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: When overriding from another class, always remember to call the base function
    first. In the case of the preceding code, this can be done by calling `super.setupCaptureSession()`
    as highlighted.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当从另一个类覆盖时，始终记得首先调用基类函数。在前面代码的情况下，可以通过调用 `super.setupCaptureSession()` 来实现，如高亮所示。
- en: 'You''ll notice some functions in the ViewControler.swift file that we''ve not
    yet created. Let''s go through these now one by one:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到在 ViewControler.swift 文件中一些我们尚未创建的函数。让我们现在逐一过一遍：
- en: 'First, we''ll add a detection layer to our `rootLayer` that we created earlier.
    This `CALayer` will be used as the drawing plane for our detected object area:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将向之前创建的`rootLayer`中添加一个检测层。这个`CALayer`将用作我们检测到的对象区域的绘图平面：
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As you can see from the code, we create its bounds based on the height and width
    taken from our `bufferSize` property (which is being shared back over in our `ViewController`
    class).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码所示，我们根据从`bufferSize`属性（在`ViewController`类中共享）中获取的高度和宽度创建其边界。
- en: 'Next, we need to add some geometry to `detectionLayer()`. This will re-adjust
    and scale the detection layer based on the device''s current geometry:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要在`detectionLayer()`中添加一些几何形状。这将根据设备的当前几何形状重新调整和缩放检测层：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, let''s hook up our `startVision()` function:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们连接我们的`startVision()`函数：
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: With this comes a new function, `visionResults()`. Go ahead and create this
    function in `VisionViewController` too.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将带来一个新的函数，`visionResults()`。请在前面的`VisionViewController`中也创建这个函数。
- en: We could have simply used an extension in our original `ViewController` to house
    all these new functions, but we'd run the risk of overloading our view controller
    to the point where it could become too unmaintainable. Also, our logic and extension
    for `UIImagePicker` was in here, so the separation is nice.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以直接在我们的原始`ViewController`中扩展所有这些新功能，但这样做可能会使我们的视图控制器过载，变得难以维护。此外，我们的`UIImagePicker`的逻辑和扩展也在这里，所以这种分离是很好的。
- en: 'With this, let''s build out `visionResults()` function. We''ll do this a section
    at a time so it all makes sense:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们构建`visionResults()`函数。我们将分部分进行，以确保一切都有意义：
- en: '[PRE28]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We start with some basic housekeeping; performing `CATransaction` locks in memory
    any changes we're going to make to `CALayer`, before we finally commit them for
    use. In this code, we'll be modifying `detectionLayer`.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一些基本的维护工作开始；执行`CATransaction`会将我们即将对`CALayer`所做的任何更改锁定在内存中，在我们最终提交它们以供使用之前。在这段代码中，我们将修改`detectionLayer`。
- en: 'Next, we''ll iterate around our `results` parameter to pull out anything that
    is of class type `VNRecognizedObjectObservation`:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将遍历`results`参数，提取出任何属于类类型`VNRecognizedObjectObservation`的对象：
- en: '[PRE29]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: From this, we'll continue to use Vision to obtain the `Rect` and position of
    the identified object(s) using `VNImageRectForNormalizedRect`. We can also grab
    some text information about the objects detected and use that too.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们将继续使用Vision通过`VNImageRectForNormalizedRect`获取已识别对象（的）`Rect`和位置。我们还可以获取一些关于检测到的对象的文本信息并加以利用。
- en: 'Finally, we''ll gracefully close off any changes to `detectionLayer` and update
    the geometry to match the detected objects. You''ll notice there are two new functions
    we''ve just introduced:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将优雅地关闭对`detectionLayer`的任何更改，并更新几何形状以匹配检测到的对象。您会注意到我们刚刚引入了两个新函数：
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'These again are helper functions, one to draw the rectangle of the detected
    object and the other to write the text. These functions are generic boilerplate
    sample code that can be obtained from Apple''s documentation. Feel free to have
    a play around with these to suit your needs. One thing I will mention: you''ll
    notice how we do all this again using layers rather than adding `UIView` and `UILabel`.
    This again is because UIKit is a wrapper around a lot of core functionality. But
    adding a UIKit component on top of another component is unnecessary and with what
    is already an intense program, this could be performed much more efficiently by
    updating and manipulating the layers directly on a UIKit object.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这些同样是辅助函数，一个用于绘制检测对象的矩形，另一个用于写入文本。这些函数是通用的样板代码，可以从Apple的文档中获得。请随意根据您的需求进行尝试。有一点我要提到：您会注意到我们如何再次使用层而不是添加`UIView`和`UILabel`来完成所有这些。这再次是因为UIKit是围绕许多核心功能的一个包装器。但在另一个组件之上添加UIKit组件是不必要的，而且考虑到这已经是一个密集的程序，通过直接更新和操作UIKit对象上的层来执行这可以更加高效。
- en: These objects can be found in the sample project on GitHub; just copy them into
    your project (either in `VisionViewController` or your own helper file).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象可以在GitHub上的示例项目中找到；只需将它们复制到你的项目中（无论是`VisionViewController`还是你自己的辅助文件）。
- en: 'With our AV Foundation camera streaming in place and Vison and CoreML ready
    to do their magic, there is one final override we need to add to our `VisionViewController`:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的AV Foundation相机流就绪，Vision和CoreML准备施展魔法之后，我们还需要在`VisionViewController`中添加一个最后的重写：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Using the delegate for AV Foundation, we grab each frame again, converting this
    to `CVPixelBuffer` in order to create `VNImageRequestHander`. This now kicks off
    the requests in our `startVision()` function, stitching everything together nicely.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 AV Foundation 的代理，我们再次抓取每一帧，将其转换为 `CVPixelBuffer` 以创建 `VNImageRequestHander`。现在，在
    `startVision()` 函数中启动请求，将一切很好地拼接在一起。
- en: 'We''re almost done; let''s finish off with some bits and pieces to tie all
    this together now:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎完成了；让我们现在完成一些将所有这些联系在一起的片段：
- en: 'Head on over to `ViewController.swift` and add the following `IBAction` and
    logic from `UISegmentedControl` that we created earlier:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到 `ViewController.swift` 并添加我们之前创建的 `UISegmentedControl` 的以下 `IBAction` 和逻辑：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, create a function called `startLivePreview()`:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，创建一个名为 `startLivePreview()` 的函数：
- en: '[PRE33]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Remove `captureSession.startRunning()` from `setupCaptureSession()`.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `setupCaptureSession()` 中移除 `captureSession.startRunning()`。
- en: Finally, in our `Main.storyboard` view controller, change the class from `ViewController`
    to `VisionViewController`.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在我们的 `Main.storyboard` 视图控制器中，将类从 `ViewController` 更改为 `VisionViewController`。
- en: 'Now, go ahead and run the app. All going well, you should be live-detecting
    images with an overlay that looks like this:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，运行应用程序。一切顺利的话，你应该能够实时检测图像，并带有如下覆盖层：
- en: '![](img/c44f615f-9831-4031-8041-8e91c52658ee.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c44f615f-9831-4031-8041-8e91c52658ee.png)'
- en: Figure 11.2 – Vision detection
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 视觉检测
- en: As you can see, both Vision and CoreML have successfully detected my cell phone
    and its location in the image (all in real time).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，视觉和 CoreML 都成功检测到了我的手机及其在图像中的位置（全部为实时）。
- en: How it works...
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'A high-level overview goes something like this:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 高级概述大致如下：
- en: Capture a real-time camera feed (using AV Foundation).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 捕获实时相机流（使用 AV Foundation）。
- en: Use a trained CoreML model to detect whether the image contains an object (that
    it recognizes).
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练好的 CoreML 模型来检测图像中是否包含（它所识别的）物体。
- en: Use Vision to detect the position of the object in the picture.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用视觉检测图片中物体的位置。
- en: We covered the camera streaming elements in the previous recipe, but let's take
    a deeper look at how *steps 2* and *3* work.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的配方中涵盖了相机流元素，但让我们更深入地看看步骤 2 和 3 是如何工作的。
- en: Let's actually start with *step 3*. We saw in the last section how we use `VNImageRequestHander`
    to pass back `CVPixelBuffer` of each image frame. This now fires off calls in
    our `setupVision()` function, so let's take a closer look in there.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们实际上从 *步骤 3* 开始。在上一个部分中，我们看到了我们如何使用 `VNImageRequestHander` 来传递每个图像帧的 `CVPixelBuffer`。现在，这将在我们的
    `setupVision()` 函数中触发调用，让我们在那里仔细看看。
- en: 'First, we grab our model from the apps bundle so that we can pass this over
    to Vision:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从应用的包中获取我们的模型，以便我们可以将其传递给视觉：
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Next, we head back to *step 2*, where we create an instance of `VNCoreMLModel()`,passing
    in our `localModel`. With this `visionModel`, we can now create our `VNCoreMLRequest`
    call, along with its completion handler, which will fire from requests that come
    in via our AV Foundation delegate.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回到 *步骤 2*，在那里我们创建一个 `VNCoreMLModel()` 的实例，传入我们的 `localModel`。有了这个 `visionModel`，我们现在可以创建我们的
    `VNCoreMLRequest` 调用，以及它的完成处理程序，它将来自通过我们的 AV Foundation 代理传入的请求。
- en: This one simple request does the work of both the Vision Framework and CoreML
    – first detecting whether an object is found, then supplying us with the details
    on where that object is located inside the image.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的请求完成了视觉框架和 CoreML 的双重工作——首先检测是否找到了物体，然后提供有关该物体在图像中位置的详细信息。
- en: This is where the bulk of our work is done. If you look again at our `visionResults()`
    function and all the helper functions within, these are merely just ways of parsing
    data that has come back, and in turn, decorating our view.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们大部分工作的地方。如果您再次查看我们的 `visionResults()` 函数以及其中的所有辅助函数，这些只是解析返回数据的几种方式，并进而装饰我们的视图。
- en: In our "results" from the `VNCoreMLRequest()` response, we take an instance
    of `VNRecognizedObjectObservation`, which in turn gives us two properties, a label
    (of what CoreML thinks it has found) along with a confidence score.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们来自 `VNCoreMLRequest()` 响应的“结果”中，我们取一个 `VNRecognizedObjectObservation` 的实例，它反过来给我们两个属性，一个标签（CoreML认为它找到了什么）以及一个置信度分数。
- en: See also
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: For more information on `CALayer`, refer to [https://developer.apple.com/documentation/quartzcore/calayer](https://developer.apple.com/documentation/quartzcore/calayer).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于 `CALayer` 的信息，请参阅 [https://developer.apple.com/documentation/quartzcore/calayer](https://developer.apple.com/documentation/quartzcore/calayer)。
